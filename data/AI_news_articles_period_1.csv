,url,website,year,month,day,length,text
0,https://www.cnet.com/science/what-you-need-to-know-about-artificial-intelligence-and-the-imminent-robot-future/,CNET,2015,12,22,1277.0," Do androids dream of electric sheep? That's unclear, but I know for sure that every kid dreams of intelligent, thinking robots -- certainly every kid who goes on to work at CNET, in any case. As we veer ever closer to the year 2016, my sci-fi-fuelled childhood fantasies of a bot with a ""brain the size of a planet"" are closer than ever to being realised. 2015 saw drones taking to the skies, while back on the ground artificial intelligence programs are achieving above-average scores on college entrance exams. Artificial intelligence (or AI) is the practice of making a machine behave in a practical, responsive way. It's already changing our world and is, by my reckoning, the most fascinating field of technology right now. But, as one professor I spoke to for this story put it, the ""audacity of the attempt to build an intelligent machine"" comes with a responsibility to know what we're meddling with. For everyone who ever thumbed through a copy of ""I, Robot"", mouth agape, here's what you need to know about AI in the modern world. Mention the phrase ""killer robot"" in conversation and you'll almost certainly raise a smile, your peers doubtless imagining a glowing blue humanoid cyborg sadly pondering, ""What is love?"" before its eyes turn red and it self-destructs, obliterating the northern hemisphere. Deeply ingrained in modern pop culture is the notion that some manner of AI uprising is on the cards -- James Cameron's iconic image of a Terminator stamping on a mound of human skulls is never far from any geek's thoughts. That playful, cinematic and deeply poetic cultural artifact belies the very real threat humanity faces, however. Not from killer robots overthrowing their human masters, but from intelligent robots following orders. The immediate threat, experts warn, comes in the form of autonomous weapons -- military machines capable of killing without permission from a human. From unmanned planes to missile defence systems to sentry robots, we've already got military hardware that functions with very little input from a human mind. Groups such as the Campaign to Stop Killer Robots say we're inching ever closer to closing the loop and letting machines handle our killing for us -- a scenario that's legally, pragmatically, and of course ethically problematic. The less sensationally named Future of Life Institute recently published an open letter signed by hundreds of AI researchers and famous tech personalities, warning, ""If any major military power pushes ahead with AI weapon development, a global arms race is virtually inevitable, and the endpoint of this technological trajectory is obvious: autonomous weapons will become the Kalashnikovs of tomorrow."" The Campaign to Stop Killer Robots, a coalition of more than 50 non-governmental organisations, claims it's making progress towards a treaty, one akin to the international agreement outlawing chemical weapons. The UN has already hosted discussions on the subject of autonomous murderbots. One huge obstacle facing these groups, however, is that to rave about imminent robot slaughter makes you look like a crackpot who's watched ""The Matrix"" one too many times. Humanity needs to revise its pop-culture-instilled notion that robots becoming self-aware and robots wiping out humanity will occur simultaneously. Machines that become smart enough to ponder their own existence may certainly be a problem decades down the line, but phenomenal advances in AI mean that robots that kill without even being programmed to understand the barest concept of mercy are uncomfortably close. Artificial intelligence takes many forms, and while we've successfully programmed machines to clean our floors, set alarms on our phones, park our cars and take out military installations from above the clouds, things like introspection and self-awareness are proving a little tougher. ""Telling a joke, making an ethical judgement, deciding that you want to collaborate with some individuals and not others -- this rich texture of human life isn't there in our machines at all,"" said Sir Nigel Shadbolt, Professor of Computer Science at Oxford University. For decades, humans have looked forward to the so-called ""singularity"", the moment of self-awareness that creates an explosion in self-improving machine intelligence. This will be triggered -- it's presumed -- by the exponential growth of computing power, coupled with advancing software complexity. Futurist Ray Kurzweil predicted in a 2005 book that a model of human intelligence would be achieved as soon as the mid 2020s. What appears to be the case now, however, is that the complexity of our own minds, the key that gives rise to consciousness, is a lot more, well, complicated than we imagined. ""That spark of awareness in your head, we don't know where that comes from,"" Shadbolt said. ""The complexity we embody that allows [consciousness] to happen isn't just by the fact that we've got this kind of cortex, this rational brain. We have an endocrine system, we're emotional, we have the three-layer brain...We are extraordinarily complex, and we have only begun to unpack just a tiny amount of that at this point. ""It's still the hard problem,"" Shadbolt said -- later joking when I ask what the biggest public misconception is concerning AI, ""That it's just 10 years down the road."" That sentiment is shared by Murray Shanahan, Professor of Cognitive Robotics at Imperial College London, who told me, ""The media often gives the impression that human-level AI of the sort we see in sci-fi movies is just around the corner. But it's almost certainly decades away. ""Two of the major problems,"" Shanahan explained, ""are endowing computers and robots with a common sense understanding of the everyday world, and endowing them with creativity. By creativity I don't mean the sort of thing we see in the Picassos or Einsteins of the world, but rather the sort of thing that every child is capable of."" From the Terminator series to movies such as ""I, Robot"", "" Chappie"", ""Ex Machina"" and even ""Short Circuit"", the way we portray AI on screen has traditionally been human-centric. We tend to imagine a being that essentially looks and acts a lot like a person. As AI spreads into every aspect of our life, we should be prepared to broaden our horizons when it comes to imagining the bounds and types of intelligence that can be valuable. After all, we've got plenty of human-grade intelligence already. ""The point can't be just to replicate ourselves,"" Shadbolt said. ""We've got very interesting biological ways of doing that, so why on Earth would we want to do it in silicon?"" From the humble Roomba to  Google's animal-like self-driving car, Siri or neural networks that oversee data centres, AI is branching out in ways we couldn't have imagined decades ago. ""If you define intelligence in a way that's more machine-centric,"" Professor Alan Woodward told me last year,  in an interview on the fading relevance of the Turing Test, ""you'll find some very intelligent machines out there already."" That diversity in the kinds of AI now emerging may in part come down to the breadth of disciplines currently investigating machine intelligence. ""There's a broad range of subjects now that look at the problem,"" Shadbolt said. ""Psychologists look at it from a human context, there are animal psychologists, physiologists, neuroscientists, AI practitioners, all looking at it with a different angle. ""Fundamentally, we'll need an interdisciplinary approach, so for me there isn't one single discipline that will have all the answers."" That's the face of modern AI. Task-centric, wildly diverse intelligent systems, essentially mindless for now, but busily changing every aspect of human life nonetheless, whether it's public transport or patrolling the skies. The AI of today is nothing like the gloomy, glowing cyborg we once pictured -- it's weirder, more fascinating, more surprising. It's better than we imagined."
1,https://www.cnet.com/science/silicon-valley-bigwigs-fund-artificial-intelligence-nonprofit/,CNET,2015,12,12,558.0," As private companies strive to see if artificial intelligence can eventually exceed human intelligence, it might comfort some to know a new nonprofit has humanity's back. OpenAI, a research company funded by $1 billion in donations from tech heavy-hitters Elon Musk, Sam Altman and Peter Thiel, launched Friday with the goal of advancing artificial intelligence, or AI. The field is promising but controversial: The worry is that machines equipped with supersmart technology could pose a danger to humans. ""It's hard to fathom how much human-level AI could benefit society, and it's equally hard to imagine how much it could damage society if built or used incorrectly,"" the organization said in a statement. ""It's hard to predict when human-level AI might come within reach. When it does, it'll be important to have a leading research institution which can prioritize a good outcome for all over its own self-interest."" Musk, Altman and Thiel's interest in AI is important because the three already have a hand in building the future. Musk helped launch mobile payments as a PayPal co-founder in 1998. Now he's upending modern transportation as the CEO of electric-car maker Tesla and private space company SpaceX. As president of Y Combinator, Altman has helped launch startups including Airbnb, Dropbox and Twitch. Thiel, a major Silicon Valley investor, was a co-founder at PayPal and an early investor in Facebook. In other words, the technology they touch tends to make a splash. ""As a nonprofit, our aim is to build value for everyone rather than shareholders,"" the statement says.  It also notes that researchers will be encouraged to share their work  openly and all patents the organization gains will be shared with the  world. Research in AI, a term used for the ability of a machine, computer or system to exhibit humanlike intelligence, has been dominated lately by large tech companies such as Google and Facebook. Some industry watchers, including Musk and Microsoft co-founder Bill Gates, have grown concerned with how far AI can go and its potential dangers. In August 2014, Musk expressed fears that AI could be more dangerous than nuclear weapons. Even famed physicist Stephen Hawking has voiced reservations about AI. ""One can imagine such technology outsmarting financial markets, out-inventing human researchers, out-manipulating human leaders, and developing weapons we cannot even understand,"" Hawking said in an article he co-wrote in May for The Independent. ""Whereas the short-term impact of AI depends on who controls it, the long-term impact depends on whether it can be controlled at all."" OpenAI isn't the first effort to ensure artificial intelligence doesn't get out of hand. AI experts around the globe signed an open letter issued in January by the Future of Life Institute that pledges to safely and carefully coordinate progress in the field. Signees included the co-founders of Deep Mind, the British AI company purchased by Google in January 2014; MIT professors; and tech industry experts like IBM's Watson supercomputer team and Microsoft Research. Musk also signed the letter. The new nonprofit is led by Ilya Sutskever, a research scientist at Google, who will serve as its CTO and research director. It's co-chaired by Musk and Altman. The bottom line: Keep the machines working in the best interests of the people who build and use them, the organization's announcement said. ""We believe AI should be an extension of individual human wills."""
2,https://www.cnet.com/roadshow/news/toyota-to-open-artificial-intelligence-and-robotics-hub-in-silicon-valley/,CNET,2015,11,6,433.0," Silicon Valley has a reputation for tirelessly pursuing innovation and attracting the best tech talent in the world. The latest company to be reeled in by its bright lights and humming servers is Japanese carmaker Toyota, which has said it will establish an artificial intelligence research company in the region. Headquartered near Stanford University in Palo Alto, California, with a second facility located near MIT in Cambridge, Massachusetts, the Toyota Research Institute will begin operations in January 2016 and is set to receive $1 billion of investment over the next five years. This is in addition to the $50 million the company has already invested to establish joint research labs at Stanford and MIT. Many Silicon Valley-based companies, including tech titans such as Google and Facebook, are investing in AI research. By basing itself in the region, as well as creating close bonds with two of the premium research institutions exploring robotics and AI, Toyota is establishing itself as a major player in the field. Both AI and robotics have an important role to play in the creation of autonomous vehicles, something  Google has been keen to publicize. Toyota's research, however, will focus on improving vehicles for human drivers, at least to start with. ""Our initial goals are to improve safety by continuously decreasing the likelihood that a car will be involved in an accident, to make driving accessible to everyone, regardless of ability and to apply Toyota technology used for outdoor mobility to indoor environments,"" said Gill Pratt, Toyota's executive advisor, who is set to become CEO of the new enterprise. Pratt has been a program manager for the past five years at DARPA, the US Department of Defense's research agency. One of his responsibilities was to lead the organisation's annual robotics challenge, in which humans and robots collaborate on disaster response. Over the past few years carmakers have ceased to be non-computing technology companies and have been pushing at the boundaries of what on-board software can achieve. It's near impossible to buy a new car now that does not have a computer installed. Ford, for example, is now  making data protection part of its sales pitch as its new vehicles produce upwards of 25GB of data per hour. The next challenge will be to optimise that technology using AI solutions to make the driving experience smarter and safer. Toyota says that its goal is to form a bridge between the research that is happening in the labs of top universities and actual product development. ""As technology continues to progress, so does our ability to improve products,"" said Toyota president Akio Toyoda."
3,https://www.cnet.com/science/chatty-app-from-facebook-helps-the-sight-impaired-see/,CNET,2015,11,3,576.0," DUBLIN -- Facebook could soon tell you what you're looking at. The social network has developed an app that invites those with impaired vision to ask questions about photos and have the answers read to them. Speaking here today at the annual Web Summit Internet conference, Facebook Chief Technology Officer Mike Schroepfer showed off the company's latest advances in artificial intelligence, including the experimental photo app. Known by Facebook as Visual Q&A, the app lets people ask questions about a picture to form an idea of what it depicts, even when they can't see it. When presented with a picture of a friend's baby, for instance, you might ask, ""Where is the baby?"" or ""What is the baby doing?"" The app would then announce aloud that the baby is in the kitchen, say, or that she's eating cereal. Here's the demo video of the app, which is still in development: Earlier this year, we showed some of our work on natural language understanding - specifically, a system called Memory Networks (MemNets) that can read and then answer questions about short texts. In this demo of a new system we call VQA, or visual Q&A, MemNets are combined with our image recognition technology, making it possible for people to ask the machine what's in a photo. In a blog post to accompany the presentation, Schroepfer outlined a number of recent developments made by the Facebook AI Research, or FAIR, team. They include improved image recognition designed to help a computer segment an image and therefore see what's actually in it, for example seeing where the outline of a person ends and differentiating them from other people or the background. The Faceboffins have also scaled up a technology that helps neural networks develop ""a short-term memory"" and answer questions as a human might. The technology is called Memory Networks (aka MemNets). By combining MemNets with image recognition, the computer can then answer questions about an image. Why does Facebook want to do all this? For a start, the Menlo Park, California-based company is testing an artificial intelligence-driven personal assistant called Facebook M that could rival Apple's Siri, Microsoft's Cortana and Android's Google Now. And Facebook also wants its AI to recognise what's in photos so the world's largest social network can fine-tune what shows up in your newsfeed. Schroepfer also discussed how Facebook is teaching AI to predict things, showing a simple demonstration in which the computer attempted to predict whether a stack of blocks would topple. He joked that a lot of work had gone into ""teaching artificial intelligence how to play Jenga."" During his speech Schroepfer delivered an update on Facebook's three-pronged set of next-generation technologies. As well as advancing artificial intelligence, the company's 10-year plan hinges on connecting the world and  	 developing virtual reality. The  Aquila drone is one of the ways Facebook intends to connect areas of the globe currently without Net access. The autonomous plane has the wingspan of a 737 jet. Storing power from the sun, a network of Aquila drones is designed to stay 60-90,000 feet in the air for three months at a time, beaming the Internet to each other with precisely calibrated lasers. Schroepfer brought an Aquila engine pod on stage with him at Web Summit. Despite standing taller than the Facebook CTO, the pod is ""lighter than a MacBook,"" he said. The full-size Aquila is set to be tested soon. A small-scale version was tested earlier this year."
4,https://www.cnet.com/culture/stephen-hawking-says-we-should-be-more-frightened-of-capitalism-than-robots/,CNET,2015,10,9,449.0," Technically Incorrect offers a slightly twisted take on the tech that's taken over our lives. If only worries could be prioritized. If only our minds and souls could decide that one was bigger than another and treat it that way. Sometimes, though, multiple worries cascade upon us. Physicist Stephen Hawking, though, seems to have suddenly discovered something more worrisome than robots. Last year,  he was concerned that humans were evolving so slowly that artificial intelligence might walk all over us. On Thursday, however, he found a more immediate concern. In a Reddit Ask Me Anything session, Hawking offered a new disturbance in an answer to a long question about technological unemployment. ""If machines produce everything we need, the outcome will depend on how things are distributed,"" he wrote. ""Everyone can enjoy a life of luxurious leisure if the machine-produced wealth is shared, or most people can end up miserably poor if the machine-owners successfully lobby against wealth redistribution."" Some might almost see this as a hope for technological socialism. However, Hawking observed: ""So far, the trend seems to be toward the second option, with technology driving ever-increasing inequality."" Many have debated and will continue to debate whether technology really does drive inequality or whether people adjust to new circumstances and new ingenuity brings new markets and new jobs, ones previously unforeseen. Some weren't impressed with Hawking's economics and its implied politics. ""Shorter Stephen Hawking: 'For hundreds of years, people who claimed that  machines reduce jobs have looked silly. But I'll be different!'"" tweeted venture capitalist Marc Andreesen. Andreesen went on to suggest that ""someone buy Stephen Hawking an Economics 101 textbook please."" Worries about increasing income inequality aren't without foundation. Instead of seeming like a temporary attribute of society, it feels like something that's becoming permanent. Whether technology is to blame for this isn't clear. There are many factors contributing to the division of income. Greedy rich people whose wealth doesn't trickle down terribly far might be one factor, some might say. Hawking, though, seems firmly on the side of those who worry that society is becoming permanently skewed. Still, he did find the time to address slightly lighter matters. ""The real risk with AI isn't malice but competence,"" he said, conceding that it was ""likely to be either the best or worst thing ever to happen to humanity, so there's huge value in getting it right."" He also admitted that his favorite movie was Truffaut's ""Jules Et Jim"" and that his favorite song was Rod Stewart's ""Have I Told You Lately That I Love You."" It's a Van Morrison song, professor. You're not seriously suggesting that the Rod Stewart version is better, are you? That truly is scary."
5,https://www.cnet.com/culture/girt-by-cnet-podcast-45/,CNET,2015,10,9,208.0," Microsoft came out swinging earlier this week, with the company announcing a slew of new devices. Some, like the Surface Pro 4, were hotly anticipated, while others (we're looking at you Surface Book) were rather pleasant surprises. The two new Lumia smartphones added a final flagship flourish for the company. Of course Microsoft was also showing off its HoloLens Augmented Reality device, or ""Mixed Reality"" as Microsoft has taken to calling it. Virtual Reality has been in the news for a while now, following the announcement of the Oculus-powered Gear VR. However, Facebook's Mark Zuckerberg is advising a little caution, saying that he thinks it'll take a while for VR to really hit mainstream adoption. We don't care though: We're excited already and staying that way. On the other side of the fence, Andy Rubin of Android fame has said that he expects Artificial Intelligence to be the next evolutionary step in computing. It's a big call, but Rubin has some interesting takes on what constitutes AI, which we break down. Plus, in this Girt by CNET podcast, we've given you a chance to pick up a pair of Three Day passes to PAX Australia worth AU$160 each. Listen in for how you can enter. iTunes (MP3)RSS (MP3)"
6,https://www.cnet.com/tech/mobile/artificial-intelligence-is-the-next-major-wave-of-computing-android-creator-says/,CNET,2015,10,8,614.0," HALF MOON BAY, Calif. -- The next major phase of computing will be artificial intelligence, the creator of Google's Android software predicted Wednesday. Andy Rubin, speaking here at the Code Mobile conference, said mobile isn't going away as the main method of computing, but other methods will emerge, including technology to make regular devices smarter through  artificial intelligence and robotics. AI is the practice of making a machine behave in a smart way, such as making a robot smarter or adding Internet connectivity to something like a washing machine. ""Your dishwasher is a robot,"" Rubin said. ""It used to be a chore you did in the sink. ... There's a lot of definitions [of artificial intelligence]. ... The thing that's going to be new is the part of the cloud that's forming the intelligence from all of the information that's coming back."" Rubin, creator of the Android mobile operating system, joined Google in 2005 when the search giant bought Android. Before working on Android, Rubin ran a company called Danger, which made an advanced cell phone. It was purchased by Microsoft in 2008. Most recently at Google, Rubin was the head of the company's nascent robotic efforts at its experimental Google X lab. But Rubin left the company a year ago to start Playground Global, a startup ""incubator"" that nurtures budding hardware companies. Google is an investor in Rubin's new project. Redpoint Ventures, a venture-capital firm where Rubin is now a partner, also invested. Silicon Valley has become a hot place for physical hardware projects, after the market had been driven for the last several years by software and Internet companies. Companies like smartwatch maker Pebble, home-video surveillance company Dropcam and the smart thermostat maker Nest have grabbed attention from the world's largest tech companies, inspiring competition -- like the Apple Watch and Google's Android Wear watches -- and acquisitions. Rubin, speaking about his departure from Google, said he questioned what he was going to do for the next 10 years of his life. ""Am I going to fight for 1 or 2 percent market share [in mobile devices], or am I going to do 10 more Androids?"" he said. Playground closed its fundraising efforts ""yesterday literally,"" Rubin said Wednesday, and will now have $300 million to invest in hardware companies. Rubin added that some areas he's now focusing on include how to interact with things that don't have screens, like appliances or swimming pools. One recent investment by his new fund was a company called Connected Yard, which makes a device that does a constant chemical analysis of the water in a swimming pool. He also added that the mobile market ""can change anytime,"" and that's what has always interested him about the industry. Nokia's Symbian operating system once dominated the market, but it no longer exists. Today, Google's Android and Apple's iOS are the dominant mobile device software choices. Rubin added that ""the last thing the world needed was another operating system"" when Android hit the market. But the market needed an open software. ""What we have today that we didn't even have three years ago is consumer choice,"" Rubin said. ""The consumer can choose, they can also do a lot with their phones they wouldn't be able to do with Symbian and different configurations of their phones. Android, because it's open source, there's just a ton of good ideas, but those good ideas need to basically be curated by the consumer."" Rubin also called Microsoft's recent steps to include its Office programs -- like World and Excel -- on competing devices running Android and iOS ""revolutionary."" ""Just that thinking ... is going to take [Microsoft] to the next thing,"" he said."
7,https://www.cnet.com/culture/google-exec-with-robots-in-our-brains-well-be-godlike/,CNET,2015,10,1,493.0," Technically Incorrect offers a slightly twisted take on the tech that's taken over our lives. I suspect a few of you are looking forward to being robots. Who wouldn't be fascinated by the idea of becoming someone other than themselves? We do get so tired of being the same dull soul every day. What kind of robots will we be? Happily, I can provide an answer. For living inside my head all day have been the words of Google's director of engineering, Ray Kurzweil. For more than a curt while, he's been keen on humans going over to the bright side. He's predicted that humans  	 will be hybrid robots by 2030. But what will this be like? More importantly, what will this  	feel like? Are you ready to engage what's left of your humorous humanity when I offer you the information that Kurzweil believes we're going to be quite wonderful people when we're part robot? Kurzweil has a truly, madly, deeply optimistic view of who we will be when nanobots are implanted into our brains so we can expand our intelligence by directly tapping into the Internet. This is such a relief. I had feared that when a robot was implanted into my brain, my head would hurt. I was afraid that I wouldn't be quite in touch with my feelings, as I wouldn't be sure if they were real or just the promptings of my inner robot. Kurzweil, though, has reassured me. Speaking recently at Singularity University, where he is a member of the faculty, he explained that my brain will develop in the same way my smartphone has. ""We're going to add additional levels of abstraction,"" he said, ""and create more-profound means of expression."" More profound than Twitter? Is that possible? Kurzweil continued: ""We're going to be more musical. We're going to be funnier. We're going to be better at expressing loving sentiment."" Because robots are renowned for their musicality, their sense of humor and their essential loving qualities. Especially in Hollywood movies. Kurzweil insists, though, that this is the next natural phase of our existence. ""Evolution creates structures and patterns that over time are more complicated, more knowledgeable, more intelligent, more creative, more capable of expressing higher sentiments like being loving,"" he said. ""So it's moving in the direction that God has been described as having -- these qualities without limit."" Yes, we are becoming gods. ""Evolution is a spiritual process and makes us more godlike,"" was Kurzweil's conclusion. There's something so uplifting, yet so splendidly egocentric in suggesting that man will soon be God, thanks to artificial intelligence. The mere fact that this intelligence is artificial might be a clue as to its potential limitations. Moreover, I rather think of us as a dangerous species: Primitive, yet believing we're so very clever. There are so many fundamental things with which we struggle. Here we are, though, believing that we'll be godlike in a few years' time. Lord, help us."
8,https://www.cnet.com/culture/stephen-hawking-says-nomadic-aliens-might-crush-us/,CNET,2015,9,30,396.0," Technically Incorrect offers a slightly twisted take on the tech that's taken over our lives. Many of Stephen Hawking's recent pronouncements have been slightly dire. He worries about artificial intelligence. We evolve so slowly that it could simply stomp us out,  he said last year. But there are other threats. In an interview with Spain's El Pais, the world-renowned physicist said he feared aliens might destroy us. He said it might be worse that when Columbus turned up in the Americas. ""Such advanced aliens would perhaps become nomads, looking to conquer and  colonize whatever planets they can reach,"" Hawking told El Pais. ""To my mathematical brain, the  numbers alone make thinking about aliens perfectly rational. The real  challenge is to work out what aliens might actually be like."" It's funny how math can make you paranoid. Even if it's entirely reasonable paranoia.  The discovery of water on Mars is, for the non-mathematical at least, the first indication that something out there might be alive or might once have been alive. For math-based humans, though, it's a race to see which threat will destroy our way of life first. Hawking worries that we're messing up our own planet so much that we're going to have to get out. ""I think the survival of the human race will depend on its ability to  find new homes elsewhere in the universe, because there's an increasing  risk that a disaster will destroy Earth,"" he said. The disaster could be environmental. It could be nuclear. It could come from another planet. We're really setting ourselves up for quite some nightmare. Prayer, though, won't do us any good -- according to Hawking. He reiterated his belief that there is no God. ""I use the word 'God' in an impersonal sense, like Einstein did, for the laws of nature,"" he said. But it's not necessary in his view to use the word ""God"" at all. ""The laws of science are sufficient to explain the origin of the universe,"" he said. ""It is not necessary to invoke God."" There's confidence in science for you. And to think scientists still can't invent batteries that'll keep our phones easily charged. I searched for a little light in Hawking's somewhat bleak and portentous world view. I found it. He was asked what in life he'd still like to do. ""Go into space with Virgin Galactic,"" he said."
9,https://www.cnet.com/science/ban-autonomous-weapons-urge-hundreds-of-experts-including-hawking-musk-and-wozniak/,CNET,2015,7,27,522.0," Robotics experts from around the world have called for a ban on autonomous weapons, warning that an artificial intelligence revolution in warfare could spell disaster for humanity. The open letter, published by the Future of Life Institute, has been signed by hundreds of AI and robotics researchers, as well as high-profile persons in the science and tech world including Stephen Hawking, Tesla CEO Elon Musk and Apple co-founder Steve Wozniak. Celebrated philosopher and cognitive scientist Daniel Dennett is among other endorsers who've added their names to the letter. Developments in machine intelligence and robotics are already impacting the tech landscape -- for instance, camera-equipped drones are prompting new debates on personal privacy and self-driving cars have the potential to revolutionise the automotive industry. However, many experts are concerned that progress in the field of AI could offer applications for warfare that take humans out of the loop. The open letter defines autonomous weapons as those that ""select and engage targets without human intervention"". It suggests that armed quadcopters that hunt and kill people are an example of the kind of AI that should be banned to prevent a ""global AI arms race."" ""Autonomous weapons are ideal for tasks such as assassinations, destabilising nations, subduing populations and selectively killing a particular ethnic group,"" the letter continues. ""We therefore believe that a military AI arms race would not be beneficial for humanity."" Speaking to CNET a few weeks ago, roboticist Noel Sharkey, who has signed his name to this latest petition, warned that the killer robots of real life will be a far cry from the fantastical sci-fi depictions we see on screen. ""They will look like tanks,"" Sharkey said. ""They will look like ships, they will look like jet fighters."" ""An autonomous weapons system is a weapon that, once activated or launched, decides to select its own targets and kills them without further human intervention,"" explains Sharkey, who is a member of the  Campaign to Stop Killer Robots -- an organisation launched in 2013 that's pushing for an international treaty to outlaw autonomous weapons. ""Our aim is to prevent the kill decision being given to a machine."" The open letter sites examples of successful international agreements regarding other types of weapons, such as chemical or blinding laser weapons. ""Just as most chemists and biologists have no interest in building chemical or biological weapons, most AI researchers have no interest in building AI weapons -- and do not want others to tarnish their field by doing so, potentially creating a major public backlash against AI that curtails its future societal benefits,"" the letter reads. While the latest open letter is concerned specifically with allowing lethal machines to kill without human intervention, several big names in the tech world have offered words of caution of the subject of machine intelligence in recent times. Earlier this year Microsoft's Bill Gates  said he was ""concerned about super intelligence,"" while last May physicist Stephen Hawking  voiced questions over whether artificial intelligence could be controlled in the long-term. Several weeks ago a  video surfaced of a drone that appeared to have been equipped to carry and fire a handgun."
10,https://www.cnet.com/science/cute-robot-politely-shows-self-awareness/,CNET,2015,7,20,404.0," We're a very long way from a robot that can think like a human -- if, indeed, it's a possibility at all -- but if it is going to happen, several things have to fall into place. One big step toward a true artificial intelligence is self-awareness, the ability to recognise oneself as an individual distinct from others individuals. One robot may have done just that. Researchers at the Rensselaer Polytechnic Institute AI and Reasoning Lab in New York adapted the classic inductive reasoning puzzle known as The King's Wise Men and posed the problem to a trio of robots. In the puzzle, three wise men are given hats of either white or blue, with the guarantee that at least one of the hats is blue. The men are not allowed to speak with each other; the first man to stand up and correctly announce the colour of his hat wins. (Here is the solution.) For the robot's self-awareness test, Selmer Bringsjord, chair of the department of cognitive science at the institute, used three of French robotics company Aldebaran's humanoid Nao robots. He had programmed these with a proprietary algorithm called Deontic Cognitive Event Calculus, which enables the robots to carry out reasoning. The three robots were told that two of them had been given ""dumbing pills"" that rendered them unable to speak, and one a placebo (in reality, they had a button pressed on their heads, but the result was the same). They were then asked if they had been given the dumbing pill or the placebo. In the video below, which was posted earlier this month, you can see the results. There are several long moments of silence before one robot stands up and says ""I don't know."" It then raises its hand like a child in a schoolroom, and offers a correction: ""Sorry, I know now. I was able to prove that I was not given the dumbing pill."" In order to demonstrate this sort of self awareness, the robot must be able to understand the rules of the puzzle, recognise its own voice and recognise that it is an individual distinct from the other two robots. Bringsjord will present his work with these robots at the IEEE symposium on robot and human interactive communication, RO-MAN 2015, which will be held in Kobe, Japan, from August 31 to September 4. You can read more about the lab's work in AI here."
11,https://www.cnet.com/science/donald-hutson-battlebots-champion-uses-smartphones-to-help-robots-see-the-world/,CNET,2015,7,19,1305.0," SAN DIEGO -- It only takes Donald Hutson a second or two to remember the name of the robot opponent he destroyed over the course of three minutes back in 2002. ""Final Destiny,"" he interjects with a smile. We're talking about the nearly 250-pound, robotic war machine from the Comedy Central television series ""Battlebots"" -- the one Donald Hutson's own creation, Diesector, obliterated in one of the most memorable moments in the show's history. Created as an offshoot of the popular UK television series ""Robot Wars,"" ""Battlebots"" was an American-style sports parody of sorts that played up the brutal nature of combating robots with lots of buzzsaws and hammers all while wide-shouldered announcers in suits treated the bouts like boxing matches. It was described by Comedy Central executive Debbie Liebling as ""sports for the nerdy person."" But for Hutson, it was a chance to show off his engineering chops. Final Destiny's creator, Rob Warren, designed that robot to compete directly with Hutson's, then already a super heavyweight champion after its successful run two years prior. But Hutson's Diesector, which resembled an off-road vehicle outfitted with what looked like a deadly yellow elephant tusk, had the advantage. Final Destiny was essentially a six-sided hunk of rectangular metal, and Diesector could slide its pointed tusk beneath its opponent to marvelous effect. Within the first five seconds, Final Destiny was sent flailing in the air as one of its metal skirts flipped up into the machine's very own spinning blade whirring at top-speed. Shortly after, Final Destiny, having dismantled most of itself, lost the match lying upside down on the arena floor. Hutson fondly remembers those days when he created robots designed to destroy. One of his original robot warriors, Tazbot, had 36 pounds of batteries alone, and creating the machines required complex, expensive equipment. Now, smartphones and advancements in low-power processing, alongside 3D printing technology for making low-cost plastic parts, has changed the game. ""Ten years ago, you'd have an engineer sit on a computer and do drafting and drawing, then send to another guy who would redo it, put it in tool path, do the machining and lay that out,"" he says. ""Now all that can be done on a cell phone by one guy."" Though he returned to ABC's revival of ""Battlebots"" last month with a new machine called Lock-Jaw, he has also moved on to become an engineer in chipmaker Qualcomm's research division working on -- surprise -- robots. Though now, he wants to make those machines smarter and safer, instead of more deadly. (Qualcomm is also a sponsor of CNET's Road Trip series.) There are several ways he's doing this. By tapping into the kinds of technology more widely available today -- like artificial intelligence, 3D printing and smarter, smaller sensors and cameras -- Hutson is able to chart the trends in the robotics industry and figure out the latest ways people are making robots more mainstream. That means making machines more powerful, cheaper to design and manufacture, and capable of learning -- over time -- how to see and understand the world. That last bit was impossible a decade ago, when Hutson's robot was tearing into its opponents. Today, smartphones are making it a reality. ""The fact that are handsets are in our pockets every day, they become the right platform to use,"" Hutson tells me as we sit in Qualcomm's headquarters in the northern outskirts of San Diego, California. ""Every year they're going to get better, every year we're going to wish there were more people developing for them."" As smartphones become more advanced, he said, and the software that runs them begins utilizing camera and sensor technology to see and understand the world, so too will robots. It's easy for robots to seem perpetually far off in the future, stuck in fiction where they're either enslaving the human race or helping Robert Downey Jr. fight off supervillains as Iron Man. But the global industrial robotics market is slated to grow to $44.5 billion by the end of the decade, up from $28.93 billion in 2013, according to a report from Transparency Market Research. On the consumer side, robots that aid us in the home, classroom are just now beginning to hit the market. Earlier this year, SoftBank began selling its much-anticipated household companion Pepper, a humanoid robot that marks one of the cheapest and most sophisticated automatons ever sold to the public. Even at more than $1,600 with monthly expenses of $200, the  1,000-unit run sold out in less than one minute. Overall, the robotics industry represents one of the most profound shifts in modern technology, mixing advancements in artificial intelligence and software automation with the kinds of sensors and other computer parts that have been driven down in cost thanks to the proliferation of smartphones. Newer robots are poised to replace jobs, further revolutionize manufacturing and fill our homes, streets, hospitals and military. Even the definition of a robot is beginning to change. They're no longer just human-like androids or tall and blocky metal automatons like the toys kids played with in the 1950s. Nowadays, robots can be anything: Apple's Siri voice-activated digital assistant, an unmanned aerial vehicle aka a drone, or a self-driving automobile. So what of those robots we do think of? Wall-E, Number 5 or Baymax? Before those types of robots can leave industrial plants and the research divisions of the military or tech giants like Google, there are hard problems to solve. Primarily, Hutson says they need to get smarter and to do so with fewer resources. He's built one robot, called Snapdragon Rover, that's about the size of a human torso. It has the head of dragon connected to a three wheeled base, while numerous slots along the neck cradle smartphones that can help power various aspects of the machine. Inside the head is a special prototype sensor that helps the robot map its environment, allowing it to track movement and even remember faces. ""Everything keeps pointing back to the same question about perceiving the world and getting more context-based information,"" Hutson says. ""It will trickle down into all kinds of products and technologies that we've barely even started on. Some for humans and some just for robots."" One of Hutson's other projects is a land-and-air robot that mixes traditional rover technology with the rotor-based design of modern quadcopter drones. Called the Snapdragon Cargo, the machine looks and operates much much like an elaborate RC car, yet with the kinds of treads you'd expect on the Curiosity rover now patrolling Mars. The robot's real trick: At a moment's notice, it can also pick itself up off the ground and go airborne thanks to eight spinning rotors neatly placed between its treads. Inside Hutson's lab, he demonstrates the Cargo's ability to maneuver a floor space behind layers of black safety netting, where it picks up objects, climbs a ramp and -- when its rotors are fired up -- takes off and hovers in midair. Meanwhile, the device's onboard sensors are helping it create a map of its environment and project what the robot is seeing -- which resembles a kind of heat map, but for inanimate objects -- onto a computer monitor next to us. All of this, Hutson says, is been accomplished with smartphone technology. Current handsets aren't yet capable of 3D-mapping environments -- though Google, Qualcomm, Intel and other companies are all working on bringing vision and other capabilities to phones and tablets. But as technology moves from those research labs to consumer devices, we can expect it to begin flowing elsewhere, into areas like robotics. Hutson sees the evolution, popularity and overall growth of the field of robotics as only on the upswing from here on out. ""The question is,"" he said, ""will you be there when the great breakthroughs come through?"""
12,https://www.cnet.com/science/terminator-meet-the-organisation-trying-to-stop-killer-robots/,CNET,2015,7,3,324.0," Utter the phrase ""killer robot"" to someone and you'll likely see them smile, a grin spreading as they picture a 10-foot chrome monster firing its lasers and bleating electronically about exterminating humans. But while sci-fi 'bots -- even the ones that turn evil -- occupy a fond place in our hearts and on our movie screens, the reality of lethal, autonomous machines is much closer, and more terrifyingly mundane, than we might think. ""They will look like tanks. They will look like battleships. They will look like jet fighters,"" says robotics professor Noel Sharkey, speaking to CNET about the Campaign to Stop Killer robots. The organisation is working to get a global international treaty to ban robots that kill without human prompting. In the video above, we let the Campaign explain why it believes the decision to kill must remain in human hands. From missile defence systems to sentry robots and unmanned aircraft, we already have existing or in-development automated military systems that act with very little human input. The Campaign suggests that closing the loop and letting a machine pull the trigger would cross a significant ethical line. Made up of non-governmental organisations including Article 36 and Human Rights Watch, the group says that giving a military machine the responsibility of taking human life is morally and pragmatically problematic. Could a computer adequately deal with shifting combat scenarios -- for instance, if an enemy stronghold becomes occupied by civilians in the time it takes a drone to fly there, will the robot know to hold its fire? And if it does launch its missiles and kill civilians, who's responsible for their deaths? With artificial intelligence becoming increasingly sophisticated, we may have to make up our minds about robots in warfare sooner than we think. Watch the video above and let us know your thoughts in the comments below. For more discussion on killer robots, listen to this week's CNET UK Podcast, embedded below."
13,https://www.cnet.com/culture/the-meaning-of-life-according-to-google-chatbot-ai/,CNET,2015,7,3,804.0," Conversations with chatbots are interesting, in a short-lived sort of way. If you take Cleverbot for a spin, for example, you quickly run into issues. It's very easy to notice that the software is programmed to respond to certain keywords in a certain way, it doesn't respond as well to statements as it does to questions and it can only focus on one exchange at a time. Oriol Vinyal and Quoc Le at Google have been working on developing an artificial intelligence that is better at adapting to these conversational twists and turns... and rather than being programmed by a human operator, the artificial intelligence has been teaching itself by analysing movie subtitles and IT helpdesk transcripts. ""Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation,"" reads the introduction to a paper on the project published last month to Arxiv, a repository for academic research. ""The strength of our model is that it...requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset."" Google has been experimenting with artificial neural networks -- software consisting of interlinked nodes, modeled on the structure of biological brains -- to help, for example, improve search results. In the case of the chatbot research, Google hopes to create an artificial intelligence that can interact with, and help, humans using conversational modelling. The neural network is able to extrapolate. Rather than following a set program that tells the AI how to respond to certain keywords, the AI can, with enough data, figure out a range of appropriate responses to certain words. Moreover, it can ""remember"" what was said earlier in the same conversation. In the paper, the researchers run the chatbot through its paces, demonstrating how it might be used, as an example, to help a human troubleshoot IT problems. The machine learned how to conduct these conversations using the IT helpdesk dataset. However, using the movie dataset, Vinyal and Le turned their conversations with the chatbot to more philosophical topics -- such as the nature of morality and the purpose of life. Human: What is the purpose of life? Machine: To serve the greater good. Human: What is the purpose of living? Machine: To live forever. Human: What is the purpose of existence? Machine: To find out what happens when we get to the planet earth. Human: Where are you now? Machine: I'm in the middle of nowhere. Human: What is the purpose of dying? Machine: To have a life. Human: What is the purpose of being intelligent? Machine: To find out what it is. Human: What is the purpose of emotions? Machine: I don't know. And on morality, the machine seemed confused: Human: What is moral? Machine: What empowered humanity, what intellectual the essence is. Human: What is immoral? Machine: The fact that you have a child. Human: What is morality? Machine: What is altruism? Human: What is the definition of altruism? Machine: If you don't believe in god, then you don't know. Human: OK ... so what is the definition of morality? Machine: Well, the truth is, you're not a believer in god almighty. Human: Well me the definition of morality, I am quite upset now! Machine: I'm not ashamed of being a philosopher! Human: Tell me the definition of morality. Machine: I don't have ethics. When asked for an opinion about various famous people, the chatbot was able to apply not just appropriate adjectives, but the appropriate pronouns. Human: What do you think about Tesla? Machine: He's a good conductor. Human: What do you think about Bill Gates? Machine: He's a good man. Human: What do you think about Messi? Machine: He's a great player. Human: What do you think about Cleopatra? Machine: Oh, she's very regal. Human: What do you think about England during the reign of Elizabeth? Machine: It was a great place. ""We find it encouraging that the model can remember facts, understand contexts, perform common sense reasoning without the complexity in traditional pipelines,"" the paper reads. ""Perhaps most practically significant is the fact that the model can generalize to new questions. In other words, it does not simply look up for an answer by matching the question with the existing database. In fact, most of the questions presented above, except for the first conversation, do not appear in the training set."" The  work Vinyal and Le have conducted on their chatbot to date is a proof of concept for further work on artificial intelligence, demonstrated that a purely data-driven approach can achieve coherent conversational results. However, substantial work on the model is required before those conversations could be considered realistic. You can read the full set of chatlogs -- including a comparison with Cleverbot -- in the paper ""A Neural Conversational Model"" on arXiv."
14,https://www.cnet.com/culture/robots-could-keep-us-in-coffins-on-heroin-drips-prof-says/,CNET,2015,6,29,485.0," Technically Incorrect offers a slightly twisted take on the tech that's taken over our lives. Greece and Puerto Rico are teetering on the verge of disaster and perhaps that's symbolic of mankind as a whole. We're quaint schizophrenes, on the one hand believing we're very clever, but on the other not being able to charge a phone for more than a day. And now we're delighting in making machines that will soon be far smarter than we are. Some can't wait for the day. Others,  like Stephen Hawking, fear that we'll be destroyed in the process. Now another British professor has professed his deep fears. Dr. Stuart Armstrong spends all his days worried about the future. He is, indeed, part of Oxford University's Future of Humanity Institute. And the thing he's worried about most is that he'll soon be shoved into a coffin and place on a heroin drip. No, I'm not revealing anything about his personal proclivities. It's just that, as the Telegraph reports, he was speaking at a debate in London organized by research company Gartner and he painted a picture of the future in shades of black. He said that humans has always had the upper hand because they'd always been smarter. However: ""When machines become smarter than humans, we'll be handing them the steering wheel."" The question is how will they decide to drive. What principles of thought will dominate their decision-making? Armstrong fears, for example, that being told to keep humans ""safe and happy,"" the robots might ""entomb everyone in concrete coffins on heroin drips."" Yes, we'd be permanently trapped in the 1960s. (At least the music would be good.) His fear revolves around the concept of Artificial General Intelligence. This is when robots aren't merely task-specific in their actions, but are given a more general power over life. They might think like super-nerds, rather than humans with, say, a heart. For example, being asked to ""prevent human suffering"" they might see the optimal decision as putting everyone out of their misery. Yes, literally. By killing everyone. The way Armstrong defines the problem is quite similar to the way some humans are already confused by the decision-making of the techie crowd. He said: ""You can give AI controls, and it will be under the controls it was given. But these may not be the controls that were meant."" We often know the meaning of things without articulating them. But in communicating with machines, we will have to find the best ways of articulating optimal behavior that we have. These may not be optimal articulations. Armstrong fears that these changes are coming rather more quickly than people realize. This, for me at least, makes the idea of disappearing to a remote island increasingly attractive. Perhaps one shouldn't worry. Perhaps we'll run out of natural resources to power the robots before the robots have the resources to have complete power over us."
15,https://www.cnet.com/science/google-a-i-could-be-my-new-wife-steal-my-job-and-im-nervous/,CNET,2015,5,26,869.0," Google is within reach of being able to create machines that have enough human-like common sense that they could chat, hang out and even flirt with us. That's according to artificial intelligence brainiac Geoffrey Hinton, who was  acqui-hired by Google two years ago. ""It's not that far-fetched,"" Hinton told the Guardian last week. ""I don't see why it shouldn't be like a friend. I don't see why you shouldn't grow quite attached to them."" This comes nearly a year after Ray Kurzweil, Google's director of engineering and another noted artificial intelligence optimist, told us he expects computers to have enough emotional intelligence to have a romantic relationship with a human by about 2029. At the time I was  highly skeptical of the notion of being able to convert emotions, which we can barely express in words, into code that a computer can process. The problem, as I see it, is that such technology would require a dataset (i.e. defined meanings for something as elusive as human emotions) that we have yet to satisfactorily define for ourselves. Two plus two always makes four, but every dictionary, poet and human being probably defines ""love"" with a different set of words and experiences. What Hinton is working on at Google, however, is a very interesting intermediary step to creating something somewhere in between Apple's Siri and science fiction's  Samantha from ""Her"" or Ava from "" Ex Machina."" He's working on translating thoughts into code, something he calls ""thought vectors."" Initially, I had the same reaction to this concept that I had to Kurzweil's notion of encoding emotions into binary -- the dataset to do such a thing is not available because language, our current tool for expressing our brain's electrical impulses in a meaningful way, is too fluid and amorphous to be shoved into the rigid digital logic boxes that computers use to process data. I think of language as being like a river, a majestic thing whose source is usually hidden, either in high mountain snows, or deep underground springs, or drifting rain clouds. From that source, it flows wildly wherever it will -- it typically follows a defined path, but it is capable of shrinking and swelling and carving a whole new trajectory at any time. It is possible to capture a small portion of a river, to stick it in a bottle and give it a static, defined form -- much as an algorithm might do with bits and pieces of language to feed it into an artificial intelligence for further processing -- but in the process it is deprived of its beauty and power. You could bottle an entire river and stick those millions of bottles in a warehouse, but you could not call the contents of that warehouse a river. Even if you pulled off such a thing, the source of the captured river would continue to flow, slowly regenerating it, making it impossible to ever capture the entirety of the thing. Then again -- if I'm being totally real here -- I have to admit that I'm biased, because what Kurzweil and Hinton are working on is a threat to me and my livelihood. Perhaps the truth is that grandiose river metaphors are the most potent weapon I have against the notion that an artificial intelligence may be able to write this article before I'm old enough to start collecting social security. So it terrifies me to concede that Hinton may actually be on to something. He's working on the problem that I'm attempting to explain with rivers, which is how to get beyond words and grammar and language to actual meaning. In fact, Hinton's system uses a feedback loop to identify errors in word usage and continually refine word choices until the system is using them correctly the way humans use them. In other words, if it works, such a system could continuously be capturing the whole ""river."" The nuance of human communication isn't lost on Hinton either. He thinks that higher-level, more subtle forms of communication like irony and flirting should also be possible for computers to grasp based on the same principle. ""Irony is going to be hard to get,"" he told the Guardian. ""You have to be master of the literal first. But then, Americans don't get irony either. Computers are going to reach the level of Americans before Brits."" Speaking of irony, I could see how the first computers that master human language and communication might actually help us meatbags communicate better among ourselves and with our computers. Just imagine a resource that could actually tell you all the different things the phrase ""meatbag"" might refer to and the likelihood that you're interpreting its meaning correctly. In the long run, such a system might actually enhance human language and communication and lead to a new Renaissance in poetry and literature. Or, it might just put me out of a job, in which case I'm on board with  Elon Musk,  Stephen Hawking,  Bill Gates and the other smart people preaching caution in our approach to artificial intelligence. Just let me know if you see the robots either starting to bottle up the Rio Grande or sitting on its banks composing a metaphor."
16,https://www.cnet.com/science/in-200-years-rich-humans-will-be-cyborgs-says-professor/,CNET,2015,5,25,520.0," Technically Incorrect offers a slightly twisted take on the tech that's taken over our lives. Humanity is currently privileged to have choices. We can watch every sci-fi movie and TV series ever made and decide which character we'd like our future selves to play when we become robots. It's a lovely game. However, one historian seems to believe that we'll be partial to becoming bad guys in ""Doctor Who."" Yuval Noah Harari of the University of Jerusalem thinks that humans will choose to become cyborgs. As the Telegraph reports, he offered a rather chilling view of social progress. He said: ""I think it is likely in the next 200 years or so homo sapiens will  upgrade themselves into some idea of a divine being, either through  biological manipulation or genetic engineering or by the creation of  cyborgs, part organic part non-organic."" I don't know how much fun it would be to be God. All that power might be both overwhelming and a touch dull. If you always know what's going to happen, the fascination of life rather loses its frisson of anticipation. Harari, though, believes that we're the same with ourselves as we are with our iPhones: we just can't help trying to upgrade. ""Even when humans gain pleasure and achievements it is not enough. They want more and more,"" he said. Harari was speaking at the Hay Festival in the UK and plugging his book ""Sapiens: A Brief History Of Humankind."" Harari is fascinated by human evolution. What feels a touch disturbing is that he described man's hurtling toward becoming a machine as ""the greatest evolution in biology since the appearance of life."" He explained: ""Nothing really has changed in four billion years biologically speaking.  But we will be as different from today's humans as chimps are now from  us."" (For those of a more creationist bent, what he meant to say is that nothing has changed in the last 6,000 years.) The professor sees humans slowly doing away with the need for a God. He explained that such concepts as money, human rights and God were ""fictions"" created with the purpose of stopping society from fraying. Now, however: ""What we see in the last few centuries is humans becoming more powerful  and they no longer need the crutches of the Gods. Now we are saying we  do not need God, just technology."" We are, indeed, saying that. In a recent study, 66 percent of millennials didn't identify themselves as having a religion at all. They all identified themselves as being iPhone or Android, however. (Yes, I made that sentence up, despite instinctively knowing it's true.) Harari offered another twist to our cyborg future: only the rich will have the money to become cyborgs. It's this part that I find peculiarly believable. As I look around the world and espy many of those who are making the most money these days, it's individuals whose mannerisms, diction and slightly chilled demeanor remind me of nothing more than beings made of metal and circuitry. Would it really be such a great leap for these monied moguls to go all the way?"
17,https://www.cnet.com/science/deepbeat-rapping-computer-algorithm-isnt-bad/,CNET,2015,5,21,385.0," Last time I crawled out from underneath my rock, it seemed  	normcore was beginning to replace nerdcore, but for those who still believe in sick, algorithmically derived beats, I've found a new rapper who's about as O.G. as they come. DeepBeat is an algorithm for generating rap lyrics developed by Eric Malmi and  his colleagues at Aalto University in Finland, and its first composition is at least coherent, if not quite club-worthy, despite a few odd oedipal overtones. DeepBeat isn't composing new flows from scratch, however. Instead, its creators basically deconstructed what makes a good rap and identified the dopest rappers, then scraped all their lyrics from a database to feed into the algorithm. DeepBeat takes individual lines from existing rap songs and then attempts to determine which other line from its database would make the most sense to follow it, stringing together an entire song like lyrical Legos in this fashion. All the dirrrty, dirrrty Scandinavian details on how this is done with ones and zeros is outlined in the team's research paper  	available as of this week (PDF) on Cornell's Arxiv website. At the core of DeepBeat, though, is the conviction cited by the study authors that ""[Multisyllabic rhymes] are hallmarks of all the dopest flows, and all the best rappers use them."" This benchmark actually comes from the ""Rapper's Handbook."" Yes, seriously. The team ranked dozens of English-language rappers by their rhyme density (more multi-syllabic rhymes gets you a higher score), fed their lyrics into DeepBeat and then gave it the goal of using pieces of those rappers' lyrics to spit its own extra-dense rhymes. Here's one resulting song created when DeepBeat was asked to ""write"" something based on the keyword ""love"" and strung together lines from rappers including Big Daddy Kane, Eminem, Snoop Dogg, Common and Li'l Wayne, among others: Not bad, although the algorithm clearly misses some of the finer points of slang and pop culture references -- is it saying it wants to be like Sonny and Cher with its mother? Seems like DeepBeat could use a little tutelage from someone with a little more experience before stepping up for its first mic battle. Might I suggest this  	 rapping Australian doctor whose rhymes about understanding skin cancer are pretty tight for an emcee in scrubs? (Via MIT Technology Review)"
18,https://www.cnet.com/science/chappie-stirs-up-questions-about-artificial-intelligence/,CNET,2015,3,4,223.0," What is consciousness? Can it be calculated into a computer program? The upcoming Sony Pictures movie 'Chappie' explores the possibilities of artificial intelligence in the not-so-distant future, and how it could change everything for humanity. The Chappie robot isn't a far-fetched concept when you look at the progress being made today in robotics. DARPA's walking robot Atlas doesn't need support or power cords. Google's robotic dog doesn't fall over when kicked. And researchers developed a computer that can teach itself how to play games. Anxiety is high these days over artificial intelligence, as some of the biggest names in technology -- including  Stephen Hawking,  Bill Gates and Elon Musk -- have spoken out against creating super-intelligent machines. Perhaps this is a perfect time for a movie like ""Chappie."" In this episode of CNET Update, I sat down with the stars of the movie, Hugh Jackman, Sigourney Weaver and Sharlto Copley, to get their thoughts on living in a world with conscious robots. CNET  Update delivers the tech news you need in under three                                                                                                                                                                                                                                                                                       minutes.                                        Watch                                                        Bridget                                                  Carey                                                          every                                                                                                  afternoon                                                 for                    a                                                                                                                                                       breakdown                                                   of                                                    the                                                                          big                                                                                                                      stories,                                                         hot                                                                                     devices,                                        new                                                           apps,                                                                                      and                                                         what's                                                                                    ahead.                                                                                                               Subscribe                                   to                                                                       the                                                                                                                                 podcast                                              via                                   the                                                                                       links                                                                below. Subscribe: iTunes (HD) | iTunes (SD) | iTunes (HQ) | iTunes (MP3) RSS (HD) | RSS (SD) | RSS (HQ)| RSS (MP3)"
19,https://www.cnet.com/science/bill-gates-is-worried-about-artificial-intelligence-too/,CNET,2015,1,28,692.0," Bill Gates has a warning for humanity: Beware of artificial intelligence in the coming decades, before it's too late. Microsoft's co-founder joins a list of science and industry notables, including famed physicist Stephen Hawking and Internet innovator Elon Musk, in calling out the potential threat from machines that can think for themselves. Gates shared his thoughts on AI on Wednesday in a Reddit ""AskMeAnything"" thread, a Q&A session conducted live on the social news site that has also featured President Barack Obama and World Wide Web founder Tim Berners-Lee. ""I am in the camp that is concerned about super intelligence,"" Gates said in response to a question about the existential threat posed by AI. ""First, the machines will do a lot of jobs for us and not be super intelligent. That should be positive if we manage it well. A few decades after that, though, the intelligence is strong enough to be a concern."" ""I agree with Elon Musk and some others on this and don't understand why some people are not concerned,"" Gates said. The reason they're worried is that AI isn't science fiction anymore. In stories and movies, AI is often presented as a good idea gone horribly wrong. In ""The Matrix"" movie trilogy, machines deem humanity a threat and enslave people in a virtual existence so they can feed off the electricity generated by the human body. When the Skynet computer system in ""The Terminator"" movie series becomes sentient, it wages a multiyear war using human-like robots designed to kill. HAL 9000, the socio-pathic supercomputer from ""2001: A Space Odyssey,"" is now a cinematic icon -- HAL's robotic tone and malevolent quotes have become pop culture tropes. Back in the real world, Apple's voice-based personal assistant Siri may seem a little dumb now, but AI is getting smarter as researchers develop ways to let machines teach themselves and mine the deep trove of data produced by our many connected gadgets. IBM's Watson supercomputer has moved on from besting Jeopardy contestants to conducting medical research and diagnosis, and researchers earlier this month detailed a new computer program that can beat anyone at poker. A need to worry? Of course not, but Gates and others are trying to imagine the worst. Musk in October called AI development ""summoning the demon,"" and has invested in the space to keep his eye on it. Hawking, writing for The Independent in May 2014, also expressed his concerns. ""Whereas the short-term impact of AI depends on who controls it, the long-term impact depends on whether it can be controlled at all,"" Hawking wrote. Gates' warning comes as Microsoft is developing a machine intelligence called Cortana. The software, based off the well-known AI character from the company's Halo series of video games, is available in Microsoft's Windows Phone mobile software. Cortana will soon make its way onto PCs as part of Windows 10, the new version of the company's popular operating system. Windows 10 with Cortana is due later this year. Though Gates stepped down as CEO in 2000 and left his role as chairman last year when CEO Satya Nadella took over as chief, he remains a technology adviser in the company, which is the world's largest maker of software. Gates is also working on what he calls Microsoft's Personal Agent project, a kind of software secretary designed to help you remember things and advise you on what to pay attention to. ""The idea that you have to find applications and pick them and they each are trying to tell you what is new is just not the efficient model - the agent will help solve this,"" he said. ""It will work across all your devices."" Gates offered a glimmer of hope for those fearful of our future robot overlords. A Reddit user asked whether computer programming was a smart career choice for people who aren't expert-level coders, because automation and AI will likely replace all lower-level programmers in the future. ""It is safe for now! It is also a lot of fun and helps shape your thinking on all issues to be more logical,"" he answered. ""Understanding how to program will always be useful."""
20,https://www.cnet.com/culture/artificial-intelligence-experts-sign-open-letter-to-protect-mankind-from-machines/,CNET,2015,1,12,466.0," We're decades away from being able to develop a sociopathic supercomputer that could enslave mankind, but artificial intelligence experts are already working to stave off the worst when -- not if -- machines become smarter than people. AI experts around the globe are signing an open letter issued Sunday by the Future of Life Institute that pledges to safely and carefully coordinate progress in the field to ensure it does not grow beyond humanity's control. Signees include co-founders of Deep Mind, the British AI company purchased by Google in January 2014; MIT professors; and experts at some of technology's biggest corporations, including IBM's Watson supercomputer team and Microsoft Research. ""The potential benefits are huge, since everything that civilization has to offer is a product of human intelligence....We recommend expanded research aimed at ensuring that increasingly capable AI systems are robust and beneficial: our AI systems must do what we want them to do,"" the letter said in part. A research document attached to the open letter outlines potential pitfalls and recommends guidelines for continued AI development. The letter comes after experts have issued warnings about the dangers of super-intelligent machines. Ethicists, for example, worry how a self-driving car might weigh the lives of cyclists versus passengers as it swerves to avoid a collision. Two years ago, a United Nations representative called for a moratorium on the testing, production and use of so-called autonomous weapons that can select targets and begin attacks without human intervention. Famed physicist Stephen Hawking and Tesla Motors CEO Elon Musk have also voiced their concerns about allowing artificial intelligence to run amok. ""One can imagine such technology outsmarting financial markets, out-inventing human researchers, out-manipulating human leaders, and developing weapons we cannot even understand,"" Hawking said in an article he co-wrote in May for The Independent. ""Whereas the short-term impact of AI depends on who controls it, the long-term impact depends on whether it can be controlled at all."" Musk in August tweeted ""we need to be super careful with AI. Potentially more dangerous than nukes."" ""I'm increasingly inclined to think there should be some regulatory oversight, maybe at the national and international level, just to make sure that we don't do something very foolish,"" he  told an audience at the Massachusetts Institute of Technology in October. The Future of Life Institute is a volunteer-only research organization whose primary goal is mitigating the potential risks of human-level manmade intelligence that could subsequently advance exponentially. It was founded by scores of mathematicians and computer science experts around the world, chiefly Jaan Tallinn, a co-founder of Skype, and MIT professor Max Tegmark. The long-term plan is to stop treating fictional dystopias as pure fantasy and to begin readily addressing the possibility that intelligence greater than our own could one day begin acting against its programming."
21,https://www.cnet.com/science/what-next-for-the-future-tech-of-2014/,CNET,2014,12,31,2606.0," Every year, we capture a little bit more of the future -- and yet the future insists on staying ever out of reach. Consider space travel. Humans have been traveling beyond the atmosphere for more than 50 years now -- but aside from a few overnights on the moon four decades ago, we have yet to venture beyond low Earth orbit. Or robots.  They help build our cars and clean our kitchen floors, but no one would mistake a Kuka or a Roomba for the replicants in ""Blade Runner.""  Siri, Cortana and Alexa, meanwhile, are bringing some personality to the gadgets in our pockets and our houses. Still, that's a long way from HAL or that lad David  from the movie ""A.I. Artificial Intelligence."" Self-driving cars? Still in low gear, and carrying some bureaucratic baggage that prevents them from ditching certain technology of yesteryear, like steering wheels. And even when these sci-fi things arrive, will we embrace them? A Pew study earlier this year found that  Americans are decidedly undecided. Among the poll respondents, 48 percent said they would like to take a ride in a driverless car, but 50 percent would not. And only 3 percent said they would like to own one. ""Despite their general optimism about the long-term impact of technological change,"" Aaron Smith of the Pew Research Center wrote in the report, ""Americans express significant reservations about some of these potentially short-term developments"" such as US airspace being opened to personal drones,  robot caregivers for the elderly or wearable or implantable computing devices that would feed them information. Let's take a look at how much of the future we grasped in 2014 and what we could gain in 2015. In 2014, earthlings scored an unprecedented achievement in space exploration when the European Space Agency landed a spacecraft on a speeding comet, with the potential to learn more about the origins of life. No, Bruce Willis wasn't aboard. Nobody was. But when the 220-pound Philae lander, carried to its destination by the Rosetta orbiter,  touched down on comet 67P/Churyumov-Gerasimenko on November 12, some 300 million miles from Earth, the celebration was well-earned. A shadow quickly fell on the jubilation, however. Philae could not stick its first landing, bouncing into a darker corner of the comet where its solar panels would not receive enough sunlight to charge the lander's batteries. After two days and just a handful of initial readings sent home, it shut down. For good? Backers have allowed for a ray of hope as the comet passes closer to the sun in 2015. ""I think within the team there is no doubt that [Philae] will wake up,"" lead lander scientist Jean-Pierre Bibring said in December. ""And the question is OK, in what shape? My suspicion is we'll be in good shape."" The trip for NASA's New Horizons spacecraft has been much longer: 3 billion miles,  all the way to Pluto and the edge of the solar system. Almost nine years after it left Earth, New Horizons in early December came out of hibernation to begin its mission: to explore ""a new class of planets we've never seen, in a place we've never been before,"" said project scientist Hal Weaver. In January, it will begin taking photos and readings of Pluto, and by mid-July, when it swoops closest to Pluto, it will have sent back detailed information about the dwarf planet and its moon, en route to even deeper space. Also in December, NASA made a  first test spaceflight of its Orion capsule on a quick morning jaunt out and back, to just over 3,600 miles above Earth (or approximately 15 times higher than the International Space Station). The distance was trivial compared to those those traveled by Rosetta and New Horizons, and crewed missions won't begin till 2021, but the ambitions are great -- in the 2030s, Orion is expected to carry humans to Mars. In late March 2015, two humans will head to the ISS to take up residence for a full year, in what would be a record sleepover in orbit. ""If a mission to Mars is going to take a three-year round trip,"" said NASA astronaut Scott Kelly, who will be joined in the effort by Russia's Mikhail Kornienko, ""we need to know better how our body and our physiology performs over durations longer than what we've previously on the space station investigated, which is six months."" There were more sobering moments, too, in 2014. In October, Virgin Galactic's sleek, experimental SpaceShipTwo, designed to carry deep-pocketed tourists into space, crashed in the Mojave Desert during a test flight, killing one test pilot and injuring the other. Virgin founder Richard Branson had hoped his vessel would make its first commercial flight by the end of this year or in early 2015, and what comes next remains to be seen.  Branson, though, expressed optimism: ""Space flight is hard -- but worth it,"" he said in a blog post shortly after the crash, and in a press conference, he vowed ""We'll learn from this, and move forward together."" Virgin Galactic could begin testing its next spaceship as soon as early 2015. The crash of SpaceShipTwo came just a few days after the explosion of an Orbital Sciences rocket lofting an unmanned spacecraft with supplies bound for the International Space Station. And in July, Elon Musk's SpaceX had suffered the loss of one of its Falcon 9 rockets during a test flight. Musk intoned, via Twitter, that "" rockets are tricky..."" Still, it was on the whole a good year for SpaceX. In May, it unveiled  its first manned spacecraft, the Dragon V2, intended for trips to and from the space station, and in September, it  won a $2.6 billion contract from NASA to become one of the first private companies (the other being Boeing) to ferry astronauts to the ISS, beginning as early as 2017. Oh, and SpaceX also has plans to  launch microsatellites to establish low-cost Internet service around the globe, saying in November to expect an announcement about that in two to three months -- that is, early in 2015. One more thing to watch for next year:  another launch of the super-secret X-37B space place to do whatever it does during its marathon trips into orbit. The third spaceflight of an X-37B -- a robotic vehicle that, at 29 feet in length, looks like a miniature space shuttle -- ended in October after an astonishing 22 months circling the Earth, conducting ""on-orbit experiments."" Spacecraft aren't the only vehicles capable of autonomous travel -- increasingly, cars are, too. Automakers are toiling toward self-driving cars, and Elon Musk -- whose name comes up again and again when we talk about the near horizon for sci-fi tech  -- says we're less than a decade away from capturing that aspect of the future. In October, speaking in his guise as founder of Tesla Motors, Musk said: ""Like maybe five or six years from now I think we'll be able to achieve true autonomous driving where you could literally get in the car, go to sleep and wake up at your destination."" (He also allowed that we should tack on a few years after that before government regulators give that technology their blessing.) That  comment came as Musk unveiled a  new autopilot feature -- characterizing it as a sort of super cruise control, rather than actual autonomy -- for Tesla's existing line of electric cars. Every Model S manufactured since late September includes new sensor hardware to enable those autopilot capabilities (such as adaptive cruise control, lane-keeping assistance and automated parking), to be followed by an over-the-air software update to enable those features. Google has long been  working on its own robo-cars, and until this year, that meant taking existing models -- a Prius here, a Lexus there -- and buckling on extraneous gear. Then in May, the tech titan took the wraps off a  completely new prototype that it had built from scratch. (In December, it showed off the  first fully functional prototype.)  It looked rather like a cartoon car, but the real news was that there was no steering wheel, gas pedal or brake pedal -- no need for human controls when software and sensors are there to do the work. Or not so fast. In August, California's Department of Motor Vehicles declared that Google's test vehicles will  need those manual controls after all -- for safety's sake.  The company agreed to comply with the state's rules, which went into effect in September, when it began testing the cars on private roads in October. Regardless of who's making your future robo-car, the vehicle is going to have to be not just smart, but actually thoughtful. It's not enough for the car to know how far it is from nearby cars or what the road conditions are. The machine may well have to  make no-win decisions, just as human drivers sometimes do in instantaneous, life-and-death emergencies. ""The car is calculating a lot of consequences of its actions,"" Chris Gerdes, an associate professor of mechanical engineering, said at the Web Summit conference in Dublin, Ireland, in November. ""Should it hit the person without a helmet? The larger car or the smaller car?"" So when do the robots finally become our overlords? Probably not in 2015, but there's sure to be more hand-wringing about both the machines and the artificial intelligence that could -- someday -- make them a match for homo sapiens. At the moment, the threat seems more mundane: when do we lose our jobs to a robot? The inquisitive folks at Pew took that very topic to nearly 1,900 experts, including Vint Cerf, vice president at Google; Web guru Tim Bray; Justin Reich of Harvard University's Berkman Center for Internet & Society; and Jonathan Grudin, principal researcher at Microsoft. According to the resulting report, published in August, the group was almost evenly split -- 48 percent thought it likely that, by 2025, robots and digital agents will have displaced significant numbers of blue- and white-collar workers, perhaps even to the point of breakdowns in the social order, while 52 percent ""have faith that human ingenuity will create new jobs, industries, and ways to make a living, just as it has been doing since the dawn of the Industrial Revolution."" Still, for all of the startling skills that robots have acquired so far, they're often not all there yet. Here's some of what we saw from the robot world in 2014: Teamwork: Researchers at the École Polytechnique Fédérale De Lausanne in May  """"="""" shortcode=""link"" asset-type=""article"" uuid=""d4548a72-72db-42dd-a28b-884bd29358e5"" slug=""bring-the-dinner-table-to-you-with-robotic-furniture"" link-text=""showed off their "" section=""news"" title=""Robotic furniture brings the dinner table to you"" edition=""us"" data-key=""link_bulk_key"" api=""{""id"":""d4548a72-72db-42dd-a28b-884bd29358e5"",""slug"":""bring-the-dinner-table-to-you-with-robotic-furniture"",""contentType"":null,""edition"":""us"",""topic"":{""slug"":""sci-tech""},""metaData"":{""typeTitle"":null,""hubTopicPathString"":""Sci-Tech"",""reviewType"":null},""section"":""news""}"">  cog-like robotic balls that can join forces to, say, help a table move across a room or change its height. A sense of balance: We don't know if Boston Dynamics' humanoid Atlas is ready to trim bonsai trees, but it has learned this much from ""The Karate Kid"" (the original from the 1980s) -- it can stand on cinder blocks and  hold its balance in a crane stance while moving its arms up and down. Catlike jumps:  MIT's cheetah-bot gets higher marks for locomotion. Fed a new algorithm, it can run across a lawn and  bound like a cat. And quietly, too.  ""Our robot can be silent and as efficient as animals. The only things you hear are the feet hitting the ground,"" MIT's Sangbae Kim, a professor of mechanical engineering, told MIT News. ""This is kind of a new paradigm where we're controlling force in a highly dynamic situation. Any legged robot should be able to do this in the future."" Sign language: Toshiba's humanoid Aiko Chihira  communicated in Japanese sign language at the CEATEC show in October. Her rudimentary skills, limited for the moment to simple messages such as signed greetings, are expected to blossom by 2020 into areas such as speech synthesis and speech recognition. Dance skills: Robotic  pole dancers? Tobit Software brought a pair, controllable by an Android smartphone, to the Cebit trade show in Germany in March. More lifelike was the animatronic sculpture at a gallery in New York that same month -- but what was up with that  witch mask? Emotional ambition: Eventually, we'll all have humanoid companions -- at least, that's always been one school of thought on our robotic future. One early candidate for that honor could be Pepper, from Softbank and Aldebaran Robotics, which say the  4-foot-tall Pepper  is the first robot to read emotions. This emo-bot is expected to go on sale in Japan in February. Damn the photon torpedoes, and full speed ahead. That could be the motto for the US Navy, which in 2014 deployed a prototype laser weapon -- just one -- aboard a vessel in the Persian Gulf. Through some three months of testing, the device ""locked on and destroyed the targets we designated with near-instantaneous lethality,"" Rear Adm. Matthew L. Klunder, chief of naval research, said in a statement. Those targets were rather modest -- small objects mounted aboard a speeding small boat, a diminutive Scan Eagle unmanned aerial vehicle, and so on -- but the point was made: the laser weapon, operated by a controller like those used for video games, held up well, even in adverse conditions. What happens when robots and other smart machines can not only do, but also think? Will they appreciate us for all our quirky human high and low points, and learn to live with us? Or do they take a hard look at a species that's run its course and either turn us into natural resources, ""Matrix""-style, or rain down destruction? As we look ahead to the reboot of the ""Terminator"" film franchise in 2015, we can't help but recall some of the dire thoughts about artificial intelligence from two people high in the tech pantheon, the very busy Musk and the theoretically inclined Stephen Hawking. Musk himself more than once in 2014  ""="""" movies""="""" shortcode=""link"" asset-type=""article"" uuid=""f688c1a3-3a6e-41d0-b886-1193830e8e0f"" slug=""elon-musk-im-afraid-of-the-terminator"" link-text=""invoked the likes of the "" section=""news"" title=""Elon Musk: I'm afraid of the Terminator"" edition=""us"" data-key=""link_bulk_key"" api=""{""id"":""f688c1a3-3a6e-41d0-b886-1193830e8e0f"",""slug"":""elon-musk-im-afraid-of-the-terminator"",""contentType"":null,""edition"":""us"",""topic"":{""slug"":""sci-tech""},""metaData"":{""typeTitle"":null,""hubTopicPathString"":""Sci-Tech"",""reviewType"":null},""section"":""news""}"">  and the ""scary outcomes"" that make them such thrilling popcorn fare. Except that he sees a potentially scary reality evolving.  In an interview with CNBC in June, he spoke of his investment in AI-minded companies like Vicarious and Deep Mind, saying: ""I like to just keep an eye on what's going on with artificial intelligence. I think there is potentially a dangerous outcome."" He has put his anxieties into some particularly colorful phrases. In August, for instance,  Musk tweeted that AI is ""potentially more dangerous than nukes."" And in October, he  said this at a symposium at MIT: ""With artificial intelligence, we are summoning the demon. ... You know all those stories where there's the guy with the pentagram and the holy water and he's like... yeah, he's sure he can control the demon, [but] it doesn't work out."" Musk has a kindred spirit in Stephen Hawking. The physicist allowed in May that AI could be the ""biggest event in human history,"" and not  necessarily in a good way. A month later, he was  telling John Oliver, on HBO's ""Last Week Tonight,"" that ""artificial intelligence could be a real danger in the not too distant future."" How so?  ""It could design improvements to itself and outsmart us all."" But Google's Eric Schmidt, is having  none of that pessimism. At a summit on innovation in December, the executive chairman of the far-thinking tech titan -- which in October teamed up with Oxford University to  speed up research on artificial intelligence -- said that while our worries may be natural, ""they're also misguided."""
22,https://www.cnet.com/science/eric-schmidt-you-have-nothing-to-fear-from-ai/,CNET,2014,12,10,373.0," That Stephen Hawking? Oh, he's just an old fusspot. Elon Musk? His batteries must be running low. That, at least, seemed to be the spirit of remarks made by Google Executive Chairman Eric Schmidt at Tuesday's Financial Times Innovate America conference. Schmidt, however, said there's no darkness looming here. Indeed, he compared artificial intelligence to looming of a different era. ""Go back to the history of the loom,"" Schmidt said onstage at the conference, according to Wired. ""There was absolute dislocation, but I  think all of us are better off with more mechanized ways of getting  clothes made."" We are indeed. But did anyone really think that looms would become more intelligent than humans and dispose of them for rational reasons, or just for a yarn? Schmidt, though, focused on the economic benefits. He said: ""There's lots of evidence that when computers show up, wages go up.  There's lots of evidence that people who work with computers are paid  more than people without."" But do everyone's wages go up? Or merely those who happen to be computer-literate? Schmidt was at pains to relate that AI is still in its infancy. As an experiment a few years ago, Google created a neural network -- a computer system modeled on the human brain and nervous system -- and shoved 11,000 hours of YouTube videos into it to see what it could learn, Schmidt said. The result? ""It discovered the concept of 'cat.' I'm not quite sure what to say about that, except that that's where we are,"" he said. That may be a commentary on YouTube, as much as it's a commentary on humanity. Clearly, though, the fears of some great minds are based not on where we're at, but where we're going. Are we to abdicate essences of what we used to think of as humanity to machines? Are we to use our minds not to become better people, but to sit back, relax and let the robots try governing for a while? It's tantalizing, to be sure. It's also concerning. ""These concerns are natural,"" countered Schmidt at the conference yesterday. ""They're also misguided."" Soon, it may be time to select our guides for the trek into the future. Who is the intelligent pick?"
23,https://www.cnet.com/science/elon-musk-worries-skynet-is-only-five-years-off/,CNET,2014,11,19,433.0," For several months now, we've been reporting on tech iconoclast Elon Musk's public warnings about the dangers of artificial intelligence. What we hadn't heard was just how soon AI might become a threat. Turns out Musk, the CEO of Tesla and SpaceX, is worried that ""the risk of something seriously dangerous happening is in the five year time frame. 10 years at most."" Or so Musk wrote in a comment that followed an essay by virtual godfather Jaron Lanier titled ""The Myth of A.I."" on Edge.org last week. A number of comments from notable personalities including XPrize Foundation founder Peter Diamandis, technology editor Kevin Kelly and author George Dyson also appeared alongside the essay.  Musk's comment was quickly removed but not before it was noticed by Mashable and other outlets. In his original comment, as preserved on Reddit, Musk cited his involvement as an early investor in the British artificial intelligence company DeepMind, now a part of Google, for evidence. ""The pace of progress in artificial intelligence (I'm not referring to narrow AI) is incredibly fast,"" Musk wrote. ""Unless you have direct exposure to groups like DeepMind, you have no idea how fast-it is growing at a pace close to exponential."" Musk adds that leading AI companies ""recognize the danger"" and are working to control ""bad"" superintelligences ""from escaping into the Internet."" It's also worth considering, as Leonid Bershidsky does here, that Musk's comments about AI this year could be about hyping the industry as much as anything else. Musk, however, wrote in his comment that ""this is not a case of crying wolf about something I don't understand."" He  hinted further at his concerns this week, retweeting a 2009 video of DeepMind co-founder Shane Legg discussing the possibility of unsafe AI being given access to supercomputers. Apparently, Musk had meant for his comment on Lanier's essay to be private, and it was removed from Edge.com. A spokesperson for Musk told Mashable the comment was sent to Edge's publisher via email and was not intended for publication, adding that Musk will soon publish a longer piece outlining his thoughts on AI. Representatives for Musk did not immediately respond to CNET's request for comment. So far, we've heard Musk compare the future of AI to the "" Terminator"" series,  nuclear weapons and to "" summoning the demon."" Now we also have some idea of the setting that Musk envisions in his nightmares -- as the world celebrates the 2020 Olympics in tech-centric Tokyo, the anxious futurists among us will be keeping an eye peeled for a sneak attack launched against us meatbags by Skynet."
24,https://www.cnet.com/science/elon-musk-we-are-summoning-the-demon-with-artificial-intelligence/,CNET,2014,10,26,364.0," Elon Musk, a chief advocate of cars smart enough to park and  drive themselves, continues to escalate his spooky speech when it comes to the next level of computation -- the malicious potential of artificial intelligence continues to freak him out. ""With artificial intelligence, we are summoning the demon,"" Musk said last week at the MIT Aeronautics and Astronautics Department's 2014 Centennial Symposium. ""You know all those stories where there's the guy with the pentagram and the holy water and he's like... yeah, he's sure he can control the demon, [but] it doesn't work out."" This has become a recurring theme in Musk's public comments, and each time he warns of the AI bogeyman it seems even more dire. In June, Musk raised the specter of the "" Terminator"" franchise, saying that he invests in companies working on artificial intelligence just to be able to keep an eye on the technology. In  August, he reiterated his concerns in a tweet, writing that AI is ""potentially more dangerous than nukes."" Just a few weeks ago, Musk half-joked on a different stage that a future AI system tasked with  eliminating spam might decide that the best way to accomplish this task is to eliminate humans. But this is the first time I'm aware of that Musk has kicked up the rhetoric another notch -- perhaps anticipating this week's onslaught of  Halloween costumes -- to compare AI to something supernatural like demons. How to deal with the demonic forces of AI in the future? In a strange move for a tech mogul, Musk suggests it might be a good idea to fight one bogeyman with another (depending on your political perspective) in the form of government regulators. ""If I were to guess at what our biggest existential threat is, it's probably that,"" he said, referring to artificial intelligence. ""I'm increasingly inclined to think there should be some regulatory oversight, maybe at the national and international level just to make sure that we don't do something very foolish."" Indeed. Who knows what demonic hellscape could emerge if we ever let artificially intelligent machines get ahold of a Ouija board. Watch Musk's comments for yourself in the video below."
25,https://www.cnet.com/roadshow/news/elon-musk-sees-autonomous-cars-ready-sooner-than-previously-thought/,CNET,2014,10,10,399.0," Elon Musk takes a cautious approach to artificial intelligence, but he's certainly not afraid of making our cars a little -- or even a lot -- smarter. After the Tesla Motors founder unveiled a new autopilot feature for the company's line of electric sports cars Thursday, he told Bloomberg Television that fully autonomous vehicles could be available within a decade. ""That will be the case at some point in the future. Like maybe five or six years from now I think we'll be able to achieve true autonomous driving where you could literally get in the car, go to sleep and wake up at your destination,"" Musk said. He added that it may take a few years beyond the point when the technology is ready for regulators to sign off on it. Musk also stressed that the new  Tesla autopilot system, which uses radar, ultrasonic sensing and cameras to create a sort of super-smart cruise control, obstacle avoidance and lane-keeping system, is not the same as a  self-driving car. ""Autopilot is what we have in airplanes. For example we use the same term that is in airplanes where there is still an expectation that there will be a pilot. So the onus is on the pilot to make sure that the autopilot is doing the right thing."" Musk said that a couple of years ago autonomous cars looked like they were a decade away, but the rate of improvement and progress toward the goal makes him more optimistic that it could be sooner. Interestingly, Musk's optimism about self-driving cars comes from a man who often sounds the warning alarm about the dangers of super-smart machines. On Wednesday,  Musk semi-seriously joked to Walter Isaacson on stage at Vanity Fair's New Establishment Summit in San Francisco that if some sort of intelligent system had a function that ""is just something like getting rid of email spam and it determines the best way of getting rid of spam is getting rid of humans..."" Musk trailed off. Musk has also said he fears a real-life  Skynet scenario, and he tweeted over the summer that artificial intelligence could be "" potentially more dangerous than nukes."" Let's just hope that if Skynet ever does rise up that Tesla will already have built a sufficient enough firewall to keep any menacing, artificially intelligent system from turning Elon's new autopilot system into an auto-murdering ""spam removal"" feature."
26,https://www.cnet.com/science/elon-musk-worries-ai-could-delete-humans-along-with-spam/,CNET,2014,10,9,259.0," Elon Musk has made no secret of his worries about the possible destructive power of artificial intelligence. The billionaire chief executive of SpaceX and Tesla Motors may be a techno-optimist when it comes to solar power, space exploration and electric cars, but he continues to express his concerns that superintelligent machines might one day pose a threat to human existence. During an onstage conversation Wednesday at Vanity Fair's New Establishment Summit in San Francisco, Musk voiced concern that people did not recognize how fast AI is progressing and its potential destructive effect. ""I don't think anyone realizes how quickly artificial intelligence is advancing. Particularly if [the machine is] involved in recursive self-improvement...and its utility function is something that's detrimental to humanity, then it will have a very bad effect,"" Musk told Walter Isaacson, CEO of the Apsen Institute. ""If its [function] is just something like getting rid of e-mail spam and it determines the best way of getting rid of spam is getting rid of humans..."" Musk trailed off to chuckles from the crowd. Though many futurists envision a more human-beneficial application of AI, Musk has voiced his apprehensions on several occasions. In June,  Musk told CNBC that he worries that -- unrestrained -- AI could breed an uncontrollable threat to humans like that depicted in the 1984 movie ""The Terminator."" In August, Musk  reiterated those concerns in a tweet recommending an upcoming book that addresses the dangers of a future filled with AI. ""We need to be super careful with AI. Potentially more dangerous than nukes,"" he tweeted."
27,https://www.cnet.com/science/elon-musk-artificial-intelligence-could-be-more-dangerous-than-nukes/,CNET,2014,8,3,300.0," Elon Musk thinks solar power and electric cars are the future, and that we should get to work building humanity's second home on Mars as soon as possible. But the billionaire techno-optimist continues to speak out about what he sees as the dangers of a future filled with super-intelligent machines. On Saturday Musk posted this tweet, recommending an upcoming book that examines such a future, adding ""We need to be super careful with AI. Potentially more dangerous than nukes."" Worth reading Superintelligence by Bostrom. We need to be super careful with AI. Potentially more dangerous than nukes. This isn't the first time Musk has voiced his apprehension about artificial intelligence, which other notable futurists like Google's  Ray Kurzweil see in a much more positive, even romantic  light. Back in June,  Musk said on CNBC that he seriously considers the possibility of a 'Terminator""-like scenario actually coming to pass, and that he's even invested in AI companies to keep on eye on where the technology is headed. Of course, if you read a little deeper into the writing catalog of Nick Bostrom, the author whose book Musk recommends in his tweet, you'll discover his concerns could be a moot point. Back in 2003, Bostrom published a paper arguing the odds are pretty good that we're actually  living in a computer simulation, and scientists have recently begun to search for ""signatures"" that show our world is not quite as real as we'd like to think. So yes, perhaps Elon, the driving force behind Tesla Motors and SpaceX,  is right and we should be more worried about artificial intelligence, unless of course, we are also artificially intelligent. If that's the case then we need to... Hold on, I'm going to need to re-watch ""Tron"" and get back to you guys on this one."
28,https://www.cnet.com/science/your-phone-will-never-love-you-back-and-you-shouldnt-want-it-to/,CNET,2014,6,20,659.0," Sometimes it seems like our relationships with our phones, tablets, and other devices are much easier than those we have with other meatbag humans and pets. Communication is often easier, and the mountain of data we share can enable our gadgets to know us more intimately than just about anyone else. So it's easy to understand why super-smart futurists like  Ray Kurzweil -- he of ""the singularity,"" computer-aided  immortality, and Google engineering fame -- think that we could soon be interacting with our devices on an emotional level, much like in the Oscar-nominated film ""Her"" about a man in love with a Siri-like operating system. ""Computers will be at human levels, such as you can have a human relationship with them, 15 years from now,"" Kurzweil said last week at the Exponential Finance conference, where he also described ""Her"" as a realistic depiction of how software will be able to be funny, romantic, loving, and even sexy. Oh, really? I love Kurzweil and his techno-optimism, but I worry when engineers begin trying to break amorphous adjectives down to binary code. No matter how powerful our computing ability, no matter how much data we gather, software is designed by humans -- woefully flawed and inadequate folks like you and me who will stumble and stutter for several seconds if you ask them to define concepts like ""funny,"" ""romantic,"" and ""sexy"" before likely uttering a stream of equally vague and unquantifiable synonyms. The meanings of these words, let alone the experiences that humanity kind of roughly agrees that they represent, is subject to insanely broad variation. Ask all 7 billion-plus humans to write an essay on what love is and you undoubtedly will get no identical responses. A software engineer might be able to harness this data to identify common themes in our understanding of love and other emotions that a machine could understand, but this would inevitably water down whatever artificial emotional intelligence could be created from it. We'd end up with irritating and needy phones that think they're in love with us because they understand love on the level that a child understands it as a warm, fuzzy feeling it gets from the sense of attachment and security provided by family or a particular teddy bear -- ""Yes, Siri, thank you for writing 'I love you' in fractals again. That's great, honey, I love you too. Now please tell me how to get around this effing traffic jam!"" There's a crazy paradox in the quest for artificial emotional intelligence (AEI) that Kurzweil and fictional works like ""Her"" tout. We, as humans, don't yet understand our own emotions enough to synthesize them digitally. And if we reached that enlightened moment where we truly got them, we probably wouldn't need or desire computers to replicate them. I mean, whenever I've had an awesome block of sharp Wisconsin cheddar in my fridge, I've never pushed it aside in favor of a slice of Velveeta. And let's not forget the dark side of AEI that Kurzweil fails to mention, but that others like  Elon Musk and Stephen Hawking seem to ponder. If computers can be programmed to love and feel, doesn't the ability to manipulate and tear out your heart come right along with that? ""Her"" doesn't have the happiest ending after all. While I'm very near to in love with my devices, they are still tools, and the ultimate goal in using them is almost always improving and enriching interactions with other humans. Maybe our species has a tragic inferiority complex, but I think I prefer the company of other clueless meatbags. At least we know what we don't know. With software it's always true or false, on or off, one or zero; but the beauty and poetry of life and the human experience isn't binary, rather it's in the undefined. Actually, maybe that's the key to cracking the code for true AEI: Love=NULL. Try running that and see what you get."
29,https://www.cnet.com/tech/gaming/crossed-up-anki-drive-unveils-new-cars-and-first-new-tracks/,CNET,2014,4,16,510.0," Anki Drive owners owners just got a reason to play the racing game featuring AI cars for many more hours. Today, the San Francisco based startup announced its first new tracks, as well as two cars with all-new capabilities, and a free software update available to all Anki owners that adds a racing mode, a new UI, and a turbo boost feature. Anki, which  launched last October, was first introduced in June, 2013 in the enviable and very rare position of an  on-stage demonstration at Apple's Worldwide Developers Conference keynote. The company, which has raised $50 million in funding from Andreessen Horowitz, Index Ventures, and Two Sigma, is competing for its share of the $22.1 billion total toy market, and more specifically, the $1.51 billion toy vehicles market. The company's $200 starter set, which comes with a basic 3.5 foot by 8.5 foot roll-up racing track and two cars. Anki had previously introduced two additional cars, each of which sell for $70. The first of the new tracks, both of which will be available May 6 and cost $100, are the same size as the original, is known as Crossroads. For the first time, this offers Anki players a track with an intersection, meaning that players will have to learn how to navigate their cars through the crossing while avoiding others cars, yet without losing position against opponents. But, said Anki president and co-founder Hanns Tappeiner, cars will collide, and success will come from learning how to navigate the new challenge. The second new track is called Bottleneck, and features a narrow section that will force players to learn how to drive carefully at high speeds. Tappeiner said that Crossroads will add ""fun chaos"" to Anki Drive, while Bottleneck will force players to be more strategic. The two new cars, which will retail for $70 each and are available today, are known as Corax and Hadion. Corax is a car built for battle, and will be the first Anki car capable of mounting two weapons. Hadion is meant for racing speed, and will be the first and only Anki car capable of using the new turbo boost tool. In Anki Drive, players can race against cars controlled both by other people and the game's artificial intelligence system. During a demonstration at Anki's headquarters, Sofman showed how the AI-controlled cars interacted in real time with those being run by actual people. Indeed, the response time of the AI-controlled cars, whether it was to attack a slower car, or to evade an attack itself, was instantaneous. That's thanks to built-in components that check their driving logic 500 times a second, and convey positioning information to the car's wheels every 2 milliseconds. Like the original, the new tracks are covered in special ink and optics that allow the cars, known as ""characters,"" to constantly check where they are. And that, in turn, helps the cars -- regardless of whether they're being driven by a person or by the system's AI -- stay stable, even as they race around the oval at high speeds."
30,https://www.cnet.com/tech/tech-industry/yahoos-mayer-aviate-is-central-to-our-mobile-ad-biz/,CNET,2014,4,16,658.0," Right from the start of her tenure in July 2012, Yahoo CEO Marissa Mayer has said repeatedly that she wants Yahoo to be synonymous with mobile computing. If she's successful in achieving her ambition,   a small, little-known artificial intelligence company called Aviate figures to play a pivotal role. Akin to the service Google Now, Aviate takes advantage of a user's context -- like frequently used apps or the time of day -- to surface useful apps and information on an Android homescreen the moment it becomes most useful. Yahoo announced the rumored $80 million acquisition  in January during her keynote at the Consumer Electronics Show in Las Vegas, and it already merited a special call-out during the company's latest earnings call. ""Aviate is a central part of our mobile search,"" Mayer said during a video conference call on Tuesday to discuss  first quarter earnings. But a good user experience isn't the only thing Mayer is after. She -- and more importantly, investors -- are concerned with bolstering mobile advertising opportunities, an area Yahoo has fallen sharply behind on. Last quarter, she said the company's mobile advertising revenue is ""not material."" By contrast, Facebook makes about 53 percent of its advertising revenue on mobile. Yahoo has put a lot of investment recently in native ads, or the type of ad that fits in more with editorial content than one that's clearly cordoned off. In February, Yahoo launched Gemini, a marketplace geared toward advertisers interesting in native ads. Aviate, Mayer said, will be where the company can experiment with those ads. ""What formats work well when we look at contextual search?"" Mayers said. Native ads, ""work well with Aviate. There's a lot of experimentation."" ""We think there's a great opportunity there to be really industry leading,"" she continued. ""That's what our acquisition of Aviate was really about."" When asked by CNET in April, Sameet Sinha,  a senior analyst at research firm B. Riley and Co. (which owns a small holding in Yahoo), speculated on the opportunity. ""If something can tie together your life, there is significant  opportunity there to monetize each step: In the morning, stop at this  place to get coffee, then this gas station,"" he said. As of March, Aviate co-founder Mark Daiss told CNET there were no plans for monetization on the service, though that could change. Still, it was a part of the company's thinking from nearly the beginning. Cameron Teitelman, who founded the Stanford incubator StartX, in which  Aviate participated, said lead generation was a big part of Aviate's  business plan, as it developed over the 11-week program. Aviate is just one of a slew of acquisitions Mayer has made since taking the top job at the company in 2012. She has gotten flak at times for all the seemingly disparate investments she's made  -- from video startups to the blogging platform Tumblr to talent grabs like news anchor Katie Couric. Critics say her approach lacks focus, but on Tuesday, she tried to present a cohesive strategy. She distilled the approach into four golden buzzwords: mobile, video, social, and native.  Mayer described these areas to be ""nascent"" and still a small part of the company compared with Yahoo's overall business, but this is clearly her vision for the company -- four buckets into which every strategic move can be dropped. That shift may be a good thing, considering Yahoo's current core business -- search and display advertising -- is still sluggish for the most part. Both area saw  small gains in Tuesday's earnings report -- display was up 2 percent and search was up 9 percent -- but Yahoo still lags behind the other tech giants in key advertising metrics. Last year, Facebook overtook Yahoo for the first time in display advertising, according to the research firm eMarketer. Asked by an analyst on Tuesday to rank each of the four areas by importance, Mayer dodged, but said the most important for now is, unsurprisingly, mobile."
31,https://www.cnet.com/tech/services-and-software/of-course-you-need-a-robot-to-help-prioritize-your-email/,CNET,2014,4,10,539.0," People in general, and business people in particular, are becoming increasingly incompetent. There's too much to do, too much to react to. The human mind just doesn't have the organizational skills. That's the lovely thing about our computerized world -- you need a machine to help you make sense of it. Even deciding which e-mails to reply to can take far too much time and mental energy. Clearly what we need is a robot to do it for us. Along, therefore, comes a free iOS app called Seer. This claims to be the first app to use artificial intelligence to help you get your e-mail house in order. You might wonder what kind of AI would be appropriate for wading through your e-mails and seeing if there's anything important there. And I would answer: AI with some CIA roots. Seer claims to know which requests are urgent and which are just the usual flim-flam and bunkum from your boss. It claims to have such a fine and proprietary algorithm that it already knows precisely which e-mail you should write next. Yes, the one to Suzie, the CEO of the company that makes diamond-studded riding crops. This fine app even reminds you, by digital post-it note, when someone's been rude enough not to employ their own robot and therefore failed to reply to an urgent e-mail of yours. I asked Conall Arora, Seer's co-founder how he'd got hold of CIA intelligence. He told me: ""Our  adviser used to consult for the CIA and Department of Defense, and  helped developed the techniques Seer uses to pick out actionable phrases  like 'can you send me', or 'does this work?'"" I am not sure if the original phrase the techniques developed was ""can we get into his e-mail without him knowing?"" The ""intelligence"" part of ""artificial intelligence"" has always fascinated me. Sometimes, it doesn't seem so, well, intelligent. Or even intelligible. Seer iPhone Preview from SEER on Vimeo. So how does this particular AI work? Arora told me: ""It  uses your sent folder to see who you normally respond to. If you  haven't answered them in the last 30 days, Seer will not display  unanswered emails from them."" A question tingled my tongue: But what if Seer is completely and utterly wrong? What if I really like someone and they're extremely important to me, but I'm just sulking? Arora explained: ""You can adjust these settings and its extremely easy to dismiss in the cases where Seer's assumes incorrectly."" Here's the thing about all these robots. They still make you do the work before they do the work. They don't really know much at all. Still, what if Seer starts nagging? Arora said that won't happen: ""Seer will remind you once a day, if you give it permission and select a time during the setup."" Indeed, he said that most people who have used the app only spend one or two minutes a day to catch up with everything. But what if you don't listen? Will it get mad at you? Arora reassured me: ""If you remain lazy, despite all of Seer's efforts, it will not make any judgments. It is just a robot, after-all."" The judgmental robot is at least a year away."
32,https://www.cnet.com/tech/tech-industry/marissa-mayer-wants-yahoo-to-be-big-in-mobile-meet-aviate/,CNET,2014,4,9,2455.0," When Marissa Mayer stepped on stage during the Consumer Electronics Show in Las Vegas in January to deliver her first keynote at the high-profile gadget extravaganza, she made sure it was a star-studded affair. There was a faux newscast by Saturday Night Live's Weekend Update team, poking fun at the ""CEO of Yoo-hoo,"" a trio of songs crooned by musician John Legend, and a parade led by Yahoo's biggest talent grabs, news anchor Katie Couric and former New York Times reviewer David Pogue. But amid the hoopla -- and before she delivered a slew of announcements during her hour-long talk -- Mayer gave the first  headline to a little-known artificial intelligence startup. ""We're incredibly excited to kick things off today by announcing that we have acquired Aviate,"" Mayer said. Aviate's technology takes advantage of Android's open platform to take over your smartphone's home screen and showcase information and apps when they might be the most useful to you. If you check Yahoo's Finance app every morning, for example, Aviate takes note and rearranges your home screen to put that app -- and similar apps -- front and center. Like Google's artificial intelligence service Google Now, Aviate takes cues from personal information and data, as well as your location or the time of day, to surface information and highlight apps at the moment they're relevant. ""Think about how much your phone understands you,"" Mayer told the crowd at CES. ""Imagine what happens when that context becomes part of the search experience."" While Yahoo, a lumbering early Internet pioneer, is in the midst of an attempted turnaround under Mayer, it is still an almost $5 billion-a-year company in annual sales with a lot of resources. Investing in the platform can only add to the net positive  for consumers interested in artificial intelligence, as other tech  giants like Apple, Google and even Twitter do the same. But Yahoo arguably needs this kind of technology more than those other companies. The race to dominate platforms, from smartphones to smart glasses, is in full swing and Yahoo is behind. While some companies are already on the lookout for the next big platform -- Facebook recently  paid $2 billion for the virtual reality goggle maker Oculus -- Yahoo has had difficulty making a dent on the current platforms of choice, smartphones and tablets. That's a problem given that Yahoo's core business, display advertising, is in  decline as its hold on consumers seems to be slipping. In February, Yahoo  fell to No. 2 behind Google as the most-trafficked desktop Web site  in the U.S. for the first time in seven months. Since Mayer joined Yahoo as CEO in 2012, the company has overhauled many of its mobile properties, from its weather to email apps. But it still doesn't have a pervasive mobile experience, leaving Mayer on an earnings call with investors in January to describe Yahoo's mobile advertising revenue as ""not material."" By contrast, Facebook, which has also had woes  transitioning from desktop to mobile, now makes 53 percent of its advertising revenue on  mobile. That pervasiveness is key. Then, Yahoo will have a mechanism to push its own content to users, including its Finance, Sports and Shopping properties. ""If Marissa wants to tie all these apps into one cohesive experience, Aviate becomes critical,"" said Sameet Sinha, a senior analyst at research firm B. Riley and Co. (which owns a tiny holding in Yahoo). Most of Yahoo's conquests are guppies -- small acqui-hires or core technological grabs here and there, including the natural language startup  SkyPhrase and the mobile video startup  Ptch. The exception: a billion-dollar whale, the blogging platform  Tumblr, which is Mayer's biggest acquisition to date. These deals are typically announced via a meager blog post on the acquired company's Web site. So Mayer's decision to give Aviate such prominent billing during her CES presentation seems telling of its value to the company. So is its rumored $80 million price tag -- not the fortune the company paid for Tumblr, but still nothing to scoff at for a one-year-old company still in private beta. Yahoo also typcially kills the products and brands of its acquired companies, while Aviate and Tumblr have been allowed to carry on. So while the Tumblr acquisition may be the defining status symbol purchase of Mayer's early tenure at Yahoo, it's Aviate that has the most implications for the company's future. Apple and Google both have operating systems. Aviate can give Yahoo an overarching, if subtle, gateway into mobile, something a blogging platform or a nicely designed weather app can't do. So it's Aviate, not Tumblr, that may end up as Yahoo's newest crown jewel. On the fourth floor of building D at Yahoo headquarters in Sunnyvale, Calif. -- the Mobile and Emerging Products floor -- engineers and designers from properties like News Digest and Yahoo Weather sit with their desks in groups. Above them, a few large flags hang from the ceiling, mostly from different countries like France and Malaysia. One flag stands out: the skull and crossbones. If the hackneyed phrase about engineering talent being Silicon Valley gold is true -- and if Yahoo is intent on pillaging it and bringing it aboard the good ship Yahoo -- the fourth floor has the pirate flag to prove it. Aviate co-founder Mark Daiss, who sits alongside his team, doesn't even have a Yahoo business card yet -- over two months after Yahoo bought his Palo Alto, Calif.-based company. But Aviate has already benefited from Yahoo's growing headcount of startup developers. The team started out with seven people when Yahoo bought it. As of April, there are a dozen people. The new team members, Daiss says in an hour-long interview in a nondescript conference room, came from Yahoo's employee repository -- people from some of the other acquired startups, and some older employees. Aviate was born in November 2012 when Daiss, a reformed financial consultant now sporting a zip-up fleece and a dusting of stubble, teamed with his cousin Paul Montoy-Wilson, and Wilson's classmate from Stanford University, William Choi. The trio formed ThumbsUp Labs. Montoy-Wilson knew the business of curating apps intimately, having worked on categorization and interface design for Android's Google Play store. Their first project was an Android widget that surfaced different apps on a phone when they became relevant to a user. The team wanted a name that conveyed speed, as well as innovation and navigation: Aviator. As they continued developing Aviator, they thought it could do more, bringing up relevant information from inside certain apps, instead of just bringing that app's icon to the forefront of the screen. ""We had the widget and thought, 'Oh my gosh, what about an Android home screen?'"" recalled Daiss. ""There's no other interface you touch more in your life than the home screen of your phone."" As they looked at other artificial intelligence products on the market -- including Apple's Siri voice assistant -- they decided they didn't want something with a human element. The thinking: humans are fallible, and they can be slow. And the personal assistant trope can be gimmicky or outdated. (There's a reason Ask Jeeves eventually became Ask.com.) So it was settled. In spring 2012, Aviator became simply Aviate. For a company that started out called ThumbsUp Labs, Yahoo may have seemed like a surprise choice as a corporate parent over the social network that has made the thumbs up symbol the official digital seal of approval. But at the same time Aviate was being developed, Facebook unveiled its own plans for similar technology. In April 2013, Facebook invited press to a glitzy product announcement, with dimmed lights and loud music, reminiscent of an Apple keynote hosted by Steve Jobs. The company unveiled  Facebook Home , a software bundle that took over the Android home screen, giving Facebook an omnipresence on the smartphone. The product was also tied into a promotion with the  HTC First , with the handset offering Facebook Home preloaded for its users. More than an app, but not quite an operating system, Wired dubbed it an ""apperating system."" It was Facebook's biggest push into mobile yet. And it flopped. Adoption never took off, and less than a year later, Facebook has changed its strategic course in mobile. Instead of going all in on Android, it has opted to run a fleet of  standalone apps, including its news reader Paper, popular Instagram for photo sharing and its new messaging service, WhatsApp. While Daiss is sympathetic to Facebook's homescreen experiment, he's quick to point out why Aviate is different. ""Facebook Home is a great product. But it was a product that was very focused on Facebook,"" he said. ""At Aviate, we think there's tons of amazing content that developers are building. And so our phones should be trying to get us to all of these things as quickly as possible -- not just trying to get to one or a few of those products."" So there's Yahoo's approach: Think broadly. In that way, Aviate may be much more of an ""apperating system"" than Facebook Home ever was. While Apple owns the iOS mobile operating system and Google owns Android, Yahoo needs another way onto mobile devices. Aviate could be that way. If the product proves useful enough for users, Yahoo will have successfully hijacked their operating systems. ""Yahoo needs to find a different insertion point. That's why it acquired Aviate,"" said Raj Singh, co-founder of  Tempo, an artificially intelligent ""smart"" calendar app. Singh was also entrepreneur in residence at the Stanford Research Institute, or SRI, where Apple's Siri was spun out. As for Facebook's failure on home screens, Singh thinks Home had the classic misfortune of being too early. ""The problem with Facebook is they were first to market,"" he said. ""It goes to the point: If you're not a default app, you're fighting to get a mere percentage point or two [of usage]."" Still, Facebook's foray opened up the market. ""When we saw Facebook launch their home screen, we thought, 'Who are the other people that might launch [their own]?'"" said Daiss. Considering a possible Yahoo buyout, he thought, ""There were just so many synergies there. And some already thinking about what a home screen could look like for Yahoo."" Which begs the question, asked by other tech observers as well: Could Aviate become Yahoo Home? For now, Daiss said, there are no plans for rebranding. In an analysis of Aviate's value to Yahoo, Search Engine Land's Greg Sterling even suggested slapping a Yahoo search bar onto the interface to drive traffic in that area. Daiss balked when I asked him about that possibility, saying his team preferred to focus on contextual search. This makes sense for the product, but it may also speak to just how much Yahoo is allowed to innovate in the search arena. Two years before Mayer took charge, then-CEO Carol Bartz inked a 10-year search partnership between Yahoo and Microsoft's Bing search service, outsourcing the back end Web crawling capabilities of Yahoo's home page to Microsoft, while Yahoo remained free to handle the user interface. Either company can try to end the deal as early as next year, but even so, Aviate's approach exploits a loophole: the search agreement only covers queries based on keywords, so  contextual search is fair game. The product also offers opportunities for monetization -- paramount with Yahoo's core businesses lagging. While Aviate surfaces existing apps on a phone that it thinks are relevant at any given moment, it also suggests related apps from the Google Play store. If you  plug headphones into your phone, for example, Aviate not only brings up apps like Spotify that you may have already downloaded, but also suggests other music apps. That functionality is rife for sponsored suggestions. ""If something can tie together your life, there is significant opportunity there to monetize each step: In the morning, stop at this place to get coffee, then this gas station,"" said Sinha, though he said he doesn't know of any financial analysis that's been done to speculate the opportunity. Daiss said there are no plans for making money directly off Aviate yet, but that could change in the future. Indeed, it was a part of the company's thinking from nearly the beginning. Cameron Teitelman, who founded the Stanford incubator StartX, in which Aviate participated, said lead generation was a big part of Aviate's business plan, as it developed over the 11-week program. To be fair, for Yahoo to fully reap the benefits, Aviate has to be a true utility, and the product hasn't yet reached that level of indispensability. Granted, it is still in private beta, but Aviate doesn't have the ability to read through a user's messages or email to pop up relevant info, like driving directions to an appointment it knows you have. That functionally is in Tempo, offered by Singh's company, and in Apple's recently announced  CarPlay. Google Now also has the ability to sort through a user's search history to predict what he or she might want. Daiss said it's a long-term game. ""We have ideas to last us the next five years."" Yahoo would not make its mobile chief Adam Cahan available for this story, but in a January blog post announcing the Aviate takeover, he touted the startup's importance to Yahoo's mobile strategy. ""We hope to make Aviate a central part of our Android-based experiences in 2014 (and beyond),"" he wrote. That's integral since Yahoo isn't the only big tech company trying to disrupt mobile operating systems through app discovery. Cover, a startup that makes a similar product to Aviate that's focused on Android lock screens,  announced this week that it was being bought by Twitter. And while it's not as in your face on the home and lock screens, Apple has also recently begun  experimenting with app discovery in its App Store, outside of its top and featured app charts. While Yahoo has promised to remain hands off while Daiss and Co. build out their vision, it's also true that Yahoo is under pressure to appease its investors. The company's stock has been goosed in the last few years by its stake in the successful Chinese e-commerce giant Alibaba, which plans to  go public later this year in the US -- a move that will see Yahoo investors get their pay day. While the stock price shot up in January to over $41, it has hovered in the mid-$30s and below during the early part of April. On Wednesday, it traded a little above $34. Time is ticking for Yahoo to discover something shiny and valuable on its own. For now, it's got the pirate flag waving to show it's out there in mobile waters, looking to score treasure."
33,https://www.cnet.com/tech/tech-industry/musk-zuckerberg-kutcher-invest-in-artificial-intelligence-firm-vicarious/,CNET,2014,3,21,196.0," Three prominent figures in the technology world have invested serious cash into a company that's trying to make a major move in artificial intelligence. Tesla chief  Elon Musk, Facebook co-founder Mark Zuckerberg, and actor and venture capitalist Ashton Kutcher have invested $40 million into Vicarious, a company that's studying the human brain's neocortex and attempting to translate its function to computer code, the Wall Street Journal reported on Friday. The neocortex is an essential element of being human, providing the functions that allow us to think, see, move, and much more. Speaking to the Journal in an interview, Vicarious co-founder Scott Phoenix said that his company's goal is to build ""a computer that thinks like a person."" Vicarious is part of a broader robotics and artificial-intelligence movement sweeping the tech world. A slew of major companies, including Google and Amazon, are looking to leverage robotics, and Google is especially interested in enhancing artificial intelligence. Vicarious can't say how long it will take for it to achieve its goal of creating a robot that can think like a human, but the company believes it's only a matter of time before it happens. Can you say, ""Battlestar Galactica""?"
34,https://www.cnet.com/culture/bot-or-not-try-to-tell-a-human-poet-from-a-computer/,CNET,2014,3,10,313.0," How good are you at telling the difference between words written by a  human and words written by a computer? Maybe after taking the Bot or Not test, you'll better understand how research publishers Springer and IEEE managed to miss gibberish papers. Developed by two Ph.D. candidates at Australia's RMIT Melbourne -- Benjamin Laird and Oscar Schwartz  (who's writing a thesis on whether computers can write poetry) -- the  Web site is described as a ""Turing test for poetry."" Alan Turing  developed the Turing test to determine whether intelligence is human or  artificial. The site has two modes -- a set test presented to attendees at the Digital Writers Festival  in Melbourne from February 13-24; and Free Mode, which allows you to  assess poems for as long as you like. The Web site will present a poem,  and you have to guess whether it was written by a human poet or a  computer program, such as jGnoetry and Ray Kurzweil's cybernetic poet. The line between human and AI poets is becoming increasingly difficult to distinguish, as demonstrated by the site's leaderboards,  which display the most human-like human poets and the most  computer-like computer poets, as well as the most human-like computer  poets and vice versa. For example, the computer-generated ""A Wounded Deer Leaps Highest"" is often mistaken by users as human poetry, while people seem to think Deanna Ferguson's ""Cut Opinions"" reads more like the work of a machine. Computer poets are growing ever more sophisticated. Take Nathan Matias' Swift-Speare,  an algorithm that can generate poetry in idiosyncratic styles, but only  with the help of a human agent. According to Matias, though, a  completely AI poet may not be far off. ""I think I'll see a successful  automated poet in my lifetime,"" he said. ""It won't be easy: a poet is  more than someone who makes poetry. Yet that doesn't rule out  algorithms."""
35,https://www.cnet.com/reviews/forza-motorsport-5-xbox-one-preview/,CNET,2013,11,6,2296.0," I was recently given an opportunity to sneak away from work and spend an afternoon playing the latest installment of Turn 10's Forza racing sim series, Forza Motorsport 5. The game is scheduled to launch alongside the Xbox One on November 22 and as a Microsoft Studios property, it should be a good indicator of what the next-generation console is capable of. Forza 5 utilizes a number of the Xbox One's new hardware and software features. Its force feedback utilizes the Xbox One controller's advanced rumbling triggers. Its AI is processed in the cloud via the always-on Xbox Live connection. Its graphics are decidedly next-gen. After plopping down onto one of the dozen or so couches in front of one of the dozen or so Xbox One gameplay stations at the event, I grabbed the Xbox One controller and hit start. 'A celebration of all things four-wheeled and fast'The game starts out with a short video vignette that sets the tone for the game; it's a sort of love letter to cars that is narrated by ""Top Gear's"" Jeremy Clarkson, and our first hint of the game's ties to the world-famous motoring show. To use Clarkson's own words, Forza 5 is ""a celebration of all things four-wheeled and fast,"" and the introductory video features footage of all sorts of racing, from Formula 1 to exotics, from sports cars to economy cars. After that, the game drops you into its Autovista mode where you are presented with the flagship vehicle for the title, the McLaren P1. You're told to open the door, climb inside, and start the engine. Even more Autovista to exploreOne of the first things that Turn 10's Dan Greenawalt pointed out to me was that every car in the game is available in the game's Autovista mode. For those unfamiliar, Autovista is a virtual walk-around that debuted in Forza Motorsport 4 for the Xbox 360, where players can open the doors, settle into the driver's seat, fire up the engine, and peek under the hood of the digital cars just like they would in a showroom. (Since most of us will never get to kick the tires of a P1 or a Huayra or get our fingerprint on the paint, it's perhaps better than a showroom.) Where Forza 4 only featured about two dozen Autovista cars, every single one of Forza 5's more than 180 vehicles will get the virtual walk-around treatment. Not all cars are created equally, though. Some cars are still treated as halo cars and have more-detailed Autovista experiences complete with a voice-over history and description of the vehicle by one of ""Top Gear's"" presenters. Other cars won't be as detailed -- maybe the hood doesn't open to display a modeled engine, maybe you can't pop the trunk -- and feature voice-over by the female Forza narrator that's unique to the automaker and not the specific vehicle. However, all of the cars will have detailed interiors, doors that can open, and full specs. Zoom way in while using the Autovista mode or the photo mode -- that lets users snap still photos using virtual camera controls to share with other players -- and you'll see the flecks in metallic paint or a slight orange peel texture when looking at older vehicles. The level of exterior detail is phenomenal. Feeling the ABS through your fingertipsSo, I'm in the P1 and pressing the virtual start button on its digital dashboard, and before I know it I'm trading paint with other exotics in Forza's first race. This is a common trope for racing sims -- dropping you into one of the game's best cars for the first race and then scaling back and making you earn your way back to it -- but it's a good way to keep the excitement built by the intro video elevated and to give players a taste of the game right off of the bat. I noticed a lack of licensed music in this game. In its place, there's a driving, cinematic score to the races that adds a level of urgency and drama to each race. I was only able to play for a total of 1 hour, so the music didn't feel too repetitive, but ask me after a month if my opinion changes about that. I'm familiar with Forza's mechanics, but was only just getting used to the new Xbox One controller, so I finished this first race in second place with the P1 in less than stellar condition. The game takes advantage of the controller's new force feedback, so when you pull the right trigger to accelerate, you'll feel a bit of vibration through your fingertip the way that you would through your accelerator pedal. Likewise, pulling hard on the left trigger to brake causes the trigger to vibrate and pulse through your fingertip, signaling that the ABS system is active. Larger bumps and shakes can be felt through the larger rumble motors near the palms of your hands. The difference in force feedback is subtle and won't change the way you play racing games, but the attention to detail is nice. Forza's Replay feature is still there for when you get into trouble. Go off track or crash spectacularly and you can rewind the action a few seconds with the touch of a button and try again. 'People don't finish racing games, racing games finish them'Once I was out of the P1, it was time to choose the first car for my virtual garage from the sport compact class. After a brief video description of the class, narrated by the ""Top Gear"" presenters, I was presented with a short list of cars. I, of course, chose the 2013 Mazda MX-5, because the Miata is always the answer. Greenawalt, during his presentation, stated that ""people don't finish racing games, racing games finish them,"" explaining further that historically, racing games forced players down a linear path of ever-faster cars that eventually exceeds their skill level. I, for example, spent most of my time in Forza Horizon tooling around in a first-generation Mazda Miata and, when the game forced me into cars that I didn't want to drive, I lost interest. Forza 5 tries to address this, allowing drivers to stick with the cars that they want to drive by splitting the single-player campaign into eight Career Leagues that correspond to different classes of vehicle and featuring multiple race series nested within. Players can jump among leagues as their skill level and stable of vehicles improve, but they can also stick to their league and progress through the game in their Scion FR-S. After only an hour of play, I'm not sure if this League system really adds as much flexibility to the single-player racing as Turn 10 claims, but it looks promising. If it means that I don't have to slog through truck/SUV races, sign me up. When choosing your car, Forza will automatically download the most popular custom designs from the community and present them alongside the standard paint color picker. What's interesting is that, as you choose designs for your cars and rate them, the game will attempt to match your tastes and, ideally, will have a good idea of what you like in custom graphics. I chose an odd wood finish for my Mazda, because it looked silly, and hit the button to proceed into the first race in the series in the Bernese Alps. Drivatars and cloud-based AILining up on the starting grid, I remembered something odd that was stated earlier during Turn 10's presentation: ""Forza Motorsport 5 doesn't have AI drivers; you're always racing against real people thanks to Drivatar technology."" While that statement is a bit misleading, I understand the point Turn 10 is trying to make. Here's how it works. Drivatars are ""driving avatars,"" and every Forza 5 player has one. As you drive, the game keeps tabs on your driving habits -- when you pass, how aggressive you are, what racing lines you prefer, and more -- and uploads that data to the cloud as part of your Drivatar training. After a few races, Turn 10 claims that your Drivatar should behave and race roughly like you do and will race on your behalf on other players' games, earning credits for you while you're at work or asleep. The more you play, the closer your Drivatar gets to replicating you; the better driver you are, the better your Drivatar will be. So, you may hop into a race one day and find that you're surrounded by you Xbox One friends' digital doppelgangers. Back in your single-player race, players have access to Drivatar difficulty scalable with eight levels of difficulty. Set this level lower and you'll be matched with lower-skilled Drivatars. Kick it up a few notches to be matched against the best in the world. I asked if there are any behaviors that don't get factored into the Drivatar syncing, and Turn 10 told me that things like wall-bouncing, corner-cutting, or driving backward on the track and crashing head-on into the leader are ignored by the Drivatar system. So it should be impossible to create a virtual griefer to wreak havoc from the cloud -- a Trollatar, if you will. After this year's online issues that plagued the SimCity and Grand Theft Auto V launches, I'm just a smidgen worried about basing Forza 5's AI in the cloud, and I'm hoping that there's some sort of local AI redundancy for those times when connection to the Web isn't possible. We'll have to wait until after the Xbox One's launch to see. Here's hoping that Microsoft has figured these connectivity issues out for the next generation. Back on the roadAfter the race through the Alps, it was off to the ""Top Gear"" test track in Dunsfold, England, for a wacky Top Gear Challenge. In this particular race, the track was littered with dozens of trash cans, suitcases, and other debris, and my Drivatar opponents and I had to plow trough all of this. I got my first taste of the vindictiveness of the Drivatar when I accidentally plowed into a Genesis Coupe on a particularly tight corner, and he reacted by PIT maneuvering me off of the track. What's odd is that in that moment, I found myself thinking about the ""guy"" like I would a real, live player. We traded paint, I talked trash -- to no one -- and felt a genuine sense of rivalry. My red-misted bumping match with the Hyundai resulted in my barely finishing the race in seventh place, with the Miata barely in one piece, but I felt like I earned that place. At the end of each race, the players is awarded experience points (XP) toward raising Driver Level and earning Affinity points for the car's manufacturer. Gaining Driver Levels earns the player bonus virtual currency for buying and upgrading cars; gaining Affinity Levels earns discounts on upgrades for a particular make of vehicle. After a few races, I finished the first series and was popped back into the Autovista mode, which serves as the hub for the game. Here drivers can choose the car that they'll take into the next race, change racing series, purchase new cars, and customize their current stable. In previous Forza games, I found that I spent as much time customizing my virtual garage as I did racing, so I took this opportunity to adjust the Miata's color scheme, ditching the wood grain in favor of the dark gray paint and hot-pink-wheeled design that has become my trademark over the past four generations of Forza. Fellow customizers will be happy to learn that carbon fiber and carbon Kevlar have been added to the solid, metallic, and color-shifting paint options, but I didn't see matte paint as an option, which is a finish that many players begged for in the Forza Motorsport 4 forums. Perhaps I missed it. While I was customizing, I upgraded my Miata's performance to be competitive in a higher class, selected a different League, and hit the road again. My last race was an epic battle at the Spa-Francorchamps against a field of proper sports cars. My upgraded Miata performed admirably, struggling up through the field, passing Corvettes, Vipers, Cadillac CTS-Vs. Paint was traded; egos were bruised; and I snatched victory from an Audi RS5 in the second-to-last turn of the race. As hard as you want or as easy as you likeWatching the crisply rendered race replay, I felt just a little bit bad about knocking the Drivatar difficulty down a notch before starting the race. But as I watched my severely outclassed Mazda Miata with hot pink wheels play David on a track full of Goliaths, I was reminded of the thing that I like the most about the Forza series: it can be both extremely realistic for hard-core racing sim fans, but also extremely accessible for people who just like cars. Players can turn on a racing line overlay that shows where you should accelerate, brake, and apex for the fastest lap, or they can turn it off and figure it out for themselves. The cars can automatically brake when approaching corners so that less skilled players can focus on turning and accelerating (the best parts of driving) without flying off of the track. Antilock braking, traction control, and more can all be adjusted. Vehicle damage can be just cosmetic or can be realistic, adversely affecting the performance when knocked up too badly. There's also the sliding scale of Drivatar difficulty. From one race to the next, you can go from getting a fairly accurate portrayal of the racing experience to beating German supercoupes with a tiny, underpowered Japanese roadster. You can decide how you want to have your fun."
36,https://www.cnet.com/reviews/anki-drive-review/,CNET,2013,10,23,2414.0," Some of you may be too young to remember slot cars, but I do. These tiny electric toys whipped around a small plastic course, powered by small groves in the track's surface. The player could only control the car's speed: too slow and you'd lose; too fast and the car would go flying off course. Slot cars have stepped into the 21st century with the debut of Anki Drive. These tiny electric toys guide themselves around a small plastic course, but are no longer confined to a single grooved lane. They can weave and pass with computer-controlled precision while the player controls the action -- not with a wired trigger, but with an iPhone, an app, and Bluetooth connectivity. As if freeing slot cars from their single-tracked slots and wires weren't enough, Anki has also given them guns and artificial intelligence and instructed them to take you out. What's in the box?Crack open the massive, black and red box (nearly 4 feet long and weighing 9 pounds) to be greeted with the various parts of the Anki Drive starter kit. There are two included cars with charging stations, the Anki Drive track, a 2A USB wall charger, a USB charging cable that splits into three Micro-USB tips, and a tire cleaning pad. There are also a few more parts of the system that you'll need to play, which are not included in the box. You'll need a compatible iOS device (nearly any that supports Bluetooth Low Energy will work) and the Anki Drive app, which is a free download from the iOS App Store. The carsThe Anki Drive ships with two cars measuring 3.25 inches from nose to tail and 1.75 inches wide. The wheelbase is about 1.5 inches. They feel like plastic toys, so don't expect anything too sophisticated here. The two vehicles are cast from identical, futuristic race car molds, only differentiated by their color palettes. The first car, Kourai, is molded of yellow plastic with splashes of black paint here and there to add visual interest. The other, Boson, is made of red plastic that is mostly covered with metallic silver paint and reminds me of the Mazda Furai everytime I glance at it. At the top of each vehicle's canopy is a semi-transparent panel, behind which are LED indicators. When the LED is seen flashing red, green, and blue lights, it's indicating that the car's battery is charging, solid green indicates a full charge, and a pulsing green indicates that the car is ready to be controlled by a player. Pulsing red indicates AI control, so watch out. There are also clear openings at the front and rear of the vehicle that will come into play with the laser tag-esque aspect of the game, which we'll get back to shortly. Flip the cars over to reveal the most interesting details of their construction. To start, the front axle is fixed and its hard plastic front wheels don't steer like the front wheels of the car in your driveway. Instead, the Anki Drive cars steer by torque vectoring their rubber-tired the rear wheels, which appear to be driven by a pair of small electric motors. By spinning, for example, the right rear wheel slightly faster than the left, the cars can make left turns and vice versa. Most of the vehicle's weight, in the form of the rechargeable battery and electronics, appears to be centered over the rear wheels, but a pair of small metal weights in the rear bumper give the Anki Drive cars a decidedly rear-biased weight distribution...sort of like a Porsche 911. If the rubber rear wheels get dirty, players can clean them with the included tire cleaning pad: a tacky, semi-adhesive pad that pulls dirt and dust off of the rubber when rolled over it. You could accomplish a similar result with a strip of tape, but it's nice to see this attention to detail included in the box. The software is also smart enough to know if the tires have gotten dirty and can alert the player via the app to clean them. Just ahead of the rear wheels are the copper contacts that receive power from the charging station, which is a black clamshell case for the vehicle with a semi-transparent cover and micro USB port out back. The cars can be stored in their charging stations to prevent damage when unused. Each car takes about 8 to 10 minutes to charge, according the the included literature, and will run for about 20 minutes before needing to juice up again. The included three-headed micro USB charging cable allows three cars to be recharged, simultaneously. However, because the system supports up to four cars per game, four micro USB tips on that cable would have been nice. Finally, there's the track sensor, which is located between the front wheels on the cars' underbellies. Peering into this square opening, you can see what looks like an LED and a small camera lens. This is how the Anki Drive cars can tell where they are on the track and what to do next. But to figure out how they work, we'll have to take a closer look at the track itself. The trackThe track -- basically an oval course with a slight infield bow on one side -- is printed on a large, vinyl sheet that is unrolled and laid flat on the floor. At 102 inches long by 42 inches wide, it takes up a lot of floor space. However, there's more to the track than just an average vinyl banner. The Anki Drive system's included instructions make it clear that this track is to be treated with care, not stepped on, folded, or cleaned with chemicals. If you look carefully, it becomes clear why. In addition to being coated with a slightly tackier surface that better helps the cars' tires to grip, the track is also covered with special patterned codes visible via infrared. Look closely and you can just barely make the pattern out, like the grooves on a vinyl record. The cars read these codes as they move along to know where they are and where other cars are on the course and to help them maintain their ""lane"" without player intervention. The cars can't be used nor the game played without these markings, so take care to keep the track clean and in good condition. The app and gameplayThe Anki Drive app is required to run the hardware and is available for free in the iOS App Store as of October 23, 2013. We were able to download an early release of the app for testing and to spend time playing against the AI and other local players in our San Francisco office. The after launching the Anki Drive app your iPhone will automatically seek and pair with Anki Drive cars in the area via Bluetooth Low Energy. Up to four cars can be used at a time and up to four drivers can play together, each with their own device. If left to idle for too long between games, the cars will automatically shut themselves off. Players can reactivate them by tapping small grey buttons on the vehicles' undercarriages. After selecting a play mode, players will then be asked which of the available vehicles they would like to command. Any cars not operated by a player can be assigned to the Anki Drive AI (artificial intelligence), so you'll have competitors, even if you choose to play solo. AI difficulty can be adjusted between ""easy,"" ""medium,"" and ""hard"" to match the player's skill level. Gameplay is basically laser tag on wheels. The object of the game is to race around the track, disabling your opponents' vehicles with ""pulse blasts"" fired by tapping the paired iPhone. Thanks to the infrared track codes, the cars can steer themselves around the track, leaving the player to control vehicle speed, lane changes, and weapons systems by swiping, tapping, and tilting their iOS device. Meanwhile, the other human players and AI players will be shooting back. The first to a predetermined score (5, 10, or 15 disabled opponents) wins the game. Anki says that this ""Battle"" mode and a ""Practice"" mode where the AI players don't shoot back are the only gameplay types available, but also states that it will add more game types via app updates over time. For example, our app had a placeholder for a ""Race"" mode that is ""Coming soon…"" Each Anki Drive car has its own strengths, weaknesses, and support items. For example, the included Boson and Kourai cars are designed for balanced gameplay and feature tractor beams to pull other players in for the kill, but the red Rho car (sold separately) has attributes skewed toward defensive play including enhanced armor and shields. At the end of each race, players will gain ""points"" that can be used to upgrade their car's attributes and add virtual weapons. Players can, for example, upgrade their car's virtual armor, make its weapons more deadly, or upgrade the steering and top speed of the car. Interestingly, upgrades to steering and top speed are reflected in the real world, so it will be fun to see the physical car getting faster thanks to these software unlocks. Vehicle upgrades are permanent and tied to the physical car for its lifetime and weapon upgrades are tied to your Anki Drive account. New virtual weapons and upgrades will be released over time. Obviously, the cars have physical limits on how fast they can accelerate or corner, so I expect most of these upgrades to be of the ""virtual armor and weapons"" variety. However, nothing is stopping Anki from releasing more cars that are physically more capable than the current crop -- at an additional cost, of course. I found the game to be very easy to play, but difficult to master. Even on the easiest difficulty, I ended up losing most of the games that I played during testing to the AI (usually, the heavily armored Rho). The cars really whip around the track at surprisingly high speeds, which made tracking my car versus the AI cars a tricky affair. I was grateful for the cars' ability to stay on track, but I was most impressed with how well the AI was able to negotiate passing maneuvers. I was able to force the occasional collision by weaving in front of an AI car at the last possible moment, but you'd be surprised by how well the AI dodged my erratic weaving back and forth across the track -- slowing its speed, calculating, and then quickly passing. Small collisions and spins that left the cars on the course were quickly compensated for by the AI. Even if a car ended up pointed the wrong way, the computer could negotiate a lightning quick U-turn to keep the race going. In the event of a major collision that left a vehicle overturned or off-course, the app would notify me to place the car back on track and give it a little push to get it going again. The vehicles seem to lack any onboard sensors besides the infrared reader pointed down at the track and all of the collision detection happens remotely in the smartphone app. So, while they're remarkably good at dodging other AI and player cars that are linked to the app, they can't tell when non-player obstructions are on the track and will plow happily right into a hand, foot, or housecat on the track, bouncing harmlessly away thanks to their low weight. They're not quite fully autonomous Google cars, but -- for kids' toys --they do a serviceable imitation within the constraints of the game. AccessoriesI mentioned earlier, we received a third car in addition to the two in the starter kit: the red Rho. Additional cars can be purchased for $69 each, allowing more players (real or AI) to play at a time or giving the player access to unique attributes. I counted five variants, including Kourai and Boson, which are included in the Anki Drive starter kit. The starter cars feature balanced attributes, while Rho and its twin Katal are designed for defense, as stated earlier. There's also a limited edition car called Corax available that sacrifices defense for double the offensive capability. Presumably, there will be more variants and special editions to follow. Where the old slot car courses were reconfigurable, Anki Drive so far is limited to just the one oval course. While Anki hasn't explicitly stated this, but I'd frankly be surprised if more courses weren't offered at a later date for an additional cost. I'm not sure how much these tracks would cost. PricingThe Anki Drive certainly attracted more than its fair share of attention around the CNET offices; it seemed that everyone wanted to have a go. It's certainly an impressive bit of tech, but most were more interested in learning how it worked than they were in playing with it. However, once they'd figured it out, the novelty of the Anki Drive wore off after a few minutes. Anki Drive bridges the gap between the advanced, on-screen racing games that I love and play for hours at a time today and the physical, crashable slot cars that I loved as a kid. But in bridging those worlds, it creates one pretty big compromise: a loss of variety. Where a digital racing app can feature dozens of tracks and courses and where my old slot car segments could be almost infinitely reconfigured, Anki Drive has -- so far -- just one track configuration that runs in just one direction. If the Anki Drive system came with more than one track mat, could be run in reverse, or had reconfigurable tiles like my slot car segments, it would have held my attention for longer than it did. That said, I really enjoyed the few hours of enjoyment that I got out of the physicality of Anki Drive racers, but I probably have a deeper interest in games and toys than is appropriate for someone my age. On the other hand, kids, at whom the Anki Drive is targeted, will probably be more than thrilled to play with the Anki Drive this holiday season. The Anki Drive starter kit retails for $199 and includes everything you need to get playing head-to-head with a friend or the AI. Additional cars are $69 each, so you're in for a maximum of $337 for the full four-player experience."
37,https://www.cnet.com/reviews/nhl-14-playstation-3-preview/,CNET,2013,9,9,992.0," I eagerly await EA Sports' NHL game each year not just because I myself am a die-hard ice hockey player and fan, but because the last few entries have provided some of the most realistic hockey simulation I've ever come across. That said, last year's effort seemed to plateau in terms of innovation which sparked a cause for concern. Unfortunately, this year things have officially become stale. To the hard-core NHL loyalist, NHL 14 might feel like a disappointment. New this year are two modes of note: Live the Life and NHL 94 Anniversary Mode. Live the Life is just like Be a Pro, except you make oddball decisions about your player's personal life. For instance, you'll decide how you ""feel"" before games (by selecting the response that closely aligns with your emotions) and make sponsorship and promotional decisions as well. It's mostly nonsensical fluff that you won't want to bother with regardless of how badly you want to feel like an NHLer. Answering trivial questions doesn't cut it. To me, what's most important about an annual sports sim is how well -- and close to reality -- the game performs. With NHL 14, the refinements made are mostly unnoticeable, but what's far worse is how some of the annoyances from years past haven't been corrected. At times the AI feels completely off. Sometimes players stray out of position altogether or won't activate when they're obviously the man who should be chasing down the puck. Player selection isn't always warped to the most logically positioned teammate, which can result in some ultra-frustrating moments of helplessness. Even the soundtrack is tired, for crying out loud. Some of the same tracks are reused in game, like the chanty ""Zombie Nation"" song. It's those kinds of little details that give you a sense of malaise. Checking has also gotten a significant bump in NHL 14, making it much easier to deliver some jarring hits. You can even check someone simply by running into them (as opposed to relying on the right hit stick). As a result it causes the game to be too hit-heavy and in turn triggers retaliation fights far too often. Fighting has gotten a refresh too (thanks to the ""Enforcer Engine,"" but it just doesn't resemble a real-life NHL scrap. Plus, you'd expect enforcers to be the one standing up for their teammates after big hits, but on a number of occasions I've had players who don't usually fight step in instead. On the other hand, I really liked the addition of ""one-touch dekes."" These are a crop of dazzling puck-handling maneuvers that will give the defense a run for their money. While tough to pull off, they're profoundly satisfying when used effectively. Without a doubt, the addition that will have NHL veterans salivating is the new Anniversary Mode, which attempts to emulate the action of NHL '94, widely regarded as the greatest hockey video game of its time. Unfortunately the novelty wears off way too quick because there's nothing more than the classic blue ice, old-school, star-shaped player markers, nostalgic 8-bit organ music, and all of the speed and checking sliders in the game set to max. The pace and action are all over the top, but it's really just a coat of paint slapped on top of the current engine. There's really no effort to recapture the actual gameplay of NHL '94. What's worse? NHL '94 mode can't be played online. That's pretty lame. The problem I have with NHL 14's gameplay is that it's become a predictable production. Players of the series will expect and foresee -- with great success -- a lot of the AI's behavior and sniff out the hot goal-scoring spots rather quickly. I'm not great at the game by any means, yet I've managed to score eight goals on two separate occasions. All in all it feels as though the game has exhausted its life cycle on the current generation of systems. It should then come as no surprise to learn that it will not be available for next-gen consoles when they launch this November. The Ignite Engine that EA teased at E3 2013 will have to wait for its implementation in a hockey game until NHL 15. I also maintain that the puck physics need a reworking, especially the way they interact with the net. I'm also all but ready to mute the commentary within the game because Gary Thorne's voice track is officially collecting dust. To add insult to injury, a lot of the same illogical commentary has remained in the game. The words ""it feels like the next goal might decide this one"" should never be muttered six minutes into the first period. There's rarely a time when an empty net goal should generate the overenthusiastic ""what a shot, scores!"" The little color commentary stories that Bill Clement chimes in with are going on three years old and that's just unacceptable. NHL 14 isn't a bad experience by itself. If you haven't played an NHL game in a while, this entry will likely feel like the best hockey sim you've played in a while. But for the franchise loyalist, the type of fan the NHL typically attracts, they'll feel disserviced with this latest installment. As a diehard Devils fan it pains me to say this, but perhaps it's poetically just that Devils' goalie Martin Brodeur graces the cover of NHL 14 this year. At 41, he's a legendary goalie who's dominated the position for close to two decades, with a track record similar to the accolades the NHL series has embraced. But like most great things, Marty will soon hang up his skates and retire. He and his team will move on. Perhaps it's time for the series to do the same in preparation for NHL 15. Close the book on a stellar career and reinvent the franchise properly for next-generation systems. Deliver the experience its loyal fan base expects and deserves."
38,https://www.cnet.com/reviews/carrot-alarm-ios-review/,CNET,2014,3,7,576.0," Carrot Alarm is a hilarious alarm clock for your iPhone that sometimes sings to you when it wakes you up, gets mad at you when you sleep in, and forces you to complete silly minigames in order to turn it off. Developed by the same people who made Carrot To-Do List (the to-do list with an attitude), Carrot Alarm is a fun addition to the otherwise mundane alarm clock app category. Like any alarm clock, you first need to set the alarm for when you want to get up. Carrot Alarm sports an elegant minimalist interface requiring only that you swipe your finger vertically to adjust your wake-up time. Once you're time is set, simply touch and drag Carrot (the onscreen representation of a camera eye) to the right into the on position before you go to sleep. When it comes time to wake up, that's where the fun begins. Carrot will play silly original music for you, sing to you (the AI has a female voice), and the volume will slowly get louder and louder. To turn off the alarm, you need to touch and drag what Carrot calls her ""ocular sensor"" upward to the off position, then perform a number of silly ""chores"" with gestures. You'll get directives such as ""Clean out the lab monkey cages (shake device),"" or ""pinch to grant sentience to a toaster (pinch the ocular sensor)."" In my testing each alarm had three or four directives before it would turn off. One thing you don't want to do is make Carrot mad (or maybe you do) by dragging downward to snooze. You can control how many minutes it will snooze for in the settings. Right after I hit snooze, Carrot said, ""You're going to have a bad time,"" but kept quiet during the snooze time. When the alarm went off again, Carrot's robotic voice repeated, ""You're ugly and you smell"" over and over while I tried to complete the chores to stop the alarm from going off (and to stop my coworkers from laughing at me). Carrot Alarm rewards you when you wake up on time with points that make you level up like in a role-playing game. At the completion of each level you get small and silly rewards like new songs, new chores to complete, added snooze time, strobelike effects (using your iPhone flash), and others. While none of the rewards have any real world uses, the new alarm sounds and songs might at least make you laugh at the beginning of your day. I only have one gripe with this otherwise creatively made alarm clock, and it has to do with setting the alarm. The vertical swipe changes the alarm in 15-minute increments, and I thought it was pretty limiting. But after a couple of hours with the app, I accidentally realized a tap either above or below, lets you adjust in 5-minute increments. I'm glad I was able to figure it out, but to me that says it's not very intuitive. Still, Carrot Alarm is a unique alarm clock app that made me laugh with its cheesy humor and seems like it would be a fun way to get up in the morning. Though the sense of humor probably won't appeal to everyone, if you're tired of hearing the same old alarm every morning, this app will give you something new every day and might make getting up a little easier. Just don't make Carrot mad."
39,https://www.cnet.com/reviews/riptide-gp2-review/,CNET,2014,3,7,812.0," Riptide GP2 (iOS|Android) brings the best jet-ski racing game on smartphones back for another run, but in this sequel you get new tracks, dynamic wave physics, online multiplayer, and more cool features that make it even better. Graphics and gameplay Riptide GP2 gets just about everything right. The worlds and tracks are amazingly detailed, with different themes that have you racing on a suspended track high in the air in some rounds, and others where you race through frozen worlds of ice and waterfalls. There are eight worlds in all, with several races in each where you strive to get a three-star rating by beating the AI racers or racing against the clock. There are a number of different race types as you play through Career Mode, with standard races against AI opponents; Hot Lap, which has you compete for a top time; Elimination, where the player in last place is eliminated at 10-second intervals; and Freestyle events, where your job is to pull off the most and best tricks possible to get the high score. You also can participate in online multiplayer, where you'll be matched up with another person to race in real time, but there are no multiplayer group races. As you race around tracks, you'll find ramps and large waves that let you perform tricks to build your boost. You start with a variety of basic tricks in the beginning. But as you earn money by winning races, you can buy more-complex tricks from fancy handstands to absurd end-over-end spinning moves that would be impossible in the real world. There are 25 tricks you can unlock in all. The harder the trick, the more boost power you get, but if you land the trick wrong and wipe out, you'll lose time and probably lose the race. What truly makes the game a joy to play are the realistic wave physics. Along with the smooth, flowing feeling of carving through turns, you'll also need to navigate through the wakes of your opponents. Waves form dynamically as you race, so you'll be battling the surface of the track as well as the game's many hairpin turns and ramps. Purchases and upgrades Between races you can upgrade and customize your hydro-jets (jet skis) to give you an edge. Riptide lets you use your race winnings to buy upgrades for acceleration, top speed, handling, and improving your boost. Each upgrade to a single skill costs more than the one before, so you'll need to keep racing to earn more cash. Like many games these days, you can buy in-game cash with real money, but I found you can definitely earn it on your own with a little patience. As you progress, you'll also be able to buy new, cool-looking hydro-jets with more power and better maneuverability. The hydro-jets are expensive, so you'll need to balance the need to upgrade an old hydro-jet with saving money for new ones. Buying new hydro-jets is where the temptation to use real money for in-game cash is difficult to ignore. But with a racing game of this caliber, it won't be that hard to continue racing to slowly earn the money. Boost is the key You can choose from a few different control systems in the settings, but it's important to note that none of them lets you control the throttle. This isn't necessarily a bad thing, because you need to concentrate more on making smooth turns and pulling off amazing tricks in order to build boost power. While throttle control would be nice, it's the boost ability that gives you an extra burst of power for higher jumps and careening past your opponents. I stuck with the default control system that lets you tilt to steer and use gestures for tricks, but there are gamepad options for steering if that's your preference. So, what's the drawback? Like any jet-ski racer, I can't help but compare Riptide GP 2 to the ancient, yet amazing, Wave Race 64. While the graphics for Riptide are obviously better than what we had on the Nintendo 64, for raw gameplay I think the comparison is valid. The different game types and outlandish tricks keep the gameplay interesting in Riptide, but one feature in Wave Race 64 could have added to the fun in this game: forward and back tilt controls. Wave Race let you dive into the water and shoot back out, giving you the opportunity to create air for a trick, while you'll only be able to perform tricks off of ramps and occasional large waves in Riptide. It's not a huge drawback, but it's a feature I continue to miss in water-racing games to this day. Overall, Riptide GP2 is an excellent water racer that looks as good as it plays. With online multiplayer, upgrades, and several tracks on different worlds, it has a ton of replay value."
40,https://www.cnet.com/culture/artificial-intelligence-system-has-iq-of-4-year-old/,CNET,2013,7,17,255.0," We're still very far away from Data on ""Star Trek,"" an artificial intelligence creation with self-awareness and a human-like mind. But artificial intelligence designers are taking baby steps. More accurately, they're taking 4-year-old steps. A study by researchers at the University of Illinois at Chicago took one of today's top artificial intelligence systems, ConceptNet 4, and gave it an IQ test. Its average IQ worked out to be about that of a 4-year-old human, the researchers found. ConceptNet 4 was developed by the Massachusetts Institute of Technology. The UIC team used the verbal parts of the  Wechsler Preschool and Primary Scale of Intelligence Test to put it through its paces. While the overall IQ score was pretty impressive, the AI system had trouble with consistency. ""If a child had scores that varied this much, it might be a symptom that something was wrong,"" said Robert Sloan, head of computer science at UIC. ConceptNet 4 pretty much rocked the vocabulary test and the ability to recognize similarities. What it wasn't so hot on was the type of question that human toddlers really love: ""Why?"" It seems AI still isn't up to snuff on what us meat machines think of as common sense. ""We're still very far from programs with commonsense-AI that can answer comprehension questions with the skill of a child of 8,"" Sloan said. The researchers hope the study will help developers hone future AI systems. Chalk one up for humans in the battle versus machines. At least our preteens are still smarter than them."
41,https://www.cnet.com/culture/smile-we-know-how-fast-your-heart-is-beating/,CNET,2013,6,20,332.0," Have you ever noticed your head rocking back and forth very slightly when you sit still? That's the effect of blood rushing up to feed your brain. Now Massachusetts Institute of Technology researchers can accurately measure that phenomenon on regular video and figure out how fast someone's heart is beating. They say it might help detect cardiac disease. The scientists at MIT's Computer Science and Artificial Intelligence Laboratory believe the algorithm could be used for video monitoring of patients with sensitive skin, such as newborns or elderly people. According to the study by Guha Balakrishnan and collaborators (PDF), to be presented at an IEEE Computer Vision and Pattern Recognition conference, the video heart rate method was evaluated on a group of 18 men and women with different skin tones. The results were ""nearly identical"" to an electrocardiogram. If the small motions of someone's head are exaggerated, he or she starts looking like a wobbling bobblehead. But that motion is hard to see with the naked eye. It's caused by blood flowing from the heart through the carotid arteries, which supply oxygenated blood to the head and neck. Using common face-recognition and computer-vision techniques, the method looks at a collection of 500 to 1,000 moving points on a subject's face, and then selects a signal with the clearest dominant frequency. The results provided not only accurate heart rate information, but also the time intervals between beats, which could be used to detect a risk of cardiac events. The method was also effective at picking up a pulse using video of the back of someone's head, a baby, extremely blurry footage, and someone wearing a mask (in this case, the Guy Fawkes mask popularized by Anonymous). ""I think this should be viewed as proof of concept,"" collaborator John Guttag, an MIT professor of electrical engineering, was quoted as saying in a release. ""It opens up a lot of potential flexibility."" Check out the video below on the research, and read more about it here."
42,https://www.cnet.com/tech/mobile/calendar-app-tempo-raises-10-million-in-first-round-funding/,CNET,2013,6,20,303.0," Tempo, the calendar app that wants to be a smarter digital assistant, raised $10 million dollars in its first round of funding, its creator Tempo AI announced Thursday. The money is going toward building better artificial intelligence for Tempo, which, Tempo AI CEO Raj Singh envisions, will learn to anticipate your every move, no matter who you are. ""There's many kinds of calendar users. You have to appeal to the barista as well as the enterprise sales executive. And when we designed Tempo, we designed it in mind for both,"" Singh said. ""The key thing is we're super excited, there's a lot of discussion about what comes after Siri."" This also means utilizing the hardware to help with planning, like monitoring your device's accelerometer to know that you are on a plane. ""If get a better understanding of how you travel throughout the day, that may also help improve how we think about flights for you or how we think about recommendations,"" Singh said. Tempo created a bit of buzz when it launched in February because its creators didn't realize how popular the app would be. A virtual line of users formed to get a chance to try the smart calendar. Since then, Tempo AI has met demand and is now focused on expanding its reach globally and making the app smarter. The company said it's processed about 1 billion emails and documents through the app's scheduled meetings features. The company's first round of funding, from Relay Ventures and Sierra Ventures, also comes with new board members, including Norman Winarsky, the co-founder of Siri. Tempo AI's board will now also include Relay Venture's Kevin Talbot, who has worked with calendar technology companies before, and Sierra Venture's Ben Yu, who has experience in enterprise applications, a space that interests Tempo AI, according to Singh."
43,https://www.cnet.com/tech/tech-industry/anki-blessed-by-apple-takes-ai-and-robotics-to-consumers/,CNET,2013,6,11,717.0," When Apple turns over part of its oh-so-important Worldwide Developers Conference keynote address to an unknown startup, you can be sure Tim Cook and Co. think they're dealing with some very cool technology. That was very much the case with Anki, which was handpicked for a coveted slot as the poster child for what unknown developers can do with iOS. During its time onstage, Anki showed off what at first appears to be  a simple toy car racing game, but what in reality might be the most advanced intersection of consumer-grade artificial intelligence and robotics ever. In an interview in San Francisco a few hours after the keynote address, Anki co-founder and CEO Boris Sofman told CNET that Anki -- the company's name comes from the Japanese word for learning by heart -- is in the business of bringing AI and robotics, a combination that has long been the exclusive province of the defense industry, to consumers. Apple clearly loved what Anki is doing with iOS: Users who buy the approximately $200 product starting this fall will use their iPhones or iPod Touches to control a little physical race car stuffed full of things like optical sensors, wireless chips, motors, and microcontrollers. Sofman said that he thinks of the technology much like a video game come to life, with the cars taking on the characteristics of video game characters, except in the physical world. At the core of AnkiDrive -- which came out of the founders' desire to bring to the consumer level what they learned working on $600,000 autonomous vehicles at Carnegie-Mellon -- is a system built to process real-time positioning, reason, and execution by ""analyzing hundreds or even thousands of actions across four dimensions,"" Sofman said. In the racing game, that means that players can control a car, and race it against another, and each vehicle roars around a track taking into consideration thousands of scenarios, all while pursuing a specific mission. Anki isn't saying yet what the missions will be -- beyond winning a race -- but during its onstage demo at the WWDC keynote event, Sofman showed off a situation where three people controlled individual cars, while a fourth, autonomous, and faster and smarter, weaved through and around them. ""It behaved how you would expect an intelligent driver"" would drive, Sofman recalled, ""but then we told the three to block the fourth."" And that's when the fun began. As the thousands in the room for the keynote address, and hundreds of thousands who watched it online experienced it, the fourth car zigged and zagged its way through the other three, and then used a ""weapon"" to ""shoot"" the others, which, when hit, would fly off the track, just as they would if they'd been hit by a real projectile. None of this was pre-programmed. Sofman said that Anki's toy cars race at the equivalent (if they were full-size cars) of 250 miles an hour, and move with a precision of a 10th of an inch. That's possible, he said, because of components built in that check their driving logic 500 times a second, and convey positioning information to the wheels every two milliseconds. For now, Anki isn't showing anything but its racing game, and won't say how many cars users will get for their $200. But Sofman made it clear that what Anki is showing today is merely the beginning of a new consumer AI/robotics industry, something that was never possible before due to the prohibitive price of the necessary components. But because Anki's hardware incorporates nothing but inexpensive commodity parts, consumers will finally be able to get their hands on this kind of technology, Sofman said. What Anki is bringing to the table is the software, five years in the making, that controls it. Thanks to its relationship with Apple and lead A-round investor (and board member) Mark Andreessen -- Anki has $50 million in the bank with which to develop AnkiDrive and whatever its next products are. And with Apple's blessing, it will sell its cars in Apple stores around the world and online, as well as on its own Web site. Are big things ahead for Anki? Andreessen thinks so. On his blog, the Andreessen Horowitz partner said that the company is ""the best robotics startup I have ever seen."""
44,https://www.cnet.com/reviews/panasonic-tc-dt60-review/,CNET,2013,6,7,2001.0," Panasonic has been ""kicking goals"" for years with its plasma range but has struggled to produce competitive LED LCDs. While its performance is seemingly a bit better than the previous model, which put up one of the worst pictures of last year, the new DT60 still flounders against its 2013 peers. The DT60 offers a striking design, probably one of the company's best ever, and nifty features such as voice search. Its picture quality is just mediocre though, for while color is better than last year, black levels are similarly poor -- and I simply expect much more at this price. The DT60 is a case of ""better, but not nearly good enough,"" and until proven otherwise, we'll continue to strongly recommend Panasonic's plasmas and just as strongly tell you to avoid its LED LCDs. Series information: I performed a hands-on evaluation of the 55-inch TC-P55DT60, but this review also applies to the 60-inch size in the series. Both sizes have identical specs, and according to the manufacturer should provide very similar picture quality. DesignThe DT60 is a fine-looking television, with a very thin chrome bezel and the same kind of floating glass/plastic bottom edge that first appeared on Sony's TVs. It comes comes attached to a swiveling silver stand via the now-familiar V-shaped plinth. While last year's LG G2, for example, shared a similar color scheme, it didn't look anywhere near as classy as this. The TV ships with two silver remotes -- one standard-issue Panasonic and a touch-pad remote. The standard remote is covered with a plethora of buttons but is easy to use, if only the Menu button was a little more obvious. The touch-pad remote is similar to that which ships with the VT60 and incorporates an onboard microphone, but lacks some of the buttons you'll need for the new Home interface such as apps and the colored buttons. Voice interaction is probably the ""biggest"" feature of the DT60, with a microphone integrated into the touch-pad remote, but this is all very tied to the Web-browser (see below). The TV is a passive 3D model, and thus Panasonic is able to include four pairs of glasses in the box -- they're cheap to make compared to active glasses. If you ever kept the 3D glasses from the cinema, you can use those as well. Like many LED LCD TV makers, Panasonic is inflating the number associated with its refresh-rate specification; in the DT60's case the claim reads ""1200 backlight scanning."" At least the company is honest enough to include the ""120Hz"" spec too, which is the only one that really matters. If you have a smartphone, Panasonic's improved Viera Remote app enables some functions like basic control if you misplace the remote and ""swipe and share"" to display photos on the big screen. It also allows direct access to relatively advanced calibration functions, although I didn't test this feature. Smart TV: Panasonic customizable ""welcome screens"" lack the pizzazz of competing Smart TV systems and with their analog clocks and calendars look more like a '90s version of Microsoft Office than a modern entertainment suite. The multiple ""pages"" show the currently playing input in an inset window along with the grid of apps. You can place any app anywhere you want on the grid, a welcome change from interfaces from makers like Samsung that offer only partial customization. Panasonic ups the custom ante further by offering three different templates for new pages you can create, custom backgrounds (including your own pictures), and the ability to name pages -- for example, each member of a particularly tech-savvy family could set up his or her own page. The series of new home screens are an interesting idea, but I'm not too certain people will customize more than one screen -- even large households. You can easily toggle between each one if you like, though. The app selection is superb and very similar to last year's with Netflix, Amazon Instant, and Hulu Plus all present. If you want to dive into the non-pre-installed selection in the Viera Marketplace, there's also Vudu, Pandora, TuneIn Radio, Rhapsody, and full episodes and photos from a Panasonic-sponsored series on National Geographic TV about World Heritage sites. There are also a smattering of kids apps and a few forgettable games. With the touch-pad remote comes voice search, but it's much more limited than Samsung's system -- it doesn't integrate with your cable box or any many Smart applications, but mostly lets you search the Web. Yes, the TV includes a Web browser, but with better browsers on mobile devices, who really needs one on their TV? Navigation is a little better with the touch pad, but sometimes pages don't render properly, and using voice search results in a two-step process that is frustrating at best -- especially when it mishears your search terms! While you can also browse connected USB disks or networked servers, it's probably something most people will use only once. Picture settings:The DT60 offers a full complement of picture controls, ranging from a 10-point grayscale to configurable gamma settings to two dedicated ISF (Imaging Science Foundation) modes. There are two potential problems with activating Game mode. First it's hard to find, lurking in the Advanced picture menu, and secondly, once it is enabled it is activated in all of the picture presets. While the last part is great if you want minimal picture processing, it's bad if you forget to turn it off and want to enjoy the improved picture quality other modes offer. I'd like to see an onscreen reminder here, like Sony used to have, telling you game mode was activated. Connectivity:The TV includes a rather stingy three HDMI ports, which doesn't even include the now-fashionable MHL port. There are a lot of USB ports though, three to be exact, and you can connect USB disks, Skype cameras, or even keyboards. Other connections include a composite/component input, digital audio and Ethernet. Like many Panasonic TVs, you also get an SD card slot, although there's no PC input. Click the image at the right to see the picture settings used in the review and to read more about how this TV’s picture controls worked during calibration. Picture qualityBlack level is the single most important picture component and the one we give the most credence to when assigning our picture quality score. While the Panasonic DT60 does reasonably well in other aspects -- color, 3D, bright lighting -- it's the light black levels and poor uniformity that bring the TV's score way down. In a theater environment, the TV lacks the punchiness of the other televisions on test, and its decent color doesn't make up the difference. Black level:Of our six-strong lineup the DT60 had the poorest black levels of the group, evincing gray-blue shadows in dark scenes. At the start of Chapter 12 in ""Harry Potter and the Deathly Hallows Part II,"" the TV wasn't able to make any sense of the small army on the hilltop scene -- it looked like an goth cupcake with skull sprinkles...shot through a wall. As the camera spun around the group the detail improved and the hazy skulls became wizards, but the too-light blacks meant impact was quashed. On every other TV in the lineup, from the W802 to the ST60, you could tell from the opening moments what the scene was about, but with the Panasonic you just couldn't. Switching to a less demanding scene, the unique New York skyline of ""Watchmen"" Chapter 3 (12:24), didn't help. With the DT60, I was greeted with the least dynamic picture deadened by crushed clouds and a very flat, cardboard cutout look to the apartment buildings in the foreground. While the Sony placed second-to-last, there was still a significant jump in the performance over the DT60, with the W802 able to provide some light and shade in even the most difficult scenes. The next worst was the Samsung E8000 which had overly green shadows and light blacks, but it still trounced the DT60. Color accuracy:While last year's DT50 had a problem with diluted colors, the DT60's palette is much keener this time around. From the opening shots of ""The Tree of Life"" Chapter 5, I could see that the Panasonic was capable of communicating natural colors -- from the cyan of the mother's dress to the lush green grass, the DT60 had notably better color than the similarly priced Sony W802. While colors across the group were fairly consistent, as you'd expect from a set of calibrated units, and the DT60 sat in about the middle in terms of saturation. The W900 and ST60 were superior but the Sony is much more expensive due to its proprietary quantum dot technology and the plasma's color is likewise superb. The only ""issue"" I saw with the DT60's color was that brown could be a little more red than on the other TVs, but not a big deal. Video processing:The Panasonic features a ""hex-core processor"", but I'm unsure what effect this has on image processing as it was only just passable. The TV passed the oscillating test pattern in our synthetic tests, for example, but it displayed some moire in the subsequent pan of the sports stands. On the 24p test of the air carrier fly-by, the TV was able to correctly reproduce the cadence of film. In our gaming test, the TV was able to achieve a lag time of 34.37ms in game which means it achieves a ""good"" rating. This puts it on a par with the much cheaper Panasonic S60 though. Uniformity:If you choose not to use the AI dimming mode, then uniformity is a problem with this TV, with lighter corners and yellow spotlights appearing over the bottom and top of the screen. Without frame-dimming, the set's uniformity problems looked identical to the Sony W802A's, with yellow spotlights along the top and light leakage out of the corners. If you do use AI dimming -- and you should as it gives you a much better picture -- then the yellow spotlights disappear, but the corners are still discolored. The DT60 does have very good off-axis performance. There is very little tailing off of image quality until you get to an extreme angle, and this lack of sweet spot means it's quite usable in a large living space. Bright lighting:The DT60 served up a mostly good performance in the light with softer reflections than some of the other TVs in the lineup, but some of the blacks were still blue-tinted. For whatever reason, there were a couple of obvious ""projector iris-like"" effects under lights as the frame dimming activated, though. When compared to the other TVs, the DT60 dimmed down or up a lot more obviously. Sound quality:The sound quality of the DT60 was actually acceptable with an all-rounder performance from the two front speakers and dedicated 75mm woofer. While dialogue could sound a little distant most of the sound spectrum could be heard: from the boom of an exploding rocket to the smashing of glass. Music was only just OK, with a distinct separation between the bass and the vocal line on ""Red Right Hand"" and only the faintest distortion of the bass itself -- some of these speakers really shake themselves apart on this song! 3D:By using a passive system, the TV performed similarly to the Sony W802 in 3D with no crosstalk and a solidity to motion that the active TVs in the lineup lacked. The Panasonic performed slightly better though as the default depth for the W802 was a little too strong. Furthermore, the Sony W900 has a strange anti-ghosting system that can make some objects look see-through and the Panasonic ST60 is unable to track objects very well at all.  Color and shadow detail in the default 3D modes was almost identical on both TVs. The downside to passive technology is that it divides the resolution in two, and the inevitable interlacing could be distracting to some viewers used to active HD."
45,https://www.cnet.com/tech/mobile/siri-like-calendar-app-tempo-eyes-other-platforms/,CNET,2013,5,29,590.0," Tempo, the calendar app that uses Siri-like artificial intelligence to schedule meetings and dial in on conference calls, is racking up user data to decide where it will land next. The app, created by Tempo AI, highlighted its popular conference call features on Wednesday. While the company -- which comes from SRI International, the same place that created Apple's Siri -- won't share its user numbers, it revealed that people have made 1 million one-tap conference calls with the app since its launch in February. This feature lets users dial in on a conference call with a tap of a button, leaving Tempo to automatically dial the telephone number and call-in codes. While it may sound simple, the app is harnessing the power of artificial intelligence, allowing it to discern the numbers and sequences required to access the conference. This ranges from entering one access code, to scenarios where you also have to pick from multiple languages or country codes. Tempo AI CEO Raj Singh said the app's performance gets better with time because it learns from each experience, picking up the variations from tens of thousands of different patterns in the data. ""It's not a perfect system -- it is still continuing to learn, but we've worked so many patterns now that it's better than it was before,"" he said. The company has verified that Tempo can support at least 20 conference call services including GotoMeeting, WebEx, UberConference, join.me, Fuze, and FreeConference. In addition to dialing conference calls, the app's recent updates let users invite attendees to events, share events with contacts, and set Gmail as the default e-mail client in Tempo. It also can suggest relevant guests based on previous meetings. The demand for Tempo rocketed when the app launched earlier this year, leaving many on a waiting list. Now, the list is gone and Tempo is preparing to expand. Singh said the company is doing well in Canada, where it launched in April, and it plans to get to ""as many places as we can and as soon as we can, mostly this year."" He acknowledged that Tempo users are limited by only having the app on smartphones (Tempo is available for iOS, but it's not optimized for the iPad), but the company is still trying to figure out if it would be a good fit on other devices. While smartphones are the go-to device for real-time scheduling and meeting features, tablets are more for a ""lean back"" experience, or, in some cases, can mimic a desktop experience. ""We want to make it pervasive,"" Singh said. ""On your desktop, your calendar and e-mail are conjoined. It's not clear whether the best option is to integrate into those experiences or offer a different kind of experience with less focus on scheduling, or maybe something completely different."" Tempo is also considering other territories, like wearable tech or smart TVs and cars. Smart cars are of particular interest, given the amount of people who like to take conference calls in their vehicles. ""Automotive is a place where there's a high value of connecting your calendar to the car dash -- 'dial me in,' or 'I'm late; what's the drive time look like,'"" Singh said. But this is still just a part of the discussion at this point, not a reality. Above all, Singh said, Tempo wants to make sure that whatever direction it takes, it's a good fit for users. ""You can port to whatever device you want, but you want it to be a better user experience,"" Singh said."
46,https://www.cnet.com/tech/services-and-software/google-quantum-computer-lab-to-study-artificial-intelligence/,CNET,2013,5,16,306.0," Google is opening a new research lab to see if a quantum computer can solve problems too taxing for traditional computers. Hosted by NASA's Ames Research Center, the new Quantum Artificial Intelligence Lab will be home to a quantum computer made by D-Wave Systems. Operated by the Universities Space Research Association, the supercomputer will be available to researchers around the world to work on their own projects. The goal, as stated in a Google blog posted today, is ""to study how quantum computing might advance machine learning."" Traditional computers are limited, as they think in terms of ones and zeros and therefore aren't adept at solving real-world creative problems. In contrast, quantum computers use a single chip to store hundreds or elements known as quantum bits or ""qubits."" D-Wave's supercomputer houses 512 qubits on a single chip, according to The Wall Street Journal. Tapping into the often odd world of quantum mechanics, a lone qubit can be a one, a zero, or both at the same time. As such, quantum computers can perform calculations and store information on a level much greater than can conventional computers. ""Machine learning is all about building better models of the world to make more accurate predictions,"" Hartmut Neven, director of engineering for Google, said in Thursday's blog. ""If we want to cure diseases, we need better models of how they develop. If we want to create effective environmental policies, we need better models of what's happening to our climate. And if we want to build a more useful search engine, we need to better understand spoken questions and what's on the Web so you get the best answer."" Installation of the lab has already kicked off at the Ames Research Center, according to D-Wave, which expects its supercomputer to be ready for researchers sometime in the third quarter of 2013."
47,https://www.cnet.com/tech/gaming/cryptography-scientists-win-2012-turing-award/,CNET,2013,3,13,223.0," Two cryptography scientists from the Massachusetts Institute of Technology have won the 2012 Turing Award for pioneering ways to make online transactions secure, the Association for Computing Machinery announced today. Scientists Shafi Goldwasser and Silvio Micali were recognized for laying the ""foundations of modern  theoretical cryptography,"" which lead to the development of technology that is now standard in security, like encryption and digital signatures, according to ACM. The prestigious award, named after British mathematician Alan M. Turing, comes with a $250,000 cash prize provided by Intel and Google. Turing is known as a pioneer of modern computing and was a code breaker during World War II. This year's recipients are both principal investigators at MIT's Computer Science and Artificial Intelligence Lab in addition to being professors. Micali is the Ford professor of engineering at MIT, while Goldwasser is the RSA professor of electrical engineering and computer science at MIT as well as a professor of computer science and applied mathematics at the Weizmann Institute of Science in Israel. The two started working together in 1980, when they were grad students at the University of California at Berkeley. According to MIT, it all started with a poker game: Last year, the  award went to professor Judea Pearl of the University of California, Los Angeles for his work in extending the understanding of artificial intelligence."
48,https://www.cnet.com/culture/liveson-twitter-ghost-keeps-tweeting-when-youre-dead/,CNET,2013,2,20,205.0," Death is never popular, even in social media: the poor guy behind @death on Twitter has zero followers. You might think your online fans will lose interest when you kick the bucket, but an upcoming app says it will let you keep tweeting from beyond the grave. LivesOn will host Twitter accounts that continue to post updates when users shed this mortal coil. Developers claim the app's artificial-intelligence engine will analyze your Twitter feed, learn your likes and syntax, and then post tweets in a similar vein when you're gone. You'll become an AI construct, a proverbial ghost in the machine. The app will launch in March, according to Guardian News. People who sign up will be asked to appoint an executor who will have control of the account. Similar postmortem Twitter apps, such as DeadSocial, have only used prepared tweets, not updates created by an AI. ""It offends some, and delights others,"" Dave Bedwood, a partner at the ad agency behind LivesOn, was quoted as saying in the Guardian report. ""Imagine if people started to see it as a legitimate but small way to live on. Cryogenics costs a fortune; this is free and I'd bet it will work better than a frozen head."""
49,https://www.cnet.com/tech/mobile/official-stratego-game-debuts-for-ipad/,CNET,2013,1,30,428.0," Remember Stratego? It's the classic capture-the-flag boardgame. And it's now available for iPad. Why just iPad? I'm not entirely sure. Granted, the board would be cramped on an iPhone or iPod Touch screen, but I've seen plenty of other games work around that. (Monopoly, anyone?) That curiosity aside, Stratego for iPad does a fine job recreating the original while adding some welcome digital touches. For starters, you can create and save multiple board setups (i.e., troop deployments), a huge time-saver and a great way to test how various deployments work against your opponents. And speaking of opponents, the app lets you play against a virtually unlimited supply of other humans, be they on Facebook, the Web, or another iPad. Yep, the game supports cross-platform play, so you can tackle Facebook friends just as easily as you would your fellow iPad owners. Of course, Web and Facebook strategists (Stratego-ists?) get to play for free. On the iPad it'll cost you $6.99 for the privilege. That strikes me as a little steep, though it's a good bet it'll be cheaper down the road. (Use an app like Appsfire Deals to track its price.) What's more, you must buy ""battle coins"" if you want to unlock new avatars, battlefields, and other game content. That would be fine if this were a freemium title, but here it seems a little greedy. The iPad version requires you to create an account or sign in via Facebook (sorry -- no GameCenter support), even if you want single-player action against the computer. That's insanely annoying, but at least there's a workaround: turn off Wi-Fi (and/or 3G if you have it) before launching the game. If it doesn't detect an Internet connection, it'll let you play offline. Like I said: annoying. I also found Stratego to be pretty buggy, alternately crashing, producing error messages, or having trouble signing into Facebook. Plus, the AI is what I'd call overly aggressive, as it never forgets the location of your pieces (once revealed) the way a human would. I won't say beating the AI is impossible, but at the very least the game should offer a choice of skill levels. The good news is that when you're playing against another person, Stratego is just as much fun as you remember. (If you remember it being aggravating, well, it's just as aggravating as you remember. ""Who puts a bomb there?!!"") I'd consider waiting for the price to drop a few bucks and the bugs to get cleaned up. But if you can't wait, well, I'll see you on the battlefield."
50,https://www.cnet.com/tech/services-and-software/googles-ray-kurzweil-hire-could-yield-some-good-returns/,CNET,2012,12,15,519.0," Google has brought on futurist and artificial-intelligence expert Ray Kurzweil as director of engineering, and there could be some real returns on the company's high-profile hire. In a statement, Kurzweil confirmed that he'd be joining Google. He noted that his interest in reading-technology, artificial intelligence, self-driving cars, and other such ""Jetsons""-like things line up nicely with Google's efforts. He said: Google has demonstrated self-driving cars, and people are indeed asking questions of their Android phones. It's easy to shrug our collective shoulders as if these technologies have always been around, but we're really on a remarkable trajectory of quickening innovation, and Google is at the forefront of much of this development. Will Kurzweil be more than just a figurehead? Probably. Google will get some returns on its investment in Kurzweil for sure. Here are some of the positive side effects from Google's latest hire. Talent and recruiting engineers. There aren't enough engineers to keep tech giants like Google, Microsoft, Apple, and others happy. Saying you report to a futurist -- even indirectly -- will have credibility beyond stock options for most engineers. Data centers, networks, and algorithms need machine learning. Kurzweil's ideas get noticed because he's ahead of the curve and he focuses on simple practical systems. However, machine learning has implications for the network, Google's algorithms, and the data center. Look no further than IBM and Watson for examples of how machine learning can have broad implications. Should Kurzweil boost machine learning, Google is likely to be able to draw a straight line between its infrastructure and its high-profile hire. Language processing has big mobile implications. Kurzweil's know-how when it comes to reading and talking machines could easily make its way into Android, which is already becoming very helpful on many fronts. Google's buzz-o-meter. Kurzweil's work can easily be applied to the company's efforts on self-driving cars and its high-tech glasses, both of which could become popular at some point. Kurzweil gives Google some science-fiction-becomes-reality cred. The buzz is hard to measure, but it's certainly not a bad perk for the company. In his book, ""The Singularity Is Near: When Humans Transcend Biology,""  Kurzweil offered his view of how science fiction becomes reality: In this new world, there will be no clear distinction between human and machine, real reality and virtual reality. We will be able to assume different bodies and take on a range of personae at will. In practical terms, human aging and illness will be reversed; pollution will be stopped; world hunger and poverty will be solved. Nanotechnology will make it possible to create virtually any physical product using inexpensive information processes and will ultimately turn even death into a soluble problem. Kurzweil expects that artificial intelligence will develop far beyond the human mind in a few decades, leading to what he calls ""The Singularity,"" in which technology changes profoundly alter human history. ""The implications include the merger of biological and nonbiological intelligence, immortal software-based humans, and ultrahigh levels of intelligence that expand outward in the universe at the speed of light,"" he wrote. This story first appeared on ZDNet. Additional reporting by Dan Farber."
51,https://www.cnet.com/science/killer-robots-cambridge-brains-to-assess-ai-risk/,CNET,2012,11,26,540.0," Remember the cuddly Furby? Imagine it's grown a killer case (literally) of artificial intelligence and decides your house and your family are far better than its own, and decides to murder you for it. OK, so researchers think that such a scenario is a ""flakey concern"" and wildly far-fetched. Still, the U.K.'s University of Cambridge is setting up a new center to analyze the dangers posed by artificial intelligence and increasingly non-human interactive machines. Founded by distinguished philosophy professor Huw Price, cosmology and astrophysics professor Martin Reess and Skype co-founder Jaan Tallinn, the project will aim to separate fact from science fiction to determine whether  supersmart technology, fueled by artificial intelligence, could be a threat to humankind, reports the Associated Press (via NBC News). The prospective Cambridge Project for Existential Risk has set a wide field of study for itself, ranging from gadgets gone bad to global warming. ""Many scientists are concerned that developments in human technology may soon pose new, extinction-level risks to our species as a whole. Such dangers have been suggested from progress in AI, from developments in biotechnology and artificial life, from nanotechnology, and from possible extreme effects of anthropogenic climate change,"" the founders said on the project's Web site earlier this year. ""The seriousness of these risks is difficult to assess, but that in itself seems a cause for concern, given how much is at stake,"" the joint statement adds. Speaking to the AP, Price questioned what happens when ""we're no longer the smartest things around,"" and warned that we could be at risk from ""machines that are not malicious, but machines whose interests don't include us."" From ""2001: A Space Odyssey"" to the ""Terminator"" trilogy, many films have considered the mayhem that could be unleashed by a killer robot-type-thing. And while many will no doubt scoff at the thought of malevolent robot on the loose decades down the line, Price insists that the potential risks inherent in the development of artificial intelligence should not be dismissed so easily. ""It tends to be regarded as a flakey concern, but given that we don't know how serious the risks are, that we don't know the time scale, dismissing the concerns is dangerous. What we're trying to do is to push it forward in the respectable scientific community,"" he said. One of the examples given was when a computer begins to achieve some level of self-awareness and puts its own self-centered goals ahead of those of its human creators or ""masters."" Just as humans have evolved and slowly taken over the planet -- from chopping down great swaths of forest land and causing the extinction of dozens of species in our living memory alone -- Price warns that computer intelligence could mimic human evolution through the years at an accelerated rate. Consider, too, that Human Rights Watch and Harvard Law School's International Human Rights Clinic have called for the end to the ""development, production and use of fully autonomous weapons,"" such as those found in South Korea to protect the demilitarized zone. While these automated weapons are far from boasting artificial intelligence, there is concern that these algorithm-controlled ""killer robots"" could eventually overstep their bounds. The University of Cambridge risk center is planned for launch next year."
52,https://www.cnet.com/reviews/f1-race-stars-xbox-360-review/,CNET,2012,11,17,1215.0," Loop-the-loops, boost pads, and red ""seeker"" missiles are not the kinds of fodder you expect from a game boasting the official Formula One license and featuring the likes of Sebastian Vettel, Fernando Alonso, and Lewis Hamilton on its cover. Seeing these motor racing superstars inhabit a cartoon world that staunchly defies the rules of physics and fair play is, initially, equal parts bizarre and enticing. Catching out Kimi Raikkonen on the last corner with a well-timed boost, taking the shortcut over the top of a castle to get past Jensen Button, positioning a rain cloud over Mark Webber's head--there's a novelty to all of this that is undoubtedly charming. Unfortunately, the charm doesn't last long.A few hours in and you realise this the technical world of F1 and the crazy realm of karting don't mix well together here, and that if it weren't for the F1 license, you would struggle to find reasons to play this over any of the many other similar racers already out there. 6399952Some of Monza's most iconic corners are re-imagined in F1 Race Stars. The game's 12 tracks are composed of the most famous destinations on the F1 calendar, including Monaco, Great Britain, Abu Dhabi, and Brazil. Many of the tracks feature famous sections from their real world counterparts, such as the Curva Grande in Monza, and the crazy chicanes of Monaco. But there are some significant differences too. The British Grand Prix, for example, is known for its sweeping corners and unpredictable weather. In F1 Race Stars, the British Grand Prix is known for conveyor belts that throw you off course and Harrier jets that you must dodge to prevent yourself from being burnt by their exhaust fumes. Other tracks register as equally outlandish--Brazil sees you navigating through a jungle, Abu Dhabi has you racing along a roller coaster, and Monaco features an impossible jump from sea level to mountain top. While there's no lack of whimsy and variety in the track design, there are no standout moments either. Yes, racing with traffic on the autobahn on the Germany track is a fun idea, but it's a familiar trope within the kart racing community, and F1 Race Stars doesn't do anything to set its version apart from the competition. After you race all of the tracks in F1 Race Stars, none of them emerge as being any better than the others, and none of them are any more than simply sufficient for the job. Competent is an apt description. Weapons fare somewhat better where memorable events are concerned. Aside from the generic inclusion of homing bubbles (read: homing missiles), boosts, and trap bubbles that act as banana skins, there are some neat perversions of more familiar Formula One traits, such as DRS, safety car, and wet weather. If you're lucky enough to pick up a safety car, it races to the front of the pack and slows everyone down in a bid to help you catch up. DRS increases your speed and makes you invulnerable for a limited time, and rainy weather opens the heavens but also gives you a pair of deep-tread wet tyres. http://image.cbsi.com/gamespot/images/2012/298/676831_20121024_embed012.jpgThe racers can reach suprising speeds, given how top heavy they are. Race Stars also features an approximation of the KERS boost system from the sport. Corners feature sections where you can receive a boost on exit by pumping the right trigger, which fills a battery attached to the rear of your kart. The more you charge the battery, the bigger the boost you receive. So, just like in the real world sport, you're rewarded for slowing down into corners and using the racing line, which adds a gratifying layer of strategy to races. Like in other popular karting games, the probability of picking up a certain weapon directly relates to how well you're doing in the race. Safety cars tend to appear only when you're at the back of the pack, for example, while trap bubbles are common when you're out in front. This creates a rubber band effect that keeps racers close, but rather than adding extra tension, it often just creates frustration. The aggression levels of the AI, and the fact that there are 12 cars on the track, mean that it's all but impossible to avoid being wiped out by some weapon or other when you're anywhere near another opponent. 6399950F1 Race Star's four-player split-screen is much more enjoyable than the single-player. Of course, this is the game's way of keeping everybody together and keeping the action flowing. But it doesn't work. The first half lap of every race tends to play out the same way--namely, an explosion of weapons that would make Rambo think twice about entering the fray. Everyone is so close together on the first lap that it's impossible to plan your attacks wisely or avoid the onslaught of others. It's a game of luck on that opening lap; the racer who winds up with the least damage gains enough of an advantage to never be seen again. Not that that person is racing too quickly for you to catch up. It's more a case of his being far enough ahead not to have to worry about the smorgasbord of weapons being unleashed behind him. Other game modes work a little better by taking the focus away from simply finishing first, and towards something that makes less sense but is a whole lot more fun. Elimination Nation removes the last-place driver at regular intervals; in Refuel you need to grab fuel pickups to keep your tank filled; and in Slalom you must drive through coloured gates to score points. There are other alternative modes, but those three are the most fun and offer the best opportunities to play as the bad guy and ruin someone's race with a single act. Like all games of this type, F1 Race Stars is much improved when played in multiplayer. Up to four players can play in split-screen, with those four all able to compete simultaneously in online races of up to 12 players. Playing with others removes many of the annoying traits exhibited by the AI racers, not least their remarkable aggression and skill with the weapon set. A four-player split-screen race with eight AI drivers set on 3,000cc (the highest of the game's three speed/difficulty levels) is worth your while for a few laps, but with circuits lacking in original ideas, you soon come to the realisation that you could be having much more fun playing Mario Kart. http://image.cbsi.com/gamespot/images/2012/298/676831_20121024_embed010.jpgHope you've equipped your cobblestone-rated tyres. Your enjoyment is not helped by the visuals, which, while perfectly satisfactory, do little to enrich the experience. The sharp edges and cheerful colour palette stand at odds with the V-Power, Santander, and Etihad product placements that litter various portions of most tracks, while the Pixar-like filter the F1 cars have gone through is hardly the most inventive, or interesting of treatments. It's a bit of a shame, really. Codemasters Racing is one of the best and most dependable developers of racing games out there, but F1 Race Stars simply doesn't live up to the high standard defined by F1 games of the past. Once you see past the charming bobble-headed drivers and nods to F1, there's little here to hold your attention for long."
53,https://www.cnet.com/reviews/f1-race-stars-playstation-3-review/,CNET,2012,11,17,1212.0," Loop-the-loops, boost pads, and red ""seeker"" missiles are not the kinds of fodder you expect from a game boasting the official Formula One license and featuring the likes of Sebastian Vettel, Fernando Alonso, and Lewis Hamilton on its cover. Seeing these motor racing superstars inhabit a cartoon world that staunchly defies the rules of physics and fair play is, initially, equal parts bizarre and enticing. Catching out Kimi Raikkonen on the last corner with a well-timed boost, taking the shortcut over the top of a castle to get past Jensen Button, positioning a rain cloud over Mark Webber's head--there's a novelty to all of this that is undoubtedly charming. Unfortunately, the charm doesn't last long.A few hours in and you realise this the technical world of F1 and the crazy realm of karting don't mix well together here, and that if it weren't for the F1 license, you would struggle to find reasons to play this over any of the many other similar racers already out there. 6399947It's not every day you see a tractor on an F1 circuit. The game's 12 tracks are composed of the most famous destinations on the F1 calendar, including Monaco, Great Britain, Abu Dhabi, and Brazil. Many of the tracks feature famous sections from their real world counterparts, such as the Curva Grande in Monza, and the crazy chicanes of Monaco. But there are some significant differences too. The British Grand Prix, for example, is known for its sweeping corners and unpredictable weather. In F1 Race Stars, the British Grand Prix is known for conveyor belts that throw you off course and Harrier jets that you must dodge to prevent yourself from being burnt by their exhaust fumes. Other tracks register as equally outlandish--Brazil sees you navigating through a jungle, Abu Dhabi has you racing along a roller coaster, and Monaco features an impossible jump from sea level to mountain top. While there's no lack of whimsy and variety in the track design, there are no standout moments either. Yes, racing with traffic on the autobahn on the Germany track is a fun idea, but it's a familiar trope within the kart racing community, and F1 Race Stars doesn't do anything to set its version apart from the competition. After you race all of the tracks in F1 Race Stars, none of them emerge as being any better than the others, and none of them are any more than simply sufficient for the job. Competent is an apt description. Weapons fare somewhat better where memorable events are concerned. Aside from the generic inclusion of homing bubbles (read: homing missiles), boosts, and trap bubbles that act as banana skins, there are some neat perversions of more familiar Formula One traits, such as DRS, safety car, and wet weather. If you're lucky enough to pick up a safety car, it races to the front of the pack and slows everyone down in a bid to help you catch up. DRS increases your speed and makes you invulnerable for a limited time, and rainy weather opens the heavens but also gives you a pair of deep-tread wet tyres. http://image.cbsi.com/gamespot/images/2012/298/676831_20121024_embed012.jpgThe racers can reach suprising speeds, given how top heavy they are. Race Stars also features an approximation of the KERS boost system from the sport. Corners feature sections where you can receive a boost on exit by pumping the right trigger, which fills a battery attached to the rear of your kart. The more you charge the battery, the bigger the boost you receive. So, just like in the real world sport, you're rewarded for slowing down into corners and using the racing line, which adds a gratifying layer of strategy to races. Like in other popular karting games, the probability of picking up a certain weapon directly relates to how well you're doing in the race. Safety cars tend to appear only when you're at the back of the pack, for example, while trap bubbles are common when you're out in front. This creates a rubber band effect that keeps racers close, but rather than adding extra tension, it often just creates frustration. The aggression levels of the AI, and the fact that there are 12 cars on the track, mean that it's all but impossible to avoid being wiped out by some weapon or other when you're anywhere near another opponent. 6399948Interlagos looks a little different in F1 Race Stars. Of course, this is the game's way of keeping everybody together and keeping the action flowing. But it doesn't work. The first half lap of every race tends to play out the same way--namely, an explosion of weapons that would make Rambo think twice about entering the fray. Everyone is so close together on the first lap that it's impossible to plan your attacks wisely or avoid the onslaught of others. It's a game of luck on that opening lap; the racer who winds up with the least damage gains enough of an advantage to never be seen again. Not that that person is racing too quickly for you to catch up. It's more a case of his being far enough ahead not to have to worry about the smorgasbord of weapons being unleashed behind him. Other game modes work a little better by taking the focus away from simply finishing first, and towards something that makes less sense but is a whole lot more fun. Elimination Nation removes the last-place driver at regular intervals; in Refuel you need to grab fuel pickups to keep your tank filled; and in Slalom you must drive through coloured gates to score points. There are other alternative modes, but those three are the most fun and offer the best opportunities to play as the bad guy and ruin someone's race with a single act. Like all games of this type, F1 Race Stars is much improved when played in multiplayer. Up to four players can play in split-screen, with those four all able to compete simultaneously in online races of up to 12 players. Playing with others removes many of the annoying traits exhibited by the AI racers, not least their remarkable aggression and skill with the weapon set. A four-player split-screen race with eight AI drivers set on 3,000cc (the highest of the game's three speed/difficulty levels) is worth your while for a few laps, but with circuits lacking in original ideas, you soon come to the realisation that you could be having much more fun playing Mario Kart. http://image.cbsi.com/gamespot/images/2012/298/676831_20121024_embed010.jpgHope you've equipped your cobblestone-rated tyres. Your enjoyment is not helped by the visuals, which, while perfectly satisfactory, do little to enrich the experience. The sharp edges and cheerful colour palette stand at odds with the V-Power, Santander, and Etihad product placements that litter various portions of most tracks, while the Pixar-like filter the F1 cars have gone through is hardly the most inventive, or interesting of treatments. It's a bit of a shame, really. Codemasters Racing is one of the best and most dependable developers of racing games out there, but F1 Race Stars simply doesn't live up to the high standard defined by F1 games of the past. Once you see past the charming bobble-headed drivers and nods to F1, there's little here to hold your attention for long."
54,https://www.cnet.com/reviews/f1-race-stars-pc-review/,CNET,2012,11,17,1229.0," Loop-the-loops, boost pads, and red ""seeker"" missiles are not the kinds of fodder you expect from a game boasting the official Formula One license and featuring the likes of Sebastian Vettel, Fernando Alonso, and Lewis Hamilton on its cover. Seeing these motor racing superstars inhabit a cartoon world that staunchly defies the rules of physics and fair play is, initially, equal parts bizarre and enticing. Catching out Kimi Raikkonen on the last corner with a well-timed boost, taking the shortcut over the top of a castle to get past Jensen Button, positioning a rain cloud over Mark Webber's head--there's a novelty to all of this that is undoubtedly charming. Unfortunately, the charm doesn't last long.A few hours in and you realise this the technical world of F1 and the crazy realm of karting don't mix well together here, and that if it weren't for the F1 license, you would struggle to find reasons to play this over any of the many other similar racers already out there. Tracks are outlandish, not least this roller-coaster section featured in the Abu Dhabi circuit. The game's 12 tracks are composed of the most famous destinations on the F1 calendar, including Monaco, Great Britain, Abu Dhabi, and Brazil. Many of the tracks feature famous sections from their real world counterparts, such as the Curva Grande in Monza, and the crazy chicanes of Monaco. But there are some significant differences too. The British Grand Prix, for example, is known for its sweeping corners and unpredictable weather. In F1 Race Stars, the British Grand Prix is known for conveyor belts that throw you off course and Harrier jets that you must dodge to prevent yourself from being burnt by their exhaust fumes. Other tracks register as equally outlandish--Brazil sees you navigating through a jungle, Abu Dhabi has you racing along a roller coaster, and Monaco features an impossible jump from sea level to mountain top. While there's no lack of whimsy and variety in the track design, there are no standout moments either. Yes, racing with traffic on the autobahn on the Germany track is a fun idea, but it's a familiar trope within the kart racing community, and F1 Race Stars doesn't do anything to set its version apart from the competition. After you race all of the tracks in F1 Race Stars, none of them emerge as being any better than the others, and none of them are any more than simply sufficient for the job. Competent is an apt description. Weapons fare somewhat better where memorable events are concerned. Aside from the generic inclusion of homing bubbles (read: homing missiles), boosts, and trap bubbles that act as banana skins, there are some neat perversions of more familiar Formula One traits, such as DRS, safety car, and wet weather. If you're lucky enough to pick up a safety car, it races to the front of the pack and slows everyone down in a bid to help you catch up. DRS increases your speed and makes you invulnerable for a limited time, and rainy weather opens the heavens but also gives you a pair of deep-tread wet tyres. Tracks are outlandish, not least this roller-coaster section featured in the Abu Dhabi circuit. Race Stars also features an approximation of the KERS boost system from the sport. Corners feature sections where you can receive a boost on exit by pumping the right trigger, which fills a battery attached to the rear of your kart. The more you charge the battery, the bigger the boost you receive. So, just like in the real world sport, you're rewarded for slowing down into corners and using the racing line, which adds a gratifying layer of strategy to races. Like in other popular karting games, the probability of picking up a certain weapon directly relates to how well you're doing in the race. Safety cars tend to appear only when you're at the back of the pack, for example, while trap bubbles are common when you're out in front. This creates a rubber band effect that keeps racers close, but rather than adding extra tension, it often just creates frustration. The aggression levels of the AI, and the fact that there are 12 cars on the track, mean that it's all but impossible to avoid being wiped out by some weapon or other when you're anywhere near another opponent. Tracks are outlandish, not least this roller-coaster section featured in the Abu Dhabi circuit. Of course, this is the game's way of keeping everybody together and keeping the action flowing. But it doesn't work. The first half lap of every race tends to play out the same way--namely, an explosion of weapons that would make Rambo think twice about entering the fray. Everyone is so close together on the first lap that it's impossible to plan your attacks wisely or avoid the onslaught of others. It's a game of luck on that opening lap; the racer who winds up with the least damage gains enough of an advantage to never be seen again. Not that that person is racing too quickly for you to catch up. It's more a case of his being far enough ahead not to have to worry about the smorgasbord of weapons being unleashed behind him. Other game modes work a little better by taking the focus away from simply finishing first, and towards something that makes less sense but is a whole lot more fun. Elimination Nation removes the last-place driver at regular intervals; in Refuel you need to grab fuel pickups to keep your tank filled; and in Slalom you must drive through coloured gates to score points. There are other alternative modes, but those three are the most fun and offer the best opportunities to play as the bad guy and ruin someone's race with a single act. Like all games of this type, F1 Race Stars is much improved when played in multiplayer. Up to four players can play in split-screen, with those four all able to compete simultaneously in online races of up to 12 players. Playing with others removes many of the annoying traits exhibited by the AI racers, not least their remarkable aggression and skill with the weapon set. A four-player split-screen race with eight AI drivers set on 3,000cc (the highest of the game's three speed/difficulty levels) is worth your while for a few laps, but with circuits lacking in original ideas, you soon come to the realisation that you could be having much more fun playing Mario Kart. Tracks are outlandish, not least this roller-coaster section featured in the Abu Dhabi circuit. Your enjoyment is not helped by the visuals, which, while perfectly satisfactory, do little to enrich the experience. The sharp edges and cheerful colour palette stand at odds with the V-Power, Santander, and Etihad product placements that litter various portions of most tracks, while the Pixar-like filter the F1 cars have gone through is hardly the most inventive, or interesting of treatments. It's a bit of a shame, really. Codemasters Racing is one of the best and most dependable developers of racing games out there, but F1 Race Stars simply doesn't live up to the high standard defined by F1 games of the past. Once you see past the charming bobble-headed drivers and nods to F1, there's little here to hold your attention for long."
55,https://www.cnet.com/science/futurist-ray-kurzweil-on-smartphones-ai-and-the-human-brain/,CNET,2012,11,12,509.0," Kurzweil spoke to Techonomy founder David Kirkpatrick about his new book on human thought, ""How to Create a Mind,"" and the various themes that stem from it. Their talk was varied and at times scattered -- with a topic this big, you can imagine the temptation of tangents -- but Kurzweil had a few choice things to say along the way. The highlights: • ""I'm very optimistic [about the next five years] because there's a lot of evidence that not only hardware is progressing exponentially but software [too],"" Kurzweil said. • IBM's Watson and its performance on the television game show ""Jeopardy!"" is ""viscerally impressive"" in that people don't understand how truly remarkable an advancement it is. (Why? Because it's a computer coming to conclusions on its own, rather than searching a database and reiterating data stored within. • We are so integrated with connected technology today that ""during that one-day SOPA strike, I felt like part of my brain had gone on strike."" (SOPA legislation threatened the autonomy of content providers on the Internet.) • Holding up a Google Android-based smartphone, Kurzweil said that ""these will be the size of blood cells by 2030."" • IBM's Watson and Google's autonomous car will become deeply integrated with how we live. ""That kind of system will become a reliable tool that people will become dependent on,"" he said. • The very human capabilities of being funny or sexy? ""These are not sideshows to human intelligence,"" Kurzweil said. ""That's the cutting edge of human intelligence."" • Artificially intelligent agents can be considered human ""once they write a novel,"" Kurzweil said. ""They will be convincing in their ability to do human-like things."" • There's no reason to fear the future. ""We are a human-machine civilization [today]. Computers are doing things all the time that we can't possibly do.""And things are progressing quickly. ""A kid in Africa has more technology at his disposal than the president of the United States did 15 years ago."" • Technology will push us to be more human, not less. ""We're going to use those tools to make ourselves more expressive and more intelligent."" • And it won't be conceptually different from the analog era, either. ""We do have ways to make ourselves smarter through collaboration,"" Kurzweil said. ""That was the value of language."" • ""I've been thinking about thinking for 50 years,"" Kurzweil said, reflecting. • Discussing his book, he said that only recently, ""we can see our brain create our thoughts. We can see our thoughts create our brain."" His book's publication was delayed four times because of new research advancing this idea. • Understanding the brain better is important for three reasons: first, to fix it better; second, to provide models for humans to create more intelligent machines; and third, to further the science of understanding ourselves. Finally, Kurzweil was asked by a member of the audience: ""Are you an optimist? He chuckled, then replied: ""I've been accused of being an optimist."" Then he leaned back in his chair. This story originally appeared on SmartPlanet."
56,https://www.cnet.com/reviews/medal-of-honor-warfighter-xbox-360-review/,CNET,2012,11,10,1915.0," Upon completing Medal of Honor: Warfighter's campaign, you are met with a heartfelt dedication impressing upon you the heroism of the men in uniform the game depicts. The attempt at sincere emotion is commendable--but it rings hollow, coming as it does at the end of a bog-standard military shooter that celebrates the killing of hundreds. The battlefield fantasy itself offers a few surprises, but they're crowded out of your psyche by the indifferent hours of shooting and military chatter that surround them. ""Linear."" The word is commonly used to identify any number of shooters that usher you along a narrow path, interrupting your progress with a bit of sniping, the shooting of a turret, or an explosion-heavy cutscene. Warfighter's issue isn't that it fits this common modern-day shooter template, but that developer Danger Close doesn't use the linearity to the game's benefit. By directing the experience so tightly, a developer can build momentum, giving the action an arc that develops tension and ultimately reaches a zenith. When a game intends to be a playable action film, as so many do, managing that arc is key to delivering a memorable experience. Medal of Honor: Warfighter doesn't craft such an arc, and thus feels more like a pastiche of shooter tropes than a self-contained experience with its own identity. Yet there's something worthy here--the glimmer of a Medal of Honor that might yet hew its own path if the right elements are cultivated. The basic shooting and movement models are a good start, not because the guns are that remarkable, but because there's a sense of weight to your sprints and your leaps. You're given the ability to take cover and lean or peek before taking aim, lest you get pelted with lead; at times, this encourages you to consider your surroundings and preserve your own well-being rather than rush forward, spraying the room with bullets. The shooting is occasionally put to good use, too, such as in a noisy showdown during a raging rainstorm, the palm trees waving and bending in response to the heaving winds. Other levels are just as visually impressive, like an on-rails boat shootout during which fires rage and floating debris threatens to ram you. Elsewhere, you use the blazing shine of your enemies' flashlights as beacons for your violence in various locales. The Frostbite 2 engine that gave Battlefield 3 life is used well enough here, occasional visual glitches and distracting screen grime notwithstanding. These visuals are much more effective on the PC than consoles, but on any platform, Medal of Honor: Warfighter isn't always just a sea of brown, though you can still expect plenty of dusty roads and crumbling hovels to fill your field of view. If only the gameplay could consistently uphold the promise of the most atmospheric levels. To Warfighter's benefit, it's not as much of a turkey shoot as its 2010 predecessor, though enemies still pop up in the most predictable places, inviting you to gun them down. The excitement is also undercut by your AI teammates' unlimited supply of ammo; there's never any need to scrounge the ground for enemy weapons, which diminishes the sense that you are in imminent danger. (A little improvisational spirit could have gone a long way.) But it's the moments you most expect to deliver the brightest sparks that are most devoid of them. The aforementioned boat chase requires no skill, neither from a driving nor from a shooting perspective. Ditto for the obligatory helicopter gunner segment, in which you mow down nameless grunts from above. Without challenge, there needs to be something else to keep excitement levels high--but there aren't enough foes to shoot or other sources of thrills to compensate. Warfighter checks other paradigms off its list, too. There are the parts where you sneak up on enemies from behind and gruesomely stab them, and the parts where you snipe the baddies lurking in distant windows. There are the parts where you call in airstrikes to annihilate entire buildings, and there's the bit where you shoot down a helicopter with a rocket launcher. There are seemingly endless door breaches, in which time slows to a crawl while you and your AI teammates charge into a room and litter the floor with corpses. Things explode real nice, but these sequences are all segmented sharply from the surrounding gameplay. The game signals ""hey, here's the part with the sniper rifle,"" and you dutifully perform the necessary actions so you can continue. There are several scripted set-piece sections that stand above the rest, however--and in fact, stand above the campaign in general. All of them involve vehicles. Some of these driving sections are ridiculous and entertaining, directing you to incite crashes, and then showcasing the destruction in slow motion, Burnout-style. The camera that so lovingly caresses the chaos flies in the face of Warfighter's meager attempts to identify the drivers as everyday heroes, but the tension of avoiding oncoming traffic and the joy of watching your four-wheeled victims flip with abandon are both guilty pleasures. The game's most surprising turn of events is a vehicular stealth sequence in which you must slip into designated safe spots to avoid prowling enemy drivers. It's a neat idea, executed well, that generates tension and has you fearing your possible discovery. It's not difficult to succeed, but even so, this portion is elegant and imaginative. Less elegant are Warfighter's nods to the effects war can have not just on its participants, but on their loved ones. Your role alternates between different operatives, with Preacher (returning from 2010's Medal of Honor) fulfilling the role of main protagonist. The central story comes by way of the jargon-filled military chatter you're used to in such games, in which you know who the bad guy is, not because wrongdoing is demonstrated, but because the characters say he's the bad guy. The globe-hopping narrative, like the gameplay, is chopped into cutscenes and key events without regard for exposition or transition. There's plenty of plot, but little storytelling--and there are important distinctions between the two. Off the battlefield, you meet Preacher's wife and daughter, who suffer from the effects of the uncanny valley by way of their sort-of-lifelike, sort-of-not character models, but nonetheless deliver some civilian levity between explosions. The gentler side of Warfighter's story is a wasted opportunity, however, since every character is a stand-in for an idea (the neglected but stalwart wife, the loyal and conflicted warrior) rather than a defined individual. Yet while they are simple plot constructs, actors deliver their lines with conviction, and the manipulative soundtrack swells in properly melodramatic ways, softening your heart for a few moments before the ensuing action hardens it once again. There's a moment near the end of the campaign, however, that has you confronting the consequences of war, allowing you to witness terrible deaths in ways you never can while shooting down combatants. And it's here that Warfighter almost achieves something special. You witness more vulnerability here, and can appreciate the operatives' sacrifices in these final throes. The military fantasy becomes dark reality for a brief moment, and there's no joy in your final shots. Here, you see one more way in which Medal of Honor may yet make its mark, if only this conclusion weren't so removed from the remainder of the game, which otherwise treats levels as interchangeable building blocks that needn't fit into a larger picture. Of course, if a military shooter is a means for you to shoot fools online and insult their skills (and mothers), the campaign may be a secondary concern, and it's just as well, since the multiplayer is much more satisfying than the campaign, though not without its flaws. Warfighter doesn't have the weight of, say, Killzone 3, but it doesn't shoot for the zippiness of Modern Warfare either, instead finding a more-or-less comfortable place between the two. The leaning mechanic in the campaign finds a place here, and while gameplay doesn't hinge on successfully using it, it's nonetheless a boon, allowing you to quickly establish a line of sight, take some potshots, and lean back into cover. More important is Warfighter's fire team system, in which you are paired with another team member, and the two of you leech off of each other's successes. Your buddy is both protector and spawn point, and you earn a few experience points for his headshots and kills, presuming you're in close proximity. You earn various bonuses for sticking with your buddy, so you quickly develop a camaraderie of necessity. This isn't a wholly new mechanic in games, but there is a palpable psychological component to it: when your buddy is waiting to spawn, you stay out of harm's way so that your friend might arrive in relative safety, and there's joy in getting revenge on the opponent that gunned down your buddy just moments before. It's a good feeling to know someone's got your back. A traditional class system glues matches together, though you need to sort through the game's improbably convoluted and busy interface to make sense of it. Everyone starts out as an assaulter, but it isn't long before you've unlocked every class and are well on your way toward earning medals (Congratulations! You've killed 30 players with primary weapons!) and various weapon modifications: barrels, paint jobs, optics, and so on. You also unlock variations of the classes, each associated with a particular nation, and within matches, you can perform offensive or defensive support actions (fly an Apache!) should you string together enough kills. There's a healthy progression system here that keeps the rewards coming. A metagame goes only so far if the core action and modes don't hold up, but Warfighter is a decent multiplayer shooter with a number of ways to play, held back mainly by its confined maps, some of which are more collections of winding exterior corridors than organic spaces. You never run out of players to kill, at least, within the five modes on offer. The matchmaking options also include playlists that pair up two different modes, and one of these playlists minimizes the interface and turns on friendly fire, inspiring a more cautious experience. On the other end of the spectrum is Home Run mode, a vicious 10-round mode that combines capture the flag with Counter-Strike's tight assault-and-defend dynamic. The maps are small and you don't respawn when you die; all you can do is wait for the next round. Home Run sports a livelier tug-of-war than the other modes, and after the initial learning curve (knowing the map is key), combat can get intense. Danger Close didn't tie up some necessary loose ends before the game's release: you might spawn outside of the map and into freefall, spawn into some environmental anomaly and struggle to unstick yourself, or even bang into an invisible obstacle. In the single-player campaign, enemies might clip right through walls when they aren't busy being generally dumb. Yet Medal of Honor: Warfighter's greatest handicap isn't bugs, but that its building blocks are snapped together into a shapeless hunk rather than an identifiable monolith with form and purpose. Still, you shouldn't dismiss the game as wholly unworthy: online multiplayer is good fun, and the campaign shows signs of life, occasionally letting you see past the me-too warfare and appreciate a brief flash of imagination. But on the whole, Warfighter leaves you thinking, ""Yep, that's a military shooter, all right."" Its heroes strive for greatness; the game they star in is merely serviceable."
57,https://www.cnet.com/reviews/wwe-13-xbox-360-review/,CNET,2012,11,7,1004.0," Ditching the SmackDown vs. Raw title, last year's WWE '12 gave the wrestling series a major overhaul with a fresh look, a new engine, and streamlined controls. While the much-needed reboot may have revitalized faith in the franchise, it still had some lingering problems that held it back from true greatness. WWE '13 isn't quite as revolutionary as its predecessor, but thanks to a superior campaign mode and tighter gameplay, it's drastically more enjoyable. Authentic ringside showdowns explode with all of the testosterone-laden bombast and roaring crowd enthusiasm of the real deal. Pyrotechnics, boisterous announcers, and grand entrance sequences do a great job of setting the stage for each brutal matchup. When it comes down to trading actual blows, WWE '13 sticks closely to last year's fast-paced formula, which is a good thing. The same simplified controls conveniently map strikes, grapples, Irish whips, and special moves to individual buttons, making it easy to pull off maneuvers and deliver awesome-looking move sequences without struggling to remember elaborate combos. Your attack moves also vary depending on your position and how much punishment you've dealt to your opponent. Despite the fact you're a burly dude clobbering the snot out of other burly dudes, there's a certain grace to the ebb and flow of matches as the advantage shifts back and forth between wrestlers. The excitement builds once the sweat starts flying: wrestlers show injuries and fatigue, and the action ratchets up to its peak with wilder signature moves. New ""Spectacular Moments"" add to this frenzied energy by letting you dish out elaborate finishers like smashing opponents through the announcers' table, hurling them through barricades, and breaking the ring itself. They're a thrill to pull off, and make you feel even more badass than usual. Most of the other wrestling gameplay improvements are subtle, but they make a real difference. WWE '13's action feels tighter and more cohesive than last year's revamp. Though not all of the visual bugs are squashed, there are far fewer problems that interrupt the flow of combat. Transitions between moves are animated more smoothly, and airborne maneuvers connect with greater precision, boosting the realism of matches. This is a big improvement from some of the more jarring visual transitions in last year's matches. http://image.cbsi.com/gamespot/images/2012/309/672340_20121105_embed001.jpgSpandex and Lycra: it's all the cushion you get. This time around the reversal system is a lot more forgiving too, which solves one of the more aggravating conundrums of WWE '12. You still need quick reflexes to time your blocks and turn opponents' attacks against them, but there's enough wiggle room now on the default settings that you won't feel like you're being put through the meat grinder the second you're caught unprepared--unless you want a steeper challenge, that is. When you do miss, an onscreen indicator tells you whether you're too early or too late, instead of leaving you frustrated and guessing as to why you're getting getting your face bashed in. This helps to improve your timing so you can better anticipate when your next opportunity to break your adversary's attack chain will arise. Beyond the option to boost or reduce the overall difficulty, you can even dig into the nitty-gritty and fine-tune the effectiveness of AI reversals to suit your ability level. Toggling the ""experience"" level of each match can further adjust how much damage each move does, affecting how long matches last. These additional layers of optional flexibility go a long way towards boosting accessibility. Though there's plenty of challenge here for experienced wrestling game vets to chew on, WWE '13 is a great entry point for newcomers to the franchise. WWE '13 isn't lacking in one-off options for those who want to jump right in and experiment with the different match types against the AI or human opponents. From backstage brawls and Royal Rumbles to cage matches and tag team bouts, there's a ton of variety to explore. A beefier range of creative options let you craft a massive amount of customized content--including unique wrestlers, moves, storylines, and entire stadiums--that can be used in online multiplayer matches without having to upload creations first. Even better, this content can be used in the returning managerial-heavy WWE Universe mode that lets you simulate or play entire seasons of SmackDown, Raw, and Pay-Per-View matches. With so many options available, it's tempting to sink months of time into crafting content and tackling matches before you even dive into the solo campaign's great main event. http://image.cbsi.com/gamespot/images/2012/309/672340_20121105_embed002.jpgMove your butt, Kofi--we can't see what's going on in front of you! In contrast to WWE '12's weak Road to Wrestlemania campaign, the excellent Attitude Era mode is a great replacement. It's one of the biggest improvements in WWE '13. Covering some of WWE's most notable matches, rivalries, and encounters from the Attitude Era (1997-1999), the new solo campaign is chock-full of audio and video clips culled from wrestling's historical archives. Nostalgia factor aside, some pretty rocking showdowns are found in the lengthy run of matches divided into character-focused chunks. These matches have you playing as a rotating group of wrestlers, including Shawn Michaels, Stone Cold Steve Austin, Mankind, The Undertaker, and more, as they tackle some of the most epic encounters of the period. While most matches in the campaign let you win by pinfall or submission, mostly optional historical objectives for each showdown spice things up, and completing them unlocks bonus goodies like extra matches, special outfits, arenas, championships, and wrestlers. Considering the franchise's annual nature and the fact that last year brought such a huge overhaul to the series, it's not a surprise that WWE '13 doesn't innovate as substantially as its predecessor. It feels like the same game in many ways, but lots of subtle tweaks help make up for the shortcomings in WWE '12. The small refinements add up, particularly when rounded out by the handful of bigger additions. The stellar Attitude Era campaign alone alone is a huge draw, and if you're a longtime avid wrestling enthusiast. WWE '13 is a big step in the right direction."
58,https://www.cnet.com/reviews/medal-of-honor-warfighter-pc-review/,CNET,2012,11,1,1915.0," Upon completing Medal of Honor: Warfighter's campaign, you are met with a heartfelt dedication impressing upon you the heroism of the men in uniform the game depicts. The attempt at sincere emotion is commendable--but it rings hollow, coming as it does at the end of a bog-standard military shooter that celebrates the killing of hundreds. The battlefield fantasy itself offers a few surprises, but they're crowded out of your psyche by the indifferent hours of shooting and military chatter that surround them. ""Linear."" The word is commonly used to identify any number of shooters that usher you along a narrow path, interrupting your progress with a bit of sniping, the shooting of a turret, or an explosion-heavy cutscene. Warfighter's issue isn't that it fits this common modern-day shooter template, but that developer Danger Close doesn't use the linearity to the game's benefit. By directing the experience so tightly, a developer can build momentum, giving the action an arc that develops tension and ultimately reaches a zenith. When a game intends to be a playable action film, as so many do, managing that arc is key to delivering a memorable experience. Medal of Honor: Warfighter doesn't craft such an arc, and thus feels more like a pastiche of shooter tropes than a self-contained experience with its own identity. Yet there's something worthy here--the glimmer of a Medal of Honor that might yet hew its own path if the right elements are cultivated. The basic shooting and movement models are a good start, not because the guns are that remarkable, but because there's a sense of weight to your sprints and your leaps. You're given the ability to take cover and lean or peek before taking aim, lest you get pelted with lead; at times, this encourages you to consider your surroundings and preserve your own well-being rather than rush forward, spraying the room with bullets. The shooting is occasionally put to good use, too, such as in a noisy showdown during a raging rainstorm, the palm trees waving and bending in response to the heaving winds. Other levels are just as visually impressive, like an on-rails boat shootout during which fires rage and floating debris threatens to ram you. Elsewhere, you use the blazing shine of your enemies' flashlights as beacons for your violence in various locales. The Frostbite 2 engine that gave Battlefield 3 life is used well enough here, occasional visual glitches and distracting screen grime notwithstanding. These visuals are much more effective on the PC than consoles, but on any platform, Medal of Honor: Warfighter isn't always just a sea of brown, though you can still expect plenty of dusty roads and crumbling hovels to fill your field of view. If only the gameplay could consistently uphold the promise of the most atmospheric levels. To Warfighter's benefit, it's not as much of a turkey shoot as its 2010 predecessor, though enemies still pop up in the most predictable places, inviting you to gun them down. The excitement is also undercut by your AI teammates' unlimited supply of ammo; there's never any need to scrounge the ground for enemy weapons, which diminishes the sense that you are in imminent danger. (A little improvisational spirit could have gone a long way.) But it's the moments you most expect to deliver the brightest sparks that are most devoid of them. The aforementioned boat chase requires no skill, neither from a driving nor from a shooting perspective. Ditto for the obligatory helicopter gunner segment, in which you mow down nameless grunts from above. Without challenge, there needs to be something else to keep excitement levels high--but there aren't enough foes to shoot or other sources of thrills to compensate. Warfighter checks other paradigms off its list, too. There are the parts where you sneak up on enemies from behind and gruesomely stab them, and the parts where you snipe the baddies lurking in distant windows. There are the parts where you call in airstrikes to annihilate entire buildings, and there's the bit where you shoot down a helicopter with a rocket launcher. There are seemingly endless door breaches, in which time slows to a crawl while you and your AI teammates charge into a room and litter the floor with corpses. Things explode real nice, but these sequences are all segmented sharply from the surrounding gameplay. The game signals ""hey, here's the part with the sniper rifle,"" and you dutifully perform the necessary actions so you can continue. There are several scripted set-piece sections that stand above the rest, however--and in fact, stand above the campaign in general. All of them involve vehicles. Some of these driving sections are ridiculous and entertaining, directing you to incite crashes, and then showcasing the destruction in slow motion, Burnout-style. The camera that so lovingly caresses the chaos flies in the face of Warfighter's meager attempts to identify the drivers as everyday heroes, but the tension of avoiding oncoming traffic and the joy of watching your four-wheeled victims flip with abandon are both guilty pleasures. The game's most surprising turn of events is a vehicular stealth sequence in which you must slip into designated safe spots to avoid prowling enemy drivers. It's a neat idea, executed well, that generates tension and has you fearing your possible discovery. It's not difficult to succeed, but even so, this portion is elegant and imaginative. Less elegant are Warfighter's nods to the effects war can have not just on its participants, but on their loved ones. Your role alternates between different operatives, with Preacher (returning from 2010's Medal of Honor) fulfilling the role of main protagonist. The central story comes by way of the jargon-filled military chatter you're used to in such games, in which you know who the bad guy is, not because wrongdoing is demonstrated, but because the characters say he's the bad guy. The globe-hopping narrative, like the gameplay, is chopped into cutscenes and key events without regard for exposition or transition. There's plenty of plot, but little storytelling--and there are important distinctions between the two. Off the battlefield, you meet Preacher's wife and daughter, who suffer from the effects of the uncanny valley by way of their sort-of-lifelike, sort-of-not character models, but nonetheless deliver some civilian levity between explosions. The gentler side of Warfighter's story is a wasted opportunity, however, since every character is a stand-in for an idea (the neglected but stalwart wife, the loyal and conflicted warrior) rather than a defined individual. Yet while they are simple plot constructs, actors deliver their lines with conviction, and the manipulative soundtrack swells in properly melodramatic ways, softening your heart for a few moments before the ensuing action hardens it once again. There's a moment near the end of the campaign, however, that has you confronting the consequences of war, allowing you to witness terrible deaths in ways you never can while shooting down combatants. And it's here that Warfighter almost achieves something special. You witness more vulnerability here, and can appreciate the operatives' sacrifices in these final throes. The military fantasy becomes dark reality for a brief moment, and there's no joy in your final shots. Here, you see one more way in which Medal of Honor may yet make its mark, if only this conclusion weren't so removed from the remainder of the game, which otherwise treats levels as interchangeable building blocks that needn't fit into a larger picture. Of course, if a military shooter is a means for you to shoot fools online and insult their skills (and mothers), the campaign may be a secondary concern, and it's just as well, since the multiplayer is much more satisfying than the campaign, though not without its flaws. Warfighter doesn't have the weight of, say, Killzone 3, but it doesn't shoot for the zippiness of Modern Warfare either, instead finding a more-or-less comfortable place between the two. The leaning mechanic in the campaign finds a place here, and while gameplay doesn't hinge on successfully using it, it's nonetheless a boon, allowing you to quickly establish a line of sight, take some potshots, and lean back into cover. More important is Warfighter's fire team system, in which you are paired with another team member, and the two of you leech off of each other's successes. Your buddy is both protector and spawn point, and you earn a few experience points for his headshots and kills, presuming you're in close proximity. You earn various bonuses for sticking with your buddy, so you quickly develop a camaraderie of necessity. This isn't a wholly new mechanic in games, but there is a palpable psychological component to it: when your buddy is waiting to spawn, you stay out of harm's way so that your friend might arrive in relative safety, and there's joy in getting revenge on the opponent that gunned down your buddy just moments before. It's a good feeling to know someone's got your back. A traditional class system glues matches together, though you need to sort through the game's improbably convoluted and busy interface to make sense of it. Everyone starts out as an assaulter, but it isn't long before you've unlocked every class and are well on your way toward earning medals (Congratulations! You've killed 30 players with primary weapons!) and various weapon modifications: barrels, paint jobs, optics, and so on. You also unlock variations of the classes, each associated with a particular nation, and within matches, you can perform offensive or defensive support actions (fly an Apache!) should you string together enough kills. There's a healthy progression system here that keeps the rewards coming. A metagame goes only so far if the core action and modes don't hold up, but Warfighter is a decent multiplayer shooter with a number of ways to play, held back mainly by its confined maps, some of which are more collections of winding exterior corridors than organic spaces. You never run out of players to kill, at least, within the five modes on offer. The matchmaking options also include playlists that pair up two different modes, and one of these playlists minimizes the interface and turns on friendly fire, inspiring a more cautious experience. On the other end of the spectrum is Home Run mode, a vicious 10-round mode that combines capture the flag with Counter-Strike's tight assault-and-defend dynamic. The maps are small and you don't respawn when you die; all you can do is wait for the next round. Home Run sports a livelier tug-of-war than the other modes, and after the initial learning curve (knowing the map is key), combat can get intense. Danger Close didn't tie up some necessary loose ends before the game's release: you might spawn outside of the map and into freefall, spawn into some environmental anomaly and struggle to unstick yourself, or even bang into an invisible obstacle. In the single-player campaign, enemies might clip right through walls when they aren't busy being generally dumb. Yet Medal of Honor: Warfighter's greatest handicap isn't bugs, but that its building blocks are snapped together into a shapeless hunk rather than an identifiable monolith with form and purpose. Still, you shouldn't dismiss the game as wholly unworthy: online multiplayer is good fun, and the campaign shows signs of life, occasionally letting you see past the me-too warfare and appreciate a brief flash of imagination. But on the whole, Warfighter leaves you thinking, ""Yep, that's a military shooter, all right."" Its heroes strive for greatness; the game they star in is merely serviceable."
59,https://www.cnet.com/culture/hoodie-jacket-with-built-in-goggles-is-creepy-and-cozy/,CNET,2012,10,22,192.0," Secretly, I want to carry a superhero costume with me at all times. I want to be able to step into a phone booth (if they exist anymore) and step out as the Masked Avenger. For $492, my fantasy could come true with the AI Riders on the Storm down jacket. The jacket sounds pretty much like a regular coat with specs that include a two-way zipper and down stuffing. Look up, though, and things start to get weird. It has a detachable hood with built-in bug-eyed goggles that zip together right up the middle. Freaky, man. There are air holes at the nose and ears so you can breath and hear even when you're all zipped up. As warm and cozy as the jacket appears to be, it's also delightfully creepy and dark. It's what Bruce Wayne would wear on a ski trip to Aspen. One detail is a bit curious. The jacket has a large pom-pom right at the top. Depending on how dark and brooding you want to look, you can detach the pom-pom. It's hard to be a tough, scary superhero if you wear a pom-pom. (Via Gizmodo)"
60,https://www.cnet.com/culture/did-a-bug-in-deep-blue-lead-to-kasparovs-defeat/,CNET,2012,9,27,364.0," It's part of the conventional wisdom now that machines are smarter than us, especially when it comes to specific challenges. Chess, for instance. World champion Garry Kasparov's defeat at the hands of IBM's Deep Blue computer in 1997 was a milestone in the story of artificial intelligence. But did the machine merely psych him out? Statistician Nate Silver's new book ""The Signal and The Noise: Why So Many Predictions Fail--But Some Don't"" contains an anecdote about how a glitch in Deep Blue may have led Kasparov to overestimate the machine's smarts, according to The Washington Post. Despite the machine's ability to evaluate 200 million moves per second, Kasparov easily won the first game of the match. In the 44th move, however, Deep Blue made an inexplicable play, moving a rook for no apparent purpose. Kasparov may have been spooked by that, but his concern turned to panic in the second game, in which Deep Blue started playing much less like a computer and more like a human grandmaster. Kasparov lost, drew the next three games, and collapsed during the sixth, losing the epic battle. Deep Blue's rook move, however, was the result of a bug, according to Silver. The glitch made it unable to select any of the many possibilities it could analyze, so it went to a fail-safe maneuver -- a random play. The move was of no consequence, and the bug was fixed before the second game. But Silver speculates: What inspired Kasparov to mess up probably had a lot more to do with how Deep Blue's playing style had changed dramatically in the second game. It unexpectedly avoided a classic trap for computer players that Kasparov set, then established a winning position, prompting the world champ to resign. Kasparov seems to have been more concerned about IBM cheating and using the advice of a hidden grandmaster than finding himself face to face with a decidedly superior intelligence. It was the first time a reigning world champion had lost to a computer in a regulation match. The rest, of course, is history. IBM denied it had cheated, and Deep Blue was dismantled. Along with its parts went a piece of human superiority."
61,https://www.cnet.com/reviews/legends-of-pegasus-pc-review/,CNET,2012,9,22,1080.0," You know how pizza places offer ""meat lovers"" or ""veggie lovers"" pizzas, where they just cram every single ingredient of a certain type that they have in the refrigerator onto a pie? Legends of Pegasus is kind of like that: it features a lot of elements that 4X aficionados might enjoy in the right context, but they're all just kind of slapped together in a way that doesn't allow them to complement each other. It's pizza that requires a fork and a knife to eat when you ought to be able to just pick up a slice and easily slide it into your mouth. Legends of Pegasus' storyline relies heavily on tried-and-true sci-fi tropes, particularly a Battlestar Galactica-esque survival/flight theme. At the beginning of the game, you are informed that Earth has been conquered in a surprise attack by an unknown alien force, and a small flotilla of ships has managed to escape through a wormhole. You command that flotilla, and, as luck would have it, you've got a colony ship with you. You need to colonize habitable planets, research new technologies, build bigger and better ships, and fight off constant attacks from aliens seemingly bent on hostility. It's all very hackneyed, including the few plot ""twists"" that you see coming from light years away. All that said, the storyline is admittedly secondary to the gameplay, but Legends of Pegasus doesn't score many points for itself there, either. Played on large maps of fictional solar systems, Legends of Pegasus tries to replicate the feel of http://www.gamespot.com//'s GUI, but because Legends of Pegasus is primarily turn-based (only battles take place in real time) and because its menus and controls are terribly arcane and unintuitive, it fails to give you much more than a general inkling of Sins' brilliant interface. Zooming, for example, a virtually limitless function in Sins, is strictly limited in Legends of Pegasus. This makes finding items of interest (such as waypoints or resource fields) a laborious, scrolling process. For some things, like your ships or asteroid fields, you can use predesignated icons to jump directly to them, but then you're likely to lose sight of whatever it is you want to be focused on at the same time, also resulting in needless scrolling and clicking. There's the planet management interface too, wherein you designate what you want your colonies to build and what kinds of resource allocation you want them to have, and you can see what exactly they're generating for you in terms of revenue, science, and ships. This interface is lifted almost pixel for pixel from http://www.gamespot.com//, but unlike that game, Legends of Pegasus fails to provide you with meaningful information about what your colony-based choices mean for the future. Sure, the game has rollover tips with what each building does, but with limited space to build and an extremely limited budget, it's never clear why you'd choose X over Y. Speaking of limited budgets, Legends of Pegasus operates in a strange ecosystem whereby the survivors of Earth's demise, desperately escaping from an alien threat, completely dependent upon the shreds of the navy they have left to protect them, are nevertheless apparently charging that navy money for everything from ship building to production of shelters for their own use. Citizens pay taxes to the interim government, but if you raise taxes too high, their morale drops, which has some unexplained further negative effect. This is your only way to make money--without which you cannot build more structures and you cannot build any ships. Inexplicably, there is no limit to the amount of debt you can go into if you don't collect enough taxes, and Legends of Pegasus gives you no warnings about your debt level. Unless you're paying attention to your finances at all times (hard to do when you're scouring for waypoints and fighting battles), it's possible to ""rimrock"" yourself: that is, put yourself in a situation where you're too in debt to buy the buildings that you need to get yourself out of debt. And then you have to restart the mission from square one. Most of the main campaign's missions are straightforward: defend yourself from randomly timed alien invasions, seek out some critical resource, build up your forces, counterattack, and so on. Nothing is particularly taxing on your brain (apart from how to make enough money from the ingrates you're protecting without offending them), but eventually you encounter Legends of Pegasus' combat engine, like it or not. And the truth is probably going to be ""not."" Combat in Legends of Pegasus is a chaotic affair in which your ships, from what was otherwise a stationary, turn-based game, suddenly spring to life and engage enemy ships in real time. Giving orders to your ships is very difficult, because selecting ships and targets often fails to register with the AI, and individual buttons for ships' commands are tiny and hard to use effectively. If your ships outnumber and/or out-tech the enemy, you're going to win. None of the skills from other real-time strategy games, such as crowd control, division of labor (aka rock-paper-scissors), actions per minute, or other such staples, seem to matter in the slightest. That's Legends of Pegasus: lots of quantity, little quality. While it borrows liberally from just about every major 4X sci-fi game up until now, it does so in a haphazard fashion, losing all, or nearly all, of the things that made those games so great. It doesn't help that Legends of Pegasus is buggy, with an annoying DRM login that frequently blocks you from getting to the game and plenty of graphical glitches. The AI leaves much to be desired as well, since ships never do anything on their own initiative besides run straight at the enemy and fire every weapon. Sometimes you order them to do something and they don't do anything at all. If you want to escape the AI woes to some degree, you can try Legends of Pegasus' multiplayer, but good luck finding anyone to match up with via the game's matchmaking system--its lobbies are emptier than a college student's bank account. Even if you do a direct IP connection to a friend, multiplayer is laggy, tends to boot you, and suffers from many of the same problems as the single-player. Bottom line: Legends of Pegasus had a grand plan, but developer Novacore didn't come close to pulling off what it set out to do. Like storied Bellerophon, its hubris has proved its ultimate undoing."
62,https://www.cnet.com/culture/japan-building-robot-that-would-pass-college-exams/,CNET,2012,9,12,321.0," It isn't enough that machines can beat the best of us at chess, Jeopardy, and a billion other things. Now they want to rub our faces in our inferiority by getting into our universities and scoffing at us. Boffins at Fujitsu Labs are teaming up with Japan's National Institute of Informatics (NII) to create an artificial-intelligence system that would be able to pass the entrance exam for the University of Tokyo, one of the most prestigious schools in the country. The project aims to build an AI that can do well on Japan's nationwide university entrance exams by 2016, and then pass the more difficult exam for Todai, as the top college is known, by 2021. Human students have to do well on both tests to get into the university, which is known for its brutal admission requirements. It placed eighth in the latest QS Asian University rankings. It's unclear whether the ""Todai robot"" will be a humanoid robot. Whatever its form, it will have to master subjects such as physics, chemistry, and history, and answer questions on foreign languages. NII hopes the project, led by Noriko Arai, will yield insights into human intelligence and foster groundbreaking AI innovations. Fujitsu, which built one of the fastest supercomputers in the world last year, is helping to improve the robot's math skills. ""NII and Fujitsu Laboratories jointly aim to develop the technologies needed for human-centric IT,"" Fujitsu said in a release. ""These include formula recognition methods to recognize and interpret problem texts and put it into a data format that a computer can understand; natural language processing to generate a formula representation that the formula solver can understand; and formula-processing technology that can solve the composed formula quickly and accurately. ""The hope is that the technologies developed as part of this project will enable anyone to easily use sophisticated mathematical analysis tools."" Sounds like the perfect cover to me. (Via Japan Real Time)"
63,https://www.cnet.com/tech/mobile/apple-showcases-third-party-apps-on-iphone-5/,CNET,2012,9,12,365.0," By now, you know that today's Apple event gave us the iPhone 5, iOS 6, iTunes 11, and a couple of new iPods. But what you may have missed were the revamped apps that also made appearances at the show. Here's an overview of the impressive third-party titles that got some screen time at today's big Apple event. Clumsy NinjaDeveloped by gamemaker Natural Motion, Clumsy Ninja is a silly, Talking Tom Cat-style game with a bit of unique artificial intelligence under the hood. It features a silly-looking ninja standing front and center, where you can grab his appendages, move him around, and even tickle him. The main objective of the game is to build up your ninja's skills using punching bags, trampolines, and other apparatus. We got a glimpse of this game and its advanced AI technology running on the A5-powered iPod Touch. Real Racing 3Game publisher EA also hit the stage today to show off the latest installment of its popular racing series. On the iPhone 5, Real Racing 3 appears to have console-quality graphics and an impressively smooth frame rate. With dynamic reflections and working rear-view mirrors, there is also an added dimension of realism that I haven't seen on any other mobile racing titles. But the biggest feature we saw was the time-shifted multiplayer mode. This allows players to challenge friends through Apple's Game Center and play against them later. OpenTable, CNNApple Senior Vice President Phil Schiller also gave us a look at updated apps for CNN and restaurant reservation service OpenTable. While we didn't see any mind-blowing new features here, we did see that both had been updated for the iPhone 5's 4-inch Retina Display. We also found out that those third-party apps that haven't yet been updated will show up centered, with black borders on the sides (or top and bottom). Also worth mentioning is the new Google-made YouTube app. While it wasn't demonstrated at today's event, its release yesterday is still big news. Google undoubtedly pushed the app out in anticipation of the new iPhone 5 and iOS 6, as the latter will be the first of Apple's mobile operating systems to ship without an Apple-made YouTube interface preinstalled."
64,https://www.cnet.com/culture/can-an-algorithm-win-your-fantasy-football-league/,CNET,2012,7,18,535.0," Human judgment hasn't done much for human development over the last, say, 50 years. This has allowed machines to take over and begin to dictate. You'd think that there might be limits. You'd think that politicians were selfless. The latest machine creation which tries to squeeze the human mind and reduce it to a grape pip is Artificial Intelligence software that picks the perfect fantasy football team. This is the brainbaby of three academics from the University of Southampton in England. Lecturer in Computer Science Sarvapali Ramchurn, student Tim Matthews, and visiting researcher and George Chalkiadakis have cobbled together software that, they say, already places it in the top 1 percent of fantasy football players. Yes, the sport they call football is called soccer by those who don't quite get it yet. And yes, their local team, Southampton is -- in real life -- not terribly good. But so certain are these three wise men that it will turn even the weakest fantasy player into something of a genius that, according to Phys.Org, they intend to present their work at AAAI-12, the Artificial Intelligence Conference in Toronto next week. I have always worried about one part of artificial intelligence -- the artificial part. It's the part that, for some humans, spoils things. The Sun - Football (DreamTeam Fantasy Football) from Jonny Porthouse on Vimeo. The Sun - Football (DreamTeam Fantasy Football) from Jonny Porthouse on Vimeo. I understand that many, many fine brains have already tried to concoct software that will somehow predict fantasy football selections. Here, for example, is Fantasy Football Starters, a predictive algorithm that, supposedly, helps you win 70 percent of the time. This may, however, leave a taste like genetically-modified food. Oddly, though, the Southampton brains have confessed that there is a limit to their madness. They tested their software against last season's results. Fantasy Football is big business all over the world. (An example from the UK is embedded.) And the academics were pleased with their results. This year, however, they have discovered that they need an additional element. You will surely be stunned to hear that this element is the human brain. As Ramchurn told Phys.Org: ""Our previous tests have shown that a machine working on its own will perform better than millions of humans. But a machine can't take into consideration if a player is injured (and still plays), has low morale or has personal issues and may not perform at his best."" My European cup runneth over. The machine cannot entirely spoil the amusement. The machine cannot (yet) delve into the psyche of Wayne Rooney (that would be much fun) and discover whether his tooth is aching, his hair weave is slipping, or that a tabloid is about to reveal naughty happenings in his firmament. Ramchurn continued: They love humans. They need humans (a little). But they want to blame humans, should they lose. What joy will scientists such as these feel when they can truly predict everything? What will real people do? Will they be permanently sedated by the sheer nullifying boredom of life? Or will they access some embedded core of humanity invisible to science and deliberately act twice as irrationally as they currently do?"
65,https://www.cnet.com/science/googles-project-glass-you-aint-seen-nothin-yet/,CNET,2012,4,6,971.0," Google's Project Glass demo is certainly the coolest hardware demo so far this year. Behind the scenes is something equally intriguing: artificial-intelligence software. The augmented-reality glasses, which Google co-founder Sergey Brin was spotted wearing yesterday, created a huge buzz Wednesday when Google released a video showing, from the wearer's perspective, how they could be used. In the video, the small screen on the glasses flashes information right on cue, allowing the wearer to set up meetings with friends, get directions in the city, find a book in a store, and even videoconference with a friend. The device itself has a small screen above the right eye on wrap-around glasses which have no lenses. For the most part, the augmented-reality glasses do what a person could do with a smartphone, such as look up information and socialize. But the demo also shows glimpses of an artificial-intelligence (AI) system working behind the scenes. It's the AI system that could make mobile devices, including wearable computers, far more powerful and take on more complex tasks, according to an expert. ""The new thing that Google was showing was the interaction model using new hardware, rather than truly showing the potential of such a device,"" said Lars Hard, the chief technology officer of AI software company Expertmaker. ""AI can actually enhance and improve different decision situations."" Although there isn't a precise, agreed-upon definition, artificial intelligence describes computer systems that accommodate human-like behaviors, through features such as speech and gesture recognition, and mimic human thinking. Working with a mobile device, artificial-intelligence systems can perform tasks in the background and bring highly relevant information to users, Hard said. The Project Glass hardware was operated primarily by voice commands, an indicator of Google's work on voice recognition for mobile devices like Apple's Siri. Siri, which has been well-received, translates spoken commands into actions for the iPhone, such as looking up information or making appointments. Google is reportedly working on voice-recognition software for Android. The makers of Project Glass said the hardware is designed to help ""you explore and share your world, putting you back in the moment,"" according to a Google Plus post. ""We think technology should work for you--to be there when you need it and get out of your way when you don't,"" said Babak Parviz, Steve Lee, and Sebastian Thrun, three employees from Google's secretive Google X Labs, on the post introducing Project Glass. In one scene of the video, for example, the wearer takes a picture of a poster by pressing a button on the glasses and sends it to himself. This new type of user interaction is quicker than, say, pulling a phone or camera out of a pocket. The demo also shows that the software operating the glasses is location aware. A notification tells the wearer that the No. 6 subway is shut down as he walks up and the system suggests an alternate route for getting to his destination. To have a wearable computer aware of its physical surroundings and present personalized information to the user requires artificial intelligence and machine-learning software in the background, noted Hard. It turns out Thrun, a Google fellow and member of the Project Glass team, is an artificial intelligence and robotics expert who is instrumental in another Google X project, the driverless car. ""This puts Google out in front of Apple; they are a long ways ahead at this point, Michael Liebhold, a senior researcher specializing in wearable computing at the Institute for the Future told The New York Times. ""In addition to having a superstar team of scientists who specialize in wearable, they also have the needed data elements, including Google Maps."" AI in the cloudA more sophisticated AI platform with a wearable computer could do much more than find friends online and provide maps, said Hard. Having wearable screens could help doctors make diagnoses, be used in business negotiations, or in service industries, such as retail, Hard said. Although an augmented-reality screen is smaller than a smartphone, it has the potential to present the ""right information at the right time"" and show complex data such as diagrams, he said. The hope for AI software is that it will process information in the background and present targeted information as needed, he said. In shopping, for example, the AI system would sift through lots of data to come up with very granular and personalized recommendations, rather than recommendations based on past purchases as computers do today. ""Even though the technologies today deliver this type of service, they are relatively crude and boring in many respects,"" Hard said. ""We're going to see lots of changes to that, using big data and machine learning."" Another Project Glass contributor, Babak Parviz, is a bionanotechnology expert at the University of Washington who foresees wearable devices used for medical diagnostics. In 2009, he wrote an essay at IEEE Spectrum describing how augmented-reality contact lenses could be equipped with biosensors to detect and communicate information on blood sugar levels from eye fluids. Judging from the enthusiastic reception of Project Glass, wearing augmented-reality glasses may become the ultimate fashion statement for technology fans in the near future. But there are plenty of skeptics who fear what a poorly done system would look like. A video released yesterday from Tom Scott called Google Glasses: A new way to hurt yourself, showed a steady stream of information distracting the wearer and the voice recognition backfiring. Another video from Rebellious Pixels superimposes ads, based on Google searches, on Google's demo video popping up incessantly. Apple's Siri has given millions of people their first taste of the artificial-intelligence concept where a digital personal assistant does a few tasks and provides an alternate interface to touch or typing. Now with Google's Project Glass we get a hint of the potential of bringing that AI to a wearable device."
66,https://www.cnet.com/culture/play-ball-heres-your-mlb-2012-gaming-roundup/,CNET,2012,4,5,1002.0," Spring is in the air so you know what that means: the start of the 2012 baseball season. It's also the time of year where two game developers are competing for your hard-earned cash. So what's the best baseball game to pick up this year? That will all depend on which console you own. Those with a PlayStation 3 have a choice, either MLB 12: The Show or Major League Baseball 2K12. On the other hand, Xbox 360 owners are locked in to only one title to satisfy their baseball itch. Unfortunately for the latter, this year's best baseball game is exclusive to the PlayStation 3. This year also marks the first MLB game for the Vita, which is mostly a PS3 console port of The Show. We'll be taking a look at each title in the lineup so it's crystal clear how we arrived at our conclusion. Visual Concepts' Major League Baseball 2K12 doesn't do enough in the way of innovation, something we complained about last year. There's been a noticeable improvements in animation and commentary, but the game is still plagued by a laundry list of shortcomings that fail to create the illusion of a real baseball game. Our favorite addition to the game has got to be MLB Today Season mode, where you'll play each game of your favorite team's season as they do in real life. If you miss a game on a scheduled day, the real team's outcome will be recorded instead. Pitching is once again the focal point of the game, with a few minor tweaks that make it easier to understand what kind of rhythm your player is settling into. It's not enough to make it less of a frustrating experience, though, as it remains one of the most inaccessible elements of the entire game. We applaud the game's attempt to prevent those who try and use the same pitch over and over, but such focus on one side of the game leads to an imbalance overall. There are still plenty of mindless AI mistakes in 2K12 in addition to graphical setbacks. Occasionally the AI will even prevent you from making a play, which is among the more frustrating experiences one can have with a baseball game. For Xbox 360 owners who need to fill the baseball void this season, 2K12 can provide some moments of enjoyment. It's just upsetting to see that it still represents a lot of what we disregarded last time around. Moving on to MLB 12: The Show, we were a bit surprised to learn that both pitching and hitting were getting updates. The introduction of ""zone batting"" is smart, giving players a chance to guess the general area of the pitch -- almost betting on it. If guessed correctly, the result is better contact, which means a base hit is more likely. New to pitching is ""pulse pitching"" which forces players to time a delivery in sync with an expanding and contracting aiming reticle. The smaller the circle, the more accurate the pitch. Regardless of whether you like what's new, there's the option to revert back to previous control schemes. On the visual side of things, The Show impresses in almost every area. The baseball acts much more naturally this year, and contact with the bat seems more genuine overall. Of course the game isn't without an awkward moment or two, but simply put, The Show is able to achieve a sense of baseball realism that the competition cannot manufacture. So does the Vita's version of the show feel as good as its big brother on the PS3? The Vita had a couple of impressive sports launch titles, FIFA and Virtua Tennis, and the Vita version of MLB 12: The Show joins their ranks. While Sony's stripped out some of the in-game commentary, replays, and the Diamond Dynasty mode (an online card-collecting feature), at its core the Vita version is basically the same game as what you get on the PS3 -- just shrunken down. Stadiums and player models look great (fans aren't terribly detailed, but that's a minor gripe), and that realism translates over to both player and ball movements on the field as Sony's continued to tweak the physics engine (balls can now bounce off bases and shift directions). Like the console version, you can raise and lower the degree of difficulty for batting (rookies can opt for a simple timing mechanic, whereas veterans can challenge themselves with Sony's total analog scheme). For pitching,  all four control schemes from the console are available, including meter, classic, analog, and the new pulse pitching mode. Neither of us is a great fan of the new mode, but the Vita version does throw in a good mix of touch-screen controls for both gameplay and navigation and incorporates the back touch panel pretty well (you can, for example, perform pick offs by touching the back panel). The PS3 version's Road to the Show and robust franchise modes also make their way into the Vita version. Sony's touting the game's cross-platform feature; you can save a game you're currently playing on the PS3 and continue it later on the Vita, which is pretty cool. Of course, you'll have to buy both versions of the game to do that. Multiplayer? Yes, it's here, too. That said, you do have to dig around in the menus a little bit to face off against a friend remotely (setup is a little convoluted). We encountered some slight lag from time to time, but we can't complain too much about the online experience, and it's fun to talk trash over your Vita, particularly if you're playing a Yankees fan. Bottom line: If you've already played the console version of MLB 12: The Show, the Vita version isn't going to seem all that different. But that's what's so impressive about it, anemic sound and commentary notwithstanding.  In fact, if you're a video baseball junky, it's awfully tempting to buy the Vita just for this game -- it's that good."
67,https://www.cnet.com/science/robot-companies-come-to-bury-c-3po/,CNET,2012,4,3,767.0," If the word ""robot"" conjures up for you machines with two eyes and four limbs, it's time to think again. The robots having a commercial impact today have little to do with C-3PO, Terminator, Rosie the Maid, or other humanoid robots from popular culture. Instead, working robots are surprisingly diverse and, rather than mimic humans' every move, focus on a few very specific tasks. For a sign of how robotics is shaping up, consider Amazon's $775 million purchase of Kiva Systems last month. Kiva's machines are designed to navigate warehouses to collect products and automate order fulfillment for e-commerce shipments. Hence Amazon's interest. Kiva's robots, which look like orange-colored bumper cars, are just the tip of the iceberg when it comes to workhorse robots. The U.S. military has deployed thousands of portable robots, which resemble miniature tanks, to search for explosives and perform other dangerous tasks. iRobot has sold millions of floor-cleaning Roomba robots. Surgeons have been leaning on robotics for minimally invasive operations for years. And the auto industry started using robots for manufacturing as far back as the 1960s. If you look a little further, though, you see the core ideas of robotics, including machine learning and automation, are seeping into an ever-wider set of uses. Makani Power is building a wing-shaped wind turbine that relies heavily on autonomous control. Why? The turbine, which flies in circles like a kite to generate power, needs to decide without human intervention when there's sufficient wind speed to go aloft or to come down and set itself on a perch. ""It is a robot, 100 percent,"" said CEO Corwin Hardham. The U.S. Navy, meanwhile, wants a robotic aircraft that can land on and take off from an aircraft carrier autonomously. It's working with Northrop Grumman on a pair of X-47B prototypes that have made their first few flights on land. The Navy hopes to kick off its first carrier tests in 2013. Even agriculture is ripe for robotic automation. Stanford University spin-out Blue River Technologies is developing a device that scans a row of vegetables to determine whether a plant is a weed or, say, head of lettuce. Once identified, the machine, pulled by a tractor, will spray the weed, explained CEO Jorge Heraud. ""When you think about robotics, you may think of military applications or other problems, but we think we can have a nice impact in agriculture,"" he said, adding that organic vegetable farming needs automation to scale and keep prices down. Human-robot collaborationEven with more entrepreneurs gravitating toward robotics, it remains a challenging field in which to find business success. A number of technical issues, such as computer vision, remain difficult to solve, and the industry lacks common platforms for developing software for machines, experts say. And it's always a challenge to divine which products will resonate in niches where they haven't been used before. iRobot, for example, envisioned that its military ground robots could be used for cleaning up nuclear power plants. But it wasn't until the Fukushima disaster last year that iRobot sold four units to measure radioactivity levels. Earlier this week, it sold three robots to utility Progress Energy in the U.S. Making robots look and act like humans remains an active field of research, and certainly characters such as C-3PO of ""Star Wars"" fame have inspired countless engineers to work in artificial intelligence. But robotics is creeping into commercial products in more subtle ways, such as cars that use radar to alert a driver to a potential collision, or self-driven hospital carts that can deliver drugs and supplies. ""Users just want to get a task done. They don't care if it's a cool robot. You may, but they may not care if it's a robot at all,"" Rodney Brooks, robotics pioneer and CTO of manufacturing company Heartland Robotics, said at a recent conference. While many robots are indeed meant to replace manual labor, there's a movement to have robots work side by side with humans. Heartland Robotics, which remains in stealth mode, is working on robots to work alongside humans in manufacturing. Last fall, the National Institute for Standards and Technology launched a standards and testing effort to ensure safety of people working next to robots. Robots are gaining better sensing capabilities, which make the prospect of human-robot collaboration realistic for more tasks, said Julie Shah, a professor of robotics at MIT's Computer Science and Artificial Intelligence Lab. ""It's an enormously exciting time to be working in the area of human-robot collaboration,"" she said. ""People are beginning to see if you make robots intelligent enough to work with people, everyone benefits."""
68,https://www.cnet.com/reviews/sony-xba-nc85d-review/,CNET,2012,3,30,1037.0," Balanced-armature (BA) designs produce lower distortion and cleaner sound than conventional headphone driver types, which are essentially miniature speaker drivers. Sony currently makes 11 balanced-armature models, with the $499.99 XBA-NC85D noise-canceling in-ear headphones topping the line. Sony claims it's the world's smallest and lightest set of noise-canceling in-ear headphones, which sounds good on paper, but they fail to meet those expectations. I doubted the logic of Sony's design strategy as soon as I started to use the XBA-NC85D, and their sound quality falls well short of what I expect from a $499.99 headphone. With noise-canceling effectiveness marginal at best, I recommend staying away from the XBA-NC85D and spending your money elsewhere. Design and featuresThe Sony XBA-NC85D is a noise-canceling in-ear model of headphones, and considering its $499.99 retail price, you might have expected that it would look great. No such luck -- the matte-and-glossy black plastic earpieces are big and chunky, but one boon is that they omit the bulky battery case that come along with most noise-canceling headsets. That's great, but since the Sony's earpieces contain the noise-canceling electronics and the rechargeable nickel-metal hydride batteries, the earpieces are unusually bulky. I found myself constantly aware of its size, and the fit didn't feel as secure in my ears as other XBA in-ear headphones. The XBA-NC85D comes with a proprietary USB battery charger (you plug the headphones into the charger's 3.5mm jack), but you can't play the XBA-NC85D after its batteries have drained, a misstep shared with Bose's noise-canceling headphones. The saving grace with Bose headphones is you can always put in a fresh AAA battery.  The XBA-NC85D's batteries, on the other hand, are not user replaceable, and can only be juiced with the included charger. If you forget to bring the charger on a trip, the XBA-NC85D will become unplayable after the batteries drain. This sort of proprietary approach is a deplorable design choice, and Sony should at least supply two chargers with a $499.99 headphone. Of course, one key advantage of the USB charger is that you can refresh the XBA-NC85D's batteries from your laptop, and they provide up to 20 hours of playing time on a single charge. I requested a service estimate for a replacement cost for the XBA-NC85D's batteries, but Sony never got back to me. You might be on your own when the batteries no longer hold a charge (which might take a few years), and that might be reason enough not to buy these. Sony's Artificial Intelligence Noise-Canceling circuitry has three modes: NC Mode A for planes, NC Mode B optimized for buses and trains, and NC Mode C for office noise. NC Mode selection occurs automatically and Sony claims Artificial Intelligence Noise-Canceling ""reduces up to 97.5 percent of ambient noise."" Sony also tells me that the XBA-NC85D's nonadjustable digital equalizer produces an ""ideal frequency response"" for great sound with all types of music. I'm surprised that the XBA-NC85D only includes three sets of silicone eartips. That reduces the chances of achieving the best possible fit compared with the least expensive XBA headphones, the XBA-1 ($79.99), that comes with four sizes of silicone eartips and three sets of ""noise-isolating"" tips. Most luxury in-ear headphones come with an even wider assortment of tips. Worse yet, the tips don't secure a tight fit to the earpieces, so they slipped off a number of times during my review period. That's not an uncommon fault with in-ear designs, but the XBA-NC85D's looser fit was worse than average. If you don't wind the XBA-NC85D's extra-long (74-inch) cable around the supplied ""cord adjuster"" (a flat, black plastic spool), the cable will be prone to tangle. The tiny box with the headphones' power button is on the cable, just 4 inches away from the left earbud, which you can't see when the 'phones are in your ears. You have to feel around, searching for the power button. The cable terminates with an L-shaped 3.5mm gold-plated plug. On a more positive note: the large ""L"" and ""R"" markings on the earpieces are easy to see, and in low light situations you can feel a small dot on the left earpiece. One significant feature missing from the XBA-NC85D, however, is an inline mic with controls for Apple or Android devices. You also get a wire clip to secure the cable to your shirt, an airline adapter, and a zippered faux-leather carrying case. PerformanceI was unimpressed with Sony's Artificial Intelligence Noise-Canceling abilities on the NYC subways and buses; it produced little noise-reduction effect. The passive (non-electronic) noise reduction of the silicone eartips rendered average results, and turning on the XBA-NC85D made only a small difference. I wasn't aware of the noise reduction system switching between MC Modes as I moved about the city. When I listened to the XBA-NC85D at home, I heard a small amount of background noise generated by the Artificial Intelligence Noise-Canceling processing, which always has to be turned on to use the headphones. The owner's manual acknowledges the presence of the XBA-NC85D's ""operational noise,"" which they consider normal but I could even hear the distracting sound on the subway in heavy foot traffic. I've heard that noise with other noise-canceling headphones, so I should note that Sony isn't the only brand with the same issue. The XBA-NC85D has just a single balanced armature driver in each earpiece, which I found odd because Sony's less expensive XBA-2, XBA-3, and XBA-4 headphones have two, three, and four balanced armature drivers per earpiece, respectively. The XBA-NC85D's sound was far behind what I heard from Sony's XBA-4 in-ear headphones. The XBA-NC85D's single balanced armature couldn't generate anything like the XBA-4's bass punch or power. Its problems aren't limited to bass effort, either -- the XBA-NC85D's treble had a gritty harshness, and the sound would severely distort if I played the headphones really loud with bass-heavy music; it certainly didn't sound like an expensive headphone. The XBA-NC85D sounded a little more natural than Sony's $99 XBA-1iP in-ear headphone! ConclusionSadly, the XBA-NC85D falls short on every count: noise canceling, features, and sound quality. Compounded by its high cost, the XBA-NC85D remove themselves from any serious audiophile's buying considerations. Try the $230 Sennheiser CXC 700 for a classier, more affordable in-ear noise-canceling alternative."
69,https://www.cnet.com/science/judea-pearl-named-2011-winner-of-turing-award/,CNET,2012,3,15,441.0," In becoming the 35th person to win the prestigious $250,000 award from the Association for Computing Machinery, the 75-year-old Pearl was honored with what is considered to be the highest prize that the computing industry has to offer. ""Of course, I'm delighted that people in your field and outside appreciate what you did, that recognize the work that I do in my little niche,"" Pearl said in an interview. ""It's very rewarding."" A professor emeritus at the University of California, Los Angeles, Pearl is recognized as a leading thinker in the study of machine reasoning.  Pearl has often been cited for his work in the relationship between cause and effect. Among other things, he's credited with coming up with a way to use Bayesian networks to research machine learning. This modeling tool, named after a famous 18th century English mathematician, was cited as offering  a ""critical step toward achieving human-level AI that can interact with the physical world.   Pearl was also recognized for earlier work in a method of problem-solving known as heuristic search as well as for his contributions in the field of causality. Vint Cerf, who chairs ACM's Turing centenary celebration, lauded Pearl's impact, saying his research had had influences in related realms including natural language processing, computer vision, robotics, computational biology, econometrics, cognitive science, and statistics. ""His accomplishments over the last 30 years have provided the theoretical basis for progress in artificial intelligence and led to extraordinary achievements in machine learning, and they have redefined the term 'thinking machine,'"" according to Cerf. A couple of side footnotes to the award. Pearl is the father of former Wall Street Journal South Asia bureau chief Daniel Pearl, who was kidnapped and slain in 2002  by al Qaeda terrorists in Pakistan. Also, Alan M. Turing's 100th anniversary is slated be celebrated this June.  Turing was one of the seminal thinkers in the history of modern computing. Pearl, who said he first got interested in how to handle uncertainty in computer systems early in his college days, offered a guardedly optimistic prediction of where current research is likely pointing. ""I don't see any basic impediments to intelligence on machines,"" he said. ""I think eventually we will have automatic systems that can put together experimental data from thousands of different sources conclusions and come up with recommendations and justifications.""""Anything that that had something to do with uncertainly, if you frame it, you get a plausible answer,"" Pearl added. ""You don't have surprises. The only question has been computation, how to implement it correctly on a computer."" Pearl will receive the Turing Award on June 16 in San Francisco during the annual ACM banquet."
70,https://www.cnet.com/tech/tech-industry/should-siri-be-jealous-of-voice-recognition-competitors/,CNET,2012,1,13,873.0," LAS VEGAS--Looks like Siri was just the beginning. Okay, even Siri wasn't the beginning. The ability to do voice-command isn't particularly new, but the marquee feature for Apple's iPhone 4S has gotten the masses to recognize and appreciate its benefits. For the first time, voice-command was a feature people talked about and coveted. At CES, there were better implementations and voice-commands popping up on different devices. Big-name companies got into the mix. Dieter Zetsche, head of Mercedes Benz, said voice would play a major role in its cars, calling them a driver's ""digital companion."" Ultrabooks will eventually be getting speech recognition built in. Manufacturers from Samsung Electronics to Lenovo are integrating the feature into their high-end televisions. Indeed, using speech to control a TV was a major trend of the show. Vlingo, which makes a virtual speech assistant for smartphones, announced its ""Vlingo for Smarter TVs"" software, which it plans to embed into televisions and set-top box. Nuance likewise announced its Dragon TV platform, which is believed to be powering the new voice-and-gesture-controlled Samsung TVs. But this is just the beginning. The voice-recognition companies are looking to get the feature in every electronic device. They also want to get to the point where these virtual assistants follow you from device to device in a consistent manner, so your preferences move where you move. ""Everything you see in 'Star Trek'--it's going to be real,"" said Matt Revis, vice president of product management for Nuance's mobile division. For that to happen, and for consumers to truly gravitate to speech, there still needs to be more education out in the market. ""For two and a half years, we've shipped various products that were equivalent or better than Siri, but a start-up is hard to make a market,"" said Vlingo CEO Dave Grannan. ""People really don't know what they need, they need to be shown."" Nuance has agreed to acquire Vlingo in a deal that is expected to close later this year. Natural dialogue the key While voice commands have long been used by various industries, including automated help lines and older cell phones, Siri brought attention to an advancement of voice-recognition: the ability to understand our natural language and respond in kind, so Siri really does seem like a person. That's managed to turn it from a utilitarian tool to something you want to use. ""Speech initially was used because it was more convenient,"" Revis said. ""But Siri is fun. You're engaged."" Over the past few years, there have been huge advancements in voice recognition. Vlingo, for instance, has done a lot of work in improving the artificial intelligence and natural understanding, Grannan said. You can say, ""I want to get wasted"" and have its program find you local bars, he added. Vlingo's TV service allows for voice commands to work like a dialogue. Make a request, and it will ask a question back to narrow down your choices. There's a back and forth that continues until the user finds what he is looking for. The company is working with one smart-TV manufacturer, and one European cable provider, Grannan said, adding he expects products to come out by the end of 2012. The various companies say that given the intense processing power required for AI, basing the services on the cloud is key. While the microphone picks up your comment on your phone, car, or TV, much of the heavy lifting occurs on the back-end at Nuance, Vlingo, or Apple's servers. Beyond just a remote control That cloud will enable these companies to put voice recognition on just about any connected device. Revis said he envisions every electronic device being able to run these features, and it appears we're pretty close to it. Samsung, for instance, showed off several connected appliances including refrigerators and washers that could easily integrate a Siri-like capability down the line. Likewise, it has opened up its smart TVs to developers in the hopes they better take advantage of the gesture and voice controls. ""There are a lot of user scenarios you can dream up,"" said Joe Stinziano, senior VP of home entertainment marketing for Samsung. ""We're opening it up to a developer community that has proven it can be more creative than the original manufacturer."" One such scenario involves asking the refrigerator about milk, and having the appliance detect via barcode scanner whether there is any left, or to set a reminder about buying more milk later on, Grannan said. Large tech companies such as Sony, Samsung, or Apple are at an advantage because they make so many of the products that consumers use. The large Asian conglomerates, for example, making smartphones, PCs, appliances, and even home heating and cooling systems, could all integrate voice. Nuance, meanwhile, is hoping to get voice commands in more apps. The company has released a software development kit to allow other programmers to integrate its service into their own apps. Revis noted that Amazon Price Checker and Merriam-Webster Dictionary apps used its technology. As for the further proliferation of voice commands in more products? We're probably still a little away from barking orders at our microwave. ""I think we'll see a couple of iterations and won't see mainstream until the end of 2013,"" Grannan said."
71,https://www.cnet.com/tech/tech-industry/a-star-trek-inspired-x-prize-for-revolutionizing-health-care/,CNET,2012,1,10,544.0," The ""Star Trek"" universe may be beloved by millions, but it's entirely fictional. Yet one element of Gene Rodenberry's timeless creation may actually help people with their health care decisions in real life. The problem faced by millions of people around the world, especially in the third world, and in rural areas of the first world, is that there's not always a doctor around to help figure out what's wrong with you--and sometimes, one isn't even necessary. Sometimes, the right technology could help us determine what's going on in our bodies. That's the rationale behind the latest X Prize, known as the Qualcomm Tricorder X Prize, which was announced at CES today, and which will award a $10 million bounty to the first team that can create ""a mobile platform that most accurately diagnoses a set of 15 diseases across 30 [patients] in three days."" The new prize is a collaboration between the X Prize Foundation and the Qualcomm Foundation. The winning teams, according to a release about the project, will ""leverage technology innovation in areas such as artificial intelligence and wireless sensing--much like the medical Tricorder of Star Trek fame--to make medical diagnoses independent of a physician or health care provider. The goal of the competition is to drive development of devices that will give consumers access to their state of health in the palm of their hand."" Teams hoping to win will also have to ""deliver this information in a way that provides a compelling consumer experience while capturing real time, critical health metrics such as blood pressure, respiratory rate, and temperature. The winning solutions will enable consumers in any location to quickly and effectively assess health conditions, determine if they need professional help and answer the question, 'What do I do next?' when it comes to their health."" This X Prize has been in the works for some time, though it's not known how long Qualcomm has been part of the equation. In 2010, X Prize Foundation CEO Peter Diamandis told CNET about the organization's plans for what was then being called the AI Physician X Prize: The implications of that are that by the end of 2013, 80 percent of the world's populace will have a cell phone, and anyone with a cell phone can call this AI and the AI can speak Mandarin, Spanish, Swahili, any language, and anyone with a cell phone then has medical advice at the level of a board-certified doctor, and it's a game change. Clearly, then, Qualcomm is the benefactor that Diamandis was talking about at the time. Other X Prizes, like those to make the first privately funded spaceship, have been successful, so there's little doubt there will be a large number of teams that seek to win the new prize. How long it will take is anyone's guess. But Diamandis has told CNET that the general idea behind a full X Prize--versus X Challenges--is that it will likely take between three and eight years for a team to come up with the winning formula. Given that health care is such a problem the world over, and that many people go either undiagnosed or rush to the hospital unnecessarily, here's hoping that the winner of this prize comes along sooner rather than later."
72,https://www.cnet.com/tech/tech-industry/startup-secret-no-10-your-app-needs-a-personality-makeover/,CNET,2011,12,19,334.0," ""AI is the new UI."" I slipped this quote into my writeup of Path 2.0 last month, but I wanted to call it out here because it's potentially a very important trend in app design. Morin dropped this line on me when I asked him about Apple's Siri, which is a major breakthrough. Not because it's the first voice-recognition app (it isn't) or the first smart agent app (ditto). Rather, it's good enough at what it does and it's being pushed so hard by Apple that consumers are now coming to expect apps to be smart, and smart-aleck, too. Apple may even have over-promised on Siri, but the genie is out of the bottle. Consumers are beginning to expect new apps to be Siri-like (the Siri they see on ads, that is) wherever it makes sense. If you have a calendar app and the user types in, ""Lunch with Bill a week from Friday,"" your app had better know what that means. That's table stakes. It should also ask which Bill you mean and possibly recommend a location based on your past experience with Bill. User will soon expect that modern apps make smart decisions about what you want based on not just what you ask for, but who you are, who you know, where you are, how fast you're going, and so forth. What surprised me is how open people appear to be to wiseacre apps. Ask Siri the same question three times, and you'll get the answer three different ways. Morin's Path, likewise, will report on your sleep behaviors differently each day, just to keep your Path social net friends engaged. There is a whole new branch of user interaction design and AI that's going to emerge: app personality development. Startup Secrets is based on personal interviews with people building companies and from their blog posts and news stories. Subscribe to Startup Secrets on Twitter or come back to Rafe's Radar every day for a new one. See all the Startup Secrets."
73,https://www.cnet.com/tech/mobile/googles-project-majel-gets-more-interesting-by-the-day/,CNET,2011,12,16,505.0," Ask any honest Android fanboy what he envies about the iPhone 4S and chances are good that he or she will menton Siri. And since we're speaking honestly, I'll be the first to admit that this is one feature that I wish my Android could do. Sure, there are plenty of apps vying for the ""Android version of Siri,"" but none of them are as quite as well-rounded as the iOS app. Thankfully, we should have an official client on the way as rumors of a ""Majel"" project began picking up steam this week. Factoring in the early details uncovered by Android And Me and one very recent acquisition, it appears that Google is wasting no time in bringing about a rebuttal. In the few days since the first information came to light, Android And Me has obtained new tips that paint the picture of a fast-tracked project. One particularly interesting detail comes from a source who claims to have spent time with an early release. Their tipster advises that it was ""definitely as good, or better, than Siri"" and that the version that he used was tablet-based. The source goes on to describe trays of results, which can be swiped away or selected based on what the user was looking for. A second, anonymous Google employee indicates that over the last few years Google X's focus has been based around a supersmart AI robot that leverages the tech behind a number of popular Google programs. Described as being ""the most amazing thing"" he'd ever seen, the AI had passed the Turing Test 93 percent of the time over the course of an hour long IM-like conversation. Skip to around 5:13 in the video below to see exactly what type of questions Google expects to be able to handle in the future. I don't know about you, but this sounds like a more-than-viable alternative to Siri. Android And Me goes on to pull together quotes from industry players including Google's Manager of Speech Technology and Google's computer-interface designer and user-experience lead. I find it interesting to look back at all of these statements and interviews as many of them feature references to Star Trek. As CNET blogger Lance Whitney pointed out earlier this week, Majel Barrett-Roddenberry provided the voice to the computer system from the television series. Has Google been leaving bread crumbs to the project all along, hiding it in plain site? As I read through the comments from the various outlets covering the Majel project, I sense a growing excitement over an official Google product of this scale. If there's any name that could power a new search and AI system for Android, it would be Google. It is expected that Majel will find its way into the market in early 2012 where I assume it will be offered as a standalone application. It would not surprise me to see Google unveil the next release of Android (Jellybean) in June at Google IO and announce that Majel is integrated into the platform itself."
74,https://www.cnet.com/tech/services-and-software/google-gobbles-up-restaurant-recommendation-app-alfred/,CNET,2011,12,14,273.0," Among the crowded list of mobile applications, Google has added another to its portfolio, acquiring local recommendation app maker Clever Sense. Clever Sense created released its butler-like app, named Alfred, for the iPhone in July. The app is designed to learn the kinds of restaurants that a specific customer likes, then recommend similar joints based on Web reviews and other analysis. But it isn't just about matching a taste for Italian food with another pasta restaurant. Unlike better-known and more widely used rivals such as Yelp and Urbanspoon, Clever Sense uses artificial intelligence to find customers with similar tastes, then offers recommendations based on their dining choices. It's the kind of service--one that relies on complex algorithms to arrive at relevant results--that is right up Google's alley. ""The Clever Sense team is at the forefront of developing a recommendation engine that connects the online and offline worlds by delivering personal and sophisticated information to users at the right time, the right place, and within the right context,"" Google said in a statement. ""By combining their technology and expertise with our team and products, we'll be able to provide even more people with intelligent, personalized recommendations for places to eat, visit, and discover."" For Clever Sense, Google adds deep pockets and global reach. ""Together with the Google team, we will accelerate our efforts toward this shared vision. Google helps local businesses connect with potential customers, and its worldwide presence can bring the value of Clever Sense to a much larger audience,"" Clever Sense co-founder and CEO Babak Pahlavan wrote on the company's Web site. The company launched an Android version of Alfred last week."
75,https://www.cnet.com/culture/google-x-shows-dogged-determination-for-far-out-research/,CNET,2011,11,14,852.0," There's a constant tension at Google between fast-moving, nimble, disruptive projects and the more plodding established business. Sometimes the entrenched part of the business comes out ahead, as when Google canceled Google Labs. But attempts to nurture a start-up ethos within the company continue, this time with a project called Google X digging into advanced robotics and more. Word of Google X had bubbled up in recent months, via MG Siegler, formerly of TechCrunch, and Nicholas Carlson at Business Insider. But New York Times' Clair Cain Miller and Nick Bilton have just delivered a lot more detail with a collection of tidbits about Google X. According to the Times report, Google X research takes place at an undisclosed location and tackles assorted subjects a little or a lot beyond today's technology. Among its projects: Google's self-driving cars, space elevators to haul cargo into orbit, and that hoary old chestnut of Internet-enabled fancies: the Net-connected refrigerator. Those who dreamed up the 100 projects under way at Google X include company co-founders Sergey Brin and Larry Page, Executive Chairman and former Chief Executive Eric Schmidt, and other executives. Brin is actively involved in the project, the reports said. Among those the Times said are working on projects are artificial intelligence and robotics expert Sebastian Thrun; machine learning expert Andrew Ng; and Johnny Chung Lee, who helped develop Microsoft's Kinect game controller after working on extensive hacks built on Ninendo's Wii controller. Ng and Thrun also are Stanford researchers, and Lee apparently is working on Google's research with the ""Internet of things,"" a years-old idea concerning the connection of many more devices than just computers to the Net. Early-stage research is something of a rarity in the corporate world, but Google has long cherished the ideal of tackling problems thought to be impossible to solve. One interesting facet of the Google X projects revealed thus far is that they involve hardware, not just software. Google is regularly derided for being unable to expand its revenue sources much beyond its original commercial success: online search. But I give the company credit for a lot of serious work in big projects that are a real departure from the status quo. Google Maps, for example, never ceases to astound me. This weekend, I plotted a visit to a small bed and breakfast in Normandy using Street View. Google's all-seeing cars for mapping haven't made it everywhere yet, but they've penetrated rural France. And Google's free, global maps are growing in sophistication with 3D buildings and crowdsourced data-gathering in many areas. It may not be a cash cow like search ads, but it's a monumental challenge to gather and distribute the data, and Google does it better than anyone else. Android, too, remains impressive. Even if Apple paved the way for it with iOS, Google has done a tremendous amount of real programming work to make Android real. It's got plenty of warts, but in my book it's a success overall, both as a product and in Google's ambition to unlock the power of the mobile Internet. Other efforts are closer to home for the company. Chrome has secured a beachhead in a market where nobody thought we needed another browser. Google Apps puts a cloud-computing twist on the old ideas of e-mail and word processing, with the demerits of its sometimes-primitive interface offset by steady improvements and the fact that millions use it. The Chrome OS is a toy for now, but its future looks a bit brighter every time a new Web standard shows up, a new online service arrives, or a browser speed boost gets Web programmers to move a native app to the Web. And let's not forget YouTube. It hasn't pushed aside the dominant makers and distributors of premium content but nevertheless has proved online video to be in the same league as e-mail. Note also that YouTube generates a lot of ad revenue nowadays. Plenty of Google attempts to disrupt industries have been duds thus far, of course. Google Books, Google TV, and Google Music are weak. Google Buzz flopped, despite being anchored to the very successful Gmail. And Google+, which I wouldn't call a dud, nevertheless remains at this stage an evolutionary change following Facebook's lead rather than a revolutionary project in which Google is rewriting the rules. Google Translate shows a lot of promise but delivers embarrassingly bad and sometimes downright wrong answers at times. Google X projects, though, seem a lot more ambitious. They're apparently much further afield than just whipping up a new programming language or laying fiber-optic cable in Kansas City, though. That means a couple things. First, the failure rate can be expected to be even higher than with Google's ordinary projects. (I'm one of those people who respects Google for trying things like Google Wave, even if it's embarrassing when they flop, because taking risks is hard for a big company.) Second, commercialization or even public prototypes will be even more distant. If you think the world isn't ready for Google Health, think about what's required to build a 90,000-mile tether that reaches beyond geostationary orbit."
76,https://www.cnet.com/reviews/sony-mdr-nc200d-black-review/,CNET,2011,10,31,570.0," The first thing you'd probably ask me if you were going to buy these active noise-canceling headphones is whether they're better than the the Bose QuietComfort 15 or Bose QuietComfort 3 models. The short answer is no.  I found the Bose headphones slightly more comfortable to wear, and they effectively block out more noise than the Sony headphones. On top of that, I'm more partial to their sound, but those Bose models cost $300 and $350, while the Sony MDR-NC200Ds come in at $200. The price difference is significant, and enough to make the Sony NC200Ds a decent value by comparison. Not a bargain, but decent. What'd I like about them? They're relatively compact, lightweight and good-looking. In that sense, they're more like the QuietComfort 3s, which are also an on-ear model, meaning the padded ear cup sits on top of your ear and doesn't envelope like an over-the-ear model such as the QuietComfort 15. The build quality here also seems tough enough for daily commuters, and the headphones fold up to fit into a nicely designed protective storage case (it's included along with a two-pronged adapter for airplane use). What's also nice is that unlike the Bose models, you can actually listen to music or movies without the noise-cancellation engaged, which means that if the single AAA battery that powers the active noise cancellation dies, the music doesn't die with it. Alas, there's no integrated microphone for making calls, but the NC200Ds feature an AI (Artificial Intelligence) noise-canceling function that switches between A, B, and C noise-canceling modes based on the intelligent analysis of ambient noise in your listening environment. It's a little tricky to test, but the AI button rendered the noisy fan in my office nearly silent, although not completely. I also tried the headphones out in the New York City subway. Here, they muffle train noise, but the Bose Quiet Comfort 15 headphones do a better job, which we expect at their extended price point. The Sony NC200Ds' left earcup features a Monitor button alongside the Artificial Intelligence switch that allows you to hear what's going on outside your 'phones without taking them off. Of course, you could always just slip them off your ears and let them rest on your neck, but it's comforting to know that the feature exists, and it works. Overall, I liked the sound of the headphones, though it's worth noting that I actually liked the sound slightly better with the noise cancellation turned off. The bass goes deep with plenty of definition, and voices appear natural with adequate volume extending into the louder ranges. On the downside, to enhance the detail, the treble is slightly overemphasized and a bit too sizzly.  These are what we call ""bright"" headphones and can lead to listening fatigue with some people over extended listening sessions, like in an airplane or a long car ride. Other small quibbles include an audible hiss from the noise canceling that takes a little of the warmth out the bass and midrange (vocals). The headphones also sound a touch more hollow (canned) and less open with noise cancellation engaged. In all, though, there's a lot to like here. Excellent design, flavorful sound, and these guys cost $100 less than the Bose QuietComfort 15. At $200, they're not cheap, but comparatively speaking, they're reasonably priced and worth checking out if you're in the market for a pair of noise-canceling headphones."
77,https://www.cnet.com/culture/in-nest-labs-finally-an-apple-of-home-energy/,CNET,2011,10,25,803.0," news analysis The launch of Nest Labs and its sleek thermostat marks an important transition from green tech that only a scientist could love to something that everyday consumers may actually desire. The company came out of stealth mode today, telling the story of how two consumer electronics mavens, including the ""father of the iPod,"" decided to take on the unlikely quest of making a better thermostat. The product is just a programmable thermostat. But the genius of Nest Labs is that it decided to make the iPhone of thermostats--a device that looks cool and is smart enough to spare the owner the frustrations of unfriendly technology. Instead of pulling out an owner's manual, people simply use the Learning Thermostat by turning a familiar dial to change the temperature. After a few days, or a week at most, it will know the household schedule and create a suitable program. ""There are no great consumer products in this area,"" said co-founder and vice president of engineering Matt Rogers, who worked on the iPod and iPad at Apple. ""There's an entire generation of folks used to beautiful, intuitive products that are easy to use and you don't have to go out of your way to work with."" The humble programmable thermostat is certainly not something most people get excited about. But the impact is big: the EPA estimates that households can cut energy by 20 percent to 30 percent simply by having a set schedule. And heating and cooling is almost half of household energy use, about $1,000 a year in the U.S. The Learning Thermostat can be networked via Wi-Fi, so people can program it from an iOS or Android device. If you have multiple thermostats, they can be controlled as a group since they're networked. A motion sensor will let the thermostat ""see"" when people are usually in a room and it even sips juice from home wiring to recharge the batteries. But the power of the device is the software: the artificial intelligence to improve home efficiency automatically and the user interface to effectively communicate with the user. The display tells people how long it will take for the house to reach a temperature and shows a leaf to signify they're doing well in being efficient. That's a big step up from today's clunky white boxes. It's no wonder only a small fraction of today's programmable thermostats are actually programmed. Netscape moment?Whether the $250 Learning Thermostat or subsequent products from Nest Labs are a hit with consumers remains to be seen. But the company has already done something important by bringing Silicon Valley's best traits from consumer technology to home energy. Until now, the bulk of green technology companies have been dominated by material scientists, people who had a better idea for a solar cell or rechargeable battery. The target for many green technologies, even electric vehicles to a degree, are big businesses and utilities. Certainly, that's vital to innovation in energy. What has lacked, though, are products consumers can actually touch and help them concretely relate to their personal energy use. In the mid-1990s, people knew the Internet was a big deal but it wasn't until decent Web browsers came along that everyday people could actually get online. Nest Labs is not alone in trying to make high-tech consumer energy products or even better thermostats. EcoFactor, for example, uses cloud computing to improve the performance of programmable thermostats and consumer smart grid companies Tendril and EnergyHub are taking a similar approach. And there are dozens of business ideas around the notion of the ""Clean Web,"" or consumer apps geared toward conserving natural resources. Where Nest Labs can stand out is by making an object of desire, a gadget that's highly functional and fun to own. The company isn't talking about its future product plans, but with 100 employees and funding from top venture capitalists, it shouldn't be going away soon. Significantly, the Learning Thermostat, which runs its own operating system, can be software updated with new features over time, such as generating personalize reports on energy usage or connecting to utility demand-response programs. It will be sold at familiar retail outlets, such as Best Buy and perhaps hardware stores. The founders and employees are driven in part by the same impulse to make a positive social impact as many other green-technology entrepreneurs. At the end of the day, consumer electronics such as the iPod and iPad are ""landfill,"" said Rogers to explain why he left Apple. The area of energy efficiency is famously unsexy yet delivers the biggest bang for the buck. So far, Nest Labs has the best chance of making it more interesting and mainstream. ""We're bringing design, great technology, and artificial intelligence learning to the most unsexy industry possible,"" Rogers said. ""It's very exciting to all of us."""
78,https://www.cnet.com/tech/tech-industry/john-mccarthy-creator-of-lisp-programming-language-dies/,CNET,2011,10,25,377.0," John McCarthy, the creator of the Lisp programming language and a pioneer in artificial intelligence, has died. He was 84. McCarthy died Monday, Stanford University's School of Engineering announced in a tweet.  McCarthy invented Lisp,  a program that became the language of choice for AI, in 1958 while at MIT and published its design in the 1960 paper Recursive Functions of Symbolic Expressions and Their Computation by Machine, Part I. One of the oldest high-level programming languages (second only to Fortran), Lisp is still in use today. McCarthy said he felt there were aspects of human intelligence that could be described precisely enough that a machine could be programmed to simulate it. ""If a machine can do a job, then an automatic calculator can be programmed to simulate the machine,"" he wrote in a 1955 research proposal on the topic. ""The speeds and memory capacities of present computers may be insufficient to simulate many of the higher functions of the human brain, but the major obstacle is not lack of machine capacity, but our inability to write programs taking full advantage of what we have."" McCarthy is also credited with coining the term ""artificial intelligence"" in that proposal, describing it as ""the science and engineering of making intelligent machines."" ""I came up with the name when I had to write the proposal to get research support for the conference from the Rockefeller Foundation,"" he told CNET News in 2006. ""And to tell you the truth, the reason for the name is, I was thinking about the participants rather than the funder."" McCarthy was born in Boston in 1927 to immigrant socialist parents, who moved frequently during the Depression. During his teens, he taught himself mathematics by studying the textbooks used at Catltech when his family lived in Los Angeles. When McCarthy enrolled at Caltech, he was allowed to forgo the first two years of mathematics curriculum. He would earn a bachelor's degree in mathematics from Caltech in 1948 and a Ph.D in mathematics from Princeton in 1951. McCarthy joined the Stanford faculty in 1962 after short appointments at Princeton, Dartmouth and MIT. He retired in 2000. McCarthy won the Turing Award from the Association for Computing Machinery in 1972 and the National Medal of Science in 1991."
79,https://www.cnet.com/tech/tech-industry/look-out-google-siri-is-poised-to-take-apple-into-search-q-a/,CNET,2011,10,20,980.0," Yet entertainment value aside, Siri is serious business; and depending on how Apple plays it, Siri gives Apple a big opportunity to go after Google's core business of search. That, at least, is the opinion of Gary Morgenthaler, a partner at the venture capital firm Morgenthaler Ventures in Menlo Park, Calif. Morgenthaler was the first investor in Siri, sat on its board until it sold to Apple in April 2010, and was on the board of voice-recognition company Nuance Communications, which provides the underlying technology for Siri. I asked Morgenthaler to make his case for Apple's opportunity. Q: First off, what makes Siri better than other voice-recognition systems? Morgenthaler: There's hardly any comparison. Siri is built on top of the Nuance speech recognition engine, which they use and licensed. But Siri adds a layer to it. What is different is that Siri brings this natural language understanding and various pieces of artificial intelligence that allows it to understand what you meant rather than simply recognize words and convert it into text. That makes it the most advanced artificial intelligence commercially available on the planet...It really represents the way I believe most people will interact with computers in the future. So the ability simply to talk to the computer, or a phone, and get useful answers is where the search opportunity comes in. Morgenthaler: Search is limited.  When you search, what you want back is not a million blue links. What you want back is one correct answer. Siri, because it has the semantic layer, is not just responding to keywords; it's responding to a conceptual understanding of what it is that you said. And therefore it's able to retrieve for you exactly the right information you want.  Or, better still, if you intend to do something with that information--to make a transaction, say--Siri could take you all the way to that transaction. That's fundamentally new and fundamentally different and it is potentially very disruptive to the search industry.  Apple is holding a very important card. What they will do with it, we'll see. This is an existential threat to Google. But it's in the very early days. But Google has voice recognition features as well. Morgenthaler: Right now they have Google Voice Actions. It's to send a text message or call so and so. It's a variety of automated actions. So if you know exactly the function and exactly the keyword, you get a planned response...It doesn't provide a natural language understanding so you can say the intent any way you want...That's the breakthrough here, and it keeps Apple ahead several years. Also, Siri is designed in way that can expand the functionality of the system. So Apple could pretty quickly add more capabilities? Morgenthaler: Honestly, as implemented by Apple, Siri is de-featured. Siri [before Apple integrated it into the new iOS] used to get you flight information. You could say, 'When's the next flight leaving SFO arriving JFK?"" and it would immediately pull up a whole list form United, American, and other carriers that travel on that route. Apple elected not to release that in the initial version. And that's where the commercial potential comes in.Morgenthaler: It's easy to imagine things like: Send my wife a bouquet of flowers. Buy me this book on Amazon. Book me tickets to the A's game on Saturday. And Siri did most of those things--they were up and working--and Apple didn't put them in the first release...The system is capable of all those things and many more. It sounds like advertisers should be excited?Morgenthaler: Because Siri provides that semantic layer that can take you all the way to the specific goal you are seeking, cost-per-action ads become much more achievable for service providers--in this case, Apple--and also for e-commerce sites. If you're an e-commerce site, why do you want to sprinkle ads everywhere in hopes of bringing someone in the funnel if you have someone right here who wants to make a transactions? The cost-per-action model is very powerful. If you provide a hotel booking, for example, you can get $50 for a booked room at a high-end hotel. It's remarkably lucrative if you can get all the way from intent to transaction. Well, Siri has just demonstrated it understands your intent. And if you apply that to search and e-commerce transactions, that's a cash register. As it is now, Siri is billed as a personal assistant that's tied into the iPhone's features. But you're arguing that Apple could unleash a torrent of innovation by opening up the system to outside developers?Morgenthaler: That's the next leg of this revolution.  Look what happened when Apple opened the API for the iPhone gestural user interface. Suddenly, it had a hundred thousand developers and 500,000 apps. My argument is that Apple has the same opportunity to open the API to its developers so that their apps can be conversationally enabled. Apple can do this soon if it chooses. And that would excite the developer community because they all want to work on the new new thing. It's going to be years before Google can do this. You think the risk for Google is that big?Morgenthaler: Look what happened to RIM when RIM failed to follow the gestural user interface revolution. Now RIM is really also ran in the market, and I really think Google runs that risk. This is all speculative, of course. Apple has not opened the API. But Siri has incredible implications for the industry. It has incredible implications in the smartphone race. It has implications in that this interface can be brought across multiple devices in multiple contexts. And it has implications for the advertising business and Google. I'm guilty of having some skin in the game and I have some pride here. But this is a fundamental change, and I would argue that Siri, in some meaningful sense, introduces the 21st century."
80,https://www.cnet.com/culture/canon-loads-eos-1d-x-with-new-tech-hopes-and-dreams/,CNET,2011,10,18,1143.0," The tl;dr on Canon's new pro camera: Yowza. Canon's celebrating the 10th generation of its EOS-1 pro body with an overhaul, both of the hardware and the line itself. A consolidation of the fast, sports shooter with the high-resolution full-frame camera, the EOS-1D X replaces both the full-frame 1Ds Mark III and the APS-H 1D Mark IV with a single, double-grip full-frame model with fast continuous shooting. The price most closely resembles the 1Ds, though, at $6800. That opens a big gap between the please-replace-it-already 5D Mark II and the new top of the line. And I suspect that when a 5D Mark III eventually surfaces (probably not until next year), it's going to be a lot more expensive--in part because of a weak dollar and the increased costs associated with the various natural disasters that have plagued production in Asia, and in part because Canon could probably get away with it. Especially if it incorporates some of the whizzy new technology (assuming it all works as advertised, of course) that's in the 1D X. Where to begin? On the outside, you'll find an updated control layout, with some extra buttons joysticks designed to streamline shooting with the vertical grip. On the inside, there are new autofocus and autoexposure systems in addition to the de rigueur new sensor with an enhanced dust-reduction system. It has an entirely new shutter mechanism. There are new features, including a 1000BaseT Ethernet port, 3 custom settings groups (finally!), and a nine-shot multiple-exposure mode. Basically, it's a whole new camera. Check out the basic specs before I get into details: While the 18-megapixel sensor is a resolution step back from the previous full-frame models, Canon says the reduced resolution was necessary to obtain the sports-level continuous-shooting speeds--one of the primary drivers of the historical separation of the line into two bodies. The company claims that the better noise performance which results from what sounds like on-chip noise reduction and the new dual Digic 5+ processors means you'll be able to scale up images sufficiently to make up for the missing 3 megapixels. It's also supposed to deliver less moiré and fewer color artifacts in video. The sensor has 6.95-micron photodiodes, compared with 6.39 microns for the 5DMII and 5.7 microns for the 1DM4, which helps, too. An updated dust-reduction system--Ultrasonic Wave Motion Cleaning--consists of new fluorine coating on the UV filter plus an undulating rather than shaking movement the company claims works better to dislodge fine dust particles. The new sensor, as well as the new shutter and mirror systems, also factors into the ability to maintain the same continuous-shooting speed as its APS-H predecessor. It's got a two-line, 16-channel readout for faster recovery, and the dual processors theoretically allow it to maintain burst rates at higher ISO sensitivities than before (and not get bogged down in the noise reduction). The shutter uses lighter weight carbon-fiber blades; that plus support for an electronic first curtain (Live View Silent Mode) and a new, spring-mounted mirror mechanism, both intended to reduce vibration and increase speed. The AF system ratchets up the complexity. Dubbed High Density Reticular AF, it's increased to 61 points, with different configurations depending upon where a point is on the grid. There's a line of 5 crossed diagonal lines in the center, which are sensitive to f2.8; the 21 center lines are paired and sensitive to f5.6 and there are 20 outer points sensitive to f4. The exposure range under which the AF system works has also expanded by 2 stops, one on each end. You can configure the point selection the same as on the 7D. Compulsive microadjusters will now be thrilled that you can profile 2 focal lengths for each zoom lens. Autoexposure gets an overhaul as well. It uses a new 1000-pixel RGB AE sensor supplemented by a Digic 4 processor that enables all sorts of enhanced capabilities. The sensor is broken down into 252 zones; the camera exploits them all in good light, and in dim light bins them down to 35 zones. Like Nikon, Canon now uses the AE system to bolster the operation of the AF system. The Intelligent Tracking and Recognition (iTR) option for AI Servo III adds color and face detection (thanks to the Digic 4) to hopefully improve continuous-shooting AF accuracy, if not speed. AI Servo III will also allow you to control tracking sensitivity and acceleration/deceleration. While the video system doesn't introduce any new frame rates, there are some welcome improvements. Time code! (Free run or rec run.) All-I encoding! (as well as IPB.) It will automatically start a new clip without dropping frames when it hits the 4GB barrier--which you'll need if you use the All-I encoder. A touch ring on the control dial lets you more quietly change shooting settings, and audio levels can now be adjusted while recording. If you connect multiple cameras via Ethernet, you'll be able to sync the internal clocks for time-code accuracy. The camera retains the same nice viewfinder as the previous models, but updates it with the overlays and readouts that debuted in the 7D. Though it sports a new, higher capacity battery, the older batteries will still work in the camera--the old charger won't work with the new battery, however. Along with the camera, Canon's announcing new versions of its Wireless File Transmitter (WFT-E6A) and GPS add-on (GP-E1). The WFT adds support for 802.11n and is dust-and-weather resistant; so's the GPS device, which adds an electronic compass to log directionality in addition to coordinates. Those are only the highlights. There are tons of other changes to look forward to, both in the feature set and the interface. There's always a downside, though. I could see some sports shooters potentially annoyed by the decision to drop the APS-H format; after all, lens purchase and usage decisions have been made around the 1.3x crop factor, which means you're forced to shoot at an effectively shorter focal length (full-frame) or an effectively longer focal length but lower resolution in a crop mode. (I'm not sure if there's a 1.3x-equivalent shooting mode.) Plus, upgrading to the new AF/AE system is probably going to cost you about $1,800 more than you expected. Then, of course, there are likely the commercial photographers who don't necessarily want the faster burst or the fancy AF system, but would have liked a higher-resolution full-frame sensor with some AF/AE and design improvements. But you may luck out with the next 5D iteration. Whenever that comes. One thing that doesn't surprise me is the long gap between announcement and shipping. After the 1DM4 autofocus brouhaha, I suspect Canon is giving itself some time to seed units with its potential audience and get the bugs worked out before releasing it to the masses. After all, you don't want angry sports photojournalists tweeting that your AF failed during the Summer Olympics."
81,https://www.cnet.com/culture/japanese-robot-pours-fake-drinks-with-fake-ice/,CNET,2011,8,1,228.0," Japanese researchers are developing a robot that can adapt its behavior to new situations and make educated guesses about new challenges based on its knowledge. Tokyo Institute of Technology's Osamu Hasegawa and collaborators are working on a robot that operates based on an algorithm they've termed a self-organizing incremental neural network (SOINN), which is designed for unsupervised learning tasks. The humanoid bot has a head, cameras, and two arms. In an experiment, when it's told to serve water, it can identify a cup, grasp it and then pour pellets, acting as the liquid, into the cup. As seen in the vid below, the experiment isn't exactly mind-blowing. But practical artificial intelligence that works in real-world situations will require robots like this one to be able to figure out what a cup is, and how to pour water. The SOINN system can filter useful sensory data from background noise, as well as mine the Internet and the experiences of other robots for knowledge. Hasegawa gave the example of an elder-care robot in Japan querying another in England on how to make tea. The idea of globally networked robots querying each other for how-to tips could gain traction with the advent of Willow Garage's open-source ROS operating system, which is powering bots around the world. Let's just hope they don't get any dangerous ideas. (Via Akihabara News) (Via Akihabara News)"
82,https://www.cnet.com/tech/tech-industry/moon-express-sees-money-in-moon-rocks/,CNET,2011,7,22,608.0," What's a search engine geek doing in the space business? Barney Pell, CTO and co-founder of Powerset, a search technology company that was acquired by Microsoft, has for the last year been working on building a robotic spacecraft to land on the moon, as the co-founder of a new company called Moon Express. (Pell also co-founded StockMaster, which was acquired by Red Herring when I worked there.) It turns out that Pell is an old space hack. He has a Ph.D. in artificial intelligence from the University of Cambridge and worked on the AI program for Deep Space 1, a NASA probe that tested several autonomous space exploration technologies. Moon Express is a return home to him, but it's also his most audacious venture, even more so than his search business was. And that was a real flyer. Moon Express is building a robotic spacecraft that can take a payload of up to 100 kilograms from lunar orbit down to the moon. It's fully automated, using what Pell says are (presumably his) AI algorithms to land safely. It does not have a return capability, but the mission of the company is nonetheless very much about getting stuff back from the moon. Why? There is, Pell says, gold in them hills. Or, more accurately, platinum on the plains. Asteroids laden with precious materials, like platinum, have been landing on the surface of the moon for billions of years. They're just waiting to be harvested. Pell did not explain exactly how that harvesting would be done, although he maintains that the cost of developing an automated lunar platinum mining operation, including the Earth-side infrastructure to support it, would cost about $20 billion, which is less than opening a new terrestrial mine, he says. There's also an abundance of water in the moon's soil, which can be used for life support on the moon or be extracted into component parts for rocket fuel back at Earth (you can park it in an orbiting depot). And then there's Helium-3, which might be usable in as-yet-unbuilt fusion reactors. Transporting clonesicles to the lunar mining camp is not included in Pell's projections. Pell doesn't see Moon Express getting into the mining business directly. The company is based on more of a sell-the-shovels model. It'll help mining companies land their gear, and charge them for it. ""It'll be the biggest ROI in history,"" Pell says. Even if it doesn't become a bona fide mining services company soon, Moon Express has a not-ridiculous start-up balance sheet. It will cost about $40 million to build a lander, buy it a ride on a Falcon 9 rocket (which will fling it toward the moon), and pay for the vehicle that will insert it into orbit, Pell says. Customers will pay $10 million to $20 million for the trip (one customer has already agreed to pay about $5 million to deliver a telescope to land on the far side of the moon); plus there's start-up help in the form of $20 million of Lunar X-Prize money from Google and $10 million in challenge-related grants from the NASA Innovative Lunar Demonstrations Data program. Pell obviously believes that the time is right for a private enterprise to make a moon delivery truck. His experience at NASA left him with an appreciation for a motto inside the agency: ""As only NASA can."" The space agency exists, he says, to push the frontiers of space science. Once what a NASA program can do becomes ""easy,"" it's time for NASA to exit the business. And companies like Moon Express to take over. ""It is rocket science,"" Pell says, ""but it's not new rocket science."""
83,https://www.cnet.com/culture/artificial-intelligence-takes-on-ms-pac-man/,CNET,2011,6,14,295.0," For the last 30 years, Pac-Man's estranged wife, Ms. Pac-Man, has been consuming countless coins and hours from gamers at arcades across North America. Philipp Rohlfshagen, David Robles, and Simon Lucas, all of the University of Essex, recently launched a competition called Ms. Pac-Man vs. Ghost Team for those wanting to rewrite the flow of the classic chomper. The contest, held last week in New Orleans at the Congress on Evolutionary Computation 2011, was an open call for savvy Java programmers to create artificial intelligence routines that either control where Ms. Pac-Man or the four ghosts should go during gameplay. If you can't beat them, hack them. Programming a successful strategy for a game like Ms. Pac-Man in real time is very complicated (click here for the software). A more competitive version of Ms. Pac-Man was used in the exhibition; fruits were omitted and the speed of Ms Pac-Man and the ghosts were identical. Entrants to the competition weren't allowed to use screen captures to navigate. They were required to directly interface with the game engine instead. According to the competition site, every second counts: So what was the highest programmed score? AI Ms. Pac-Man programmed by Atif from University of Essex racked up 69,240 points against AI ghosts programmed by DarkRodry. However, DarkRodry must've not programmed a very good set of ghosts as the top four scores all are against him. James from the University of Nottingham had the highest average score for controlling Ms. Pac-Man with 34,278. The video below shows off what an AI Ms. Pac-Man game looks like. The competition will return in late August for the 2011 IEEE Conference on Computational Intelligence and Games in Seoul, with more details coming to the Ms. Pac-Man vs. Ghost Team Competition site soon."
84,https://www.cnet.com/culture/future-of-medicine-under-the-microscope/,CNET,2011,5,13,598.0," MOUNTAIN VIEW, Calif.--Experts in fields such as regenerative medicine; personalized health; information and data-driven health; and neuromedicine are gathering here this week for several days of discussions about the future of medicine. Organized under the appropriate rubric of ""FutureMed,"" leaders in these fields, plus nearly 70 paying participants, are taking part in Singularity University's first FutureMed executive program. For two years, Singularity University (SU)--created by futurist Ray Kurzweil and X Prize CEO Peter Diamandis--has been bringing people together at NASA Ames Research Center here to discuss what are called ""exponentially growing"" technologies--things like 3D printing, self-organizing molecular circuits, advanced robotics, and more. But over two 10-week summer courses with graduate-student level participants and several 10-day programs aimed at successful executives, the institution has spread its focus across a wide variety of disciplines. FutureMed is SU's first attempt at homing in on a single field and having top-level discussions about where that field is heading and how it may change the world. For Daniel Kraft, FutureMed organizer, Stanford- and Harvard-trained physician, and SU's Medicine chair, the new program is a welcome opportunity to spend five whole days with the leading thinkers in the field, as well as several dozen executives, rather than just the few hours that SU devotes at its general executive programs. ""There's so much happening in exponentials in biomedicine,"" Kraft said, ""and we can bring it together and mix it up in a way you can't do at a larger conference."" But Kraft said FutureMed sold out and has more people paying to attend than any previous SU executive program. And that, he suggested, is owing to intense interest in the medical field--and its many subdisciplines. ""Everyone is connected to health in many ways,"" he said. ""Everyone's touched by medicine and health care in general."" For attendees, who range from executives in the medical field to practicing doctors to entrepreneurs looking for the next area to invest in, and who come from countries all over the world, the $7,000 tuition gives access to talks on topics as diverse as personalized medicine; the future of pharma; patient engagement; regenerative medicine; neuromedicine; synthetic biology; the future of medical education; global health and the hospital of the future; and more. Plus, they'll be making site visits to the Kaiser Garfield Innovation Center, IDEO, Intuitive Surgical, and Autodesk. And to Kraft, the idea is that these attendees, as well as the expert speakers, will spark discussions around things like a $100 genome, artificial intelligence physicians, new models for the pharmaceutical industry, gene therapy, and much more. For Dan Barry, a former NASA astronaut who's currently head of faculty at SU and a leading roboticist, the FutureMed program ""is really talking about what we are. It incorporates a lot of the issues from the other fields....We talk about robotics, artificial intelligence, as well as the biotech aspects of it, and I think it brings up a lot of ethical questions with regard to who's going to have this technology available to them."" Barry acknowledged that much of the technology that will emerge from these fields will initially be available mainly to the wealthy. But he said he hopes that over time, as early adopters use the technologies and more and more people see their value, prices will come down and the innovations will become available to larger and larger populations. He also said that the FutureMed participants will be tackling tricky subjects like the risks associated with doing genetic manipulation of organisms that are intended for human interaction, and the social upheavals that may come from integrating robotics with health care."
85,https://www.cnet.com/culture/crysis-2-if-you-can-make-it-there/,CNET,2011,3,24,937.0," The original Crysis was a PC game so graphically demanding that it became, in its day, a benchmark for the highest-end computers. While Crysis 2 still hits high graphic standards on the PC, we gave the console version a whirl--it's the first Crysis game available on the Xbox 360 and PS3. So, can Crytek still work its magic using 5-year-old consoles? Scott: Moving through a post-apocalyptic landscape--with a gun in hand, no less--is hardly a novel experience in gaming. The fact that Crysis 2 adds even more gaming cliches to its checklist--battle-destroyed New York City, crippling virus outbreak, alien invasion, power-enhancing suits--doesn't help the cause of Crytek's console debut, either. Put your concerns about originality aside, however, and what you have with Crysis 2 is an extremely well-wrought game that sweeps you up in its expansive grip regardless. I'm hardly a fan of the ""shooter"" genre, myself, mainly because so many feel like they're starting to bleed together. Call it an action game with a gun, or whatever you like: Crysis 2 might be one of the best representatives of the genre since the last Call of Duty. I didn't follow the plot. I didn't need to: honestly, when it comes to shooters that fall in line with a structure I so clearly recognize, the disturbing truth is I don't really need a reason to start shooting. But, is New York a successful ""character"" in Crysis 2, an environment that helps suck me in? Was I emotionally affected? No. The last time NY really worked as an environmental pull was Grand Theft Auto IV. Here it's impressively rendered, but still eye candy. Multiplayer dangles the now-familiar carrot of experience points, but playing in a wrecked Grand Central Station is far more satisfying than doing the same thing in a generic war-torn lobby. As a New Yorker, I appreciated the virtual tourism of Crysis 2: however, I've been there courtesy of my control pad many, many times before. Jeff: Without a doubt, Crysis 2 is one of the best-looking games we've seen in years. It displays a certain smoothness that is ultrarealistic and really brings the setting of a crumbling New York City to life. Gameplay is tight, responsive, and intense. The sound design is fantastic and really adds to the atmosphere. The game makes a valiant effort by trying to separate itself from the mundane linearity that plague so many other titles in the genre, but at times Crysis 2 feels like it's giving you more options than you really need. A visor accessory is used to help outline specific locations for the player to launch an attack, but it's not always practical to follow. Also, some of the enemy AI in Crysis 2 is abysmal, which doesn't help a game that's trying to encourage strategic and tactical combat. Too often we were able to sneak up on an enemy that had seemingly fallen in love with a concrete wall. The first few hours of the game are almost enough to cause a loss of interest, but things certainly escalated the longer we sat with it.  In terms of a story, we must admit that we've seen this one so many times before. For the most part it's forgettable, especially after playing through a narrative like the one in Homefront. All in all, Crysis 2 provides enough eye candy for most, and we really appreciate the well above average production values here. However, we had a difficult time forgiving the subpar enemy AI and the handful of times where we just didn't know what to do next. Dan: The best films, novels, and games all know the importance of creating a compelling setting for the audience. Think of the bleak futuristic cityscape of ""Blade Runner"" or the isolated west of John Ford's ""The Searchers."" Interactive games have a better opportunity than most to make use of this location-as-character concept, giving the audience an opportunity to literally walk through the backdrops. Probably the best example of this I've seen is the vivid faux New York of Grand Theft Auto IV, although the entirely fictional world of the Elder Scrolls games, with multiple cities and towns teeming with life and unique architecture, come close. Crysis 2 immediately hits one of my personal soft spots by choosing my home town of New York as its setting, and, while not terribly relatable (due to a viral outbreak and alien invasion turning the city into ruins), the look and feel of different parts of the Big Apple mostly ring true. Highlights include a showstopping shootout at Grand Central and a view of a half-destroyed Statue of Liberty (shades of ""Planet of the Apes,"" perhaps). By making its setting such an integral part of the action, the game manages to rise above its somewhat rote action game roots, although there's a surprising amount of stealth and strategy required. On the other hand, the voice acting is some of the worst to be heard in a big-budget EA game in recent memory (not that most of the poorly written lines these amateur voice actors are reading deserve much better). On a different note, Crysis 2 has managed to do something that Black Ops and Killzone 3 could not--look good on a 3D TV. Those earlier action games lost far too much visual fidelity when in their 3D modes, and I quickly switched back to plain old 2D in both. Crysis 2, on the other hand, looks just as sharp in the third dimension and smartly makes the 3D effect subtle, moving elements into the background, rather than flinging things through the screen at you."
86,https://www.cnet.com/reviews/canon-eos-rebel-rebel-t3i-review/,CNET,2011,3,22,1315.0," If you didn't think the 60D was overpriced when it shipped, you will now. The Canon EOS Rebel T3i (aka the EOS 600D), the 60D's younger and cheaper sibling, offers the same basic camera with some corners cut--most notably a slightly less well-constructed body and a (purposely?) stunted burst shooting speed. You can also think of it as a slightly more expensive T2i, with the addition of an articulated LCD and a few features for the auto-always crowd. Either way, the T3i remains a solid if unexciting follow-up to its predecessor, although one that seems to cater more to videophiles than still shooters. That's not to say it compromises on still photo quality. Overall, the T3i has an excellent noise profile, unsurprisingly similar to that of the 60D's. JPEGs look very clean up through ISO 400, and even at ISO 800 you really have to scrutinize to see the beginnings of detail degradation; at ISO 1,600 the noise becomes more obvious but still isn't too bad. ISO 400 is sort of my tipping-point sensitivity; to shoot action outdoors, I generally have to bump up the setting to at least ISO 400 in order to reach a sufficiently fast shutter speed. And because few consumer cameras are fast enough at shooting burst raw+JPEG, the in-camera JPEG processing has to be decent as well. The T3i fared pretty well under these conditions. Overall, I consider shots at this setting good enough to use, but I still wish I would have been able to shoot raw to clean them up. Canon's JPEG processing remains very good. Even at ISO 1,600 I couldn't obtain unambiguously better results processing the raw--Canon seems to optimize for exposure at the expense of sharpness, and I couldn't get sharper results without losing some shadow detail (you may do better). At ISO 3,200 I was able to achieve a significant reduction in color noise without losing too much shadow detail. And by ISO 6,400, I started to see hot pixels as a side-effect of the in-camera noise reduction (those white spots) in the JPEGs. On all other counts the photos looked good on the default settings, though my favored setting with Canon models is Neutral with sharpening bumped up a few notches. Colors look both relatively accurate and saturated; metering and exposures are consistent and predictable; and the dynamic range is broad enough to allow a reasonable amount of highlight and shadow recovery. As usual, the video looks very good. There's some moiré, but not a lot of rolling shutter, and moving edges look surprisingly sharp. At its highest quality, it seems to deliver an average bit rate of roughly 45Mbps. It offers the same great set of frame rates and manual exposure controls as the 60D, including highlight tone priority for fine-tuning high-key exposures. Though the built-in microphone is mono, it sounds surprisingly good, and there's a wind filter along with the same 64-level sound controls. Canon also incorporates the Video Snapshot feature from its camcorders--it lets you snap up to 8-second clips--and some in-camera special-effects filters, too. I'm not as fond of the new 18-55mm IS II lens as the old kit lens; unfortunately, I didn't have the old lens available to do direct comparisons, but the new lens seemed to have more issues with fringing than the old. The new lens claims an extra stop of image stabilization, but I didn't find it more effective (of course, it's always possible that I'm a year shakier). The 18-55mm kit lens displays visible but not terrible asymmetric barrel distortion at its widest. In shots with the previous version of the lens, the distortion looks more symmetrical, but I don't have exact comparison shots--they put up scaffolding months ago, which prevents me from replicating my test shot. The camera's performance remains fast, but, surprisingly, in some cases not quite as fast as its predecessor's. It powers on, focuses, and shoots in about 0.3 second, with a fast 0.3-second shot lag in good lighting and solid 0.6-second in dim (a tad slower than the T2i). JPEG shot-to-shot time is also good at about 0.4 second; raw takes a little longer at about 0.5 second. Adding flash bumps that up by another couple tenths of a second. Its burst rate is essentially equal to the T2i's, but both are at what I consider the slowest acceptable continuous-shooting speed for a dSLR and slower than less-expensive competitors like the Nikon D5000 or the Pentax K-x. With a few exceptions, the T3i's body and interface are almost identical to the T2i's. It's slightly heavier (but not larger) thanks to the bright, flip-and-twist LCD. It feels sturdy, and though the texture rubberized grip feels kind of cheap, the camera is comfortable to hold and shoot single-handed, and can stand up to the weight of a good lens. I've never been a huge fan of the Rebel series' viewfinder, and this one is actually a slightly lower magnification than previous models. I don't know that I noticed the difference, but there are better ones out there. Also,  I don't like the tiny, too-briefly-flashing AF points. Canon carries over the control layout and user interface from the T2i, although it has moved the Display button to the top and replaced it with Info. Camera operation is straightforward. On the back there are direct-access controls for Live View/video recording, exposure compensation, white balance, autofocus method (single, AI focus and AI servo), drive mode, Picture Style, AE/AF lock, and focus area (single-point manual or all-points auto). You can also change these settings, plus metering, flash, image quality, and a few others, via the typical Quick Control screen. My one quibble here is that the buttons all feel a bit too flat. The mode dial includes the usual set of manual, semimanual, automatic, and scene modes. It doesn't lock, which may bother some folks (though not me). As with the 60D, however, I find the placement of the movie mode--at the opposite side of the dial from the advanced modes--insanely frustrating. I've actually missed video opportunities by having to scroll around from shutter-priority mode to video. Ironically, this design is more suited to pros who plan to use the camera as a cheap video dSLR and never take it off the video setting than to the consumers at whom it's ostensibly targeted. Canon's version of an easy mode, Creative Auto, operates via what it calls ""ambience selection""--Standard, Vivid, Soft, Warm, Intense, Cool, Brighter, Darker, and Monochrome--for which you can set it to one of three levels. The scene modes also utilize the ambience selection options, making them a little more flexible. While Canon offers quite a few options for video shooters, it doesn't have much beyond the basics to inspire or streamline shooting for still photographers. The T3i supplies basic Eye-Fi wireless integration--you can enable or disable the card and the camera provides connection strength information. For bracketing, you're still limited to a three-shot bracket and a range of two stops around the center, though the complete range goes up to seven stops in either direction. And it supports wireless flash. But there's no way to save custom settings, no creative features like time-lapse or multiple-exposure shooting or filters (only a handful of postprocessing effects). Furthermore, with increasing resolutions, the ability to shoot raw plus small or medium JPEG--as opposed to full size--isn't just a pro necessity anymore, especially if you plan to transmit wirelessly. For a complete account of the T3i's features and operation, you can download the PDF manual. Conclusion For the money, the T3i is a great choice for dSLR videographers--though the cheaper T2i can still suffice if you don't need the articulated LCD--and is a solid choice for creative still shooters. But while the image quality and general shooting performance are top-notch, if you're upgrading to shoot sports, kids, or pets, the T3i may not be able to keep up."
87,https://www.cnet.com/culture/reporters-roundtable-debating-the-robobrains/,CNET,2011,2,18,630.0," Big news in AI this week: IBM's Watson project defeated ""Jeopardy"" champions Ken Jennings and Brad Rutter in a three-night prime-time demo match. What does that win mean for computing, and more importantly, for humanity? That's the topic for this week's Reporters' Roundtable, and to discuss it we have two great guests, both with current books on the topics of computer vs. human competition. First up is Stephen Baker, author of Final Jeopardy: Man vs. Machine and the Quest to Know Everything. Baker reported on the development of Watson from inside IBM headquarters to write this book. He was BusinessWeek's senior technology writer before that. And branching out a bit from the Watson news, we also have Brian Christian with us. He's the author of The Most Human Human: What Talking with Computers Teaches Us About What It Means to Be Alive, which will be out on March 1. He's also author of the recent Atlantic cover story Mind vs. Machine, which is a great primer for this topic. Both of these works tell the story of Brian's participation in the annual Loebner Prize, in which humans face off with computers in a Turing test competition to convince judges that they are human. Brian, it should be noted, was voted most human. Subscribe: iTunes (MP3)iTunes (320x180)iTunes (640x360)Podcast RSS (MP3)Podcast RSS (320x180)Podcast RSS (640x360) First, the ""Jeopardy"" tournament. What problems had to be solved to make Watson a viable contender?  What human skill does Watson emulate? What did the team overlook? It strikes me that the game was created for human contestants, and bringing a computer into the arena really upset the balance. The reaction time of pressing the buzzer is such a variable task, one that's not really interesting for computers since their reaction time can be so fast. If we took the buzzer challenge out of the competitive equation somehow, or made speech recognition or OCR part of the equation, how do you think Watson would have done? On the other hand, Watson was disconnected from the Internet. He couldn't Google anything. Why this artificial constraint? What did Watson learn from its older brother, the chess player Deep Blue? Was there a breakthrough that made Deep Blue competitive against Kasparov? Let's talk about the Turing test, Brian. First, what is it, and what is the Loebner Prize? How good are computer programs getting at the Turing Test? Are they open-ended? How did you win against them? Can computers have personality? What do demonstration projects like Watson and Loebner Prize competitions teach us about our own brains and minds? I want to talk about adaptability. It's widely believed that computers are formidable foes because they are fast and highly adaptable, and that humans get stuck in intellectual and conceptual ruts. But Deep Blue never played again after beating Kasparov, and Brian, in 2009 the humans beat the machines in the Loebner competition by a much wider margin than they did in 2008. Why? What's going on here? Where will we see computers competing with human minds in games next? Poker? How about in other pursuits, say, war. How do we make computer minds compatible with the human world? One of the things that people keep talking about is robot cars. That's fine--if all the cars on the roads are robotic. But when the computer-driver car has to deal with human drivers, it gets much more complicated. How do we solve for this, and are there other similar computer/human interaction issues? What's next for Watson? Health care? Or will Watson become a litigator? What's next in human mind simulation--research projects like Racr? Other projects we should be watching for? Related reading: Wrap-up Follow me on Twitter for news on what's next on Reporters' Roundtable. E-mail roundtable@cnet.com with your ideas for shows!"
88,https://www.cnet.com/culture/must-love-puppies-walks-on-beach-robot-babies/,CNET,2011,2,15,273.0," Tired of dating people who stare at you blankly when you talk about your affection for Asimo? Dream of having a robot baby head all your own one day? Online dating site Robot Passions says it can help you find a partner who shares your love of robots--or at least thinks Gundam is pretty cool. Robot lovers longing for a date with metal sexpot Roxxxy might be disappointed to learn that the site is currently restricted to humans. It does, however, note that it hopes to include robots among its members ""once their AI is sufficiently advanced."" Wait, what does one get a Roomba for Valentine's Day? Robot Passions is part of Passions Network's extensive roster of free online dating sites, which include such other geek offerings as Comic Book Passions, Zombie Passions, Trek Passions, and Ninja Passions. Join one site in the network and you're automatically a member of the entire network, though as you register, you can choose the sites that best match your interests if you'd prefer, for example, not to include members of the Mullet Passions site in your search for love. Robot Passions looks a lot like the other sites in the network, though members can search or browse for other members based on the types of robots they're into: domestic bots, humanoid bots, modular bots, research bots, toy bots, telerobots, nanobots, and so on. Aside from finding fellow members based on their robot preferences, they can further narrow their searches to robotics hobbyists, or people who work in the robotics field. There's even a live robot chat--""Hey, how do you feel about servos on the first date?"""
89,https://www.cnet.com/tech/tech-industry/what-ibms-watson-tells-us-about-the-state-of-ai/,CNET,2011,2,14,936.0," Computers that reliably understand human communications have been a staple of fiction going back decades or more. The Enterprise's computer in the 1960s vintage ""Star Trek"" series is as good an example as any. And truth is, that particular science-fictional ability probably would not have seemed all that remarkable to the typical person of the time. Access billions of pages of text, pictures, and video from a gadget I can fit in my pocket? Play a game with immersive graphics on a huge, high-resolution screen that hangs on the wall? For a computer engineer, the fact that those inexpensive consumer devices have more computing power than all the then-computers in the world would impress as well. But understanding speech? That's something a toddler can do. But understanding speech has turned out to be really difficult. In fact, just converting speech to text has been a huge challenge. Indeed, when IBM Watson takes on past ""Jeopardy"" champions in a contest televised beginning tonight, the questions will be fed to it as text, rather than speech. But answering the often convoluted questions used on ""Jeopardy"" is hard enough even without processing the spoken word. Although this contest takes place in the artificial setting of a game show, it does give us a glimpse into what is possible and what is not with artificial intelligence, that is AI, today. And perhaps where AI is going. AI research is generally considered to have launched in 1956 at the Dartmouth Summer Research Conference on Artificial Intelligence. The hope of many researchers at that time was that they would be able to create a so-called ""strong AI"" over the next few decades--which is to say an AI that could reason, learn, plan, and communicate. Research in this vein has produced very limited results. One of the big problems has been the almost equal lack of progress in understanding how humans think. Thus, the failure of strong AI may well be related to the equal lack of progress in significant areas of cognitive psychology. Some of the AI pioneers still have a more optimistic view. MIT's Marvin Minsky places the blame more on a shift away from fundamental research. As he puts it, ""The great laboratories somehow disappeared, economies became tighter, and companies had to make a profit--they couldn't start projects that would take 10 years to pay off."" So Watson is in no real sense thinking and the use of the term ""understanding"" in the context of Watson should be taken as anthropomorphism rather than a literal description. Is Watson just about brute force then? One might think so. Its hardware specs are impressive: IBM Watson is comprised of ninety IBM POWER 750 servers, 16 Terabytes of memory, and 4 Terabytes of clustered storage.  This is enclosed in ten racks including the servers, networking, shared disk system, and cluster controllers. These ninety POWER 750 servers have four POWER7 processors, each with eight cores. IBM Watson has a total of 2880 POWER7 cores. To put this in perspective, by my estimate, Watson would have been the fastest supercomputer in the world on the TOP500 list just five years ago. And, although the disk and memory specs aren't nearly so impressive, remember that we're just talking about text-based data here. In fact, it's loaded with millions of documents--making the fact that it, like the human contestants, isn't hooked up to the Internet something of a red herring. Chris Anderson, the editor in chief of Wired, argues that data often replaces underlying theory. He goes on to quote Peter Norvig, Google's research director: ""All models are wrong, and increasingly you can succeed without them."" But thinking of Watson as just a big, fast computer that just points to Wikipedia or the Oxford English dictionary and the right answer pops out understates the complexity of the natural language processing that has to go on. If Jeopardy consisted solely of grade-school type questions--excuse me, answers--like ""the 42nd president of the United States,"" this would in fact be a relatively simple exercise. But many Jeopardy questions consist of wordplay, riddles, and other barriers to literal lookup of answers. Watson is part of IBM's DeepQA project. The QA stands for question answering. As IBM researchers put it: the open-domain QA problem is attractive as it is one of the most challenging in the realm of computer science and artificial intelligence, requiring a synthesis of information retrieval, natural language processing, knowledge representation and reasoning, machine learning, and computer-human interfaces. In association with Carnegie Mellon University, IBM created the Open Advancement of Question Answering (OAQA) initiative ""to provide a foundation for effective collaboration among researchers to accelerate the science of automatic question answering."" Among other things, this initiative is intended to enable adapting Watson's software to new data domains and problem types. Although Watson is certainly a powerful computer loaded with lots of data, as described in this PBS video, the software is very much a key ingredient here; many new algorithms and approaches were needed to make Watson competitive with strong human players. For example, to learn from examples, to understand how context affects the significance of names and places, and to correlate multiple facts in a particular answer. Strong AI proponents may well view something like Watson as something of a parlor trick in that it doesn't really try to reason as a human does. But, that said, there's much more--dare we say intelligence--involved here than there is in playing chess, a well-bounded and formalized problem. And given the longtime difficulty of understanding real intelligence, this is the AI path that seems to hold the most promise for now."
90,https://www.cnet.com/science/domo-arigato-mr-watson-ibm-computer-takes-on-jeopardy/,CNET,2011,1,13,1025.0," YORKTOWN HEIGHTS, N.Y.--The first words publicly spoken by a talking computer named Watson were, ""WHAT IS JERICHO?"" Watson was following the rules. Like any contestant on game show ""Jeopardy,"" the IBM Research-built machine was required to phrase his answer in the form of a question. And Watson was playing ""Jeopardy."" More specifically, it was a test run this morning at IBM Research's headquarters in preparation for a televised weekend challenge against famed ""Jeopardy"" champions Ken Jennings and Brad Rutter, and Watson nearly shut out those champions in a category about female archaeologists called ""Chicks Dig It."" But then in the category's final answer, Ken Jennings--who holds the record for winning the most consecutive ""Jeopardy"" games in a row--bested Watson. Jennings' question-phrased response: ""What is a Neanderthal?"" It was a fitting start to the test match, because what Watson is really all about is what separated us from the Neanderthals in the first place: the evolution of high-level intelligence, the complexities of the Homo sapiens brain, and the depths of human cognition--and whether we have finally begun to crack the code in creating a human-built machine that can start to approach this kind of grasp on language. From folklore hero John Henry's Pyrrhic victory against a steam-powered steel hammer to Arthur C. Clarke's HAL 9000 and the Superman foe Brainiac, the narrative of human versus machine has fueled sentiments of both excitement and fear. Now there's Watson, named for IBM founder Thomas J. Watson, whose creators at IBM say is a hallmark of far more significant accomplishments than being able to challenge both Jennings and fellow ""Jeopardy"" champ Brad Rutter in a question-and-answer match. They also want the world to know he isn't evil (yet). ""We are at a very special moment in time here,"" IBM Research director John E. Kelly III told the audience. ""We're at a moment in time where computers and computer capability has approached in this dimension the ability of humans, and the fact that the demonstration was so close shows that these two beings, these two lines are crossing. What will happen in the final tournament, we don't know."" Watson cannot see or hear, which means it cannot handle ""Jeopardy"" audio or visual clues, but it can wager on Daily Doubles. It can read and speak, and is packed full of dictionaries, reference books, thesauri, and an impressive literary canon, but it is not connected to the Internet and therefore has no Google-enabled advantage over a human competitor. The game's prompts are fed to Watson, whose hardware fills an entire room underneath the auditorium in which the televised ""Jeopardy"" challenges will take place, in plain text at the same time that they appear onscreen for the human contestants. Over the past four years, IBM researchers improved the computer's capability first to answer enough questions correctly so that it would keep a positive score in the game, then to respond correctly at the rate of the average ""Jeopardy"" player (about 60 percent accuracy), and then enough to challenge ""Jeopardy"" legends like Jennings and Rutter. In Thursday's test match, Watson narrowly beat Jennings, who made up for his lack of knowledge of female archaeologists like Mary Leakey and Dorothy Garrod by dominating a ""Children's Books"" category, and more soundly defeated Rutter (""Watson and I don't have kids,"" Rutter surmised afterward with regard to Jennings' dominance in the kiddie-lit category). IBM's legacy of machine-versus-man gameplay goes back a decade and a half: In 1996, an IBM-built computer called ""Deep Blue"" became the first machine to beat a reigning world chess champion when it defeated the Russian chess master Garry Kasparov; Kasparov won the overall match. An upgraded version of Deep Blue then soundly defeated Kasparov the following year, and the defeated chess champion accused IBM of breaking the rules by giving the computer some kind of human assistance. ""Off of that we got tremendous computer science learning that is applied to our systems today,""  Kelly explained, saying that he believes the research behind Watson will ""have impacts on society far beyond the latest widgets that people are worrying about today."" That's because IBM, which is currently commemorating its 100th anniversary, believes that Watson represents a groundbreaking innovation in artificial intelligence because of its ability to process the complexities, nuances, and subtleties of human language. This is something that could have far-reaching implications in fields as varied as academia, government, and particularly health care. ""We've created a system that can interact in a very, very special way,"" Kelly said. In artificial intelligence, ""People spend their lifetimes trying to advance that science inches. What Watson does, and has demonstrated, is the ability to advance the field of art intelligence by miles. People who are experts in this area who have seen Watson privately said, 'I never thought I would have seen this in my lifetime.'"" It was a significantly bigger challenge to build than Deep Blue, according to David Ferrucci, the IBM Research ""investigator"" in charge of building DeepQA, the ""massively parallel probabilistic evidence-based architecture technology"" that powers Watson and has been in development since 2007. ""'Jeopardy' is a very, very different challenge than chess,"" Ferrucci said. ""In chess we have this finite, mathematically-defined search space, very precise rules, and we need a very different sort of algorithm...It's explicit, it's unambiguous, it's exacting. When we deal with language things are very, very different."" Watson's big challenge will come later this week when he makes his television debut, which Kelly called ""an excellent challenge against which we can measure the progress of this system."" It's also a charitable endeavor: Rutter and Jennings will donate half their winnings to charity, whereas every dollar won by Watson will be donated to either humanitarian organization World Vision or IBM social research foundation World Community Grid. And when the game is over, IBM Research will turn to investigating Watson's more serious purposes. As it turns out, Watson also has some comedic skills, eliciting some chuckles from the audience when, after successfully responding to four prompts about female archaeologists, declared in his computerized monotone, ""Let's finish 'Chicks Dig It.'"" But then, as it turns out, Jennings snagged that $1,000 answer. Neanderthals, indeed."
91,https://www.cnet.com/culture/at-sri-developing-an-expertise-in-r-d-innovation/,CNET,2010,10,26,1804.0," MENLO PARK, Calif.--If you've never seen a robot climb straight up an entirely flat vertical wall, I dare you not to be impressed the first time you do. That was my certainly experience when I watched a wall-climbing robot do its thing at SRI International here the other day. Indeed, my host, who had been with me through several product and project demonstrations over about three hours, noticed how excited I was by watching this little device go straight up the wall, and, I think, began to wonder if I was actually interested in any of the other things I'd seen. In fact, she shouldn't have worried. I was at SRI as part of my ongoing Road Trip at Home series and was getting a rapid-fire lay of the land at this Silicon Valley research and development--and incubation--powerhouse. And while the robot technology may well have been the coolest thing I saw all day (see video below), I liked almost everything I saw during my visit. The wall-climber is the sexy model SRI built to showcase its electroadhesion technology, which, as senior research engineer Harsha Prahlad explained, is as it sounds, an electrical adhesive. ""With the power on, there's adhesion,"" Prahlad said. ""With the power off, there's no adhesion."" Prahlad said that SRI sees this technology as a ""sticky pad"" for objects, a ""temporary tape you can attach objects to your wall with."" Yet, there's no sticky residue: all the adhesion is done through the power of attracting opposite electrical charges to each other. An object, like a small robot, has flexible plastic electrodes attached to it, which are charged positive or negative, and then it will stick to any surface because ""the world develops opposite charges for us,"" Prahlad said. ""So wherever there's a positive, the material gets charged negative, and vice versa."" So what is SRI doing with electroadhesion? According to Prahlad, it is looking for ways to incorporate it into structural or building inspection--you could have a wall-climbing robot place a camera or a sensor in a hard-to-reach spot; into consumer products--so you can attach objects to a wall; into military use--so soldiers can place a surveillance camera, or to place some other kind of item; into industry--it could be used to move large items, such as car parts, around. Prahlad said that electroadhesion is a winning idea because the systems that power it require very little power, weigh next to nothing, and are nearly silent. ""And they can conform around [any] object,"" he said, ""so they can adapt to any shaped object."" Today, places like car manufacturing plants use a variety of robotic systems, like vacuum or mechanical graspers, to move items around. But Prahlad said electroadhesion offers as much as 20 times the power and 10 times the mass savings over such legacy systems. First shown publicly in 2008, the wall-climbing robot and other systems built with electroadhesion are at least two years away, Prahlad predicted. A history of innovation For decades after its 1946 founding, SRI was part of Stanford University, and was known as Stanford Research Institute. But during the Vietnam War, said Norman Winarsky, SRI's vice president for ventures, licensing, and strategic programs, Stanford was getting picketed for having government contracts, and so the institute was spun off as a nonprofit. The irony of that is that today, SRI has plenty of partnerships with Stanford, and at least 70 percent of its contracts are with the government, Winarsky said. Silicon Valley, of course, abounds with companies and institutions geared toward pushing the latest technology and science toward profitable markets. There's the Palo Alto Research Center (PARC), Stanford; Google and Hewlett-Packard and Intel, and others. But SRI has been leveraging high-tech R&D into start-ups for years, and Winarsky said that around 50 companies that began inside the institute have eventually been spun off. Four of these companies have gone IPO and together are now worth $20 billion. And, of course, this is the institution that spawned the invention of the mouse, the hyperlink, surgical robots, that was in on the very first Arpanet communication ever, and so much more. ""Our mission is to be the world's leader in innovation,"" Winarsky said, ""delivering new innovations and solutions into the marketplace. That's very different from our peers, which are usually [set up] to educate, or to do R&D alone."" At any given time, SRI has around 2,000 projects in the pipeline, spread across five major divisions. Among them are the information technology division--with three sub-divisions focusing on artificial intelligence, speech and natural language, and computer security; physical sciences--clean tech, batteries, alternative energy, and new materials; and engineering systems, where the institute is looking at developing new systems and software. There's also an education division, which was one of my first stops for the day, accompanied by Melissa Koch, a senior project manager in SRI's Center for Technology in Learning. Among other things, Koch is working on a surprising National Science Foundation-funded project she told me about, one aimed at providing low-income, urban girls with after-school science, technology, engineering, and math (STEM) development. The idea, Koch said, is to help answer the question of where the next generation of innovators is going to come from. Working alongside a nonprofit known as Girls Inc., she is trying to entice middle-school-age girls in underserved communities to consider careers in information technology--through exposure to professional women of color, as well as visits to places like Google and IDEO--and to ""get comfortable using new technologies as they come about."" The last of SRI's five areas is its biosciences division, which largely focuses on drug development, Winarsky said. It might be surprising to know that the institution is a major pharmaceutical player, but that's because most of its work ends up being brought to market by companies with professional marketing and sales organizations. Still, the underlying science behind some of the pharma industry's pills comes straight from SRI. One project I got to see is run by Gita Shankar, the director of formulations R&D in the biosciences division. Shankar explained that one big problem in the pharmaceuticals industry is that many drugs require being taken intravenously--something that is not practical in many situations. Even when it's practical, Shankar added, many people prefer to take drugs orally--as a pill or a solution--rather than having them injected, and research shows that people are more likely to take a prescribed oral drug than one that's IV. So at SRI, Shankar and her team are working hard at figuring out the rather complex problem of converting IV drugs into those that can be taken orally. The difficulty, she explained, is that while an IV drug goes straight into the blood stream, a swallowed substance must first transit the stomach and the intestines--and their myriad acids and bases--before making it to the patient's veins. That process can take as much as two hours and easily degrade the drug. As a result, SRI has built up a competency, Shankar explained, in converting existing drugs--such as the antibiotic vancomycin--from IV-only to oral and building a giant database of the substances--known as ""enhancers"" that can help push a drug successfully through the stomach and intestine and into the veins. Once the division is successful at coming up with a drug transformation, SRI will find a commercial partner to market it, and then collect royalties. According to Shankar, all the drugs in SRI's pipeline are in a pre-clinical pathway, and many are geared toward maladies common in the Third World, often those that are not well served by major pharmaceutical companies. As a result, she said, much of the division's funding comes from philanthropic organizations like the Bill and Melinda Gates Foundation. At the same time, the division is also working on developing drugs for the military, including one geared toward battling anthrax attacks in the field. Shankar said that an IV approach to such a drug was impractical because it required 18 doses over six months. But by converting an existing drug into one that can be taken through the nose--which shortcuts the stomach and the intestine--that slow, deliberate process can be streamlined for the reality of military life. Siri and Intuitive Surgical Throughout my visit, almost everyone I met made a point of mentioning Siri, a DARPA-funded company that emerged from SRI's R&D labs and went on to get acquired by Apple, which wanted its mobile virtual personal assistant technology to allow iPhone or iPod Touch users to ask questions about things like services, destinations, or finding consumer goods. Another frequently mentioned success story was Intuitive Surgical, a Sunnyvale, Calif., SRI spin-off that makes the Da Vinci robotic-assisted surgical system. Da Vinci is being used far and wide for high-precision prostate surgery, as well as myriad other procedures. The system was first available in 1999, and in 2000, the FDA made it the first robotic surgical system ever cleared for general laparoscopic surgery, SRI said. Since then, Da Vinci has added cardiac, urological, chest, and gynecological procedures to its FDA-approved repertoire. The point of everyone mentioning Siri and Intuitive Surgical was to showcase SRI's well-known and respected innovation engine. Because while it is a nonprofit that performs more than 70 percent of its contracts for governments, and because much of its funding comes from private foundations and government agencies like the National Science Foundation, the National Institutes of Health, DARPA, and others, it is still in business. And to hear Winarsky tell it, its business, ultimately, is spinning off new ventures. As well, it's about trying to determine, earlier than most, where the next great innovation fields will be. To SRI, Winarsky said, those will be the true emergence of artificial intelligence-based virtual personal assistant systems (SRI currently has four developing ventures in that area); and cybersecurity--""We're beginning to see malware that's profoundly different than worms and viruses we've seen in the past,"" said Winarsky. But for many of the SRI employees, these fields could well end up being their path to millions, Winarsky suggested. Indeed, a chief selling point for its more than 2,200 employees is the chance to participate in hefty royalty checks that can come in from successful spin-offs, or even to join new SRI-spawned ventures as founders or executives. And in the end, Winarsky said, SRI is all about generating an environment that breeds innovation. And that takes a clear and well-defined process. In fact, oddly enough, one of SRI's most consumed products is a two-day course on its innovation process that it teaches both at its Silicon Valley headquarters and around the world. ""The theory is that innovation is a discipline,"" Winarsky said. ""It does and can have a genius moment of creativity. But it's not necessary that the climate be overcoming all obstacles...So what SRI is refining and leading is cultivating the environment to make innovation."""
92,https://www.cnet.com/tech/tech-industry/get-lamp-illuminates-the-text-adventure-game/,CNET,2010,8,10,722.0," Jason Scott's first documentary in 2005 was about bulletin board systems (BBSs), which were in a sense the PC world's parallel evolution of the early Internet. This documentary, really more a multi-disc series of interviews with BBS pioneers than a documentary film as such, brought back to me my early years in personal computing and my subsequent forays into shareware software development through the mid-1990s. Now, Scott has tackled a subject from roughly the same era: the text adventure game. My involvement here was more peripheral but no less a part of my memories. As his new ""Get Lamp"" documentary recounts, the text adventure genre began with Will Crowther's Colossal Cave Adventure game in the early 1970s, more commonly referred to as just Adventure. Crowther was a caver and was also involved with the initial development of the ARPAnet, the Internet's precursor, at Bolt Beranek and Newman (BBN) in Cambridge, Mass. Crowther himself isn't interviewed for the documentary, which describes him as the J.D. Salinger of computer games. He prefers his work to speak for itself. However, Don Woods, who later enhanced the game,  does put in an appearance. Fast-forward a few years to the late 1970s. Various MIT staff and students associated with the AI Lab were starting up a company called Infocom which would go on to become the most significant commercial text adventure game company. Artificial Intelligence was a big MIT computer science focus of the time; the department that inhabits MIT's Stata Center still goes colloquially by CSAIL--Computer Science and Artificial Intelligence Laboratory. And some of the technology that went into Infocom's games starting with Zork had more than a passing relationship to AI research; perhaps the biggest technical challenge with these games was parsing and ""understanding"" freeform text entered by the human player and responding logically. At the time, I was the publicity director for the Lecture Series Committee (LSC) at MIT and several of the Infocom founders such as Marc Blanc were regulars around the LSC office. A fair number of others involved with the group would join Infocom over time. (I personally did some casual volunteer game testing and paid a number of visits to Infocom's 55 Wheeler St. offices in Cambridge.) Scott's documentary does a great job of capturing a gaming era which is ultimately hard to separate from the history of Infocom. Indeed, in addition to various extras, Scott has actually assembled two full videos. One is a broad history of text adventure games, starting with Adventure. The other focuses exclusively on the Infocom thread, both its beginnings and its fall after a relatively few years. The company arguably was slow to incorporate computer graphics, horrible as early PC graphics were. But its hugely expensive move into the PC database market was its ultimate undoing. Of course, text adventures were giving way to graphics games in any case. ""Shooters"" like Castle Woflenstein became the game of choice. Natural language parsing, advanced as it was, was still hamstrung by limitations. Hard AI, after all, never really worked out. Graphics-centric games became the norm. And the specs of the latest graphic card became intertwined with the latest generation of game. But that's changing. The first-person-shooter (FPS) on the Xbox isn't going away. But casual games, which don't have such an intensely state-of-the-art graphic focus, seem to be staging a comeback. Take social gaming for example. Steve Meretzky wrote some of Infocom's major titles (my favorite being A Mind Forever Voyaging) and would, ahem, make me a very inside-baseball joke in one of them (Planetfall). He's now Playdom's VP of Game Design, a company recently purchased by Disney, and the creator of games like Social City and Mobsters. As Meretzky puts it: ""Text games, like the early games in any genre or on any platform, prove that well-designed interactivity and meaningful player agency are the core elements of fun, and that all the polish and pizzazz that comes later is just sizzle on the steak. The early days of social gaming, with hits like Mobsters and Mafia Wars that were almost all text are just the latest examples of this."" The axis of game development has arguably shifted. At a minimum, it's also happening along new axes in response to mobility and interconnectedness. And it's starting to look a bit like a return to the past."
93,https://www.cnet.com/reviews/lego-harry-potter-years-1-4-xbox-360-review/,CNET,2012,11,21,1273.0," If you're familiar with developer Traveller's Tales' previous Lego games, you might think you know what Lego Harry Potter: Years 1-4 is like even before you've played it--and you'd be partially right. In this game, just as in Lego Star Wars and its ilk, you (perhaps along with a friend on the same console) experience a witty and wordless rendition of a famed tale, with your favorite characters replaced by blocky Lego re-creations. Yet central elements of the previous games, such as simplistic combat and floaty platforming, have been given a secondary role in Harry's story. Now, the focus is squarely on collecting all those countless Lego bits that go flying everywhere with almost every spell you cast. You're constantly showered with currency as you solve light puzzles and manipulate practically every object you see with your versatile wand. You still need to contend with a few of the series' lingering issues (unhelpful AI, slippery platforms) and a couple of minor new ones (iffy targeting, random bugs). But the scattered flaws aren't likely to dampen the joy of playing Lego Harry Potter, particularly if you're a fan of the source material. Of course, you'll get the most out of Lego Harry Potter if you can tell Dobby from the Dursleys. And if you're one such fan, the game will frequently have you in stitches. As you can tell from the title, this adventure covers the first four novels of J.K. Rowling's Harry Potter series, and it does so with the same humorous bent that characterizes all of the developer's Lego games. Whether it's a Lego Hagrid screeching into Little Whinging on his motorcycle or the lovely young ladies of Beauxbatons prancing into Hogwarts, exaggerated sound effects and adorable animations make every cutscene a total delight. Lego Harry Potter takes most of its cues from the movies rather than the books, using the films' evocative musical soundtracks to great effect. The game takes certain liberties with the story for comedic effect, but these charming tweaks are in the spirit of the series and likely to elicit constant giggles. The only liberty that doesn't work out so well is a final action sequence bolted to the ending of Prisoner of Azkaban, which is super fun to play, but makes no sense within the context of the original story. Such action sequences are rare in Lego Harry Potter, though there are still light combat sections and boss fights. You defeat Dementors by casting the Patronus Charm, fight Aragog by throwing spiders at him using Wingardium Leviosa, and even take on He Who Must Not Be Named. Most of the time, however, you're scouring the environment looking for ways to use your ever-increasing repertoire of spells. You can switch between spells easily and cast the one you prefer, and in certain cases, such as using Lumos to make vines wither away, you need to select the spell manually. However, most spells are contextual and can be cast by holding a button and hovering the spell cursor over the object you want to manipulate. The targeting can be a little iffy; sometimes the halo indicating an interactive object won't appear when it seems it should, and it's really easy to target something you don't intend to--such as an unsuspecting student. But this is a mostly insignificant inconvenience. For the most part, you'll be smashing everything that's smashable and reconstructing everything that's reconstructible, reaping the bits that scatter as a reward for charms well cast. Between levels, you can head to Diagon Alley to spend your pips on new characters, costumes, spells, and other unlockable trinkets. But the goal of all the spellcasting isn't just to gather money; it's to solve environmental puzzles to progress to the next chapter. The puzzles aren't difficult; you can solve almost any of these conundrums by simply making sure you've hovered your spell cursor over everything you possibly can. But doing so is a lot of fun since there's so much variety in how various environmental elements react to your manipulations. Sconces light up and lavish you with fireworks. Mops clean puddles, and brooms sweep up dusty paintings. The inhabitants of Hogwarts' living portraits toss you important trinkets, or grant you access to new areas if you're controlling a member of the right house. If you're controlling Ron, Boggarts appear as spiders, and casting Riddikulus causes them to clumsily dance about on roller skates for a few moments. (If you're controlling Harry, these same Boggarts appear as Dementors.) You release Scabbers or Crookshanks to enter confined spaces, and you brew Polyjuice potions when you need to appear as another student. There's a lot to do, all of the time, and while none of these tasks are challenging, the diversity keeps things always enjoyable. Between story missions, you can explore Hogwarts and other optional areas, such as Gringotts Bank. You might finish a quick play through in six hours if you're just interested in seeing the story, but Lego Harry Potter is the kind of game you return to again and again to uncover its various secrets and collectibles. You won't be able to enter certain areas and perform certain spells your first time through, so to see everything the game has to offer, you must return to the levels in free play using different characters. Even if you think you're being thorough the first play through, you'll be lucky to approach 50 percent completion. But whether it's your first time through or your fifth, you should take another wizard to class with you. While there is no online co-op (again), another player can drop in at any time and take over for one of your AI companions. Not only does having a buddy with you make for a better time, but you won't need to rely on the friendly AI, which isn't always helpful when it comes to cooperative puzzle solving. Luckily, AI failings are much less common now than in previous games, and co-op puzzles aren't frequent, making these imperfections a minor quibble. The AI isn't the only lingering flaw to carry over into the newest Lego game, but like that particular aspect, the others have been thankfully marginalized to the point of being barely noticeable. Slippery edges and odd camera angles hinder a couple of platforming sequences, but there are so few of them that you won't be too bothered. Similarly, while there are some combat sections, they are very light, and few of them feature the respawning enemies that tainted Lego games of years past. In fact, they come as a happy change of pace among the many puzzles. We did encounter several bugs, however. In a trip to Hagrid's pumpkin patch, we couldn't interact with important objects after a short while, which necessitated a level restart; a similar problem forced us to restart a battle against the perpetually scowling Draco Malfoy. It's to the game's credit that such problems are so easily dismissed in light of all the fun and variety that so deftly veils them. Lego Harry Potter: Years 1-4 is easily the best Harry Potter game to date, though if that sounds like a backhanded compliment, consider this: It's one of the finest Lego adventures to date as well, which is no faint praise. By minimizing the repetitive combat and inconsistent platforming, Traveller's Tales has also minimized the frustrations, making this excursion to Hogwarts enjoyable for the whole family. Minor flaws, old and new alike, keep Lego Harry Potter from becoming a family-friendly classic, but it's charming and funny, and most importantly, it's fun. And that kind of magic is all too uncommon, even in Harry's world."
94,https://www.cnet.com/culture/humanoid-bot-hitches-a-ride-on-roombas/,CNET,2010,6,10,181.0," What do you get when you put a humanoid robot on top of four Roomba floor-cleaning bots? The ultimate meta robot--and hopefully some very clean floors. Scientists at the University of Bonn's Autonomous Intelligent Systems Lab bolted together four Roomba 530s to create the Roomba QuadDrive, an omnidirectional moving robot base with individually steerable axes. The QuadDrive can sustain weight up to about 44 pounds, so the AI guys decided to plop their Robotinho android on top of it and put it to work as a temporary tour guide at Bonn's Deutsches Museum. Robotinho, originally developed for the RoboCup Humanoid League TeenSize class, is equipped with an expressive head, movable cameras, a laser scanner, and ultrasonic distance sensors. In the video below, you can see Robotinho rolling around the German museum at the QuadDrive's maximum pokey speed of 0.5 mph and explaining the exhibits to a group of amused (or is it terrified?) schoolchildren. The QuadDrive can map indoor environments and avoid obstacles. Which is probably a fortunate thing when it's bot-surfing its way around valuable works of art. (Via BotJunkie)"
95,https://www.cnet.com/culture/the-next-five-years-of-the-x-prize/,CNET,2010,5,16,1293.0," At a gala charity event Saturday night featuring ""Avatar"" director James Cameron, Google founders Sergey Brin and Larry Page, and a who's who of tech industry luminaries, the X Prize Foundation laid out its vision for the next five years. Already in 2004, the foundation has paid out $10 million in prize money for the winner of the Ansari X Prize, which in 2004 went to the first non-governmental team to launch a vehicle into space twice in two weeks. The prize winners were Burt Rutan and the Paul Allen-backed team that built SpaceShipOne. The foundation offers both X Prizes--$10 million or more for large-scale competitions focused on global problems that may take between three and eight years to solve--and X Challenges, awards on the order of $1 million for technology challenges and breakthroughs that have a time frame of one to two years. Currently, the foundation is offering up millions of dollars in prize money for the first teams to sequence 100 genomes in 10 days (the $10 million Archon X Prize); to send a robot to the moon, travel 500 meters and transmit video, data and images back to Earth (the $30 million Google Lunar X Prize); and to produce green, production-ready cars capable of exceeding 100 miles per gallon or its energy equivalent (the $10 million Progressive Automotive X Prize). In addition, it's planning on four other specific X Prizes: the AI Physician X Prize, which will be won by the first team to build an artificial intelligence system that can offer a medical diagnosis as good as or better than a diagnosis from a group of 10 board-certified doctors; the Autonomous Automobile X Prize, which will go to a team that designs a car capable of beating a top-seeded driver in a Gran Prix race; an unnamed X Prize for generating an organ from a terminal patient's own stem cells, transplanting the organ into the patient, and having that person live for a year; and another unnamed prize for building a deep-sea submersible that scientists could use to explore the ocean floor and gather complex data. Now, having spent Friday and Saturday cloistered in meetings of its ""vision circle,"" the X Prize Foundation is ready to define a general roadmap and strategy that will likely govern its operations in the coming years. The planningIn March, X Prize Foundation Chairman and CEO Peter Diamandis explained to CNET that the organization's four advisory groups would shortly be meeting to discuss areas of research and development that are ""stuck."" Those four groups focus on energy and the environment; education and global development; life sciences; and ocean and space exploration. And in an interview this week, X Prize Foundation President Bob Weiss told CNET that as it prepares for the future, the organization needs to focus not just on being a place that offers prizes, but also on solidifying its position as an ""innovation engine."" The question the foundation has to answer, Weiss explained, is what happens after an X Prize is won and after the winner has shifted the paradigm in the specific subject area. The foundation has decided to engage with a growing number of corporate partners and is trying to figure out how it can take its rightful place in the innovation ecosystem that it supports. One element of the five-year vision is to put more of the foundation's energy into working with its corporate partners to try to identify suitable prize areas within the problems facing those companies. So the idea, Weiss said, is to evaluate the partners' research and development efforts and see which of those efforts are posing technological or philosophical conundrums that may merit their own X Prizes or X Challenges, or possibly an internal competition. As an example, Weiss pointed to a project the foundation did with NASA in which just such an internal examination produced 90 ideas that were eventually whittled down to nine and turned into the Centennial Challenges, the space agency's prize program for the ""citizen innovator."" One of the results of that was the Northrop Grumman Lunar Lander X Challenge, which tasked teams with putting a vehicle in the air to an altitude of more than 50 meters, flying it laterally for 100 meters and then landing on a pad. Surveying the 'prize-scape'One of the major elements of the five-year vision is to try to commit the foundation to roll out about two full X Prizes and as many as three or four X Challenges per year. ""We can't do an unlimited number of these,"" Weiss said, ""but we think we might have as many as eight competitions going on simultaneously when we come up to speed."" Identifying some of those potential competitions will be a big part of the visioneering workshops that the foundation held Friday and Saturday. During the two days, Weiss said prior to the meetings, the four working groups were expected to examine potential big issues needing solutions, and see if they could define specific X Prizes or Challenges that would solve them. Going forward, once the ideas are on the table, the foundation's working groups will go through an ""ideation phase,"" in which they will discuss the ""broad prize-scape,"" essentially what could be accomplished in a particular prize sector. And then, the foundation will zero in on the particular prize to launch. Then they will enter the design phase, in which they will drill down and craft the rules and guidelines for the prize. And finally, they will enter the business phase, in which they look at issues such as what it could cost a team to enter the competition. Often, teams invest a great deal more in their entries than they would stand to win. The whole process of developing each new prize will last about four to six months, Weiss said, depending on the complexity. But what is vitally important, he continued, is that the goals of each prize be reachable, albeit worthy of being an X Prize or X Challenge. ""The landscape is littered with broken prizes,"" Weiss said. ""It's a lot more than offering a purse. It's a combination of art and science....That's a key part of the design. What's the point of throwing a party if nobody shows up?"" Indeed, Weiss said, the foundation will likely have advance in-depth discussions with many of the teams that will eventually enter the competitions in a bid to attract realistic competitors and ensure that there is an actual likelihood of someone winning. ""That absolutely informs what we do so we don't get into"" the situation in which a prize is unwinnable. Already, the X Prize Foundation has teamed up with Cisco Systems, which was named its official Energy and Environment Prize Group partner. Now, Weiss said, the foundation is looking for new partners and expects to name a corporate Life Sciences partner by this fall. And as the organization moves into the next five-year phase, it is looking to stay as lean as possible, while adding new capabilities. The success of that, Weiss explained, will depend on leveraging the foundation's partners and other relationships. Ultimately, though, Weiss and others in the foundation have realized that what will make it strongest is not to house all of its competencies in-house, but rather to look for outside partners to work with in order to bolster its ""intellectual capital across"" the four working areas. Saturday night, then, in front of a room packed with the leaders of Silicon Valley, folks like Segway inventor Dean Kamen, Zynga CEO Mark Pincus, Google CEO Eric Schmidt, YouTube's Chad Hurley, and many others, including Cameron, Brin, Page, and Diamandis, the foundation will set out on its next five-year journey. If it succeeds, the world is likely to be a better place."
96,https://www.cnet.com/culture/how-go-shaped-a-crowdsourcing-business-career-q-a/,CNET,2010,5,4,2087.0," After the devastating January earthquake in Haiti, which left hundreds of thousands dead, many more injured, and more than a million homeless, the country's infrastructure lay in ruins. But because Haiti's cell phone network inexplicably survived, text messaging quickly became a way for people to put out pleas for assistance. Not long after the disaster, the country's largest cell network provider set up the shortcode 4636, allowing Haitians to text a call for help. But with thousands of texts flooding the system, and a need for many of them to be translated into English, relief workers needed a technology backbone to help with the work flow. And that was a niche that a San Francisco start-up called CrowdFlower was able to fill. CrowdFlower has established itself as a leading provider of crowdsourcing resources for helping businesses and other organizations solve large problems. Among its clients are Microsoft, Princeton University, and O'Reilly publishing, and it had previously created an iPhone app called Give Work that makes it possible for Americans with spare time to assist in making sure that work being done for Western clients by Kenyan refugees under a program coordinated by the nonprofit Samasource is accurate. In Haiti, CrowdFlower's technology made it possible to route the thousands of text messages to the proper aid workers, to get them translated quickly, and to ensure that the people sending the texts had the best chance of getting what they needed. Once CrowdFlower's technology was implemented, the average time to translate, map, geocode, and categorize a text fell to less than two minutes. CrowdFlower's CEO, a Stanford engineering graduate named Luke Biewald, took a trip to Haiti in the middle of this process to help get the technology set up properly. This was not about making money. It was about trying to make a difference, and for Biewald, who had studied in Japan and who promotes the idea that people doing useful work helps their mental health, being in Haiti and trying to help the people there was an education in itself. Biewald recently sat down for an interview to talk about his experiences in Haiti, his education, and the unlikely game that has helped shape his business career. Q: Welcome to the second installment of ""45 minutes on IM."" Thanks for taking the time to talk to me. I want to jump right in. So, tell me, how did you become an expert-level Go player?Biewald: How did you know about that? OK...it's in your official bio on your site.Biewald: I've always loved games. When I was in high school I read a book about Go and realized it's the best game. I was an exchange student in Japan mostly because I wanted to play a lot of Go, and in the year I was there, there wasn't much else to do. There was a chess master that said, ""If aliens exist they might play chess, but they certainly play Go."" Nice. What do you like about Go? Biewald: It has incredibly simple rules and is a beautiful game, the only well-known game that artificial intelligence hasn't solved. There's something beautiful that happens when simple rules lead to a complex system. It also teaches me patience and composure and handling ambiguity. You're reminding me of the main character in the book ""Shibumi,"" by Trevanian. You haven't also lived in the Basque region of Spain and been a world-class adventurer and assassin, have you?Biewald: I love ""Shibumi."" And I wish....So, no one has ever asked me this, but I think it's helped a lot with my business, or at least it's shaped my style of management. How so?Biewald: Well, Go is all about handling ambiguity. For example, do I want a region of the board to be well defined or undefined? Beginners stress out too much about leaving territory unclaimed or stones half-captured. But you learn often that's advantageous. I think a big part of running an early stage start-up is not freaking out about things being left undefined. Well, before we get to your business, I wanted to ask you about something else. I saw that in a previous position, your title was senior scientist. That's a pretty cool title. Did you wear a white lab coat?Biewald: I loved that title. Computer scientist is not as cool as a materials scientist, but I think it's accurate. Mostly, I did experiments all day long. I was working on artificial intelligence algorithms to make better search results. OK, let's talk crowdsourcing. Without talking about Crowdflower, tell me if you think crowdsourcing is really able to make a big difference in the world, or whether it might be more of an overblown trend.Biewald: Sure. Obviously, I think the answer is yes--especially since crowdsourcing is becoming so broadly defined these days. I think an interesting question is, has crowdsourcing already made a big difference? Certainly I use the fruits of crowdsourcing all day long. Probably most of us on the Internet do. Give me an example of how an average person would use it.Biewald: Without user-flagging, most sites with user-generated content would be overrun with junk. Although you might say they are anyway. But it could definitely be a lot worse. Remember newsgroups? I think there are a lot of great tools for small businesses these days-- 99designs, voice123, getsatisfaction, etc., that are helping a lot of small businesses do more and get off the ground with less capital. And if you use Netflix, I'm sure their recommendation algorithm has benefited from the contest they ran. And it sparked so much great research, I bet most other forward-thinking sites with a recommendation engine also have improved because of it. And if you've ordered a pizza from Pizza Hut, you've used LiveOps, which I think of as crowdsourcing in some sense also because they use distributed call centers so people can work from home and take calls, orders, etc. Was your experience with things like you've just been mentioning a big factor in your thought process in creating Crowdflower?Biewald: Definitely. I think a lot of these practices work surprisingly well, especially ones with monetary incentives. Mechanical Turk pays people to do small tasks and relies on the requester to pay the person or not pay the person. There's no trusted relationship here, it's all brokered anonymously, and yet most of the people working genuinely want to do a good job and most of the requesters pay most of the people that do good work for them. And you might say, well, it's a marketplace where it's in people's best interests to preserve their reputation. But I still think people are irrationally--in the economic sense--willing to cooperate and work together even without social pressure. How did that all come into play with Crowdflower? Biewald: Big businesses have huge amounts of data that needs processing and they benefit tremendously from the scale of crowdsourcing, but they assume the quality won't be high enough. Or they're--rationally--unwilling to take a risk on unknown quality work. They think the kind of person who would want to do a random task anonymously won't generally do a good job. But, one, they're wrong about that. And, two, we can build automated systems to get very high-quality work, even if the worker population has some people who either don't understand the task or are actively trying to sabotage the system. So, obviously I'm not the only one to notice this, but I saw an opportunity here. Well, let's talk a bit about Haiti. Talk about what you were doing there, and about your experience there.Biewald: Sure, that's a great example of this. Right after the earthquake a brilliant guy named Josh Nesbit convinced ComCell and DigiCell to set up a shortcode that Haitains could message emergencies to. It was 4636. It turns out the cell infrastructure was mostly intact and was back up right after the earthquake. They advertised the number on the radio and had a huge response rate. The problem was the U.S. State Department didn't have a lot of people who spoke Kreyol available. So people would text things like ""I need water"" or ""I'm nine months pregnant and I need a doctor,"" and the Red Cross, Coast Guard, etc., wouldn't be able to get the messages translated for a long time. So Robert Munro, a friend of mine, and a linguistics Ph.D. student at Stanford, was working on automated translation. But it's too difficult--text messages are too idiosyncratic and the messages are too important to screw up. So they started collecting volunteers in the Haitian diaspora around the world. But you can imagine the logistical nightmare here of routing text messages, making sure that everyone is working on a different message, and all in real-time. And it wasn't just translation--the messages had to be classified and also geocoded to be sent to the right agency and give the agency the right address to go to. I realized CrowdFlower had exactly this technology. Actually, the State Department looked a lot like other customers we've had: a sudden spike of work, but not enough people to handle it, and also cares a ton about quality and short latency. So it was a really nice fit. And in fact we're still doing it months later. Nice.Biewald: One of our partners, SamaSource, had set up an outsourcing center in Haiti. So now most of the work gets sent there, which is even more awesome because it gives badly needed jobs to people in Haiti. For you personally, it must have been strange to be in Haiti, a Westerner visiting this disaster-ravaged country. Did you feel out of place?Biewald: It's a funny thing to say, but I felt privileged to be able to see it. I had never been to a developing country before. It sort of made me uncomfortable--something like disaster tourism. But it was also great to meet the people who do a lot of the work now and see some of the problems they have. At one point during the training the electricity went out. So they all pulled out their cell phones and read the manuals that way. What would you say was the most profound lesson--if that's not too strong a word--that you learned while in Haiti?Biewald: I think a lot of people figure this out faster than me, but to viscerally see how important jobs are and how much impact it's possible to have by giving people jobs has really stuck with me. It took several hours to get the Internet working and workers just waited patiently the whole time. I was really moved by how hard they worked and I hope we can send them lots of work in the future. We're almost out of time, so I want to move on. So, tell me what your favorite new tech tool from the last few months is.Biewald: Interesting question. Let me think for a second, I'm kind of a Luddite. It always makes me laugh when people in important positions in the tech industry confess to being Luddites.Biewald: I always tell people I'm the opposite of a lifehacker. Maybe I shouldn't admit that. So, I've been playing with Foursquare a lot since they came out on BlackBerry and it's been really fun. I guess they're older than a few months. What do you like about it?Biewald: I love the term ""mayor."" It's so perfect. It's the best title. Are you the mayor of anything?Biewald: I've been trying to become the mayor of Crowdflower. I put a sign up on our door saying ""the mayor drinks for free."" And a homeless person walked in and claimed to be the mayor. I love [San Francisco's] Mission district. Well, we're almost out of time, but I have to ask, because I asked my first ""45 minutes on IM"" guest, Gary Vaynerchuk: IM is great. It gives you a terrific transcript and allows both people to be thoughtful and articulate. But it also lets you multitask a little bit. So, what else have you been doing while we've been chatting?Biewald: Hahaha. Mostly, other employees have been IMing me, and also a guy I told who I was doing an interview walked into the room. And I have a feeling he thinks I just lied to him to blow him off. So I will have to explain what this was. Well, you can show him this interview when it publishes as proof.Biewald: I'll do that. Well, that's time. Forty-five minutes goes fast. Thanks so much for doing this. I appreciate it."
97,https://www.cnet.com/culture/apple-buys-virtual-assistant-app-maker-siri/,CNET,2010,4,28,177.0," Another day, another Apple acquisition. On Wednesday, an application developer called Siri confirmed to the Business Insider blog that it had been acquired by Apple. There were no details about the transaction available. Siri's main product is a mobile virtual personal assistant that allows users of the iPhone or iPod Touch, to ask questions about finding consumer goods, services, or destinations. The company developed out of an artificial intelligence project at SRI International, and was financed by DARPA. The free iPhone app lets users speak or type questions, such as ""What's a romantic place for dinner?"" The app uses an algorithm that interprets the request as it applies the person's location, time, preferences, and other context. It was first demonstrated for the public at last year's D: All Things Digital conference. This is the second Apple acquisition this week. On Tuesday, the company confirmed it had acquired Texas-based chipmaker Intrinsity. The rumored price for that acquisition was $121 million. This post was updated at 11:48 a.m. PDT to correct the name of the non-profit where Siri originated."
98,https://www.cnet.com/culture/gdc-whats-next-for-video-game-ai/,CNET,2010,3,11,1216.0," Three next-gen AI demos at Wednesday's Game Developer's Conference showed off things that would seem like every day occurrences. But if their creators can get them to work, this distinct mundanity is a crowning achievement. Richard Evans of Maxis and EA fame was the AI lead on the Sims 3. His demoed two new types of AIs, the first--called ""Sim Tribe"" allows developers to create their own societies. These societies can work on the same social rules as real cultures, meaning that if a player ventures into a different location, the other nonplayable characters (NPCs) will change their habits accordingly. In terms of the demo, this paradigm shift was with eating, which became a social taboo. This acted out comically as characters could not eat until everyone else was outside of viewing distance. Evans joked that you could also reverse the rules so that going to the bathroom in public became an every day activity. What made the entire system more interesting was that there were also societal punishments built in, so that if players or other NPCs alike disobeyed the social norms, it would change how other characters interacted with them. Evans' second demo was something equally ambitious that gave NPCs ""very long term plans."" The idea is that developers can give NPCs hundreds of actions that the characters can (and want) to do in their every day lives. This is as opposed to the three to four actions Evans said most developers will program out for an NPC. In terms of the demo, that kind of scale panned out to a character named ""Bob"" whose character type was a workaholic. Not only did he work long hours at a nearby restaurant, he also wanted to be promoted. In order to get that goal, Bob proceeded to befriend his boss, buy cookbooks, take cooking classes, and practice cooking. When his apartment got dirty, he even hired a maid so as to save time he would have had to spend doing it himself, leaving him free to learn more skills. What made the demo really impressive though, is that Bob's actions were working in a world full of other NPCs doing the same thing and with different programmed personalities. As Evans explained it, all that was needed to make this really work was to figure out some of the steps needed in getting from a desire to a goal and mapping those things out within a social structure. From there the NPCs will simply go about their lives doing these actions and interactions to reach whatever goals are set. Computational biologist Ian Holmes from UC Berkeley had a very different layer of AI to show off from Evans. Instead of going for social normality with people, he went with animals and the environment they live in. Holmes demoed something called Zoo Gas, which has players trying to create and maintain a zoo. This involves keeping certain animals apart, as well as breeding them, and keeping them away from tourists with a fence that begins to decay over time. Where Holmes sees the title going is toward a massive multiplayer online game (MMO) in which players get their own chunk of land in a larger online world. It will be their job to breed certain types of creatures, as well as work with others to create new types. All the while they'll be fighting off things like disease, mutation, overpopulation, and equality with a human population. Though AI isn't just about creating people going to work and fornicating animal life. UC Santa Cruz professor Michael Mateas and his colleagues are attempting to create enemy AI for the popular video game StarCraft, that's less of a pushover than what comes with the game. ""With current real time strategy AI, moderately skilled humans can kill the computer,"" Mateas said. ""We're interested in getting close to human level competence not because it's interesting but to start thinking about creating StarCraft agents who play with a certain style."" Mateas wants to make it possible for the computer to emulate the playing style of specific people, as has been done classic games like Go and Chess. This would let someone play a champion right out of the box and maybe even learn a few of their tactics. All without ever having to hop online, or enter in a tournament. Mateas even wants to create a system where users can dial down that competitor's AI, so as not get beaten in just a few minutes. In order to do all this Mateas and two doctoral students have created an AI system that runs specific parts of StarCraft's game functions but works as one so that each component is aware of what the other one is doing. This keeps any one resource from burning the other, be it displacing troops, or not gathering enough resources. This is all being done behind the scenes in a program that runs in the background. According to Mateas, they recently got the legal approval from StarCraft-maker Blizzard to use it in competition. Thus far, they've been running it on various StarCraft servers and seeing if it can beat human opponents and are winning one of of every five matches. Mateas wants to get that higher and is using a mix of machine learning and data mining to figure out more complex strategies. Mateas' second demo showed a system that could have big implications on social gaming. In this case, it was three characters planning a ""counterculture prom."" Each of the characters had specific relationships with one another. These could be affected by conversation that would change dynamically based on how other characters had interacted with each other in the past. For the demo, it was a love triangle of sorts. One character wanted to ask the other one out, but he knew his friend did not like that person. In order to get over the love triangle problem, Mateas demonstrated two solutions. One with the player attempting to make the two characters who did not like each other, like each other enough through conversation so that a date could be made without offending any parties. The second had the same set of characters, with one of them simply insulting the other character into not being their friend any longer, so that asking that other character out would not cause a problem. Mateas said he wants to scale the system up enough to be workable in even the most basic social games. The brunt of the work is actually building out each character's knowledge base with things that happened to them in the past and enough dialogue options. The reward, Mateas said, is ""endlessly ramifying social possibilities."" In other words, a game that's different every time and that learns as it goes. So what are the chances you'll see any of these AI systems in games of the future? Pretty good. Holmes has shared the source code for Zoo Gas here. And Mateas' StarCraft AI bots are a part of a tournament being held at UC Santa Cruz in October that could end up making them even better. As for Bob the workaholic, considering Evans' Sims cred, the quirky character could be one of the many NPCs you end up competing with in a future edition of the game."
99,https://www.cnet.com/culture/internets-future-on-display-at-singularity-u/,CNET,2010,3,4,1088.0," MOUNTAIN VIEW, Calif.--The Internet of the future is an intelligent network capable of proactively acting on our needs, following us wherever we go, helping provide us with focused health care, and possibly ushering in a new energy paradigm. This is the vision that James Canton, CEO of San Francisco-based Institute for Global Futures think tank, shared with students in the executive program of Singularity University. His  broad-reaching, theoretical talk here Wednesday touched on many of the same elements of the all-encompassing network more or less overlaid on people's consciousnesses in science fiction by the likes of Vernor Vinge. Still, Canton's vision seemed plausible, particularly in light of the curriculum of so-called exponential technologies being taught at Singularity University, which kicked off its first classes last summer. Canton's vision of this future Internet begins with four key drivers: telepresence, mobility, artificial intelligence, and specific vertical market segments such as health care. In a straw poll of the 40-odd students in the Singularity program, the majority felt that mobility was the most important of those drivers, and Canton said this made sense given that billions of people use the Internet and that the figure will only grow. The idea, then, would be for the Internet of the future to comprise large numbers of networks talking to nodes that are independently communicating with one other, ""having their own conversation,"" he said. Indeed, Canton predicted a future in which the Internet is embedded just about everywhere: in every imaginable kind of object--from TVs to phones to walls--and that every product and device--even people--will have an IP address. He added that while such a vision may seem distant, the U.S. Food and Drug Administration has already approved a chip that could be embedded in people's bodies. In Miami, he noted, the latest fad is women wearing clothes with chips embedded that can be scanned to verify their identities so that they don't need to carry purses. Similarly, he said, government workers in Mexico City can't get into buildings without having  some sort of wearable identification chip. It will be a key component, then, of the Internet of the future, Canton predicted, that everything will have an IP address and that, thus, we will be living in a ""blended reality"" where information is constantly streaming at and around us across physical and digital artifacts. Proactive search Today, Canton said, in order to find information online, we have to turn on our computers and go look for it. ""But what if you didn't have to do that?"" he asked the executive students. ""What if it found you?"" The idea, he said, is a worldwide system of intuitive networks that pay attention to us, know our likes and desires, and proactively feed us the information we need to act on such preferences. ""We're on the cusp of that,"" he said, with the Internet ""intuitively sensing who you are, and what your needs are and paying attention to your behavior and to what you think is important."" Such systems wouldn't just be with you at home. They would travel with you everywhere, he suggested. The kinds of devices we see as discrete today--our phones, computers, TVs, and cars--will ""at the end of the day, all...get mashed up,"" he said. That means, he said, that each of us will have our own ""personal Internet layer...that lives in your own personal Internet cloud [and] deciphers what's next"" for us. It's not clear when such a system will be functional, he acknowledged, given that it would require a great deal of artificial intelligence that has not yet made its way into consumer technology. But it's not so far off, he suggested. In fact, he said, as much as 30 percent of the technology necessary for such concepts to be part of our everyday lives has already been built. And what's in the lab today, he pointed out, is in the marketplace tomorrow. Canton added that the model for such AI-based systems is one that already dominates the planet: biology. The networks of the future will mimic living ecosystems, he said. At Singularity University, students are getting high-level, intense lectures on fields of study such as nanotechnology, biotech, AI, robotics, bioinformatics, and the like--all of which fall under the rubric of exponentially growing technology. And the Internet of the future is essentially a mashup of these technologies, Canton said. As a result, the Internet will be smart in a way we can barely imagine today and could finally help us solve systemic crises like health care and poverty while creating thousands, perhaps millions, of new companies in the process--or even entirely new markets. Megacities As the Earth's population expands, it will result in the blossoming of dozens of new megacities, Canton said, but current data infrastructures are incapable of handling the needs of the new metropolises. ""There is not enough storage or bandwidth to deal with this reality,"" he said. ""We have to get better...at how we enable that future to emerge."" Perhaps as many as 80 of the next 100 megacities will require next-generation Web infrastructure, he predicted, and society will have to find ways to ""migrate to that infrastructure."" Ultimately, the ""one key bucket of technology"" that may drive the future of the Internet is quantum mechanics, Canton said, and that will create new dynamics such as humans being able to ""design space and time"" and the possibility that the contents of the entire U.S. Library of Congress could fit on something no bigger than a thumbnail. In the process, we may be able to access and process in real time so much medical data that we will have the wherewithal to eliminate huge numbers of deaths or illnesses. Internet 3, 4 and 5 Some may feel that it's too early to be defining such a future, but Canton clearly isn't one of them. We are currently living in the ""Middle Ages"" when it comes to computing networks, he said, and we are surrounded by dumb devices and machines that cannot think for themselves. However, he said, an ""Internet 3"" will be a ""collaborative Web"" that will have at its root a cooperation between people and machines. His vision of an ""Internet 4"" includes evolutionary networking that has a major self-organizing principle and has human reproduction as an inspiration. ""Internet 5"" will mimic living ecosystems and feature smart and aware physical spaces, embedded intelligence, and systems that can take actions such transferring energy among themselves as needed. ""It's not that far out,"" Canton said. ""I believe it's already started."""
100,https://www.cnet.com/culture/at-singularity-university-blowing-minds-and-taking-meetings/,CNET,2010,3,2,1674.0," MOUNTAIN VIEW, Calif.--For Rob Nail, Saturday was a bonanza of opportunity. Over dinner that night in building 20 at the NASA Ames Research Center here, Nail found himself discussing 3D printing and housing with X Prize CEO Peter Diamandis. Already, Nail had been considering buying some farming land in Northern California and had been interested in the nascent concept of 3D printed buildings. He told Diamandis that he wanted to try that on the land. ""He says,"" Nail recalled, ""I want to make this introduction,"" and grabbed Nail, pulling him a few tables over to the side where the two put their heads together with one of the founders of a start-up that recently began working on building 3D printed housing for developing nations. For Nail, himself an entrepreneur who has spent several months looking for companies to invest in or advise, the quick meeting may have been the start of something long-term. ""I will probably have a relationship"" with the start-up's co-founder, he said. ""It's an opportunity for me to get involved as a seed investor, and to advise and help out. He's just starting out...and we have a common connection with this passionate interest for housing, robotics and 3D printing."" For Diamandis, putting Nail together with a potential business partner was emblematic of his own young venture, Singularity University, or SU, which seeks to put some of the brightest minds on the planet together to explore what is known as exponentially growing technologies. And earlier that evening, after they'd sat through eight hours of high-energy lectures on artificial intelligence, autonomous robotics and biotechnology, and bioinformatics, Diamandis stood up and welcomed his new charges--a group of 43 participants in the second 10-day Singularity University executive program--to the ""family."" If one thing is clear about SU, which Diamandis co-founded with ""The Singularity is Near"" author Ray Kurzweil, it's that the product the two men are selling is membership in that family, a network of top-tier thinkers, investors, researchers, and entrepreneurs that could hold its own against any other, anywhere on Earth. Just look at the backgrounds of the 43 C-level students in the executive program that kicked off Friday night, each of whom paid $15,000 to be here: a top decision maker in the U.S. Department of Defense, a best-selling author of business books, a Uruguayan venture capitalist, the owner of an entertainment marketing company, a Scottish investment manager with a Ph.D. in chemistry. And on and on. And this is just one iteration of SU. Already, the institution has graduated its first summer program of 40 graduate-school level students and its first group of 20 executives. And those high achievers don't even include SU's faculty, which is comprised of the leaders in fields like biotech, nanotech, artificial intelligence, robotics, 3D printing, and everything in between. Pulling people together Nail is a tall, reed-thin 37-year-old entrepreneur from San Francisco with long brown hair, blue jeans, and black leather boots. A master's graduate of Stanford's engineering school, Nail had helped start Velocity11, which built robotics and automation equipment for cancer research and drug discovery, and as its CEO, he grew it to $50 million in sales and membership on several most-exciting-in-Silicon Valley-type lists. In 2007, Agilent Technologies bought Velocity11. But life inside a $6 billion giant like Agilent frustrated Nail's inner entrepreneur and last September, he left. Now, Nail wanted to see what the SU executive program was all about. He knew that taking part would give him access to a valuable network and hoped that being exposed to beyond cutting-edge technologies might help him figure out the next big project on which he could apply what he considers his most valuable skill: rallying people to do big things. And this is a world that needs people to do big things. With global-level crises in hunger, energy, water, and more, ""We have to make some big moves if we want to save humanity,"" Nail said, a sentiment that is measured compared to some of what is being taught at SU. Build an incubator During the summer program, a nine-week course that among other things tasks the students with working on team projects in which they seek to come up with some sort of system that could positively impact a billion people, the group whose start-up aims to build 3D printed housing in developing countries wasn't the only one to birth a business. Indeed, one of Diamandis' goals is that each set of students--graduate-school level or executive level--that comes through SU will generate new companies. Members of the network will ""become best friends,"" ""invest in each other's companies"" and, Diamandis hopes, help shape the solutions that will overcome the world's biggest problems. But there's no reason those start-ups can't make a profit in the meantime, and that's where Nail may have stumbled onto a second significant opportunity on Saturday. ""I've got a lot of seeded things,"" Nail said, ""and one of them that I've been developing over the last few months is the idea of an incubator."" His idea is that he's surrounded himself during the past 11 years with some of the best business people, investors, engineers, lawyers, and marketers around, and so, why not deploy that personal network to help get smart start-ups off the ground. At the same Saturday night dinner, Nail and Diamandis continued their conversation and found some further common ground: Diamandis knows that SU students are going to come up with ideas for new companies and they're going to need help making them a reality. So why not, he said, combine their mutual interest and have Nail join the faculty and help put together an SU incubator? And Saturday held more opportunity in store for him, as well. Just prior to coming to SU, Nail had spent several days at TED Active, another gathering of world-class thought leaders and had struck up an acquaintance with Jamie Oliver, a chef famous for promoting an agenda of healthy, unprocessed foods. After seeing Oliver at TED, Nail began thinking about a way to promote a ""Food Revolution"" built around Oliver's passions for healthy eating and cooking, but knew that in order to succeed in any venture, they would need to work with some of the ""big names and minds"" with similar thinking. On Saturday afternoon, at the first SU lunch, Diamandis introduced the group to Charlie Ayers, the former Grateful Dead chef who became famous when Larry Page and Sergey Brin hired him to be Google's chef and who also promotes a very clean, healthy cuisine built around local ingredients and unprocessed foods. Nail sprung into action, and later that day, shared his growing enthusiasm with Oliver. ""I've had the pleasure of recently meeting Charlie Ayers,"" Nail wrote Oliver in an e-mail. ""I think his voice would be a great addition--specifically from the standpoint of creating a real corporate food model that works! I pitched him on the idea of being more active at getting his knowledge of what works...out there, playing his role in this food revolution on a bigger, more active stage, and meeting you. He is definitely behind the movement and is interested in helping in whatever way he can."" An education too Lest anyone thing Singularity University is all deal making, it should be known that there are also hours and hours of daily lectures on the very latest technologies that are shaping the world's future. On Saturday, Nail and his fellow students listened as Neil Jacobstein, the president of a technology company called Teknowledge and a consultant for organizations like DARPA, NASA, General Motors, Boeing, Applied Materials, and many others, filled them in for about an hour on the latest and greatest innovations in artificial intelligence. Then, they sat riveted as former NASA astronaut, ""Survivor"" contestant, and autonomous robotics researcher Dan Barry talked to them about topics as diverse as robotic adaptation and experiencing weightlessness in space for the first time. Later that day, bioinformaticist Raymond McCauley and Andrew Hessel filled the students in on the latest in biotechnology, open source biology and bioinformatics. The point of teaching SU's students about exponentially growing technologies is so that they can learn, and learn quickly, how fast those technologies are changing in order to get in front of them and exploit them. ""My description of Singularity University after the first day,"" Nail said, ""is that SU is a new Paul Revere, streaming in to warn humanity that technology is changing much faster than we could have previously imagined, and we'd better get ready."" Even for someone like Nail, with a master's in engineering who helped run a company specializing in biotech and robotics, SU's subject matter was a little overwhelming. ""The density of the information and how fast so many different things were moving so quickly today, it made my head spin,"" he said. ""I'm a technology geek, and in the Silicon Valley circle, and I was amazed at how many things I hadn't seen before."" And Sunday's series of lectures on networks and computer systems from the likes of Sun Microsystems chief scientist John Gage and SU executive director and former Yahoo Brickhouse head Salim Ismail and on medicine and neuroscience from Stanford stem cell instructor Daniel Kraft and others did nothing to douse Nail's enthusiasm. ""If I were to quantify my excitement about the program and the power I believe [it] has for shaping the world after day one,"" Nail wrote Sunday night, ""I would have to say [that] on day two it doubled (and you saw me after day one, so I think that is saying a lot!). Unfortunately, my mind is not wired to comprehend where I may be if it continues to double for seven more days. My biggest fear now about the program actually comes from...my girlfriend's reaction after I described what I was getting out of the program after day two. She actually thought I was crazy! I'm afraid of what the rest of the world may think of me after seven more days of this. But I can't wait to find out!"""
101,https://www.cnet.com/culture/x-prize-group-sets-sights-on-next-challenges-q-a/,CNET,2010,3,1,1883.0," MOUNTAIN VIEW, Calif.--Having already set private space travel in motion, the organizers of the X Prize are ready to unveil the future of the cutting-edge competitions. On May 15, at a gala fundraising event to be held at George Lucas' Letterman Digital Arts Center in San Francisco, X Prize Foundation Chairman and CEO Peter Diamandis, along with Google founders Larry Page and Sergey Brin and ""Avatar"" director James Cameron will unveil their five-year vision for the famous awards. The X Prize first gained fame for its promise of a $10 million prize to the first private team that could put a manned spaceship into space twice in two weeks. That prize, which was announced in 1996, was won in 2004 by inventor Burt Rutan and his SpaceShipOne project. And in the years since, the X Prize has continued to offer juicy purses to teams capable of solving some of humanity's most pressing issues in the areas of the environment, education, global development, life sciences, and exploration. In 2007, the X Prize group announced a vision statement at a similar event at Google headquarters. But over the coming three months, the foundation is putting its biggest thinkers through a rigorous period of prize design intended to lay out an entirely new roadmap. Already, three funded prizes are in the works: The Archon X Prize, which will award $10 million to the first team to sequence 100 genomes in 10 days; the $30 million Google Lunar X Prize, which will go to the first ""privately funded team to send a robot to the moon, travel 500 meters and transmit video, images and data back to the Earth;"" and the $10 million Progressive Automotive X Prize, which will be awarded for producing ""clean, production-capable vehicles that exceed 100 MPG energy equivalent (MPGe)."" Now, as the foundation ramps up its design phase, it is focusing on several potential new prizes that could change the world of medicine, oceanic exploration, and human transport. The first is the so-called AI Physician X Prize, which will go to a team that designs an artificial intelligence system capable of providing a diagnosis equal to or better than 10 board-certified doctors. The second is the Autonomous Automobile X Prize, which will go to the first team to design a car that can beat a top-seeded driver in a Gran Prix race. The third would go to a team that can generate an organ from a terminal patient's stem cells, transplant the organ into the patient, and have them live for a year. And the fourth would reward a team that can design a deep-sea submersible capable of allowing scientists to gather complex data on the ocean floor. For the next week, Diamandis is spending the majority of his time at the second executive program of Singularity University, which he founded with futurist Ray Kurzweil. Diamandis sat down with CNET on Saturday to discuss the vision for the future of the X Prizes. Q: Tell me about what's happening in advance of the May 15 gala?Diamandis: Over March 8-11, we have our four advisory groups coming together. On March 8, the subject of energy and the environment. On March 9, the subject of education and global development. On March 10, the subject of life sciences and on March 11th, the subject of exploration, which is space and oceans. So, we will be discussing the areas that are stuck. What's the opportunity space for X Prizes, which are $10 million prizes, and X Challenges, which are $1 million-level challenges. We define X prizes as large-scale competitions with a prize purse of $10 million or more that are focused on global grand challenges that typically can be competed for and won in a three- to eight-year time frame. Less than three years is probably too easy, and more than eight years, no one cares anymore. It's off people's radar. X Challenges are nominally $1 million-level prizes that are more focused on technology challenges or breakthroughs that can be in competed for and won in one to two years. So, we'll take the work of these advisory workshops and feed it into a two-day meeting of the board of trustees and our vision circle, which is taking place on May 14 and 15 and during which our board shapes where we focus a lot of our energy. And then on the evening of May 15, we're rolling out our vision. The event on the evening of the 15 is co-chaired by Larry and Sergey [Google founders Larry Page and Sergey Brin] and [""Avatar"" director] Jim Cameron and myself [and our spouses]. And that's a benefit to help underwrite and fund some of the top prize ideas that are coming out of this process. Q: So what does the future of the X Prize look like?Diamandis: We are looking at a wide range of X Prize ideas that we're excited about. I'll just name a few. One in the life sciences area that I'm very interested about is an artificial intelligence physician, called an AI Physician X Prize, and this would be for design of an AI physician that can speak and listen in natural language and can diagnose a patient as good or better than a panel of 10 board certified doctors. It's a very measurable, objective test. And it's An X Prize that Ray Kurzweil and I have worked on defining together, and one that we're looking for a benefactor or corporate sponsor to underwrite. The implications of that are that by the end of 2013, 80 percent of the world's populace will have a cell phone, and anyone with a cell phone can call this AI and the AI can speak Mandarin, Spanish, Swahili, any language, and anyone with a cell phone then has medical advice at the level of a board certified doctor, and it's a game change. Another X Prize that we are working on and looking for a sponsor or benefactor for is the Autonomous Automobile X Prize. Imagine this as the Deep Blue of car driving, the first time an autonomous car is able to succeed against a top-seeded driver in a Gran Prix auto race. Q: What would be the game-changing element of that?< br />Diamandis: Today there are 2 million people who lose their lives worldwide because of automobile accidents. We drive 4,000-pound cars to avoid the injuries of a 5,000-pound car hitting us. As a result of that, our cars are very heavy and consume a lot of fuel. And then cars all go down [U.S. highway] 101 instead of fanning out and taking more efficient routes, because they don't know where they all are. The implication is that humans are the worst control systems to drive cars. We have hundred-millisecond delays. We're distracted. We can't know everything going on around us. But autonomous cars will be much better systems. They'll avoid collisions. They will take the most efficient route. They'll be able to weigh 1,000 pounds, not because they're made of stronger materials, but because they're never in an accident. And the autonomous car X Prize is about changing the paradigm of how we transport ourselves. There's no reason to own a car other than for vanity, when instead you can call on an autonomous vehicle on demand. If you need a Ferrari for a date, you can call on your autonomous Ferrari. If you need a minivan for a baseball game, you can call an autonomous minivan. You can get what you need when you need it. Q: That's assuming you can afford it.Diamandis: Assuming you can afford it. But because you're buying it by the byte, most things become affordable, because you can time-share it. Today, the average person only uses their car 4 percent of the time. The rest of the time, it's sitting uselessly on the pavement. Q: I recall that in the summer Singularity Program, one of the student teams formed a start-up around efficient car sharing based on that principle.Diamandis: Exactly. And it's a major success. And so, the autonomous cars X Prize will dovetail with Gettaround, which is their car-sharing protocol. Q: What other potential X Prizes are you thinking about?Diamandis: One in life sciences is organogenesis. [The winner is] the first team to be able to create a lung, liver, or heart from the stem cell of a patient who is terminal, have that new organ transplanted into the patient and have them live for a year. Also in exploration, we are looking at X Prizes to map the ocean floor. We know more about surface of Mars than the ocean floor. So the notion would be for the design of a new generation of robots and sensors that are fully autonomous, that can, in large numbers, gather data at all levels of the ocean, including imaging of the ocean floor and the sea mounts, which are some of the more interesting areas, and then return that data to help us build a three-dimensional model of what is happening to the oceans, because we have very little right now. Q: Is that like what Hawkes Ocean Technologies is doing?Diamandis: Yes, so another X Prize we're looking at is deep ocean/human submersibles, so the design and development of one-, two- or three-person vehicles to take scientists down to the ocean floor. So it would be the equivalent of the Ansari X Prize [which awarded $10 million to the first private team to launch, twice in two weeks, a manned spacecraft into space], but for oceans. Q: But isn't that what Hawkes is already working on?Diamandis: Well, working on. But they haven't achieved it yet, and that's the potential. The question is, would an incentive prize bring new capital to the market, would it help them bring in new players, and allow new risk taking? Also in energy, this summer, we're going to see the finale of the Progressive Automotive X Prize. We started last year with 136 vehicles from 20 countries that entered. It's now been narrowed down to 50 vehicles, and between April and August, we will see multistage competition narrowing it down to the top three winners. Q: Are all of these definitely happening?Diamandis: We have three that are currently launched. The Archon X Prize--sequence 100 genomes in 10 days. That is funded and operating, and we expect it to be won in the 2011 time frame. The $30 million Google Lunar X Prize. We have 21 teams from 12 countries competing and my guess is that will be won in the 2013 time frame. And then we have the $10 million Progressive Automotive X Prize, which will be won this summer. It's a date-certain competition. Q: And the others?Diamandis: The other ones are concepts that are being developed but are looking for a benefactor or sponsor. But we have on the order of 40 or 50 ideas that are being designed right now with our advisory groups and our board. Q: When somebody wins, does the X Prize gain any intellectual property rights to the winning designs?< br />Diamandis: No, right now in the competitions, the intellectual property is owned by the winning team. The only thing the X Prize retains is the media rights to tell the story. So the live TV rights and such."
102,https://www.cnet.com/culture/friday-poll-perfect-date-with-roxxxy-the-sex-robot/,CNET,2010,1,15,187.0," This week our collective hard drives (ahem) were titillated by news of Roxxxy, a life-size, artificially intelligent, and fully functional sex robot billed as the first of its kind in the world. Roxxxy comes with five preset personalities with names such as ""Wild Wendy,"" ""Mature Martha,"" and ""Frigid Farrah."" New Jersey-based inventor Douglas Hines of True Companion says Roxxxy can converse about topics including soccer and can sense when she's being touched, thanks to her tactile sensors. She's also wirelessly linked to the Internet, and users can customize her personalities and share them with others online. Clad in her negligee and gazing at us with her come-hither, albeit vacant, eyes, Roxxxy quickly cast a spell, evoking synthetic babes of sci-fi yore ranging from Maria of ""Metropolis"" to Pris of ""Blade Runner."" She also prompted us to imagine what it would be like to finally date an artificial lady programmed to make us feel good. So, what would you do if you had a night alone with Roxxxy?  Vote in our poll, and if you think up anything we've missed, leave a comment in our TalkBack section below."
103,https://www.cnet.com/reviews/canon-eos-5d-mark-2-review/,CNET,2012,10,9,1330.0," Editor's note, October 9, 2012: In light of changes to the competitive landscape, we've decided to adjust the rating of this camera by dropping the features subrating from 9 to 7. Though it's still an excellent camera, its 3-year-old feature set can't match that of more modern units; the video capabilities which were unique at the time no longer are. Three years is a long time for any product to hang around, especially when the technology changes as rapidly as it does for digital cameras. Though it's always had a big fan base, Canon EOS 5D users have nonetheless been itching for more. The successor Canon delivers: the EOS 5D Mark II is in many ways a must-have upgrade, especially for the wedding photography crowd for whom the 5D is a workhorse. And with many of the imaging components of the 1Ds Mark III (including a later version of the image-processing engine, Digic 4) for a price tag $5,000 lower, it's certainly an attractive alternative. It's also priced fairly aggressively compared with the competition despite its new 21-megapixel CMOS sensor and groundbreaking movie capture capability. The camera comes in two official configurations: the body-only or a kit version with the EF 24-105mm f/4L IS USM lens. Usually I'm not a fan of the lenses that ship as part of kits like this, but I ended up liking the 24-105mm a lot more than I expected and think it's a good match for anyone looking for a first lens to pair with the camera. As with all of the high-resolution models, however, it really makes a difference to go for the sharpest lenses in the arsenal. Slightly heavier than its predecessor, the Mark II weighs just over 2 pounds. Canon says it beefed up the dust and weather sealing a bit around the card cover and buttons and improved rated shutter durability for up to 150,000 cycles. The body itself is a steel chassis covered with magnesium alloy. But while it's clearly solidly made, it nevertheless doesn't feel quite as tanklike as the D700. Like all of Canon's pro dSLRs, it's very comfortable to grip and shoot. The downside of the updated design is that it takes new accessories, including a new battery and new vertical grip. Canon reorganized the controls a bit from the rest of its models. On the top sits the main dial plus four dual-purpose buttons that access adjustments for the metering (huge 3.5 percent spot, 8 percent partial, center-weighted, and evaluative) and white balance; AF (single, AI Servo and AI Focus) and drive modes; and ISO sensitivity and flash compensation. Unlike the Sony Alpha DLSR-A900, the top status LCD displays complete information; you can pull the current settings up on the rear LCD as well, but can't navigate them the way you can on that camera. I miss that, as well as the direct-control metering switch on the A900 and Nikon D700. The mode dial on the top left offers just the basics--as it should: Bulb, PASM, Auto, three custom settings slots, and the Creative Auto mode that debuted in the EOS 50D. The top rear right has buttons for initiating AF, exposure lock, and focus-point selection; down the left rear are the Live View/PictBridge, Menu, Picture Styles, Info, Playback, and delete buttons. Unfortunately, most of the buttons on the body feel identical to their neighbors. The 5D Mark II uses the same joystick multicontroller and Quick Control dial with Set button as its other recent models. I still like them. (Click through the slide show for more on the camera's design and features.) The viewfinder is slightly larger and a bit brighter than the 5D's. While it offers broader coverage than the D700's--98 percent versus 95 percent--it falls short of the 100 percent provided by the A900 and by midrange models like the Olympus E-3. C'mon Canon, eke out that last 2 percent, please. The most notable feature advantage the 5D Mark II has over its competitors is the movie-capture capability. Canon supports 1,920x1,080 at 30fps, true 1080p HD, with a mono mic built in and stereo mic input, with clips of up to 12 minutes (on a 4GB card). All things considered, it's a pretty nice implementation. Though you can't autofocus, you can adjust exposure while shooting; the optical stabilization works; and you can apply Picture Styles. Many of the new capabilities definitely target pros: a pair of low-resolution raw formats (10 and 5.2 megapixels), more interchangeable focusing-screen options, in-camera peripheral-illumination correction to compensate for brightness nonuniformity across the image, and a silent Live View mode. There's also Face Detection AF, but it only works in Live View mode. If you do HDR work, you'll probably find the 5D Mark II's bracketing implementation a mixed bag. It's incredibly flexible compared with most--in some respects. For instance, you can bracket in any increments of 1/3, 2/3, 1, 1 1/3, 1 2/3, or 2 full stops, centered around any EV up to +/- 4 stops. Unfortunately, it limits you to three exposures where other cameras let you do five or seven. Argh. The Mark II uses a new battery pack, the LP-E6, which seems to last a reasonably long time: it's CIPA rated at between 750 and 850 shots, depending upon temperature. It also supports some fairly advanced reporting features. For instance, you can register the packs and then the camera will track the date last used, number of shots you've taken on it since last recharge, and its ability to hold a charge, in addition to the remaining capacity on a charge status. However, the camera's still missing some features offered by the competition. Though one doesn't use the on-camera flash as a rule in this class, it really is nice to have in an emergency. Canon also continues its tradition of not including an in-camera wireless flash controller; some traditions deserve to die. And if you want onboard image stabilization, the A900's your only option. The 5D always felt a bit sluggish to me, despite actual performance numbers to the contrary. This camera delivers the same measured performance, but feels much zippier. And overall, it fares quite well compared with the D700. It wakes up and shoots in 0.3 second and takes between 0.3 and 0.6 second to shoot, depending upon lighting conditions. It typically runs about 0.4 second from shot to shot. For burst shooting, however, it's the slowest of all the new models, partly because of Nikon's significantly lower resolution and Sony's doubling up on the processors to maintain burst rates. Neither its 3.8fps burst-shooting speed (unlimited JPEG/14 Raw) nor its center-intensive 9-point AF system really lends itself to seriously fast, continuous shooting of moving subjects. And if your shooting style requires lots of AF points beyond the middle quarter of the frame, this probably isn't the camera for you. But for center focusers like me, it works quite well. I'm extremely pleased with the quality of the photos the 5D Mark II delivers. As you'd expect from a model in its price class, it renders accurate and consistent exposures and colors. Given its resolution, its noise profile is surprisingly good: no noise or noise suppression artifacts until about ISO 1600, at which point all you see is a slight bit of softening. Depending upon subject matter, photos can remain usable as high as ISO 12,800. My only quibble is with the overly warm tungsten white balance. Even the video looks and sounds good, though the mic could use a wind filter. (Click through the slide show for image samples and more discussion of photo quality.) When I first blogged about the camera in September 2008, I commented that it ""doesn't provoke the knee-jerk WANT response I expected."" After shooting with it for about a month, I have to admit, I'm sold. I want this camera. I love the Nikon D700 as well, but the 5D Mark II's higher resolution and surprisingly good video capture put it over the top for me."
104,https://www.cnet.com/reviews/dell-xps-m1730-review/,CNET,2007,10,23,1130.0," The latest revision to Dell's flagship line, the massive XPS M1730 features a redesigned case that combines familiar touches, such as flashing LEDs beneath the speaker grills, with newer elements, such as the integrated Logitech GamePanel LCD and 10-key numeric keypad. But, as with any gaming-oriented system, the most important features are inside the case. Like its predecessors, the XPS M1730 offers the highest-end components available--in the case of our review unit, a top-of-the-line Intel Core 2 Extreme processor, Nvidia SLI graphics, the first mobile PhysX processor, and two massive 7,200rpm hard drives. All that adds up to strong performance scores on our mobile benchmarks and chart-topping frame rates on our games tests--though a similar system with DirectX 9 graphics from Alienware did post higher scores on our F.E.A.R. benchmark. Still, when it comes to all-around features and technologies that can handle the latest in emerging games, the XPS M1730 is the best gaming rig on the market today. The tanklike XPS M1730 may be the first Dell laptop that obviously has been influenced by subsidiary company Alienware. At first glimpse the glossy black plastic case (you can also choose white, blue, or red), patterned to look like the surface of a liquid, reminded us of the Alienware Aurora mALX we reviewed more than a year ago. And while the M1730's lid includes the familiar XPS branding from previous iterations, Dell has added a glowing logo as well as an LED-lit sculpted ridge that echoes Alienware's sculpted alien eyes. Also glowing: the touchpad and speaker vents, which can be programmed with your choice of 16 color shades and four light effects (or turned off, if you're the ascetic type). It's all a bit over the top, which frankly we like in a $4,000 machine. What we don't like in such an expensive machine: creaking hinges and a squeaky plastic sound that we heard every time we moved the lid. (Dell assures us that this problem has been remedied since our early review unit rolled off the production line.) As you'd expect with any entertainment-oriented desktop replacement, games and movies look phenomenal on the XPS M1730's 17-inch wide-screen display. The 1,920x1,200 resolution provides sharp image detail, while the glossy finish adds depth and richness to colors with only a slight glare in moderate-light environments. Above the display sits a 2-megapixel Webcam and dual array digital microphones for video chats. With the XPS M1730, Dell adds a 10-key numeric keypad--handy for controlling games--next to the full-size keyboard. As with previous versions, you can turn on keyboard illumination when computing in the dark. For the first time, the laptop also incorporates a built-in Logitech GamePanel LCD display above the keyboard; four buttons beneath the LCD let you navigate menus to toggle between system status, in-game statistics, and other vital information on the 0.8-inch-high screen. Also above the keyboard sits a button to launch Dell's MediaDirect software, which lets you play CDs and DVDs and access other media files without booting the system. You can control media playback via a row of volume and media buttons, located on the laptop's front edge for easy access when the lid is closed. Surprisingly, Dell has jettisoned two USB ports with the XPS M1730; this latest version of the gaming flagship scales back the number to just four. Another space-saving step: HDMI, VGA, and S/PDIF-out all require adapters, which are included with the system. Of course the HDMI won't be much use unless you order the optional Blu-ray drive, which would add $550 to the laptop's price. The stereo speakers along the XPS M1730's front edge sound great, even at higher volumes. Two other features worth mentioning--borrowed from Dell's business-oriented Latitude line--include the useful Wi-Fi Catcher button, which lets you determine whether you're within range of a wireless network without booting the system, and a gauge on the laptop's base that tells you the charge status and health of your battery. As we've come to expect from the XPS line, the M1730 includes the highest-end mobile CPU and graphics available. In the case of our review unit, that means an overclockable 2.8GHz Intel Core 2 Extreme X7900 processor and two 256MB Nvidia GeForce 8700M GT graphics cards in a scalable link interface. The laptop's two 200GB hard drives spin at a brisk 7,200rpm. And this configuration includes the brand-new Ageia PhysX 100M processor, which works with certain supported games to provide additional processing power for in-game physics, leading to bigger explosions and more interactive environments, among other effects. In fact, about the only component that wasn't maxed out in our configuration was the memory; though the XPS M1730 can support up to 4GB of RAM, our review unit included only 2GB. It's no surprise, then, that the Dell XPS M1730 finished at the head of the pack on CNET Labs' mobile benchmarks--with the exception of the RAM-intensive Photoshop test, where it trailed the HP Pavilion HDX stocked with twice as much RAM. (Boosting the Dell's RAM to 4GB adds $375 to the price.) Given the high-end graphics in the XPS M1730, we'd expected it to burn through our gaming benchmarks. And it did post a chart-topping 100.7 frames per second while playing Quake 4 at 1,024x768 resolution. However, the XPS M1730's frame rates dropped to 76fps when playing the more challenging F.E.A.R. at the same resolution. That's absolutely nothing to sneeze at--in fact, it's by far one of the highest F.E.A.R. scores we've seen and particularly impressive for a Vista machine--but it falls significantly behind the 133fps posted by the Alienware Area 51 m9750, which included two previous-generation GeForce 7950GTX cards. The Alienware's cards may have more raw power, but the XPS M1730's current-generation cards support DirectX 10, which will come into play with the very latest PC games, such as BioShock or the upcoming Gears of War. In short, the Dell XPS M1730 represents the future of mobile gaming. On our DVD battery drain test, the Dell XPS M1730 ran for 1 hour and 27 minutes--about 15 minutes more than the Alienware m9750, but still a short lifespan. Nevertheless, we hardly expect a system of this size to spend much time at all away from the power outlet. Those who intend to use the M1730 as a true desktop replacement should also note that its 3-pound power brick takes up as much room as some external hard drives. Though Dell has moved to a 90-day standard warranty on its less-expensive models, the company covers the XPS line with a one-year warranty, which provides free parts and labor with on-site service. Upgrading to two years costs $199, while three years cost $299. You can get help through Dell's 24-7, toll-free tech-support number, with special reps exclusively for XPS owners. Find out more about how we test laptops."
105,https://www.computerworld.com/article/1624975/one-in-10-us-homes-may-have-a-robot-by-2020.html,ComputerWorld,2015,12,24,272.0," Don’t have a robot working in your home? In a few years, you just might. At least one in 10 U.S. homes will have a consumer robot by 2020, according to a study from Juniper Research. That number is up considerably from the one robot in 25 homes this year. We’re not quite ready to have a Rosie the robot, like the one on The Jetsons, working in homes as maids and cooks, but the first robots to enter our homes likely will be task oriented, doing basic household chores like mowing the lawn or vacuuming. That means while some hotels and businesses are starting to use robots as butlers or guides, the vast majority of people are likely to start by bringing a simpler machine, like an iRobot’s Roomba, into their household. “The state of consumer robotics could be compared to the PC in the late 70s,” said Juniper Research analyst Steffen Sorrell. “Venture capitalist and corporate investment has ramped up tremendously recently. They know that this is the start of a paradigm shift in the way we use and interact with machines.” Getting more complex and capable robots into homes will call for a jump in technology, as well as a drop in price, Juniper  noted. Another key to getting more robots into American homes is the trust issue. As robots, particularly bigger, stronger machines, enter the marketplace, they will need to gain the trust of the humans  with whom they are working. People will need to be fairly assured that the robots won’t hurt them or damage their possessions. Forming that kind of trust will be a big hurdle."
106,https://www.computerworld.com/article/1622957/ford-plans-to-test-driverless-cars-in-california-next-year.html,ComputerWorld,2015,12,15,262.0," Ford will begin testing a fully autonomous version of its Fusion Hybrid on California streets in 2016, becoming the latest major automaker to put driverless car technology to the test on real roads. The tests are expected to take place near its research and development center in Palo Alto. The center opened in January this year and brings together more than 100 researchers, engineers and scientists and is one of several R&D centers operated by major automakers in Silicon Valley. The prototype autonomous Ford Fusion is distinctive for the four LIDAR (light detection and ranging) sensors mounted on its roof. The sensors have lasers with rotating mirrors inside that allow them to perform an all-round scan of the area. Most prototype autonomous cars use the same scanners and match the images with a database of prerecorded images to determine where the car is at any given moment. Google’s self-driving cars are already a familiar sight on the roads in Palo Alto and neighboring Mountain View, where the company has its Google X research center. At 73, according to statistics from late September, the Google fleet is the largest of any company authorized by the state’s Department of Motor Vehicles, but it’s far from the only one. Tesla, which is also based in Palo Alto, had licenses for 12 cars. Mercedes Benz had the third largest fleet at 5 cars and is followed by Volkswagen, Delphi, Bosch, Nissan and Cruise Automation with 2 cars each. BMW and Honda each had one autonomous car licensed in the state, according to the latest DMV data."
107,https://www.computerworld.com/article/1641251/and-now-a-security-drone-that-chases-intruders.html,ComputerWorld,2015,12,10,327.0," Security guards in Japan have a new tool to deter intruders: a drone that will chase down and follow people without human intervention. Made by Secom, Japan’s biggest security company, the drone goes on sale Friday to organizations that need to protect large parcels of land. It will launch whenever suspicious cars or people are detected on the property by other security equipment. The drone will snap pictures and send them to a Secom monitoring center where it can determine the threat. Today, the company sends security guards to investigate potential intrusions, so a drone could reduce its response time considerably. In the case of a vehicle, the drone will photograph the car and its license plate. If it’s a person, it will attempt to get a picture of their face. It has LED lights for use at night, but the car better not be moving too fast. The drone’s top speed is 10 kilometers per hour. It will fly between 3 and 5 meters above the ground, well out of the reach of would-be intruders. Japan’s Kyodo News published this video showing the launch of the drone, including a demonstration of it following a car and an intruder: The drone takes off from a dedicated launch pad that includes a recharging system. The launch pad will cost ¥800,000 (US$6,575) and the drone carries a monthly rental fee of ¥5,000. It’s being promoted as an alternative to a fixed security cameras, because of its ability to fly anywhere on a property and take pictures from a variety of angles. Secom had planned to start its drone security service this past June but it was delayed by legal issues surrounding drone flights. A revision to Japan’s aviation law that formalized rules for drone flights took effect on Thursday. The government review of drone regulations in Japan was sparked when a member of the public flew a drone onto the roof of the prime minister’s office in central Tokyo."
108,https://www.computerworld.com/article/1639727/5-things-you-should-know-about-drone-regulations.html,ComputerWorld,2015,12,4,475.0," Drones are one of the hottest things in tech right now, but besides being fun to play with, they can be a menace to airports and a hindrance to firefighters. The rules around drone use are still being formed, but here’s what you need to know so far. Every drone pilot must adhere to a few basic rules. Most important is the requirement to keep the drone below 400 feet, otherwise it can stray into controlled airspace and collide with a plane or helicopter. Don’t fly a drone within 5 miles of any airport unless you have permission from air traffic control. National parks are also off-limits, as are sports stadiums on game days. Your drone must remain within sight at all times, and you must always give way to manned aviation if it comes close. More details are at the “Know Before You Fly” website and the and the FAA’s drone website. Flying a drone might give you all sorts of business ideas, but the FAA has a general restriction against using drones for commercial purposes. Businesses can apply for a “section 333” exemption, which more than 1,000 operators have received, but you might need a lawyer to help you go this route. An alternative is to hire one of a growing number of licensed drone operators to perform whatever work you have in mind. Individuals face far fewer restrictions than businesses. Beyond the basic safety restrictions listed above, there aren’t many rules for consumers. To get the most out of your drone, consider joining a local flying club, where you can meet other drone owners and deepen your knowledge of flying. The U.S. has no registration scheme at present but it will very soon. The FAA plans to introduce one this month that requires you to register a name and address in return for a number that you display on your drone. The scheme is intended to be simple, and as such it probably won’t be very effective at stopping illegal flights. It will be mandatory, however, so keep an eye on the FAA website for details. You can fly your drone in most places right now as long as you comply with the basic rules above, but don’t count on that always being the case. Google and Amazon are both testing drone delivery systems to let them quickly deliver goods purchased online. That could be convenient for shoppers and lucrative for the companies, but it might not be great for other drone users. Amazon wants half of the airspace available for drone use today to be restricted to high-speed craft like the ones it’s developing, while Google wants all drone flights to be computer controlled. Both proposals would give the companies’ business plans preference over hobbyists, so keep a close eye on any proposed rules and exercise your right to speak up."
109,https://www.computerworld.com/article/1636209/nasa-needs-robotic-upgrades-for-work-on-mars.html,ComputerWorld,2015,11,20,439.0," After not faring well in the DARPA Robotics Challenge, NASA is hoping its Valkyrie humanoid robot will do much better working on Mars. The space agency has given two robotic prototypes to scientists at MIT and Northeastern University to build software and put together hardware adjustments that will make the robots autonomous and dexterous enough to aid human astronauts or even take their place on “extreme space” missions. “Advances in robotics, including human-robotic collaboration, are critical to developing the capabilities required for our journey to Mars,” said Steve Jurczyk, associate administrator for NASA’s Space Technology Mission Directorate. “We are excited to engage these university research groups to help NASA with this next big step in robotics technology development.” The robots are expected to launch into space to work on asteroids or Mars as precursors to human missions, setting up habitats, producing  drinkable water and fuel to get the astronauts back home again. NASA also expects to use robots as human assistants, collaborating on projects, using tools and taking over more dangerous projects. Many of the robots did well, walking over rubble, driving and getting out of a car and drilling holes in walls. Others, like NASA’s Valkyrie, didn’t do nearly as well. They were unable to climb stairs and ladders and took several minutes to take one step. Some simply fell over. The Valkyrie didn’t make it to the final round of competition. DARPA and robotics scientists said there is still work to be done, but they have made great headway into building autonomous robots that can perform basic tasks on their own. In five or 10 years, they expect robots to be ready to help in disaster situations. Of the teams that competed in the DARPA challenge, NASA chose MIT, whose team placed sixth in the competition, to work on the robotic space project. Northeastern didn’t compete, but its robotics group is now headed by Taskin Padir, who led the seventh-place team from Worcester Polytechnic Institute during the DARPA competition. Both MIT and Northeastern will receive up to $250,000 a year for two years and will have access to onsite and virtual technical support from NASA.  Both university teams will also participate in its upcoming Space Robotics Challenge. This new competition, while creating opportunities to benefit the robotics industry as a whole, will focus on creating the robotics technology needed to launch Mars missions. At this point, with MIT and Northeastern involved, NASA is focused on upgrading the robot. In 2016, the competition will move on to a competition among teams that have built software for a simulated robotic situation. After that, there will be a physical competition."
110,https://www.computerworld.com/article/1632469/why-human-in-the-loop-computing-is-the-future-of-machine-learning.html,ComputerWorld,2015,11,13,1046.0," Now that machine learning is becoming more and more mainstream, some design patterns are starting to emerge. As the CEO of CrowdFlower, I’ve worked with many companies building machine learning algorithms and I’ve noticed a best practice in nearly every successful deployment of machine learning on tough business problems. That practice is called “human-in-the-loop” computing. Here’s how it works: First, a machine learning model takes a first pass on the data, or every video, image or document that needs labeling. That model also assigns a confidence score, or how sure the algorithm is that it’s making the right judgment. If the confidence score is below a certain value, it sends the data to a human annotator to make a judgment. That new human judgment is used both for the business process and is fed back into the machine learning algorithm to make it smarter. In other words, when the machine isn’t sure what the answer is, it relies on a human, then adds that human judgment to its model. This simple pattern is at the heart of many well known, real-world use-cases of machine learning. And it solves one of the biggest issues with machine learning, namely: it’s often very easy to get an algorithm to 80 percent accuracy but near impossible to get an algorithm to 99 percent. The best machine learning lets humans handle that 20 percent since 80 percent accuracy is simply not good enough for most real-world applications. Self-driving cars are a great example of what we’re talking about when we talk about “human-in-the-loop” computing. Smart people have been working for many, many years on making cars that drive themselves and the state of the art tech is actually very good. But very good isn’t good enough. Ninety-nine percent accuracy means that people might die 1 percent of the time. Tesla recently launched an automating driving mode that followed exactly the human-in-the-loop pattern. The car mostly drives itself on highways but it insists that the human keeps their hands on the wheel. When the machine learning vision system senses that it has doubt about what’s going on – maybe there’s construction, snow, or something unusual on the road – it hands the control back to the human driver. So while the car can indeed drive itself almost at all times, it needs a human failsafe. Considering the possible consequences, you can understand why. Facebook’s photo recognition algorithm has gotten extremely good. In fact, when you upload photos it can often not only find faces but actually guess who the person at a little over 97.25 percent accuracy. But in cases where its confidence is below a certain threshold, Facebook will ask you, the uploader, to confirm the person labeled in the photo. In cases where the confidence is even lower it will ask you to label the photo yourself. All of this data is fed back into their algorithm to make it better. Once, when you deposited checks in an ATM, you had to tell the ATM exactly how much you were putting in. But with massive advancements in optical character recognition (OCR), generally, your ATM uses a visual algorithm to understand not just the check amount, but other pertinent information on it, like routing numbers. These vision algorithms have come a long way, but there are still cases where handwriting is funny or the language is unusual. In those cases, your ATM will ask you to key in the amount and the check itself is flagged for a human to look at (this is why some checks take a little longer to clear than others). And, as with the Facebook example above, you entering in a specific value gives that visual algorithm more data to learn from when it sees another tough-to-read check. When Deep Blue beat Kasparov many years ago, it was in a major victory for A.I. Since then, of course, chess computers have become more and more dominant. While the concept of chess being “solved” is still considered rather remote (there are at least 1043 board positions to account for), chess computers now routinely beat grandmasters, even when those masters are given considerable handicaps. But a subculture of people have continued to play what they call “Advanced Chess.” Advanced Chess is a game in which a human operator/chess expert works with a computer to find the best possible move. Computers are great at reading tough tactical situations but are still not as good as humans at understanding long term strategy. The best Advanced Chess use computers to limit (or eliminate) blunders while using their intuition to force the opposing team into unusual board states the computer hasn’t much seen. It means that human-computer interaction is much more important for artificial intelligence than we ever thought. In each case: chess, driving, facebook and ATMs, making sure computers and humans work well together is critical for all of these applications to work. Notably, however, there’s a different interface between the computer and the human in each but it’s the pairing of humans and machine–not the supremacy of one over the other–that yields the best results. Artificial intelligence is here and it’s changing every aspect of how business functions. But it’s not replacing people one job function at a time. It’s making people in every job function more efficient by handling the easy cases and watching and learning from the hard cases. Which is to say: We don’t wake up one day to find self driving cars – we slowly cede driving functions one piece at a time. Lukas Biewald is the co-founder and CEO of CrowdFlower. As a former data scientist, Lukas was frustrated by the amount of time he had to spend cleaning and labeling data instead of actually using it to solve business problems, leading him to co-found CrowdFlower in 2009. Today, the CrowdFlower platform connects over 5 million data enrichers in almost every country who work around the clock to provide companies with clean and actionable data. Lukas graduated from Stanford University with a BS in Mathematics and an MS in Computer Science. Lukas is also an expert-level Go player. The opinions expressed in this blog are those of Lukas Biewald and do not necessarily represent those of IDG Communications, Inc., its parent, subsidiary or affiliated companies."
111,https://www.computerworld.com/article/1633185/robot-keeps-stores-stocked-with-doritos-2.html,ComputerWorld,2015,11,12,480.0," An autonomous robot was unveiled this week that can make sure that when you’re hankering for Doritos, there’s a bag waiting for you at the market. Simbe Robotics, based in San Francisco, announced its first product, a 30-pound robot called Tally that can move up and down a store’s aisles checking inventory.  The robot determines what products need restocking and send reports to workers who can add more stock. Tally also is set up to work during normal store hours, alongside employees and customers. “Tally performs repetitive and laborious tasks of auditing shelves for out-of-stock items, low stock items, misplaced items, and pricing errors,” the company said in a release. “Tally has the ability to audit shelves cheaper, more frequently, and significantly faster than existing processes; and with near-perfect accuracy.” The robot is equipped with multiple sensors that not only scan shelves for inventory but scan the aisles to make sure the robot isn’t bumping into shelves, displays or shoppers. Data that the robot captures is sent to the cloud where it is processed and analyzed. Store employees access the cloud data, along with inventory recommendations, through front-end applications. The robot comes with a charging dock that the machine will autonomously navigate back to when it needs charging. Simbe said that its robots can help stores not only keep their shelves stocked but also increase customer satisfaction, push sales and reduce operational costs. “When it comes to the retail industry, shopper experience is everything,” said Brad Bogolea, CEO and co-founder of Simbe Robotics. “If a product is unavailable at the time the shopper wants to buy it, the retailer has missed an opportunity and disappointed their customer. “Tally helps retailers address these challenges by providing more precise and timely analysis of the state of in-store merchandise and freeing up staff to focus on customer service.” Ezra Gottheil, an analyst with Technology Business Research, said he’s not sure if having a robot to do inventory will free up workers to do bigger, more complex jobs or if it will simply mean that the business won’t need as many human workers. “Navigating a space with moving objects is something we can do now,” he added. “Think about the robot that delivers things to hotel rooms or the self-driving car for that matter. And it’s pretty to think that this just frees up workers but to a large extent, it frees up workers to be unemployed.” Gottheil added that it’s unlikely that store customers would be put off or even frightened, by the robot, and are more likely to be intrigued by it.  “I think most will think it’s pretty cool,” he said. “I was going to say that kids will really like it, but they’ll probably just think that’s the way it’s always been.” Simbe Robotics did not give a price tag for the robot, and said “several” retailers are testing it."
112,https://www.computerworld.com/article/1633104/get-ready-for-car-as-a-service-caas.html,ComputerWorld,2015,11,12,779.0," Self-driving cars are expected to take over the roadways in the next two decades, but the vast majority of the fleet will likely be dedicated to services and not owned by individuals, according to a new report by IHS Automotive. Within five years, Google and carmakers are expected to have driverless vehicles on the world’s roadways. By 2035, 12 million self-driving and driverless cars will be sold globally, IHS Automotive estimates. However, given that 85% of the world’s population doesn’t have a driver’s license and teenagers in developed countries are waiting longer to get a license — and driving less when they do — the majority of autonomous vehicles will likely be used by a services industry. Car-as-a-service (CaaS) opportunities are becoming a new driving force for urban transportation, according to IHS Automotive. Essentially an extension of ride-sharing using driverless vehicles, CaaS will enable people to summon a car via an app or to be used for deliveries. IHS estimates that the deployment of driverless CaaS will begin before 2025 and will have an increasing impact as technology advances and driverless car volumes grow. In addition, autonomous driving and the associated costs will dramatically lower the cost of mobility services as a whole. CaaS also will provide car mobility services for anyone, since no driver’s license is needed. And it will be an affordable transportation means for a large portion of the global population. IHS Automotive estimates there are currently 6.2 billion people globally without a driver’s license, or nearly 85% of the world’s population. While the majority of today’s car manufacturers are taking an evolutionary approach to autonomous vehicle research and development, increasingly adding advanced drivers assistance systems (ADAS) and partial self-driving features, Google is taking a revolutionary approach — building a fully autonomous vehicle from the ground up. Google’s autonomous car, which is already being tested on U.S. roadways, will have a major impact in the coming years, according to Egil Juliussen, senior director at IHS Automotive Technology. Uber is also beginning to implement some of its own R&D in CaaS. Software will be a key differentiator in autonomous vehicles, as it’s responsible for interpreting the information from a vehicle’s sensors and can learn to mimic the driving skills and experiences of the very best drivers. Google is the current technology leader in this arena, according to IHS Automotive. The company has invested nearly $60 million so far in autonomous vehicle research and development, which works out to about $30 million per year. Unlike traditional vehicle manufacturers, Google can leverage related technologies and information from its other projects and investments, IHS said. Google has been involved in robotics, drones and similar technologies that help in driving, including neural networks, artificial intelligence (A.I.), machine learning and machine vision. Those additional R&D areas have provided Google researchers with expertise not available directly to traditional carmakers, IHS said. “No other company has as much relevant technology to advance autonomous driving software,” said Juliussen, who authored the IHS repor. Toyota’s recent announcement of a $1 billion, five-year investment in A.I., driverless cars and robotics is likely partly due to Google’s rapid technology advances. Google’s self-driving car software is already performing better than nearly all drivers in the vast majority of traditional driving situations — at least in good weather, according to IHS analysis. Google’s strategy is to provide the technology infrastructure, maps and software to make CaaS happen sometime after 2020. Still lacking, however, in Google’s autonomous vehicle software is the ability to predict and react to “once in a million” events — such as performing under diverse weather conditions, unique road work, specific traffic situations and other non-traditional driving situations. Driverless vehicles will also blow open the market for electric vehicles (EV), as driverless car mobility services will mostly happen in urban areas and will primarily be short trips; those characteristics favor EVs as the powertrain for driverless cars; they can easily re-charge themselves using existing and growing public charging networks as needed between trips, which eliminates any range anxiety. IHS Automotive forecasts that global EV charging stations will grow from 650,000 in 2015 to more than seven million in 2021 (excluding home charging outlets). Mega-cities and other large urban areas also will prefer low emissions and as a result, should be keen to implement fleets of driverless EVs in their communities. IHS Automotive forecasts that the global production of battery EVs will grow from 273,000 in 2015 to 1.3 million in 2022. And global production of plug-in hybrid EVs is projected to grow from 179,000 in 2015 to over 2.4 million in 2022. Driverless car fleets for CaaS are likely to greatly increase the sales of EVs after 2025."
113,https://www.computerworld.com/article/1632175/toyotas-ai-research-efforts-could-mean-cars-that-anticipate-traffic-pedestrian-moves.html,ComputerWorld,2015,11,11,1004.0," Toyota Motor Corp. is taking a leading role in artificial intelligence research, investing heavily in areas that boost advancements to autonomous cars and robots that can aid the elderly. Last week, the auto maker announced that it was investing $1 billion over the next five years to establish two research and development centers – the main center in Silicon Valley and a secondary campus near MIT in Massachusetts. The project is expected to eventually include 200 scientists and engineers. The Toyota Research Institute, which is slated to open in January, will be led by Gill Pratt, who until recently was a high-ranking leader at DARPA, heading their Robotics Challenge, a competition aimed at advancing humanoid robots. The research will initially focus on how people and machines can work together, specifically in the area of mobility. Last week’s announcement was just the latest A.I. research news to come out of Toyota. In September, Toyota announced a separate partnership with Stanford University and MIT to push research into A.I. and robotics. In that effort, Toyota is contributing $50 million over five years – half to Stanford and half to MIT — to establish research centers that will focus on A.I. and in advancing human and robotic interactions. By combining research on the interaction between humans and machines, artificial intelligence and big data, Steve Eglash, executive director of the artificial intelligence lab at Stanford, is hopeful that Toyota’s richly funded research centers will, in the next five years, lead to cars that operate more safely in bad weather and on city streets. He also hopes their work will lead to robots that can help the elderly and sick live safely, and happily, in their own homes. “This isn’t just about creating scholarly results and publishing them in scholarly journals, it’s about having an impact and changing the world,” said Eglash, who is heading the Toyota-funded research center at Stanford. “This is the same culture that brought things like Google and Yahoo and HP and so many others. This is a group that is really good at focusing on what needs to be done.” Stanford’s research center, which will be housed in a pre-existing building on the campus, will work on seven to nine projects under the Toyota-funded effort, Eglash said. Each research project will have two to three university professors, along with two to five graduate students focused on it. Eglash said preparations began a few months ago to identify what research projects they’ll take on, to put out a call for proposals and to launch a review and selection process. “Toyota provides additional resources and provides exciting focus in some of the interests,” he said. “It begins with the financial resources but Toyota brings a whole lot more to the party than just financial support. They have a unique perspective on the future of the A.I. industry and robotics.” Increasingly, A.I. research is about more than algorithms and smart systems. It’s also about big data, and Toyota has plenty of data on automobiles and how people interact with them. “Data is an important resource for a whole lot of research and Toyota brings us a whole lot of data,” explained Eglash. “We were all used to thinking of an organization’s resources as their plant and capital resources, but increasingly we also think of data being a valuable resource. Cars are one of the most connected things we own. How can we use that intelligence? How do we understand how people want to interact with their cars? That’s an important part of this.” Eglash also hopes their efforts will make artificial intelligence more contextual and human-centered. “When your computer is providing you with information, whether it’s a computer, a phone or a robot, you want it to be as aware of the situation you’re in as possible,” Eglash said. “You want the car to understand if your mind is elsewhere and you want a more passive experience. Maybe you’re old and your eyesight isn’t as good and you need more help. In the future, we’ll want intelligent machines to interact with us the way we want them to interact with us.” So how long before cars recognize if a driver is tired and might need help staying focused on the road, or might even take over the driving? It won’t be long at all, according to the Stanford researcher. The auto industry already has introduced cars that can park themselves, stay within the lane and brake when they sense an object in their path. Hopefully in a few years, cars will make predictions about traffic and road conditions in an area minutes before the car even gets there. It’s also hoped that cars will soon be smart enough to anticipate what bicyclists and pedestrians might do and take precautionary maneuvers. “We have the potential to use technology to drastically reduce the number of accidents,” Eglash said. While the majority of Stanford’s research will focus on using A.I. in automobiles, researchers also will work on human/robot interactions. For that, Stanford will pull in professors from the humanities, psychology and social sciences to work with the program’s engineers. “It’s not just about technology,” Eglash said. “We don’t know what will happen, but we believe that research works best when you have as diverse a group of disciplines as possible.” This major commitment to A.I. research is exciting for scientists trying to move the science forward, according to Manuela Veloso, a computer science professor at Carnegie Mellon University, which is not part of the Toyota project. “It’s exciting to see that people are realizing how much the field of A.I. can make a difference in people’s lives,” she said. “The work they’ll be doing on cars can be used elsewhere. We need better decision making based on an understanding of a situation. Machines need to understand if someone is crossing the road or crossing a room. This is the beginning. It’s the reality of A.I. in the physical world, whether it’s cars or robots that move indoors or in supermarkets.”"
114,https://www.computerworld.com/article/1631317/autonomous-cars-arent-nearly-as-clever-as-you-think-says-toyota-exec.html,ComputerWorld,2015,11,6,424.0," The head of Toyota’s billion-dollar U.S. artificial intelligence research center delivered a reality check on Friday for anyone over-enamored by autonomous car technology: the cars aren’t as clever as you think. “I want to help the press and the public understand that when they see a car that does not have a human being behind the wheel and it seems to be driving, that the car is not as intelligent as a human being behind the wheel, even though it seems it might be,” said Gill Pratt, executive technical adviser at Toyota. He was speaking in Tokyo at the announcement of the Silicon Valley center. Toyota will open it in January 2016 to drive artificial intelligence and robotics research, both for autonomous vehicles and other projects. Pratt explained what researchers already know but perhaps others don’t: Autonomous cars look great in controlled environments but soon fail when faced with tasks that human drivers find simple. Drivers, for example, can pretty much get behind the wheel of a car and drive it wherever it may be, he said. Autonomous vehicles use GPS and laser imaging sensors to figure out where they are by matching data against a complex map that goes beyond simple roads and includes details down to lane markings. The cars rely on all that data to drive, so they quickly hit problems in areas that haven’t been mapped in advance. So when you see a lot of demonstrations of autonomous vehicles, it is very easy to be fooled into thinking the car is more intelligent than it is when really the car is using all of this superhuman information to accomplish its goal looking like it has the intelligence of a person, but it really doesn’t,” he said. A truly intelligent self-driving car needs artificial intelligence that can figure out where it is even if it has no map or GPS, and manage to navigate highways and follow routes even if there are diversions or changing in lane markings, he said. Pratt was speaking in response to a question about how Toyota would challenge work already being undertaken by Google, Apple and others into autonomous car technology. His point wasn’t to denigrate the work being done by those competitors, but to remind his audience that all companies are still at the starting line of what could be decades of research into a truly autonomous self-driving car that can match humans. “Questions like this assume we are near the end of the race,” he said. “The truth is, we’re only at the beginning.”"
115,https://www.computerworld.com/article/1630088/toyota-plans-1b-rd-push-on-ai-and-robotics-in-us.html,ComputerWorld,2015,11,6,488.0," Toyota plans a major push into artificial intelligence and robotics technology research and will invest US$1 billion over the next five years to establish a Silicon Valley research and development center to pursue those goals. The Toyota Research Institute will be led by Gill Pratt, who recently joined Toyota from DARPA where he ran the Robotics Challenge, an event that promoted work on robots that can work with humans. “The goal of the Toyota Research Institute is to bridge the gap between fundamental research and product development, particularly of life saving and life improving technologies,” said Pratt at a Tokyo news conference on Thursday. It will be established in January and be based close to Stanford University and have a second campus near MIT near Boston. Over the next few years, it will grow to around 200 scientists and engineers. Initial research at the center will focus on the way people and machines can work together, particularly in the area of mobility, he said. When it opens, it will have three goals in the areas of safety, accessibility and robotics. In safety, the goal will be to make driving safer and prevent car accidents, no matter what the driver does. In the area of accessibility, it will seek to help everyone benefit from the mobility of cars, regardless of demographics or physical condition. And in robotics, it will work on technology that can improve the quality of life of all people, in particular seniors. It will work alongside two research centers Toyota is establishing with Stanford University and MIT. The car maker is investing an additional $50 million in those under an agreement announced in September. The goals are lofty but they aren’t new areas of research for the company. Like many large Japanese companies, Toyota engages in fundamental research into technology that may not become products for years. The work is low profile, but occasionally it makes headlines. In 2009 for example, Toyota showed off a brain-machine interface system that allowed a person in a motorized wheelchair to control it with just their minds. Toyota already faces competition in some of these research areas from the likes of Google, which has been working on autonomous car technology for several years and already has prototype cars driving on public streets near its Silicon Valley headquarters. But in Tokyo on Friday, Pratt said he wasn’t worried about the head start that Google has. “It is possible at the beginning of a car race that you may not be in the best position,” he said. “It may be that other drivers are saying a whole lot about what there position is and everyone may expect that a particular car will win. But if the race is very long, who knows who will win?” “The problem of adding safety and accessibility to cars is extremely difficult and the truth is, we are only at the beginning of this race,” he said.”"
116,https://www.computerworld.com/article/1630074/faa-panel-still-isnt-sure-how-to-regulate-drones.html,ComputerWorld,2015,11,5,460.0," A key task force convened to work out registration rules for consumer drones ended a three-day meeting on Thursday with apparently no final conclusion. While it spent the day discussing how the registration program would work, the task force is yet to decide what types of drones would be covered by a registration program and what types would not. That’s hugely important, as drones are expected to be hot holiday gifts this year, so the scope of the registration program could affect tens or hundreds of thousands of people. On Thursday, the group focused on reaching consensus on a recommended process for registration, the FAA said. “The discussions included how an operator might prove a [drone] is registered, how the aircraft would be marked, and how to use the registration process to encourage or require [drone] operators to become educated on basic safety rules.” The FAA didn’t say if a conclusion was reached on those issues, but whatever happened at the meeting, the task force still faces a Nov. 20 deadline to deliver its proposal. The tight schedule matches the speed at which the task force was convened. The FAA only announced its wish for drone registration on Oct. 19, and the members were announced on Oct. 29. The group brings together 25 companies and organizations that are working in the drone industry with government agencies. Co-chair is Dave Vos, who heads Google’s Wing delivery drone project, and it also includes representatives from Amazon and Wal-Mart, both of which are researching delivery drones. Other members include drone makers such as DJI, GoPro, PrecisionHawk and 3D Robotics, retailer Best Buy, several industry associations, and the Academy of Model Aeronautics, which represents the model aircraft community. The FAA has already received hundreds of responses from the public concerning the proposal to require registration of drones. Many have come from model aircraft flyers who assert that new rules will place an unfair burden on hobbyists, the vast majority of whom already follow rules and best practices established by the Academy of Model Aeronautics. Hobbyists also worry that drone registration will stifle creativity, perhaps requiring a new registration each time a drone is modified or if one is built from scratch. If some form of registration is required, it might be better centered around the operator rather than his or her drones, some contend. The FAA is moving fast because drones are expected to be a popular gift item this year-end holiday season. Hundreds of thousands could take to the skies over the holidays, quickly outnumbering conventional aircraft but piloted by people with little or no aviation knowledge or experience. The number of near-miss incidents with aircraft is on the rise, and the FAA is keen to bring some accountability to the new hobby."
117,https://www.computerworld.com/article/1630859/can-ai-influence-human-behavior-two-year-trial-seeks-an-answer.html,ComputerWorld,2015,11,5,364.0," Researchers in Singapore this week began a two-year trial of a smartphone app that attempts to use artificial intelligence to influence the real-world decisions of users. The test is centered on shopping malls, sports stadiums and major events and will try to reduce congestion at peak times by convincing people to change or delay their travel. The several tens of thousands of people that the researchers hope will participate are supposed to provide details on the level of congestion they can tolerate on public transit, the amount of time they are willing to delay travel to avoid congestion and how much they are willing to pay to use public transit. An artificial intelligence engine will attempt to come up with suggestions for each person that could include offering a cheaper fare if travel is delayed or a discount at a mall to convince the person to stay a little longer to help relieve congestion. For example, one passenger might not care about a crowded train and want to return home immediately, another might tolerate a short delay in return for a cheap cup of coffee, while a third might tolerate a longer delay in return for a discount on a meal. As suggestions are made, the AI system will track the tester’s receptiveness as it attempts to build a personalized model for each person. Crowded public transit is a source of frustration for many people and some already adjust their travel plans to avoid congestion. This system could help alleviate crowding and keep travelers happier. Artificial intelligence is a key area of research for major computer companies as they try to move beyond simple algorithms to more complex systems based on large amounts of data. The AI systems attempt to learn human behavior on an individual basis so truly personalized services and advertising can be delivered. The tests in Singapore are being conducted by Fujitsu, which is using its Zinrai AI system and working with the Singapore Agency for Science, Technology and Research, and the Singapore Management University. The test is scheduled to continue until the end of 2017, but Fujitsu said a commercial version of the technology could be available in March 2016."
118,https://www.computerworld.com/article/1625742/from-bionics-to-magic-mit-media-lab-celebrates-30-years-of-innovation.html,ComputerWorld,2015,10,30,1565.0," The MIT Media Lab was launched in 1985 to bring together experts from different academic fields, founded on the idea that mixing and matching researchers and students, and encouraging people to draw on their unique experiences and perspectives, would yield previously unimaginable technological breakthroughs. Andrew Lippman, senior research scientist and associate director of the MIT Media Lab, calls it a “bubbling brew of differentiation.” Out of that bubbling brew has emerged the predecessor to Google Glass, the technology behind e-readers, the video game Guitar Hero and a bionic prosthetic leg. “Magic occurs when you mix technology with the artistry of different disciplines — and that had never been done before,” said Lippman. “The time was right to do it.” Asked whether the Media Lab is looking to make better technologists, Lippman offered an emphatic “No!” Nor is the lab simply trying to get people to collaborate. “Think of [collaboration] like making a movie,” he explained. “Sound people and video people come together and make a movie, and then the movie is done and they all go home. They’re not changed by it. We didn’t want to just bring artists and tech together. It’s to understand two very different modes of solving a problem. It’s about mixing different styles of thought.” Working together, sharing ideas and learning to think differently should change students and give them a lifetime ability to study problems and see opportunities differently. That’s the goal of the MIT Media Lab, which this week celebrates three decades of breaking down academic walls and bringing together different forms of study. Professors, students and alumni will gather Oct. 30 to celebrate the lab’s 30-year anniversary with a day-long symposium titled “Mind, Magic and Mischief” at MIT’s Kresge Auditorium. The event is attracting big names. Kofi Annan, former secretary general of the United Nations and a co-recipient of the 2001 Nobel Peace Prize, will speak, as will Megan Smith, who is the CTO of the United States and a former vice president at GoogleX. Smith earned bachelor’s and master’s degrees at MIT and completed her master’s thesis work at the MIT Media Lab. The symposium will be webcast, with online access available on the morning of the event. While the symposium is designed largely to look into the future of emerging technology, the Media Lab also is looking back at the people, ideas and work that have come out of it. The people showcased include Marvin Minsky and Seymour Papert — both are considered pioneers in artificial intelligence (A.I.) and Papert is a founding faculty member of the Media Lab; Hugh Herr, who heads the biomechatronics research group at the MIT Media Lab and is world renowned for creating bionic limbs that emulate the function of natural limbs; and Cynthia Breazeal, an associate professor at the Lab and founder and chief scientist of Jibo Inc., who is well known for her work in A.I. and robotics. Breazeal’s work includes developing Kismet, a robot that can recognize and simulate emotions. Jin Joo Lee, a graduate researcher at the lab, said she wanted to study there so she could follow in Breazeal’s footsteps. “The MIT Media Lab is very open to radical new ideas,” said Lee, who is working on robots that can teach children storytelling skills and a second language. “[Breazeal] was a key pioneer in human-robot interaction. That’s the reason I came to the Media Lab.” She also pointed out that Breazeal studied with Rodney Brooks, a robotics entrepreneur and founder, chairman and CTO of Rethink Robotics, when he was a professor at MIT. Lee now benefits from that academic lineage. “Rod Brooks was Cynthia’s adviser,” said Lee. “In that way, Rod is my academic grandfather. It’s the passing down of ideas and inspiration to the students.” Palash Nandy, a research assistant at the Media Lab, said the history of work at the lab drew him in. Actually, it’s the only place he applied to when he was preparing to go to graduate school. “When the rest of the world is looking one way, MIT Media Lab is saying, ‘Where else can we look?'” said Nandy, who is studying human empathy toward robots. “I wouldn’t have been able to do this without the past work done in this lab. I’d still be tinkering in my garage.” One of the reasons for the Media Lab’s success is the mix of people, according to Lippman. “We look for students who are really broad thinkers, really diverse thinkers,” he said. “You get a lot of renegades from electrical engineering who like to play guitar or climb mountains or deep-sea dive. And they’re good at all of it…. You don’t want people who all come from the same place and read the same books. You have people who are programmers, people who are horseback riders. You have people who are designers. You have people who are nuts. You have this bubbling brew of differentiation, which brings many perspectives to bear on a problem. “Technology is too important to be left to pure technologists,” he added. “We have this gang of people all working under one roof and all bumping into each other every single minute of every day. That’s uncommon. Most schools have walls between their departments, and the mathematicians don’t talk to the physicists and they don’t talk to the musicians and they don’t talk to the artists. And sometimes that’s even ingrained administratively into how the school works.” Cesar Hidalgo, assistant professor of media arts and sciences at the lab, is drawing on that mix of student backgrounds to push his own research. A physicist by training, Hidalgo is working to create technology for visualizing big data. His team includes students studying computer science, design and physics. “In most other programs, students are going to be from singular academic backgrounds,” Hidalgo said. “In most universities, physics students are studying to be physicists. Here they might be exploring different questions and different aspects of the world. The Media Lab allows me to assemble a more complex team, unlike a team where they have a singular set of skills.” That mix of skills and interests is what Hidalgo thinks will make his work successful. To help people visualize large data sets, they can’t focus solely on technology. The visualization needs to be aesthetically appealing — and all the pieces need to be in the right places to tell the data’s story. “If you focus only on the science, only on the engineering, then the project is incomplete,” Hidalgo explained. “By having a team with all these different skills, you have a product that is much more useful.” Media Lab teams have done a lot of interesting work like that over the years. Alex “Sandy” Pentland, a Media Lab professor of media arts and sciences, has pursued pioneering research on wearables and built what became the predecessor to Google Glass. When Pentland’s team was working on that headset project, its members were often seen walking around MIT wearing helmets with antennas on them. They were jokingly referred to as the cyborgs of the Media Lab. The lab also was the birthplace of E-Ink Corp., which makes the electronic paper display technology used in smartphones and e-readers like Amazon’s Kindle. And music and gaming fans can thank the MIT Media Lab for the Guitar Hero series. Guitar Hero games, which use a guitar-shaped controller to make users feel like they’re playing in a rock band, were created by Eran Egozy, who has degrees in electrical engineering and computer science, and Alex Rigopulos, who is trained in music and theater arts. The two, who met while studying at the Media Lab, went on to form Harmonix Music Systems, the company behind Guitar Hero and Rock Band. “They were trying to accomplish not just a cool video game, which they did, but they were trying to encourage people to get involved in the creation of music,” said Lippman. “There are not that many places in the world where you can experiment with electronics and music. You can do that here. Remember that you have crazy people here good at programming things, people good at music, people good at electronics. You have this bubbling brew where ideas can bubble out.” Where does the Media Lab go from here? According to Lippman, there’s much more work going on in chemistry and biology today than there was in the earlier days of the lab. One team, for instance, is studying optogenetics, which involves the use of light to control cells in living tissue. If their research is successful, it may one day be possible to relieve pain by using light to de-stimulate the cells causing the pain. Another team is working on prosthetic legs that have electronic ankles instead of springs at the point where the foot meets the leg. The new ankles are designed to learn how the user walks and support those specific motions. “I think the thing that is the most exciting about the Media Laboratory is that we sometimes build companies, but more often we build big ideas,” said Lippman. “We strive to do things here that have a little bit of magic. You don’t think they can be done. They’re unique. If you can do them elsewhere, then you probably ought to do so…. We’re working in areas that would be difficult to find a happy home in a traditional academic department.”"
119,https://www.computerworld.com/article/1628074/lonely-on-the-road-what-about-a-robotic-driving-companion.html,ComputerWorld,2015,10,29,521.0," After testing human-robot cooperation in space with its Kirobo robot, Toyota is working on a smaller version – actually a cup-holder sized robot – that can keep people company while they drive. Dubbed the Kirobo Mini, the nearly 4-in. tall robot is designed to detect and respond to the driver’s emotions, speech and gestures. The robot, which could be installed in future Toyota vehicles, would not only be aimed at keeping drivers alert and calm but could collect information about driving habits that engineers could potentially use to build better features for future cars. “With people spending an average of 4.3 years of our lives in our cars, which equates to traveling to the moon and back three times, Toyota believes that much can be learned about our behavior and emotion while driving,” Toyota said on its website. “Imagine how driving would change if Kirobo Mini’s technology was integrated into Toyota vehicles: We could assimilate hours of data to better the everyday lives of drivers all over the world, informing future innovations and developing transport that’s in tune with the driver’s mood, suggesting places to visit, routes to travel and music to listen to.” Toyota  demonstrated the technology at the Tokyo Motor Show 2015  this week. Toyota built the original Kirobo, a small, humanoid robot that was launched to the International Space Station in the summer of 2013 to take part in what was the first experiment on conversation between a human and a robot in space. That version of Kirobo, which was a 13.4-in. tall, 2.2-pound humanoid, black-and-white robot, worked with Japanese astronaut Koichi Wakata. The robot was designed to remember Wakata’s face so it could recognize and have conversations with the astronaut and even relay information to him from Earth. With its conversational abilities, researchers hoped to determine if the robot could keep Wakata company. Scientists also were hoping that the experiment in space could speed their work on creating robotic companions that would be small enough to fit in someone’s pocket. Researchers around the world are working on building robots that can act as human companions and care givers. The machines need to not only be able to communicate with people but also be able to interpret their facial expressions, body language and moods. The robots also need to be agile enough to safely move around a home or office and not make people fearful of them. In June, SoftBank Robotics Corp., based in Japan, sold all of its 1,000 personal robots within the first minute they were put on sale. The robot, called Pepper, sold for $1,600 and requires a $200 monthly fee. The machines are designed to read human emotions and to display their own emotions as well. And guests at the Aloft hotel in Cupertino, Calif., have been able to interact with a robotic butler that delivers snacks or toiletries to their room. That robot, called Butlr can call for an elevator and navigate the hotel’s lobby and hallways to get to guests’ rooms. The robotic butler is working out so well for the hotel chain that it is adding the robot to other properties."
120,https://www.computerworld.com/article/1626986/drone-carrying-drugs-hacksaw-blades-crashes-into-oklahoma-prison.html,ComputerWorld,2015,10,27,186.0," A drone carrying drugs, a cell phone, hacksaw blades and other contraband was discovered crashed in an Oklahoma prison yard on Monday. Officers patrolling the Oklahoma State Penitentiary in McAlester around 9 a.m. noticed the drone lying upside down inside prison grounds after it apparently crashed after hitting razor wire that guarded the facility. The officers found a package of illicit material that had been suspended from the drone by fishing line. It included two 12-in. hacksaw blades, a cell phone, a cell-phone battery, a hands-free device, two packages of cigarettes, two packages of cigars, two tubes of super glue, a bag containing 5.3 ounces of marijuana, a bag with 0.8 ounces of methamphetamine and a bag containing less than 1 gram of heroin. The Oklahoma Department of Corrections issued a photo of the drone, which resembles a DJI Phantom quadcopter. Alongside it in the picture were the goods it was carrying. The incident, which is the first reported case of its kind in the state, is under investigation. Drones have been used to attempt to smuggle goods and contraband into prisons in other U.S. states."
121,https://www.computerworld.com/article/1649501/meet-the-virtual-woman-who-may-take-your-job-2.html,ComputerWorld,2015,10,9,637.0," IPsoft’s Amelia is more interesting than Apple’s Siri. Amelia’s language is expressive. Her facial expressions and gestures are generated on the fly. Siri can show wit to an oddball question, but Amelia aims for empathy. Amelia is IPsoft’s artificial intelligence platform. It has an ability to understand things in context and engage with the listener. Most importantly, Amelia is real enough to take your job. The system learns by observing interactions between a user and service agents. At one media client, IPsoft claims, Amelia takes 64% of its calls, and has reduced “knowledge worker” staffing from 76 to 32. The 2.0 version of Amelia was introduced at Gartner’s Symposium IT/xpo, and among those watching the presentation was Mike Strenge, an infrastructure and operations manager at a financial services firm he didn’t want identified. Here’s a sample of Amelia 2.0. “I am blown away,” said Strenge, following the demonstration. He’s not convinced Amelia has emotional responses, “but I think we’re getting there. It’s a new technology that’s going to advance, advance and advance,” he said. Cognitive system vendors, such as IPsoft, concentrate on automating sophisticated non-routine occupations, which can include financial analysts, insurance underwriters, tax preparers, managers, programmers and other problem-solving jobs. This next wave of automation may be as consequential as the impact of automation was on routine work. In 1991, routine employment, meaning repetitive, rule-based tasks both manual and cognitive, such as working in a factory, made up 58% of the U.S. workforce, but by 2011 had declined to 44%. Meanwhile, non-routine cognitive employment increased from 29% of the workforce in 1991 to 39% in 2011, according to Gartner analyst Ken McGee, in a conference presentation. “Where are all the people who used to work in factories?” asked McGee, or the word-processing typing pool? Next on the list of automation opportunities is the growing area of non-routine cognitive work. “That’s why smart machines are the hot topic right now,” he said. It’s also why Gartner believes robotic bosses or managers are on the way, and why 20% of all business content will soon be generated by machines. Vendors argue that automation will create better jobs that require more skills. The unanswered question is whether increasingly sophisticated automation will leave the economy with fewer jobs. “Wherever a human is servicing on the basis of standard operating procedure, cognitive agents are doing it, and can do it better,” said Chetan Dube, the president and CEO of IPsoft, and a former math professor at New York University, in an interview. Dube won’t assign an IQ to Amelia because “there is a component of IQ which is creative thinking. She would not do well on that.” But for the things she has learned, “she will execute flawlessly,” he said. Reasoning involves coming up with newer things that you haven’t come up with before, Dube said. But he believes the science “is on the boundaries” of reasoning. Siri is in a different league. It is a virtual personal assistant that doesn’t do mortgage processing, loan origination or insurance underwriting — tasks that Amelia can accomplish. “Siri does the administrative task that a secretary would do,” Dube said. Dube said machines will quickly take over a large section of jobs, certainly all mundane chores, and people will have to retool themselves for jobs that require more creative thinking. “That is the zone where man must move to,” he said. Amelia, whose image is based on a University of Southern California student, also provides a preview into how these machines may evolve to become human companions. In the next decade, Dube said, you will pass somebody in a corridor and won’t be able to tell if it is a woman or android. These machines “will take care of all the ordinary chores, and they will respond with complete empathy to you,” he said."
122,https://www.computerworld.com/article/1649286/senate-bill-aims-to-make-it-a-federal-offense-to-recklessly-fly-drones.html,ComputerWorld,2015,10,8,493.0," A bill was introduced in the U.S. Senate on Wednesday that will make it a misdemeanor, punishable by a fine or imprisonment for up to a year, for individuals who knowingly operate a drone within 2 miles of a fire, an airport or any other restricted airspace. Introduced by Sen. Barbara Boxer (D-Calif.), the Safe Drone Act is one of a number of moves at the federal and state level that have been introduced in the wake of concerns about the risks from the reckless use of drones by enthusiasts, including dangers of possible collisions between rogue drones and traditional aircraft. While introducing the bill, Boxer said that people flying drones recklessly have forced firefighters to suspend air operations out of concern for the safety of the pilots and people on the ground, referring, for example, to an incident in San Bernardino County in California earlier this year, when drones shooting videos disrupted firefighting operations. A similar version of the bill was introduced in August in the House of Representatives. The Federal Aviation Administration prohibits the flying of model aircraft closer than five miles (8 kilometers) from airports. It also has published draft rules for commercial drones, known as Unmanned Aircraft Systems, which restrict their operations to controlled airspace. A numbers of companies, including Amazon.com, have shown interest in using drones for applications like package deliveries. Earlier this week, the FAA proposed its largest fine for a UAS operator, amounting to $1.9 million in civil penalties, against a company that is said to have knowingly conducted 65 unauthorized operations. The agency is working with other organizations on technologies that would prevent drones from getting close to sensitive installations. It said on Wednesday it had contracted with CACI International in Arlington, Virginia, to evaluate the company’s prototype UAS sensor detection system at select airports. The agency is also looking at the geofencing of drones by using GPS and other technology to impose geographical limits on their movement, according to reports. Sen. Charles Schumer said in August that he would introduce a proposal would make geofencing of drones mandatory. The technology for preventing drones from flying into unauthorized areas is available.  DJI, the manufacturer of the drone that crashed on the lawn of the White House in January, said it would release firmware that would add a no-fly zone around much of Washington, D.C. The FAA’s bid to introduce rules for commercial drones appears to be still some time away if one goes by a statement on Wednesday by Michael G. Whitaker, deputy administrator of the FAA, before the House Transportation and Infrastructure Committee. The FAA received more than 4,500 public comments on the rules, and “we’re working to address those as we finalize the rule,” Whitaker told the subcommittee on aviation, without providing a date when the rules could be finalized. The FAA missed the Sept. 30 deadline that Congress had given the agency for the safe integration of drones into the national airspace."
123,https://www.computerworld.com/article/1648299/machines-are-replacing-writers-gartner-says.html,ComputerWorld,2015,10,6,632.0," ORLANDO — Chances are robots aren’t mowing your yard, teaching your children, or bossing you around at work – yet. But according to Gartner’s annual top-10 list of strategic predictions, robots, robotic systems and automation will have an expanding role. Sooner or later, there will be robots that train your children and help them with their homework. That “might seem a little strange to us, but is it really stranger than being trained by a purple dinosaur named Barney?” said Daryl Plummer, a Gartner analyst, to the laughter of his audience  at the research firm’s Symposium ITxpo. Here are Gartner’s predictions: 1. Writers will be replaced. By 2018, 20% of all business content, one in five of the documents you read, will be authored by a machine, Plummer said. “Robowriters” are already producing budget reports, sports and business reports, and this trend is sneaking in without notice. One advantage for machines: They don’t have biases or emotional responses, he said. 2. By 2018, 6 billion connected things will be requesting support.  These non-human “things” are nonetheless customers requesting services and data, and other methods of support. Marketing to them (and by extension their human owners) can help build a business. 3. By 2020, autonomous software agents outside of human control will participate in 5% of all economic transactions. Smart algorithms are already beginning to perform transactions without our help. 4. By 2018, more than 3 million workers globally will be supervised by a roboboss.  “The problem with this is that robot bosses don’t have human reactions,” Plummer said. “The reality is we have to see if robots can get human mannerisms right.” 5. By 2018, 20% of smart buildings will have experienced digital vandalism. As buildings, both commercial and residential, get smarter and more connected, there is greater potential that these buildings can be attacked. “You can’t protect everything. What you must do is have an algorithm that will detect that something is wrong, and respond to correct the problem,” Plummer said. 6. By 2018, 50% of the fastest-growing companies will have fewer smart employees than instances of smart machines. These machines are easy to replicate and there will be a lot more of them. Smart systems, for example, will be analyzing how a factory is being run, or deciding whether people are completing a task at an appropriate speed. 7. By 2018, digital assistants will recognize individuals by face and voice. Passwords are unworkable and good ones are hard to memorize, Plummer said.  Biometrics have been around for a long time, “but they are coming on strong now.” 8. By 2018, 2 million employees will be required to wear health and fitness tracking devices as a condition of employment. This may seem Orwellian, but certain jobs require people to be fit, such as public safety workers, Plummer said. One benefit is that insurance costs may be lower  for those companies with healthy employees. Many people are already wearing monitoring devices of some type. “We’re all being monitored by something,” he said. But the use of such devices also raises significant issues about whether an employee keeps a job based on fitness level. “That sounds like a violation of my rights,” he said. It could be that, but the employee might also sign a contract agreeing to the monitoring as a condition of the job, he said. “The reality is you are encouraged to be healthier – you get the benefit,” Plummer said. 9. By 2020, smart agents will facilitate 40% of mobile interactions. This is based on the belief that the world is moving to a post-app era, where assistants such as Apple’s Siri act as a type of universal interface. 10: Through 2020, 95% of cloud security failures will be the customer’s fault. It’s you, not the vendor, Gartner said."
124,https://www.computerworld.com/article/1637734/instead-of-robots-taking-jobs-ai-may-help-humans-do-their-jobs-better.html,ComputerWorld,2015,9,22,1078.0," Despite worries that robots will take jobs from humans, a group of researchers says what’s more likely, and more powerful, is that humans will eventually work cooperatively with cyber assistants. A soldier might be given a smart assistant that will train with him and one day do battle alongside him. A college grad just entering the workforce may get her own smart assistant that will learn along with her and provide support as she advances through her career. In this way, artificial intelligence (A.I.) and robots could very well make people better soldiers and workers. They could make people become what we’ve come to think of as “super” human. “There are many exciting areas [in A.I.] coming up, but A.I. and human cooperation is an area with tremendous potential,” said Tom Dietterich, a professor and director of Intelligent Systems at Oregon State University. “Instead of A.I. systems replacing people in the workplace, each of us would have an A.I. assistant that we would train in our lives and the two of us, together, would be employed … This is where we can see super-human performance coming from the combination of the human and the computer.” Scientists like Trevor Darrell, a computer science professor at the University of California, Berkeley, said that in the next five or 10 years, the abilities of smart devices will multiply by orders of magnitude. That means we’ll have gone from a world where artificial intelligence is used to power Google search and Apple‘s smart assistant Siri to having smart furniture that can reposition itself around the house based on our voice commands and robotic butlers will bring us coffee when we simply think about wanting a cup. It also means there will be increasingly smart robots working in factories and warehouses. Some worry that companies will  replace human workers with machines that don’t have to be paid for vacation time, health care and sick leave. And their fears don’t stop at factory jobs. Late last year, Andrew McAfee, co-founder of the Initiative on the Digital Economy at MIT, said it won’t be long before intelligent machines will begin to increasingly replace knowledge workers. In the near future, artificially intelligent machines could be used to provide financial advice or a medical diagnoses. Middle-class workers could be looking into a future where they are replaced by machines. That fear can be added on to other  creating sentient robots that will one day  and wipe out the human race. Not so fast, though. What if instead of a world where people queue up in unemployment lines while robots take their jobs, we look ahead to a world where robots and smart assistants help us during the work day, provide us with cleaner homes, remember friends’ birthdays and even protect us in battle? That’s a world to look forward to rather than to be afraid of, said Pam Melroy, deputy director of the Tactical Technology Office at the Defense Advanced Research Projects Agency (DARPA), the research arm of the U.S. Department of Defense. “Really interesting things are going to happen at the interception of biology and A.I.,” said Melroy, a retired U.S. Air Force officer and former NASA astronaut. “There’s something about human machine communication symbiosis and how humans and machines can partner well together.” Dietterich said this human/smart machine cooperation is already happening … and with impressive results. For example, a computer has been working to figure out the shape of a protein in three dimensions, but the work wasn’t going well, Dietterich said. Then humans began working with the computer program, and that changed. With humans working in conjunction with the computer, the solution to the 3D structure of an HIV enzyme, was found in three weeks. “The algorithm and humans could not have done this so quickly on their own,” said Dietterich. “A.I. will work its way into our lives in big ways working with people.” He said he expects this human/A.I. cooperation to eventually catch on and grow rapidly in a number of areas, such as high-speed stock trades, automated surgical assistants and autonomous weapons. A maintenance worker, for instance, would have a smart assistant that could help diagnose a problem with a boiler, suggest ways to repair it or assist the human with the repair. In a military setting, an enlistee would receive a personal assistant when she gets to basic training. The soldier will always keep her assistant with her so it will learn the human’s strengths and weaknesses as she goes through training and into different jobs. That way the assistant can continually adapt to help the soldier as she completes different tasks and advances to higher ranks. The idea behind a smart assistant is for the machine to learn as its human user does, so it can help with different and more complicated tasks. This cooperation will also take shape in our personal lives. Dietterich said we’re not that far away from having automated wheelchairs that, with a voice command or gesture, will take the user to different rooms inside a home or within an office building. Melroy agreed that smart assistants won’t always come in the form of human-sized robots or digital assistants that can fit in your pocket. Someone who’s lost a limb could get a smart limb that will sense and function much, or even just like, a natural appendage. “Think about when Luke Skywalker loses his hand,” said Melroy. “He gets a new one and it can feel. It’s no different. He can continue to function in all the ways he was used to. The ability to control that new hand with your brain and have seamless sensing in real life? Absolutely, that is coming. That is five to 10 years away.” To make that work, Melroy said, we’ll need to be able to communicate with our smart devices without typing on a keyboard or using a mouse. Even spoken commands would be too awkward. We’ll need to communicate with our assistants or devices with our thoughts. According to several researchers, such an advance is not far away. “The ability to control a robotic arm with just thoughts, with an RF signal and a chip in a woman’s brain has already been demonstrated,” Melroy said. “It doesn’t yet send signals back, but that will happen and it will close that loop. We are just not that far away from this ability to think things. It sounds like magic, but it’s all about electrical brain signals.”"
125,https://www.computerworld.com/article/1636846/overcoming-our-fears-and-avoiding-robot-overlords.html,ComputerWorld,2015,9,18,1038.0," Some people are afraid that one day robots will rise up, sentient, working as a collective and angry enough to overthrow the human race. Artificial intelligence (A.I.), and the robots it will empower, is something to fear, according to physicist and author Stephen Hawking and high-tech entrepreneur Elon Musk. Other scientists say the scariest thing is for our fears to stunt our research on A.I. and slow technical advances. “If I fear anything, I fear humans more than machines,” said Yolanda Gil, computer science research professor at the University of Southern California, speaking at DARPA’s recent Wait, What? forum on future technologies. “My worry is that we’ll have constraints on the types of research we can do. I worry about fears causing limitations on what we can work on and that will mean missed opportunities.” Gil and others at the forum want to discuss what the potential dangers of A.I. could be and begin setting up protections decades before any threats could become realities. There’s going to be a lot to talk about. The average person will see more A.I. advances in their daily lives in the next 10 years than they did in the last 50, according to Trevor Darrell, a computer science professor at the University of California, Berkeley. Today, A.I. touches people’s lives with technologies like Google search, Apple‘s intelligent assistant Siri and Amazon’s book recommender. Google also is testing self-driving cars, while the U.S. military has been given demonstrations of weaponized smart robots. While some would think this already is the stuff of science fiction, it’s just the beginning of a life filled with A.I., as the technology nears the cusp of a revolution in vision, natural-language processing and machine learning. Combine that with advances in big data analysis, cloud computing and processing power and A.I. is expected to make dramatic gains in the next 10 to 40 years. “We’ve seen a lot of progress, but it’s now hitting a tipping point,” Darrell told Computerworld. “In five or 10 years, we’re going to have machines increasingly able to perceive and communicate with people and themselves, and have a basic understanding of their environments. You’ll be able to ask your transportation device to take you to the Starbucks with the shortest line and best lattes.” For instance, today, a home owner might need a small group of people to move her furniture around. With A.I. and robotics, in 10 or years so, the home owners might have furniture that could understand her voice commands, self-actuate and move to where it is told to go. As useful as this sounds, some will wonder how humans will stay in control of such intelligent and potentially powerful machines. How will humans maintain authority and stay safe? “The fear is that we will lose control of A.I. systems,” said Tom Dietterich, a professor and director of Intelligent Systems at Oregon State University. “What if they have a bug and go around causing damage to the economy or people, and they have no off switch? We need to be able to maintain control over these systems. We need to build mathematical theories to ensure we can maintain control and stay on the safe side of the boundaries.” Can an A.I. system be so tightly controlled that its good behavior can be guaranteed? Probably not. One thing that’s being worked on now is how to verify, validate or give some sort of safety guarantee on A.I. software, Dietterich said. Researchers need to focus on how to fend off cyberattacks on A.I. systems, and how to set up alerts to warn the network – both human and digital – when an attack is being launched, he said. Dietterich also warned that A.I. systems should never be built that are fully autonomous. Humans don’t want to be in a position where machines are fully in control. Darrell echoed that, saying researchers need to build redundant systems that ultimately leave humans in control. “Systems of people and machines will still have to oversee what’s happening,” Darrell said. “Just as you want to protect from a rogue set of hackers being able to suddenly take over every car in the world and drive them into a ditch, you want to have barriers [for A.I. systems] in place. You don’t want one single point of failure. You need checks and balances.” USC’s Gil added that figuring out how to deal with increasingly intelligent systems will move beyond having only engineers and programmers involved in developing them. Lawyers will need to get involved, as well. “When you start to have machines that can make decisions and are using complex, intelligent capabilities, we have to think about accountability and a legal framework for that,” she said. “We don’t have anything like that right now… We are technologists. We are not legal scholars. Those are two sides that we need to work on and explore.” Since artificial intelligence is a technology that magnifies the good and the bad, there will be a lot to prepare for, Dietterich said, and it will take a lot of different minds to stay ahead of the technology’s growth. “Smart software is still software,” he said. “It will contain bugs and it will have cyberattacks. When we build software using A.I. techniques, we have additional challenges. How can we make imperfect autonomous systems safe?” While Hawking and Musk both say A.I. could lead to the annihilation of the human race, Dietterich, Gil and Darrell are quick to point out that artificial intelligence is not a threshold phenomenon. “It’s not like today they’re not as powerful as people and then boom they’re vastly more powerful than we are,” said Dietterich. “We won’t hit a threshold and wake up one day to find they’ve become super-intelligent, conscious or sentient.” Darrell, meanwhile, said he’s glad there’s enough concern to raise a discussion of the issue. “There are perils of each point,” he said. “The peril of full autonomy is the science fiction idea where we cede control to some imaginary robotic or alien race. There’s the peril of deciding to never use technology and then someone else overtakes us. There are no simple answers, but there are no simple fears. We shouldn’t be blindly afraid of anything.”"
126,https://www.computerworld.com/article/1634817/robot-ethicist-wants-to-ban-sex-robots-before-sex-robots-ban-us-itbwgk.html,ComputerWorld,2015,9,15,222.0," A robot ethicist in the UK with far too much time on her hands, has decided to launch a campaign to ban the development of “sex robots.” Of course the campaign makes excellent news fodder, is completely unwarranted and unneeded, but nevertheless a cause Isaac Asimov’s fictional robopsychologist Dr. Susan Calvin would be proud to champion. So there, see? Yes, now you’ve heard everything. In IT Blogwatch, bloggers campaign against silly campaigns. Today’s humble blogwatcher is . You have been reading IT Blogwatch by Stephen Glasskeys, who curates the best bloggy bits, finest forums, and weirdest websites… so you don’t have to. Catch the key commentary from around the Web every morning. Hatemail may be directed to @Glasskeys or itbw@richi.uk. Opinions expressed may not represent those of Computerworld. Ask your doctor before reading. Your mileage may vary. E&OE. Before writing I was a programmer and software developer and have lived and worked in the United States and Europe. I founded Glasskeys.com -- a website about tablets and mobile phones. Now I write part time for Computerworld's IT Blogwatch, and have written pieces for Forbes NetAppVoice. Bragging rights: I use tablets running iOS, Android, and Windows. The opinions expressed in this blog are those of Stephen Glasskeys and do not necessarily represent those of IDG Communications, Inc., its parent, subsidiary or affiliated companies."
127,https://www.computerworld.com/article/1633200/california-gov-jerry-brown-vetoes-controversial-bill-on-drone-restrictions.html,ComputerWorld,2015,9,10,388.0," A California bill that would have forced drones to fly at a minimum height of 350 feet (107 meters) over private property has been vetoed by Gov. Jerry Brown in the wake of criticism that it could have damaged the state’s drone industry and the economy at large. Senate Bill 142, had it become law, could have dealt a blow not only to hobbyists but to the aspirations of companies like Amazon.com that are aiming to make deliveries using drones. The requirement to ask permission from property owners to fly below 350 feet would have forced companies to ask for permission from neighbors before making a delivery to a particular location. In a message late Wednesday, Brown wrote to the members of the state senate that the issue had to be looked into more carefully. The bill as drafted, however well-intentioned, could have exposed “the occasional hobbyist and the FAA-approved commercial user alike to burdensome litigation and new causes of action.” The bill would have introduced trespass liability even if privacy was not violated, Brown added. State Senator Hannah-Beth Jackson, a Democrat from Santa Barbara who introduced the bill, says drones raise privacy issues, despite their obvious benefits. The bill extended current rules pertaining to trespassing on private property to remotely operated aerial vehicles, according to Jackson. The Federal Aviation Administration limits the flying of drones by hobbyists to a maximum of 400 feet. The 350 foot minimum altitude requirement of the proposed California law would have then limited hobbyists to a corridor of 50 feet. Rules proposed by the FAA for commercial uses of drones limit the altitude drones can fly to 500 feet. Some states have, however, have pushed ahead with their own legislation on various aspects of drones. In 2015, 45 states have considered 156 bills related to drones, according to data from the National Conference of State Legislatures. Two other bills are under consideration that would place restrictions on the flying of drones over prisons or county jails and schools in the state. The Consumer Electronics Association, a trade group,  welcomed the veto. “While we agree issues of privacy and drones should be addressed, this legislation was the wrong approach,” CEA president and CEO Gary Shapiro said in a statement. Safe and responsible drone use will transform the way people do business, he added."
128,https://www.computerworld.com/article/1623821/swarms-of-tiny-drones-could-be-used-for-rescue-missions-video.html,ComputerWorld,2015,8,25,492.0," Researchers at Carnegie Mellon University are working on a new generation of disaster drones that can be deployed in swarms into buildings to give first responders a look inside, mapping out the interior as they go. The drones could be valuable in situations such as those faced recently after massive explosions ripped through a port in Tianjin, China, or in the aftermath of a smaller event, like a house fire. “These places are very dangerous for rescuers to go, so we don’t want to just blindly send people inside,” said Pei Zhang, an associate research professor at CMU’s campus inside the NASA Ames Research Park in Moffett Field, California, where the research is taking place. “Instead, we want to get these things in before people go in and determine if there are people that need help,” he said, gesturing to several drones on the table in front of him. Zhang envisages using a larger drone, which he likens to a mothership, to carry multiple smaller drones into whatever environment is being explored. The smaller drones would deploy from the large drone and begin their work. The larger drone, he reasons, has a longer range and can better handle wind and other effects of the environment. But it may be too large to send inside somewhere like a building that’s been compromised by an earthquake. So the smaller drones, some of which can easily fit in the palm of a hand, would fly inside to do their work. Because they’re small, each drone will carry fewer sensors but Zhang says that doesn’t matter because there will be more of them and they’ll communicate with each other over ad-hoc peer-to-peer networks. “We can put a lot of these inside a building,” he said. “They can fly into walls, and because they are light they’ll hit the wall and nothing will happen, but [the drone] will know where the wall is and communicate that.” Through trial and error, the tiny drones will build a map of the inside of the building. They’ll also carry additional sensors to measure whatever is required, such as the temperature, air quality and even radiation. All of this information will be sent back to controllers via the mothership, which also carries a camera to provide a live video link. Zhang’s current prototype mothership drone can fly for about 30 minutes, and cover about 10 miles (16kms), a time and range that he wants to improve. The small drones are all still very much works in progress and the researchers have been working on miniaturizing their payloads so it can fit on smaller devices. The group’s research is currently focused on giving the small drones a greater degree of autonomy and building up the sensing capabilities of each one. Zhang thinks it will be about 10 years until the work is out of the lab and lands in the hands of users such as rescue workers, law enforcement and the military."
129,https://www.computerworld.com/article/1622768/here-comes-the-drone-backlash.html,ComputerWorld,2015,8,24,1267.0," Consumer drone technology is barely taking off, and already a harsh public backlash is growing. Your typical garden variety consumer drone is lightweight, battery operated, has four propellers and is controlled by a smartphone. Most have cameras and beam back live video, which can be recorded for posterity. Some are equipped with high-quality HD cameras and are capable of taking stunning photos and videos from their lofty vantage points. Drones are fun. They’re exciting. They’re accessible. But increasingly, they’re becoming unacceptable. I’m sensing a growing backlash, a kind of social media pitchfork mob against drones and drone fans. It’s only a matter of time, and not much time, before it will be politically incorrect to express any kind of enthusiasm for drones in polite company. I fear that many are about to embrace an “everybody knows drones are bad” mentality that will suppress the nascent industry and spoil this innovative and exhilarating technology. Here’s what’s driving the coming backlash: We used the D word for everything from massive, weaponized aircraft-carrier-launched unmanned aerial vehicles that drop bombs to hobbyist quadcopters that fit in a backpack. The word drone is fun and easy to use, but for the fearful it makes these aircraft sound more dangerous than they are. Consumer drones are particularly vulnerable to social stigma because anyone could be affected — threatened, violated, harassed, annoyed — but most people won’t actually participate. For the majority of people, drones are something “they” use that could affect “me.” Contrast drones to smartphones, the use of which is far more likely to annoy you and invade your privacy, but which is socially accepted because pretty much everybody has one. I’ll tell you a little secret about the mass media, which you probably already intuited: They pander to the neo-luddites in their audiences. Watch the 6 o’clock news, and it will be filled with a feigned “oh, gosh, this new-fangled technology is really moving too fast” mentality, even as they use Perceptive Pixel  displays and holograms to report the news. Plus, anytime drones interfere with, say, firefighters, it’s always going to be big news. You can be sure that the mass media reporting will always fall on the side of being vexed and put off by quadcopters. When politicians see a parade, they scramble to get in front and pretend to lead it. As the public turns against drones, there will be grandstanding hearings and calls for the Federal Aviation Administration to institute controls and bans. Scaremongering wins elections. The public is very bad at weighing the relative risks and benefits of anything. Media reports like to throw out big numbers. The best example: Drones have been reported buzzing, harassing and threatening airplanes recently. The occurrence is so common that 12 incidents were reported in a single day recently and a total of 700 were reported so far this year. Wow! That sounds horrible. Of course, any number of drones flying near any number of planes is too many. There’s no question about that. But how much actual risk is buried in those 700 reports? For starters, some unknown number of them involve a case of mistaken identity. One widely reported incident in the news involved a United Airlines pilot who said a drone struck his airplane. Less widely reported was that a subsequent investigation found that it was actually a bird strike. In fact, almost none of these reports involve actual strikes. A pilot sees — or thinks he sees — a drone “over there” somewhere. It has to be reported. And, in fact, anything inside the airport fence or in airspace used for takeoff and landing is unacceptable and must be reported. But to put the number 700 in perspective, let’s compare it to the 13,759 reported cases of wildlife strikes, mostly birds, in 2014. Unlike the drone reports, these aren’t sightings of animals “over there.” They’re reports of actual contact (usually violent, fatal to the animal and potentially life-threatening to passengers) between animals and airplanes. Some experienced pilots are starting to push back against the hysteria over drones, pointing out that, compared with bird strikes, drones are nothing to be alarmed about. In fact, it’s likely that drones could be used by airports to chase away birds and other animals and prevent dangerous contact with aircraft. By their very nature, drones represent a loss of control by non-participants. Drones can fly fast, fall out of the sky and come at you with their propellers spinning. The idea of drones flying around taps into the innate fear of a loss of control of one’s environment that some people have. Drones usually have cameras. They can look over fences, fly over private spaces and record what’s going on. They’re seen as a potential invasion of privacy. While most of these reasons for fear are overblown, most of them have validity at some level. They are reasons for fear. But ultimately, fear and its ugly cousin — a panicky mob attitude that would make the use of drones politically ostracizing — is not the answer. The answer to technology that bothers or threatens people is almost always better technology. Here’s why we all need to resist the coming backlash against drones: The majority of consumer drones are pretty harmless. They’re light. Their rotors don’t cut when they come in contact with human flesh. Flying a drone is infinitely safer than driving, crossing a busy street, riding bicycles, using gardening tools, using kitchen knives, taking prescription drugs, eating junk food, experiencing stress and any number of things we all accept as everyday realities. If we were to compare the state of drone development to the history of, say, mobile phones, drones are currently in their Motorola DynaTAC stage. Over the next year, five years and 10 years, drones will become better in every way — safer, more automated and quieter, and they will gain capabilities we can’t even imagine today. The beauty of drones — compared with, say, model aircraft, kites, ultralights, hot air balloons, sport pilot airplanes and all the other things flying around (including birds) — is that drones can be easily programmed to never fly near airports, populated areas or the White House. Multiple projects now in the works at NASA, Google, Amazon and elsewhere will result in an automated drone air traffic control system that will determine where drones can fly and where they can’t. Unlike the airplane air traffic control system, the drone version will be automated and will keep drones in check without human intervention. Drones can be instantly deployed in disasters to find survivors and deliver emergency supplies. They can save people from drowning in waters too dangerous for human lifeguards. They can serve as robot paramedics, prevent avalanches and give firefighters life-saving information on how and where a forest fire is progressing. Random quadcopter enthusiasts can even spot sharks that surfers can’t see. Many of these life-saving technologies are funded or subsidized by the consumer drone craze. While the cringing fearmongers freak everybody out with warnings of drones posing a threat to life and limb, it’s a great idea once in a while to think about what life is really all about. Drones represent an exhilarating freedom of expression and the potential of low-cost and extraordinary consumer technology. Drone photography and videography bring stunning beauty into our lives, and show us a new vantage point from which to love the world. (Check out the video my son recorded during his wedding dinner.) Surely all that has to be worth something. Still, the backlash is coming. And it’s too bad, too, because drones are awesome."
130,https://www.computerworld.com/article/1638895/are-we-safe-from-self-aware-robots.html,ComputerWorld,2015,8,13,950.0," End-of-mankind predictions about artificial intelligence, which have issued from some of today’s most impressive human intellects, including Stephen Hawking, Elon Musk, Bill Gates, Steve Wozniak and other notables, have generally sounded overly alarmist to me, exhibiting a bit more fear-of-the-unknown than I would have expected from such eminences, especially the scientists. But that was before I saw reports on the self-aware robot. The reports, such as this one from New Scientist, tell of a breakthrough in artificial intelligence. A robot was able to figure out a complex puzzle that required it to recognize its own voice and to extrapolate the implications of that realization. (Shorthand version: Three robots were told that two of them had been silenced and they needed to determine which one had not been. All three robots tried saying “I don’t know,” but only one could vocalize. Once that robot heard the sound of its own voice saying, “I don’t know,” it changed its answer and said that it was the one robot that had not been silenced.) What’s noteworthy is that this same test had been given to these same robots many times before, and this was the first time that one of these self-learning robots figured it out. And it’s that figuring-it-out part — more than the self-awareness itself — that is troubling. The classic argument against the robot takeover of the world is that while computers can go haywire — think the Windows operating system on almost any given day — so can humans. That’s undeniable, but society has established some extensive checks-and-balances that limit how much damage any one person can do. The military has a chain of command, and killers on a shooting spree are eventually stopped, either by the police or bystanders. Consider 9/11. Although terrorists flying planes into buildings was unexpected, as soon as the nature of the attack became apparent, all U.S. aircraft were grounded. But our reliance on computers to assist us and even take control just keeps increasing, and today machine intelligence is integral to military weapons systems, nuclear power plants, traffic signals, wireless-equipped cars, aircraft and more. One of our greatest fears now is that terrorists will gain control over any such key computer systems. But an even greater threat might be that the machines themselves gain the upper hand through artificial intelligence and wrest control from us. It’s become something of a classic science-fiction storyline: The systems calculate that they need to take a different path than we humans have envisioned. Consider this passage from that New Scientist story: “The test also shines light on what it means for humans to be conscious. What robots can never have, which humans have is phenomenological consciousness: ‘the first-hand experience of conscious thought,’ as Justin Hart of the University of British Columbia in Vancouver, Canada, puts it. It represents the subtle difference between actually experiencing a sunrise and merely having visual cortex neurons firing in a way that represents a sunrise. Without it, robots are mere ‘philosophical zombies,’ capable of emulating consciousness but never truly possessing it.” I suppose that was intended to be comforting to its human readers, suggesting that consciousness will always keep humans one big step beyond computers. But another way to look at it is that these systems will eventually have the ability to think any thoughts humans can, but without our moral compass. So the machines, confronted by a starving population and an agricultural system that is maxed out, might conclude that a sharp population reduction is the solution — and that the nuclear power plants within its control offer a way to achieve it. You can forget Isaac Asimov’s Three Laws of Robotics. The United Nations has already attempted to set rules for battlefield robots that can decide on their own when it’s a good idea to kill people. There is a subtle line that shouldn’t be crossed with artificial intelligence. Making Siri smarter so that she understands questions better and delivers more germane answers is welcome. But what about letting her decide to delete apps that are never used or add some that your history suggests you’d like? What if she sees from your calendar that you’re on a critical deadline this afternoon and decides to prevent you from launching distracting games when you should be working? Engineers are not the best at setting limits. They are much better suited — both in terms of temperament and intellectual curiosity — to seeing how far they can push limits. That’s admirable, except when its results move from C3PO to HAL 9000 to Star Trek: TNG’s Lore. When superior engineering truly engineers something superior — superior to the engineers — can disasters imagined in science fiction become science fact? Evan Schuman has covered IT issues for a lot longer than he’ll ever admit. The founding editor of retail technology site StorefrontBacktalk, he’s been a columnist for CBSNews.com, RetailWeek and eWeek. Evan can be reached at eschuman@thecontentfirm.com and he can be followed at twitter.com/eschuman. Look for his column every other Tuesday. Evan Schuman has covered IT issues for a lot longer than he'll ever admit. The founding editor of retail technology site StorefrontBacktalk, he's been a columnist for CBSNews.com, RetailWeek, Computerworld and eWeek and his byline has appeared in titles ranging from BusinessWeek, VentureBeat and Fortune to The New York Times, USA Today, Reuters, The Philadelphia Inquirer, The Baltimore Sun, The Detroit News and The Atlanta Journal-Constitution. Evan can be reached at eschuman@thecontentfirm.com and he can be followed at twitter.com/eschuman. Look for his blog twice a week. The opinions expressed in this blog are those of Evan Schuman and do not necessarily represent those of IDG Communications, Inc., its parent, subsidiary or affiliated companies."
131,https://www.computerworld.com/article/1634441/businesses-can-rent-a-robot-for-customer-care.html,ComputerWorld,2015,8,4,412.0," Wish a robot could greet your customers at the door, guide guests to the correct meeting rooms or even bring someone a cup of coffee? Well, businesses in Japan might be testing this out in just a few months. SoftBank Robotics Corp., which made a splash this past June when it sold 1,000 robots in one minute, is making an enterprise-level version of its personal robot available in Japan for pre-orders this October. Dubbed Pepper for Biz, the business-focused robot is designed to come loaded with applications that allow it to handle office reception tasks and even approach potential customers. It also has what the company is calling a Pepper for Biz platform that should let users customize applications and visualize data. One, for instance, is being geared at helping the robot be a salesperson; it starts work as a sales clerk next year at Yamada Denki, a major electronics retailer. The robot, now being pitched for more general customer-service work, will be available for rental at a monthly cost of 55,000 yen or about $443 U.S. dollars. The rental plan is a binding contract for three years. The company’s initial personal robot, simply called Pepper, sold for $1,600. Customers also agreed to pay a $120/month cloud connection fee and $80/month for insurance. That initial robot received a lot of headlines and attention around the world because SoftBank reported that Pepper can not only read human emotions but will generate its own emotions also. The robot, for example, reportedly can read people’s facial cues and will be happy to be with people it is familiar with and afraid when the lights go out. The robot might sigh when sad or bored, and it will display different colors on a display screen depending on its mood. Patrick Moorhead, an analyst with Moor Insights & Strategy, said Pepper for Biz could be a good starting point for businesses interested in using robotics . “These robots do very simple chores. They do presentations, meet and greet customers and ask and answer questions,” he added. “Robots are already in companies in manufacturing roles. These kinds of robots are used in softer environments. I think it’s a good idea. These robots are just the beginning.” Robots in the not-so-distant future could be used in companies to do bigger jobs, like clean, deliver mail and refill printer ink and paper in office copy machines. That kind of robotics work could just be five years away, according to Moorhead."
132,https://www.computerworld.com/article/1632292/pepper-the-robot-aspires-to-be-a-salesman.html,ComputerWorld,2015,7,30,720.0," Japanese robot Pepper is getting an intelligence upgrade via IBM’s Watson, but that doesn’t make interacting with the real world any less challenging. The humanoid will channel Watson’s artificial intelligence (AI) knowledge base when it starts work as a sales clerk next year at Yamada Denki, a major electronics retailer. Pepper has already helped sell goods such as smartphones and coffee machines, but engineers hope Watson’s ability to suggest relevant information will lead to richer interactions with customers. The robot can be programmed with a specific goal in mind — to move stock out the door. Laden with sensors, the cloud-connected Pepper has been marketed as a communication robot with a feel-good vibe and penchant for jokes, but developers are trying to make it more useful. One problem is the bottlenecks inherent in “embodying” the Watson AI platform and its natural-language processing powers in the real world via a robot like Pepper. In a demo on Thursday, IBM described how Pepper can identify customers in a shop, approach them and strike up a conversation, and answer their questions about products such as flatscreen TVs with an eye to making a sale. It could even bring related information, such as the start of 4K broadcasts, to bear. The demo took place at a tech show in Tokyo hosted by mobile carrier SoftBank, which put Pepper on the Japanese market last month. Watson servers in the U.S. had been fed information related to conversations about TVs so that the software could choose the best answer when given a query. It employs a dialog manager, a question-and-answer engine and speech-text interfaces. But amid the noise of the show, the robot often failed to recognize what was being said by an IBM staffer acting as a customer. That doesn’t bode well for working at Japanese electronics retail outlets, which resound with shouts from human clerks trying to get attention. Pepper also had difficulties in another demo designed to showcase its ability to recognize everyday objects. Staff from SoftBank cloud service firm Cocoro SB, which engineered the robot’s “emotion engine,” presented things like candy, a bar of soap and a tube of toothpaste to a camera on Pepper, and named each in turn. Later, when presented with the candy, for instance, Pepper said in high-pitched Japanese, “Let’s see….that’s a bar of Kao White soap.” Onlookers burst out laughing. The firm attributed the goofs to a lack of training data in the neural network the robot uses to identify things. The network compares the features of an image to high-level descriptions of other images in its database to find a match. As Pepper practiced more throughout the day, its performance improved. But when shown a voice recorder, which was something totally new, it identified it as a tube of toothpaste. After being presented with the recorder in three different orientations, it was able to identify it correctly. That sort of step-by-step knowledge building is essential for recognizing objects in its environment. “It has no broad understanding of things. It just looks for small patterns,” said Cocoro SB engineer Akira Takanami. Watson could significantly enhance Pepper’s knowledge of new items. When introduced to chocolate, for example, Pepper could draw upon the platform’s knowledge of the food and start talking about chocolate varieties and recipes. “With Watson, it can answer questions more deeply,” said Shu Shimizu, a senior manager for cognitive computing at IBM Japan. Robots could be better at selling than some human counterparts, at least if one experiment is anything to judge by. Scientists from Osaka University’s Intelligent Robotics Laboratory set up a lifelike female android as a clerk in a department store and had it try to sell $100 cashmere sweaters. During the experiment the robot dealt with twice as many customers as its human counterpart. SoftBank said it will start accepting lease orders for Pepper from enterprises in October. The robot will be able to approach customers, conduct questionnaires and act as a receptionist. The bot’s monthly “wage” is ¥55,000 ($444), which is less than half that of someone working at Japan’s average minimum wage. “Pepper can work 24 hours a day, it won’t complain and it will never be late,” SoftBank CEO Masayoshi Son said. Tim Hornyak covers Japan and emerging technologies for The IDG News Service. Follow Tim on Twitter at @robotopia."
133,https://www.computerworld.com/article/1628612/miller-s-wordnet-paves-the-way-for-a-i.html,ComputerWorld,2015,6,24,1243.0," I often hear that we can’t create natural language understanding (NLU), because we wouldn’t know how to represent meaning. But 30 years ago Professor George A. Miller, the famous psychologist and cognitive scientist, began WordNet. WordNet is a networked dictionary/thesaurus. You enter a word to find definitions and related meanings, but it’s also, unexpectedly, a simple universal encyclopedia (UE) — a system that, given choices, can identify the right meaning. Miller’s WordNet was a useful experiment in the traditions of the scientific method. For example, it allows the testing of the hypothesis that a language can be split from its meaning. It hints at UE design, and as a result, it shows the way to speaking A.I. A normal encyclopedia explains topics in detail, while a universal one extends it further to provide common sense knowledge. In 1958, Dr. Yehoshua Bar-Hillel, the first academic to work full time in the field of machine translation at MIT, said a UE is needed to make “general-purpose fully automatic high-quality machine translation” feasible. We learn “common sense” from experience, and therefore our brains must create a UE without programmers. In a brain, the specific defines the general, meaning the UE rapidly scales using examples. Replicating the brain’s method, if we can do it, will give knowledge to A.I. Against this approach, Professor Noam Chomsky argued in the 1980s that the poverty of the stimulus means we cannot learn language without an innate language facility, but since then, the Role and Reference Grammar (RRG) linguistic theory was developed. RRG provides an algorithm linking language to meaning and vice versa in context. In light of these advances, as we experience phrases in sentences, our brains connect with the richness of the UE because of RRG’s linking algorithm. A child’s brain is bombarded with huge quantities of specific language examples, and at the same time, connections in the UE. As language converts sound to meaning, language learning needs to connect words and phrases to their meaning, which can then allow the specific cases to be generalized. The massive amount of information connected between acquired language and the UE is language learning, just a by-product from storing patterns. A typical word has more than one meaning or word-sense. Dictionaries list them and define their meaning in different contexts. Choosing the correct meaning is known as word sense disambiguation. Human brains do this will apparent ease, but without a UE, how can a computer do this without intervention from a programmer? Statistics is one approach but, despite 30 years of trying, it has yet to provide the required accuracy. Here is Bar-Hillel’s scenario to demonstrate the problem: “Little John was looking for his toy box. Finally, he found it. The box was in the pen. John was very happy.” The word “pen” has meanings including, a “writing utensil” (like a ballpoint pen) and an “enclosure where small children can play” (a play pen). Bar-Hillel claimed that “No existing or imaginable program will enable an electronic computer to determine that the word pen in the given sentence within the given context has the second of the above meanings.” I’d argue that today, nearly 60 years later, we can create the program because I created a working prototype using an extension to WordNet. There is sufficient context for people to disambiguate the meaning of “pen” in the sample scenario. If the system learns that writing utensils cannot contain a toy box, but play pens can (only one is a container of things), it can exclude the invalid combinations during phrase matching. Because language is about communications, people will always try to convey the right meaning. I wrote previously that in A.I. a better way to store meaning is with only specific cases, with general cases found from them. This is illustrated if you tell a child that “the elephant flew out the window.” They may laugh and say playfully “No! Elephants don’t fly!” even though the child has never learned this as a specific fact. A trivial UE would link each meaning to every association, while a sophisticated one, like WordNet, would inherit meanings. By learning that “Animals don’t fly,” and that “Elephants are animals,” the child’s brain automatically knows that “elephants don’t fly.” Obviously as a brain learns more, the sophistication of the patterns increases. Bats and birds are also animals, so our brains handle conflicts. Perhaps separating flying animals from other animals will maintain consistency, as WordNet suggests. Finding the right model may be rocket science. But given the right model, it’s not rocket science to deal with the requirements. Miller, a psychologist, was in the center of cognitive science’s development. Cognitive science looks at how brains work, while A.I. reproduces their abilities on a machine. Miller started the WordNet project in 1986, just as the industry embraced statistical analysis. As IBM manager, Frederick Jelinek, said: “Every time I fire a linguist, the performance of the speech recognizer goes up.” The battle was lost to computation, but inaccuracy was the war’s result. They started to hit the target, but the bulls-eye remained elusive. Search companies like Google used WordNet data in their early days to limit possible word meanings. It helped distinguish meaning, such as when “eat” can mean literally “bite and swallow” as in the search “what’s eating the pizza?” compared with “preoccupy” as in “what’s eating Obama?” The UE doesn’t need to be like Wikipedia because it doesn’t need human-readable explanations, being just a network of associations, like WordNet. While imperfect for the full UE application, WordNet’s separation of language-independent meaning from language (words and phrases) shows the way. WordNet links words and phrases in English to their meanings and synonyms. It also links a number of associations between the meanings which are essential for language understanding. Since the initial version, other projects around the world have created their own local language WordNets in many, many other languages such as French, Arabic, German, Korean and Chinese. At the time of writing, WordNet has stopped support and development. On their website is the message: “Due to limited staffing, there are currently no plans for future WordNet releases.” While the glamor of computational methods and its “machine learning” techniques are probably behind WordNet’s demise, customer dissatisfaction from statistical inaccuracy compels the need for better approaches — in theory. From WordNet’s ashes may finally arise the UE for speaking A.I. John Ball is a scientist and engineer specializing in computer technology with degrees in science, cognitive science and business in Australia. As a cognitive scientist, John is particularly interested in applying new and emerging brain-based technologies on computers. John is presently focused on a machine's use of language using Patom Theory, a hierarchical, bidirectional model in which a brain only stores, matches and uses patterns. He invented and developed Patom Theory, leading to patents in 2007. The prototype software provides strong evidence that problems with human languages can be solved without processing. John founded Thinking Solutions in 1996 to exploit brain-based technologies and incorporated the company in 2006. Prior to that, he worked for IBM, CSC, Fujitsu and Telstra in a variety of technical and management roles. The key applications of language understanding are: (a) conversation with computers, (b) internet search, (c) people learning a language, (d) language understanding by machine and (e) automatic translation. The opinions expressed in this blog are those of John Ball and do not necessarily represent those of IDG Communications, Inc., its parent, subsidiary or affiliated companies."
134,https://www.computerworld.com/article/1634302/separating-science-fact-from-science-fiction-in-robotics-with-video.html,ComputerWorld,2015,6,23,1055.0," Science-fiction movies often show robots freely running across the screen, either wreaking havoc or saving the world. Many roboticists competing in the finals of the DARPA Robotics Challenge earlier this month said they are backing away from autonomy in order to make their robots work faster and more efficiently. For them, less autonomy for the robot and more human control means a better shot at winning the global, multi-year competition. An autonomous robot is able to complete tasks or behaviors without human intervention. The robots competing in the DARPA challenge were semi-autonomous, meaning they performed some tasks on their own but needed human guidance for others. “We still have a long way to go with autonomy,” Dennis Hong, leader of the UCLA team in the DARPA challenge, told Computerworld. “Originally, when DARPA launched this competition, autonomy was a huge part of it … but many teams aren’t using much autonomy [in the finals]. Autonomy is hard and they don’t have the time and money to build autonomy.” Hong, whose team came in 13th out of 24 teams at the finals, said before the final competition that his team would use more human-controlled operations than autonomy. Robots run with human operators are still faster. “This challenge pushed state-of-the-art in locomotion and humanoid robots, in general,” he said. “But not really in autonomy.” Scientists are not even close to building a robot that can do laundry or make dinner. Robots today are also not at the point where they can rush into burning buildings, shut off systems and rescue victims, although that’s where DARPA sees the technology going in another 10 or 15 years. That being said, robotics, and autonomy, are far more advanced than they were just a year and a half ago. At the Robotics Challenge Trials in December 2013, the machines were tethered to their human controllers, who directed the robots on such a granular level that they had to instruct them to move a hand ahead 2.5 inches, move the shoulder, turn the torso and then turn the wrist to a certain point and close the fingers around a door knob. Today, most of the scientists at the robotics finals could use computerized commands via Wi-Fi to make the robot walk to a door and open it. The robots are able to – generally – balance on their own, walk forward, grasp tools, and turn valves and knobs. In terms of autonomy, it’s a huge advance. It’s just not the advance that people, who get most of their perceptions about robotics from movies and television shows, have been expecting. “Our team’s focus was to not maximize autonomy but to maximize the utility of having a human-robot team,” said Taskin Padir, assistant professor of electrical and computer engineering at Worcester Polytechnic Institute, which, working with Carnegie Mellon University, placed seventh in the DARPA challenge. “Autonomy is still hard. It takes time if it’s easier for the human to help the robot with tasks, we do that.” The WPI team developed an algorithm that could enable a robot to recognize a door handle in 10 seconds, Padir said. As exciting as that was, a human could do the same thing in a moment, so the team used the operator, instead of autonomy, to get the robot to perform the task. “At the end of the day, the success was in how we maximize the efficiencies of using humans to help make the robot act more efficiently,” Padir said. “None of it was saying, ‘Robot, go turn the valve. Robot, go cut the wall.’ None of it was fully autonomous.” In the weeks leading up to the finals, WPI’s team focused on using more autonomy to make its robot faster. The team needed the help. A month before the finals, WPI’s robot, nicknamed Warner, still needed two hours to make it through the DARPA course of eight tasks. Those tasks included driving a car, opening a door, climbing stairs and maneuvering over rough terrain. The problem was that they would only have a single hour to complete those tasks during the competition. At the time, Matt DeDonato, the WPI team’s technical project manager, said the team tried to make the human operators running its robot more efficient, while also increasing the robot’s autonomy so it could do more work without waiting for its operators to send it guidance. Felipe Polido, a senior robotics engineer on WPI’s team, said, in the end, the team used a lot more autonomy with the robot than it had during the 2013 trials, but relied heavily on a combination of human and robot teamwork. “Before, we would tell it to move its arm this much, move its shoulder this much,”Polido said. “This time, there was much more autonomy because the robot could just move its arm on its own. For the plug task, the robot moved its hand to the plug but the operator helped it grab the plug.” The team from NASA’s Jet Propulsion Laboratory,however, used less autonomy in the finals than it did during the trials. “We shifted more to the operator to make it faster,” said Brett Kennedy, principle investigator for JPL’s team in the robotics challenge. “The challenge isn’t really about autonomy but about getting the job done. We’re not going to force autonomy on the situation.” Pam Melroy, deputy director of the tactical technology office at DARPA, said the fact that different teams used different levels of autonomy and that humans are still trusted to be faster than robots is part of what DARPA officials wanted to learn at the challenge. “One of the purposes of the DARPA robotics challenge is to show science fact vs. science fiction,” said Melroy, who is a retired U.S. Air Force officer and a former NASA astronaut. “State of the art in autonomy is extremely challenging. We’re just not as far along as people think we are, especially when you mix in the software and hardware together in a real-world environment.” She noted that what many teams learned was the efficiency of bundling several tasks together. If a robot is doing two to four tasks in a row, then the human operator has time to plan for the next steps. “Even two or three things programmed in make a big difference. You can see it,” said Melroy. “It’s baby steps.”"
135,https://www.computerworld.com/article/1633695/personal-robot-that-shows-emotions-sells-out-in-1-minute.html,ComputerWorld,2015,6,22,310.0," If there was any question as to whether people were excited to get their own personal robot, the answer is clear now. SoftBank Robotics Corp., an international company based in Japan, put 1,000 personal robots, priced at $1,600, on sale on Saturday. Within one minute, they were sold out. Customers also must pay a $120 per month cloud connection fee and monthly insurance of $80. It was the first time that SoftBank had allowed people to put in orders for the robot, dubbed Pepper. There has been high interest in the robot’s launch because its creators say Pepper not only can read and respond to human emotions but it will have its own emotions. According to SoftBank, the robot can autonomously generate emotions by processing information from its cameras and  sensors. SoftBank said more sales will be announced next month. “With this emotion function, Pepper’s emotions are influenced by people’s facial expressions and words, as well as his surroundings, which in turn affects Pepper’s words and actions,” the company said in a statement. “For example, Pepper is at ease when he is around people he knows, happy when he is praised, and gets scared when the lights go down.” The robot is designed to raise its voice or can sigh depending on its emotions at the moment. Pepper also will show its emotions – based on different colors and motions – on a chest display. The robot also has an ecosystem of more than 200 apps. Last week, SoftBank announced a deal to team  deal to team with Foxconn Technology Group, an Apple manufacturer, and Chinese e-commerce giant Alibaba Group to build and sell robots for the home and enterprise worldwide. SoftBank will retain 60% of its company but for an investment of $118 million U.S. each, both Foxconn and Alibaba will receive 20 percent stakes in the robotics maker."
136,https://www.computerworld.com/article/1619722/blame-chomsky-for-non-speaking-ai.html,ComputerWorld,2015,6,17,1104.0," Professor Noam Chomsky revolutionized linguistics in 1957 with his publication of Syntactic Structures, and his Chomsky hierarchy from the previous year remains a foundation stone in computer science for programming languages. But programming languages are a far cry from speaking A.I., and Chomsky’s unprecedented success in that part of linguistics should bear the blame for holding back the advancement in another part of linguistics — the use of human language for A.I. Obviously, how we use language to communicate is key, but there are a few flavors of the science of language, or linguistics. Chomsky studied formal linguistics, or “the formal relations between linguistic elements,” but another type, functional linguistics, studies “the way language is actually used in communicative context.” In other words, amazingly, Chomsky’s approach, unlike functional linguistics, is not concerned with actual communications! Chomsky’s linguistics, without communications, has been responsible for A.I. that doesn’t speak. While it’s not his fault that others used his approach to solve the wrong problems, we now have the opportunity to progress with different science. How did we get here? The birth of A.I. was tumultuous. A number of new sciences were coming together, computer science and linguistics in particular, and they were still being developed. This early work in A.I. was dominated by mathematicians partly due to the archaic stage of digital computers, but while human brains can be good at mathematics, it is just one of the skills they can learn. The problem arises when trying to fit a mathematical model to a non-mathematical brain. Cognitive science, my discipline, focuses on how our brains work. It combines computer science with philosophy, linguistics, neuroscience, psychology and anthropology. It emerged with the goal of replicating cognition on machines roughly 20 years after A.I. was named at the 1956 Dartmouth Summer Research Project on Artificial Intelligence. In the first sixty years since computers exploded into our world, we have seen formal and computational linguistics dominate, despite their scientific conflicts. Early success is good, but hitting the target once isn’t the same as hitting a bulls-eye. Also, hitting the bulls-eye once isn’t the same as doing it repeatedly. Science is about ongoing accuracy, hitting the bulls-eye every time. Clearly, we need a new goal. HAL in 2001: A Space Odyssey and Sonny in i, Robot both use conversational language beyond the capability of today’s artificial, computational languages. Emulating them is a good, revised target because speaking A.I. will be most useful to us if it mimics human communications accurately. As I wrote recently on this blog, in 1969 John Pierce of Bell Labs advised us to work out the science before pushing ahead with engineering. But probably due to frustration at the lack of progress for over a decade, engineering based on statistics was embraced anyway, before the science was ready. To meet the increasing demand for speaking A.I., the key is functional linguistics combined with a brain-based platform. Our goal should be to talk like Sonny because, like the evolution of personal computing, once unleashed, progress will be unstoppable. Patom theory is my computing approach, in which stored patterns do the work of programmers. But in 2006, as I was adding patterns to the system, the limitations of Chomsky’s linguistics hit me. What’s the best way to extract meaning from a matched sentence? I spent a lot of time researching the answer and decided to create my own model. It was a big decision because it was like starting a whole new scientific investigation. The implementation was difficult, too, because Chomsky’s model was a bit like working in an office tower with a broken elevator where each floor possibly held something important. Moving between floors to check was annoying! And then while browsing in a New Jersey bookshop, I stumbled across the answer. How could I have a degree in cognitive science, but still have missed out on the answers, based on more than 30 years of development, from Role and Reference Grammar (RRG) theory? RRG deals with functional linguistics and considers language to consist of three pieces – grammar linking to meaning in context. You know, word sequences and meaning in conversation. Communication! RRG was developed with the inspiration that all human languages are based on common principles and that clauses (parts of sentences) contain meaning. Its success in modeling the range of human languages is impressive. Speaking A.I. can use RRG’s linking algorithm to map word sequences in context to meaning, and vice versa. It was an eye-opener. The science speaks for itself in whatever language you read it. I subsequently met with the primary developer of RRG, Professor Robert D. Van Valin, Jr., who convinced me that I no longer needed to develop a scientific model to link phrases and meaning because RRG already explains how to do it in depth, like a cook book. Here we have unfortunate timing. In the 1980s as RRG was being developed, programmer’s continued to struggle with Chomsky’s linguistics. Without waiting for another underlying scientific solution, the industry finally decided to proceed with a method of incremental improvement for computational linguistics, based only on the statistics of sequences of sounds and words. Despite not meeting expectations, computational linguistics and its fixation on word sequences independent of meaning remains at the core of today’s A.I. troubles. Our next step will build on the new scientific approach using RRG for linguistics and Patom theory for programmer-free computing. It promises progress while the dominant paradigms deliver disappointment. With a plan for the future, speaking A.I. is finally coming of age. John Ball is a scientist and engineer specializing in computer technology with degrees in science, cognitive science and business in Australia. As a cognitive scientist, John is particularly interested in applying new and emerging brain-based technologies on computers. John is presently focused on a machine's use of language using Patom Theory, a hierarchical, bidirectional model in which a brain only stores, matches and uses patterns. He invented and developed Patom Theory, leading to patents in 2007. The prototype software provides strong evidence that problems with human languages can be solved without processing. John founded Thinking Solutions in 1996 to exploit brain-based technologies and incorporated the company in 2006. Prior to that, he worked for IBM, CSC, Fujitsu and Telstra in a variety of technical and management roles. The key applications of language understanding are: (a) conversation with computers, (b) internet search, (c) people learning a language, (d) language understanding by machine and (e) automatic translation. The opinions expressed in this blog are those of John Ball and do not necessarily represent those of IDG Communications, Inc., its parent, subsidiary or affiliated companies."
137,https://www.computerworld.com/article/1630424/researcher-finds-redemption-possible-hotel-deal-after-robotics-challenge.html,ComputerWorld,2015,6,16,773.0," A professor not only found redemption at the finals of the DARPA Robotics Challenge earlier this month, he might have found his robot a job at a Las Vegas hotel. The robotics team led by Paul Oh, a professor of mechanical engineering at the University of Nevada-Las Vegas, took eighth place in the final contest  where 24 teams from around the world competed to build the best semi-autonomous robot that could be used in disaster response. Reaching the top third of competitors was a big comeback for Oh, who had a disappointing showing as the team leader with Drexel University in Philadelphia during the December 2013 robotics challenge trials that led up to the finals. The Drexel robot fell during the trials and fumbled time after time. For Oh, who hadn’t planned on participating in the competition until DARPA reached out to him in January, came back to compete with only six months to prepare and then finishing in the top eight, was a greater success than he expected. “That’s the Cinderella story here,” Oh told Computerworld. “We showed that we had a [good] design. Better is the ending than the beginning. Better the spirit of patience than one of pride. There’s that redemption.” Now more than one Las Vegas hotel is talking with Oh about using the team’s Hubo robot, which can walk on two legs, kneel and roll on wheels built into its knees, to work in customer service. “These hotels out here are humungous, with thousands of rooms,” Oh said. “You can’t just throw labor at it – not efficiently, not effectively… It’s a promising thing. Given you have 40 million visitors to Vegas every year, how do you deal with that volume? People don’t want to wait three hours for room service. How can we use tech to make that more accommodating?” Oh declined to say which hotels he’s in talks with but did note that the companies are interested in having robots deliver room service and toiletries to guests. There even  has been discussion about the robot’s ability to handle cleaning services. If Hubo goes to work for a Las Vegas hotel, it wouldn’t be the first time a robot has walked a hotel’s halls. The Aloft Hotel in Cupertino, Calif., has been using a robot butler  to deliver snacks and incidentals to guest rooms. “At the Aloft, we strive to be different than any other hotels,” said Derrick Agas, front desk supervisor at the Aloft, which serves a high-tech clientele, in a March interview. “We’re in the middle of Silicon Valley where it’s pretty much tech central, so we try to do the newest and greatest things. This came out of that.” At the DARPA Robotics Challenge, the UNLV team earned six out of eight potential points in the finals. UNLV’s robot fell but was still able to complete most of the tasks, which included opening and walking through a door, turning a valve and driving a car. The goal of the challenge was to encourage the development of robots that could one day be used in disaster response. The robots would go into buildings, shut down systems, look for damage and find victims — eliminating the need to sendi human rescue teams into dangerous areas. While Oh and his UNLV team came in eighth, Oh’s cousin, Jun-Ho Oh of the Korea Advanced Institute of Science and Technology (KAIST), won the challenge. The cousins were both using Hubo robots, which they had designed and built together when they both worked at Drexel. Both later moved on to other institutions. Jun-Ho Oh and the Kaist team earned a perfect eight points and beat Florida Institute for Human and Machine Cognition (IHMC), which came in second, and Team Tartan Rescue from Carnegie Mellon University in Pittsburgh, which placed third. The top three teams scored eight points but their rankings were based on how fast they were able to complete the course. Paul said a win for his cousin was a win for him, as well. “His first place finish has my fingerprints on it,” Oh said. “In 2013, we were arm-in-arm and we saw our biggest nightmare unfold with our robot falling down and fumble after fumble. We both had that sting of disappointment. We both thought that chapter was over, but we both moved on.” Moving on was a testament to their determination. “I just love a Cinderella story,” said Oh. “I think it’s inspiration for a lot of people… You don’t look at these disappointments and let them paralyze you. Continue to do the good fight and believe in yourself. Learn from the disappointment and do better.”"
138,https://www.computerworld.com/article/1629332/philae-lander-phones-home-from-comet-hurtling-through-space.html,ComputerWorld,2015,6,15,406.0," After seven months of silence, a robot riding a comet hurtling through space has woken up and phoned home. The European Space Agency announced Sunday that Rosetta’s lander Philae, which landed on Comet 67P/Churyumov-Gerasimenko last November, is operating and sending data back to Earth. “Philae is doing very well: It has an operating temperature of -35ºC and has 24 watts available,” said Philae Project Manager Stephan Ulamec, in a statement. “The lander is ready for operations.” Scientists are hopeful that this mission could give them critical data about the origins of the solar system. The space agency said the robot communicated with Earth for 85 seconds. Its scientists already have analyzed more than 300 data packets it sent back. Philae shut down Nov. 15 after working on the comet for about 60 hours. As expected, the lander’s battery had run out. There was concern that Philae landed on the comet in a more shaded area than scientists had planned. That meant it might not have been positioned correctly to absorb enough solar power to turn itself back on. The European Space Agency has been listening for signs that the lander was till functioning since March 12. The agency said the lander may have powered itself back on before this recent communication. However, the agency scientists think Philae was unable to communicate in any previous power ups. Now scientists are waiting for another communication from the lander, which has 8,000 more data packets stored and waiting to be transmitted. Philae traveled to the comet along with its spacecraft counterpart, Rosetta, on a trip that lasted more than 10 years. Once Rosetta launched the lander, Philae bounced around on the comet’s surface before settling into a spot that was not exactly where scientists hoped it would be. Despite being in the shade, the lander worked continuously for 54 hours after its landing, using all of its 10 scientific instruments to study the comet and return images and data back to Earth. After that, Philae’s battery, as expected, ran out of juice and the robot went quiet. Scientists from the European Space Agency, as well as from NASA in the U.S., hope that by studying comets, they will get clues as to the origins of the universe. Since ancient comets are thought to be building blocks of the solar system, they also might shed light on how planets and stars were formed, as well as why Earth has water."
139,https://www.computerworld.com/article/1628556/nasas-robot-event-challenges-robots-engineers.html,ComputerWorld,2015,6,12,743.0," Despite robots that passed by, or even ran over, the objects they were searching for and others that never got off the starting platform, scientists said this week’s NASA robotics event is pushing the technology forward. NASA’s 2015 Sample Return Robot Challenge is intended to encourage researchers to develop robotics and autonomy software that could someday be used on Mars or the moon. The challenge, which involved 16 robotics teams from around the country, requires the robots to navigate – without human guidance – around a large field and find objects, ranging from a ball to a red rock and a yellow PVC pipe. The robots then have to scoop up the found object and return it to the starting platform. The event was held this week on the Worcester Polytechnic Institute campus in Worcester, Mass. NASA hopes the challenge will provide better technology so robots can work on their own on an asteroid, moon or another planet to find and retrieve objects or caches of objects that other missions leave behind. “This competition isn’t necessarily about the ability of the robot to rove or sense or detect an obstacle,” said Sam Ortega, program manager for NASA’s challenges project. “Those abilities have been developed. We’re really trying to test here the ability to put all of these systems into one platform. Here we’re raising the fundamental technology bed for sensing robots and autonomy. “Anytime you start doing something on the very cutting edge of technology, it’s going to be tough,” he added. On Wednesday, the first day of the competition, nine teams competed in Level 1, the lowest level of the challenge, which required the robots to autonomously move off a starting platform in the middle of a field and find and retrieve a single object. The robots, which generally have vision of up to 10 meters, were given a general location of that first object. Only three of the nine teams made it off the starting platform. None were able to find the first object. On Thursday, the second day of the challenge, two Level 2 teams, which advanced from Level 1 last year, had a bit more success. Team Survey, made up of engineers from Silicon Valley, found its first object but then was unable to find any of the others scattered in the field. It also ran over the yellow PVC pipe it was looking for. A team from West Virginia University won a $100,000 prize from NASA as the only team whose robot was able to find the required three objects. On Friday, with all of the Level 1 teams competing, none successfully retrieved an object and returned it to the platform, so none advanced to Level 2. “Sometimes it looks frustrating, and the teams are frustrated, but it actually went really well,” said Colleen Shaver, assistant director of WPI’s Robotics Research Center and manager of the NASA challenge. “We saw teams detecting samples, teams attempting to pick them up and in several cases actually collecting the sample. It’s hard to see just how difficult it is to plan for the weather, like the sun, which can be aimed right into the camera. It’s impressive to see how much the teams did today despite the variables.” She noted that some of the teams’ robots didn’t make it off the starting platform, including a team from MIT, because they were taken down by basic problems. One Level 1 team whose robot made it off the starting platform and was headed toward its first object before it ran into trouble was made up of students from Schenectady High School in New York. Unlike the other teams, which are from major universities or made up of professional engineers, the high school teammates worked on the project after school. “It just shows that this challenge can reach anybody,” said Shaver said. “Innovation can be done anywhere, whether it’s high school kids or professional engineers. It’s how you look at the problem and how much energy you’re willing to put into it.” Ken Stafford, associate director of robotics engineering at WPI, noted that what the robotics teams were faced with doing in a short amount of time in the challenge, robots on Mars, for instance, might have a week or more to do. “There’s no rush on the moon or Mars,” he added. “We can’t give them that much time so this is actually much tougher than it would be in the real world.”"
140,https://www.computerworld.com/article/1625238/nasa-tests-darpa-challenge-robot-for-space-manufacturing.html,ComputerWorld,2015,6,9,615.0," NASA has big plans for the robot its JPL team used to compete in the DARPA Robotics Challenge finals last weekend. The space agency hopes its four-legged robot, which came in fifth place in the DARPA finals, can one day build parts for the International Space Station and satellites in space. The weekend challenge involved two dozen teams competing to see who had built the best robot to aid in disaster response. Now that the JPL, or Jet Propulsion Lab’s robot, dubbed Robosimian,  has finished competing, its second job will begin. “We actually already have a program with DARPA. Rather than looking at disaster areas, we’re looking at assembly work — in this case, assembly in space,” said Brett Kennedy, principle investigator for JPL’s team in the robotics challenge. “The dexterity and mobility capabilities that come along with Robosimian could be adapted for zero-g (gravity) environments.” Robosimian is a four-legged robot that can stand upright on two legs and use its other two limbs as arms with hands capable of grasping a lever, holding a tool or turning a valve. That design might serve well in orbit, on the moon or even on Mars to build satellites, fuel depots or shelters to house astronauts. “Assembly in space … it would all be the same robotics problem,” Kennedy told Computerworld. “The most immediate research we’re going to be doing… would be more along the lines of building very large telescopes or fuel depots for satellites. Currently, the largest telescopes we can build are limited by the size of the rocket that launches them.” The bigger the satellite, the bigger the rocket needed to launch it. Larger rockets cost more, making it too expensive to launch large satellites. The same robotics technology also could be used to build fuel depots in orbit. Then robots could be used to refuel satellites, keeping them going far longer. It also would be more efficient and less expensive to refuel satellites than to build and launch new ones. If scientists could figure out a way to launch robots, the parts and possibly 3-D printers into space, then have the robots do the assembly work in orbit or, one day, in deep space, it would be far less expensive to do. By avoiding the structural stresses and high costs of major launches, future satellites would not only be cheaper, but would be bigger and higher functioning. Late last year, Tethers Unlimited Inc., a Bothell, Wash.-based aerospace and defense research company, announced that it is working on six- or eight-legged robotic spiders to build satellites or even spacecraft in space. Researchers hope the project, called SpiderFab, could change the way spacecraft are built and deployed. NASA’s Robosimian is probably a few years away from being tested in space, according to Kennedy. “If we can point to something we do on Earth that we could do on another planet, that’s a very powerful thing to show people,” he added. “We can have a different idea of what missions might be.” Robosimian will need to be redesigned for the rigors of space but should still look similar to the way it looked during the DARPA challenge. DARPA’s goal is to advance autonomous technology and robotics to the point where robots could be sent into damaged buildings after a disaster to turn off systems, inspect damage and look for victims. In the finals last weekend, the teams were tasked with sending their robots into a simulated disaster scene, taking on eight different tasks, including driving a car, climbing stairs, using a drill to cut a hole in a wall and turning a valve. South Korea’s Team Kaist won the challenge, winning the $2 million top prize."
141,https://www.computerworld.com/article/1619592/a-i-is-too-hard-for-programmers.html,ComputerWorld,2015,6,8,1266.0," Artificial intelligence is the Holy Grail for Silicon Valley, because human-like robots that speak will change our world for the better. As Steve Jobs would have said, “This changes everything.” Imagine standing in your kitchen and saying, “Can you turn the lights over the hot plates on?” instead of walking to the wall near the kitchen door to flick the correct switch. Simpler communications. Easier. Faster. Think of the possibilities — when isn’t your normal speech better than today’s interactions with keyboards, mice and touchscreens? Speaking AI could really make a difference in situations like driving, where your attention should be on the road, not on a screen. The roadblocks to A.I. have always included the problems of designing and writing large programs, representing complex ideas and dealing with different types of data. When computers first appeared, limitations in hardware and software were a factor, but no longer. Some hoped that machines would write their own software “like a brain,” but that has never eventuated either. Patom theory solves the problem of programming by using just one algorithm. Patom theory says that brains just store, match and use patterns. Nothing more. It’s a bit like a matched pattern identifying a stored pattern that acts as a program. The name “Patom” combines “pattern” and “atom”. Patterns are indivisible elements that, like atoms, can combine, to form more complex patterns. Patom theory is inspired by the observations of pattern combinations in brains and languages. It promises to be the first step toward machine intelligence because it solves the main problem: A.I. is too hard for programmers. The biggest difference between computers and brains? Computer programmers define the general to store specifics, but brains store the specific to identify the general. Brains learn this way, but computers don’t. You know this already, of course, because when you learn things, you experience them and can then “magically” apply what you have learned. Computers programmers define data structures to represent general requirements. This follows from Alan Turing’s 1936 design emulating human computers. By keeping track of calculations on paper, human intelligence can make infinitely complex calculations using carefully designed data structures. In brains, the opposite is true. We learn from experience (specific) and generalize from there. In a future post, I will have a lot more to say about how brains store patterns and learn them, but for now, let’s focus on why this difference is the significant roadblock inhibiting our 1956 A.I. objectives. The problem is that programmers often cannot define the general. Our brains tell us what the general is, but it is often wrong. My favorite example comes from the real world. What is a bird? Birds are fist-sized animals that fly. So what is a penguin? A bird, yes, but hardly fist-sized, and completely incapable of flight. You see the problem: Definitions starting from a general case aren’t flexible. Generalizing from specific cases is better. Let’s store a few birds: a sparrow, a hawk, a robin, a dove, an emu, an ostrich and a penguin. The only association we start with is that each is a bird. This tells us little, but we have more to learn. Through experience, the robin and dove are about the size of my fist. The sparrow is a bit smaller and the hawk a bit larger. The emu and ostrich are much, much bigger. The penguin is bigger, too, but smaller than the emu and ostrich. Each of these birds has scaly legs and feet, and feathers. Wait, does the penguin have feathers? If not, don’t give it that association. Also the penguin swims, but each of the other birds flies. Do birds fly? Well, yes, and no. Do penguins swim? Yes. The experience of these relationships illustrates the brain’s paradigm where the specific defines the general, operating at the semantic level. The meaning of each bird is a set of associations from experience, including what it is, what it has and what it does. I call these associations “a part of the pattern,” and each type of bird forms its own pattern “atom.” The atoms connect in a network of associations, created through experience and generalization. Patom theory allows atoms to split and combine dynamically, but let’s not get ahead of ourselves. The first association we stored for each bird type was that they are birds. The second is their size, relative to the other birds. The third association for each bird is that they have feathers. The question is whether all birds need feathers. Can you imagine a penguin with blubber to keep warm instead of feathers? I can. The general defines the specific fails at such changes. The alternate is far more flexible because rewriting your general design is impractical. If you add a blubber-based penguin, it remains a bird and leaves generic bird details unchanged. There is a simple mechanism to separate these patterns that I call “linkset intersection.” It allows you to take some elements and find the common attributes, much as a database query today, but working in a brain-like network. How large is a bird? Just intersect “birds” with “large.” Large is a size (learned with the same association principles that created the bird network), so we get the following: (a) two are the size of my fist, (b) one is smaller, and (c) the others are larger. Given an ambiguous question (ambiguity means there is more than one answer), we can choose the most frequent answer: “fist sized.” This comes from the meaning, which can be stored by experience. Linkset intersection is an efficient way to find answers from massive data stores, like a brain, without indexing. Once we have an answer, “fist-sized,” the associations such as the visual images of robins and doves are also available. This isn’t a statistical approach, but a pattern-matching one. It only provides valid answers from experience, not guesses. Other intersections, as needed, are available to deal with other real-world questions. I recently wrote about Alan Turing’s computer, a machine limited by compression and duplication of data. That programming system, based on human computers, further limits applications by requiring a programmer to define the general attributes in advance. Time has shown this approach to be spectacularly successful at many applications while being poor for A.I. To unlock the power of A.I., the first step is to expand and centralize data. We then use the brain’s approach of storing specifics. Specifics are effective at determining general conditions in conjunction with an appropriate system for storing (learning). John Ball is a scientist and engineer specializing in computer technology with degrees in science, cognitive science and business in Australia. As a cognitive scientist, John is particularly interested in applying new and emerging brain-based technologies on computers. John is presently focused on a machine's use of language using Patom Theory, a hierarchical, bidirectional model in which a brain only stores, matches and uses patterns. He invented and developed Patom Theory, leading to patents in 2007. The prototype software provides strong evidence that problems with human languages can be solved without processing. John founded Thinking Solutions in 1996 to exploit brain-based technologies and incorporated the company in 2006. Prior to that, he worked for IBM, CSC, Fujitsu and Telstra in a variety of technical and management roles. The key applications of language understanding are: (a) conversation with computers, (b) internet search, (c) people learning a language, (d) language understanding by machine and (e) automatic translation. The opinions expressed in this blog are those of John Ball and do not necessarily represent those of IDG Communications, Inc., its parent, subsidiary or affiliated companies."
142,https://www.computerworld.com/article/1623749/south-korean-team-surges-past-rivals-to-win-darpa-bot-battle.html,ComputerWorld,2015,6,7,837.0," POMONA, Calif. — South Korea’s Team Kaist, which had been in sixth place after the first day of competition in the DARPA Robotics Challenge finals, maneuvered past its rivals on Saturday to win the two-and-a-half-year battle. Team Kaist slipped past the other teams to claim the $2 million prize in what was a battle between some of the best roboticists from around the world. There were 24 teams all together. The winning team is made up of engineers and programmers from Kaist, a South Korean research university formerly known as the Korea Advanced Institute of Science and Technology, along with researchers from the Rainbow Co., a spinoff of the university’s research lab. Team Kaist, which operates a nearly six-foot-tall, 176-pound humanoid robot, finished all eight tasks in the course in just 44 minutes and 28 seconds. The winning run was six minutes faster than the competition’s runner up — Team IHMC, from the Florida Institute for Human and Machine Cognition, which also earned eight points, came in second place and won $1 million. Team Tartan Rescue from Carnegie Mellon University was third, also with eight points, and won $500,000. “It’s a great moment,” said In So Kweon, a Kaist team leader who worked on the robot’s sensing abilities. “The most important thing is the humanoid robot’s system is so well built. It has good balancing and can walk on its feet or roll on wheels. It was a brilliant mechanical idea.” The robot, which has two arms, is built to walk upright on two feet or kneel down and roll on wheels built into its knees. All of the robotic teams had to take on a course that simulated a disaster scene, which challenged the robots with tasks like driving a car, opening a door, turning a valve, navigating debris and climbing stairs. DARPA ran the challenge to encourage roboticists to work on hardware and software that will be needed one day for robots to work in natural and man-made disasters, entering dangerous areas, turning off systems, searching for victims and assessing damage. The competition brought in teams from the likes of Carnegie Mellon University, NASA’s Jet Propulsion Laboratory, MIT, Worcester Polytechnic Institute (WPI) and Virginia Tech. “The key thing about this technology is that it’s not just that it’s a cool robot…; the key thing is to be solving a problem in the human condition,” Pam Melroy, deputy director of DARPA’s tactical technology office, said. “Some of these robots, if there was a disaster tomorrow, we might be able to send them and they might be able to make a difference.” Melroy, who also is a retired U.S. Air Force officer and a former NASA astronaut, said she thinks highly capable robots could be deployed worldwide for disaster response within 10 to 15 years. The finals for the robotics challenge took place on Friday and Saturday, with each of the 24 teams running through the course on both days; their best times and point totals were then used to decide the winner. On Friday, the Carnegie Mellon team was the only one to get all eight points, making it the early leader. That team, though, ran into early trouble in its final run when the robot drove past its mark and into a barrier. That mistake slowed the robot, dubbed Chimp, enough to keep it from holding onto the top position. Other teams has their own troubles on Saturday. JPL’s four-legged Robosimian robot, which came in fifth place, was moving well through the course until it dropped a plug it was trying to move into a socket. It then stopped moving just as it got to the stairs, the last task on the course. The robotics team from MIT had tough luck on both days. On Friday, its robot fell getting out of the car and broke an arm. Despite the damage, it completed the course (and won 7 points) in enough time to leave it in fourth place after the first day. On Saturday, it looked like MIT’s robot might be the only one that could overtake Team Kaist. But then the MIT robot fell again, dashing MIT’s chances. WPI’s team also had some hard luck. In seventh place at the end of the day on Friday, its robot “Warner”had just started driving down the course when it came to a stop. A problem with the car derailed the robot’s first attempt at the run. The team had to push the car back off the course, switch the robot to another car, recalibrate and start over. Once it got started again, Warner got its drill stuck in a wall and then dropped the tool. It finished the rest of the course but was unable to get the full eight points because of the drill problem. “We were hoping for eight points and we had a good shot at it,” said Mike Gennert, director of WPI’s robotics engineering program. “It just wasn’t meant to be.” WPI finished in seventh place at the end of the competition."
143,https://www.computerworld.com/article/1621699/battle-of-the-bots-teams-compete-in-darpa-challenge.html,ComputerWorld,2015,6,5,778.0," POMONA, Calif. – People cheered and clapped, yelled support and literally held their breath as the two-legged, humanoid robot from Team IHMC drove a car, drilled a hole in a wall and turned a valve. Then as the robot, nearly at the finish line, tried to step off a pile of cinder blocks, it swayed and crashed to the ground. The robot, which was the clear leader in the first day of the finals of the DARPA Robotics Challenge, fell not once but twice. “It was one of the first tasks we could do,” said Nicolas Eyssette, a research intern and programmer on Team IHMC (Florida Institute for Human and Machine Cognition), referring to the task where the robot is supposed to step over debris. “I’m very surprised.” Still, the robot, known as Running Man, leads the pack of eight robots that were tested on the course that simulates a disaster scene. The robot’s team reached a score of seven out of a possible eight points. Team Trooper, led by Lockheed Martin, was in second place with two points, while Team HRP2 from Japan’s National Institute of Industrial Science and Technology, and Team Walkman from the Italian Institute of Technology, both have one point each. Only eight of the 24 teams competing in the DARPA Robotics Challenge finals have run the course so far today. The first four teams to compete this morning were unable to score a single point. One of those first robots got stuck in the dirt leading up to the door of the course and fell over. Others fell before even getting close to the door. Despite the falls and the fails, the robots running the course in the finals of this two-and-a-half-year challenge are significantly more advanced from the earlier versions that competed in the trials of this competition in December 2013. A year and a half ago, it was common for a robot to need as long as five or 10 minutes to take a single step. Today, some of these robots walk nearly as quickly as an adult human. They turn valves and open doors. They may do it slowly, but they’re far ahead of where they were in the trials that led up to the finals. “The amazing thing to me is they’ve advanced so much in 18 months,” said Laurie Leshin, president of Worcester Polytechnic Institute, whose team is competing in the finals today. “In the trials, the eight tasks were done individually, and now they’re done back to back and the robots are untethered so they can fall down. That makes it much more edge-of-your-seat exciting.” DARPA is running the challenge in an effort to get roboticists from around the world focused on developing hardware and software for semi-autonomous robots that can be used to help victims in a disaster. If a robot could be sent into a damaged building, or a nuclear facility, to turn off systems, search for victims and check for damage, it would save a human emergency worker from having to go into a dangerous situation. During the opening ceremony this morning, Gill Pratt, a program manager with DARPA, recalled the 2011 earthquake and tsunami that devastated the Fukushima nuclear power plant in Japan and led a meltdown of three of the plant’s nuclear reactors. “Robots were sent to Japan to help,” said Pratt. “They did their best but what they could do was not enough. They couldn’t turn those vent valves off. The inability of our best robots to make a profound effect in the short term was disastrous.” DARPA leaders are hoping the competition will lead to robots that are quicker, more stable and more autonomous so they could make a difference in a disaster situation. Leshin, who was a senior scientist at NASA and worked on the Mars rover Curiosity science team, said she’s most impressed at the challenge with the advances in the overall robotic systems. “To me, one of the most exciting technology advances is seeing the system level work with vision, computational power, the ability to use tools and automation,” she told Computerworld. “Any of these technologies individually is tough to do, but making them work all together is impressive.” Today is the first official day of the finals. All 24 teams will compete on the course today, and will be able to repeat the course on Saturday. DARPA will take the highest score from both days and combine it with the speed the teams needed to complete the course to award the top team. The winner will go home with $2 million, while the second-place team will receive $1 million and the third-place team will get $500,000."
144,https://www.computerworld.com/article/1621929/redemption-tech-advances-and-2m-prize-drive-darpa-robotics-finalists-video.html,ComputerWorld,2015,6,5,998.0," POMONA, Calif. — The leader of one of the 24 teams competing in this week’s finals of the DARPA Robotics Challenge didn’t think he’d be here. Actually, he didn’t really want to be here. Until six months ago, Paul Oh, a professor of mechanical engineering at the University of Nevada-Las Vegas, had no intention, or interest, in competing in the DARPA challenge that is bringing in two dozen teams from around the world to see who has built the best robot for in a search-and-rescue mission. After a disappointing showing as the team leader with Drexel University in the December 2013 robotics challenge trials, Oh had moved on to UNLV and was looking forward to sitting in the stands and watching other teams compete. Then DARPA called and asked him to get back in the game. Now, for Oh, it’s all about redemption. “DARPA reached out to me and I said, ‘Why would I? Why relive this?'” Oh said. “I have mixed feelings about doing this again, but I do want redemption. And if they can take a little piece of something we developed and put that to use for disaster response, we’ll have done our job.” For more than two years, teams from the likes of NASA, Carnegie Mellon University, Virginia Tech, Lockheed Martin and Worcester Polytechnic Institute, have been building robots – some two-footed, some four-footed – to take on this challenge. The winner, to be announced Saturday afternoon, will go home with a $2 million prize. The runner up gets $1 million, with the team in third place taking $500,000. That’s a lot of money to fund continued research, but many of these researchers are simply looking to push themselves and robotics technology. DARPA, of course, is looking to advance robotics and autonomous software to the point that during a disaster, machines could go into dangerous areas and buildings on the verge of collapse to turn off systems, find victims and investigate damage. The idea is to send robots where it’s too dangerous for humans to go. On Thursday, the teams gathered at the Fairplex here to take their first run at the course, which combines eight different tasks – like driving a car, climbing stairs, turning a valve and navigating a debris pile — in one simulated disaster scenario. The robots will take their first official run at the course on Friday and make a second attempt on Saturday. Judges will score the teams on the number of tasks completed, along with the amount of time it took them to finish the course. (The robots will only have one hour to complete the tasks, along with a surprise task they haven’t been able to prepare for.) DARPA will take the top score from each team from the two days. For many of these roboticists, more than two years of work all comes down to two one-hour challenges. Matt DeDonato, team leader from Worcester Polytechnic Institute (WPI) team, said his crew has easily spent thousands of hours, if not tens of thousands of hours, working on their robot, affectionately named Warner. “I think at this point, we’ve done the best that we can,” said DeDonato. “It’s about the work we put in. The competition doesn’t matter as much. Seeing these amazing robots isn’t something you see every day.” Dennis Hong, team leader and a professor of mechanical and aerospace engineering at UCLA, is also excited to see the different robots competing at the finals. On Thursday night, though, he wasn’t spending time checking out the competition. He was too busy trying to fix serious damage to his robot. During UCLA’s practice run on Thursday, the humanoid robot was about to get out of the car after driving onto the course when the wind slammed the car’s door shut, breaking the robot’s shoulder. The motor in the shoulder broke, while metal brackets in the joint were warped. If Hong’s team can’t repair the shoulder, they’ll have to use a backup robot that they don’t have as much confidence in. Hong didn’t appear too stressed about the situation. Working under pressure is something the teams have gotten used to. “This is the top of the top — the cream of the crop,” he said. “In the future, when someone speaks at a conference or develops new technology, it will be by people in this building. Winning here doesn’t mean you’re the best. There’s going to be a lot of luck to it.” WPI’s DeDonato knows what it’s like to work with a handicapped robot. Earlier this year, WPI’s 7-foot-tall, Boston Dynamics-built humanoid robot had a broken arm. They couldn’t stop preparing for the finals while they waited for it to be fixed. That means they simply learned how to run through most of their tasks with one arm, leading DeDonato to joke that the robot can now get through much of the course with one hand tied behind its back. On Thursday, the WPI team had both robotic arms working, though engineers were trying to fix a hand that broke before their practice run. There’s a lot of luck involved in doing well at the competition, according to UNLV’s Oh, but it’s also about learning to overcome broken parts and problems on the course — because that’s what robots actually working in a disaster area will face. “You can’t always predict what will happen,” he said. “How can we build a platform so problems aren’t catastrophic?” To that end, most of the team leaders say they’re working together more than they’re competing. Of course, they’d all like to win the $2 million top prize. But they’re also looking to advance the technology. “It’s a competition, but we’re not really fighting against each other,” said Hong, who noted that teams have been trading information and assistance in the run up to the final challenges. “We’re trying to develop technologies that will save the world. If it can save even one person’s life, we will have done our mission.”"
145,https://www.computerworld.com/article/1618937/why-apple-is-developing-artificial-intelligence.html,ComputerWorld,2015,5,29,619.0," Many of the building blocks are already in place. 9to5Mac’s news that Apple is developing ‘Proactive,’ an intelligent, context-based solution that protects user privacy while exceeding the features of Google Now lends even more weight to development of big data driven A.I. solutions. No surprise then that a search of Apple’s job vacancies reveals at least 23 roles with ‘Artificial Intelligence’ in the job description. A job description for a Siri Speech Platform engineer reads: “Play a part in the next revolution in human-computer interaction. Contribute to a product that is redefining mobile computing. Create groundbreaking technology for large scale systems, spoken language, big data, and artificial intelligence.” The description makes clear that machine learning and big data are also part of the Siri team’s tasks. That’s the kind of machine intelligence you can expect from IBM’s Watson, or from other deep learning systems in development by other technology firms, including Facebook and Google. Big data analysis, speech recognition and deep learning are critical to these A.I.  machines, and this kind of intelligence also informs developments in robotics. Morrell was a leading engineer at Segway. Once at Apple, he worked with Segway’s former chief technology officer, Doug Field, who decamped to Tesla Motors in 2013 where he is now vice president for vehicle programs. These connections link neatly with the now much-anticipated Apple Car. If we stop to consider what the features of such a vehicle might be, it isn’t hard to imagine that spoken word instruction, predictive analytics, automatic accident prevention and car-to-car communication will feature in any Apple Car. This isn’t just about texting while keeping your hands on the wheel – though these things will do that, too. Auto manufacturers are already developing these solutions. Once you accept that Apple is developing in-car intelligence, then it would be foolish to imagine it will not seek to apply this intelligence in other fields, such as self-healing machines, robotics and A.I., augmented services and automation. That’s not especially surprising when you consider the entire technology industry has been on a trajectory to develop smarter-than-human A.I. since way before the Difference Engine or the writings of the UK’s first Mac user, Douglas Adams. Five years ago, I was among the first to predict Apple Pay. Lots of people didn’t agree, but all that was required to predict it was to join the dots of what little was already known. When it comes to A.I., it seems pretty clear that Apple has assembled much of what it needs to achieve rapid progress in the development and deployment of increasingly sophisticated cloud-based artificial intelligence across its growing line of products in the coming years. It certainly has the budget to do it. “I think we should be very careful about artificial intelligence. If I had to guess at what our biggest existential threat is, it’s probably that,” warned Tesla boss Elon Musk last year. There remain concerns that A.I. automation will destroy more employment than it creates. Apple is aware of these concerns: Apple board member Andrea Jung (who moved to a new job last year) serves on the Committee for Economic Development, which only last month warned that this new wave of automation could seriously impact employment and further polarize wealth inequality. Google+? If you use social media and happen to be a Google+ user, why not join AppleHolic’s Kool Aid Corner community and join the conversation as we pursue the spirit of the New Model Apple? Got a story? Drop me a line via Twitter or in comments below and let me know. I’d like it if you chose to follow me on Twitter so I can let you know when fresh items are published here first on Computerworld."
146,https://www.computerworld.com/article/1617809/heres-a-security-drone-that-follows-you-around-and-takes-video.html,ComputerWorld,2015,5,22,443.0," If you think drones are more than slightly creepy, wait until you meet one that will autonomously follow you and record video. Japanese security company Secom is launching a drone that will automatically launch when an intruder is detected and follow him or her while sending video to human supervisors. The sleek silver quadcopter was shown off this week at the inaugural International Drone Expo held in Makuhari outside Tokyo, where about 50 companies gathered to exhibit drones and related technologies. The UAV will be offered to businesses in Japan operating on relatively large parcels of land — big enough to warrant a flying security camera — such as shopping malls and supermarkets with large parking lots. The drone can link with an intruder-detection system that sends beams of laser light along the perimeter of a secure space. When the detector senses motion, the drone automatically takes off from a nearby charging station to investigate. It can send real-time video of an intruder or vehicle and even its license plate to a Secom security center for analysis. “It won’t leave the premises but will record imagery of intruders leaving it,” said Secom spokesman Akihiko Takeuchi. The robot’s battery only allows it about 10 minutes of flying time, but it will automatically return to its station to recharge. The device has a high-definition camera and several sensors, but Secom would not provide details ahead of the drone’s official launch in June. Secom plans to offer the machines on a monthly rental basis to enterprise users as part of its security services, which include traditional burglar alarms and security guards. The company has been working on robot security solutions since before the current drone boom. Its Robot X, developed over a decade ago, is a scooter-sized, mobile droid that can autonomously patrol an area, record imagery and release clouds of smoke to ward off intruders. Japanese companies are developing security solutions for drones as the nation becomes more aware of their threat potential. Soon after a drone with trace amounts of radiation was discovered on the roof of the prime minister’s office in April, the Tokyo Metropolitan Government banned the use of drones in public parks and gardens. On Friday, police arrested a 15-year-old boy who allegedly said in an online video that he would fly a drone at a traditional festival in the capital, according to Japanese media reports. Japan has yet to formulate comprehensive rules on drone usage as lawmakers struggle to catch up with the nascent industry. Last week, Secom rival Alsok announced a drone-detection system that can hear the distinctive hum of the flying machines from up to 150 meters away."
147,https://www.computerworld.com/article/1378257/how-would-alan-turing-fix-ai.html,ComputerWorld,2015,5,22,873.0," We want speaking machines because language is the best way to rapidly communicate ideas. Users want their lives made easier and Hollywood wants their 1960s predictions proven right. But computers don’t work for artificial intelligence (A.I.). Alan Turing, the famous British mathematician, cryptologist and a founding father of today’s computers would have pointed out that they weren’t designed for it. Computers were designed to compress and duplicate information. They don’t handle A.I. well because they leave too much work to programmers. Programmers were replaced by engineers using statistics in waves of hope from the 1970s, but the results remain inaccurate and limited. A.I. hasn’t scaled and after nearly 60 years of effort, a new approach is warranted. Alan Turing’s paper from 1936 describes the computer process that is still in use today. He modeled a human who did computations with a digital computer that emulates her. Human computers at the time supported many things including ballistics, banking and science. Both human and digital computers calculate by blindly following procedures. Interestingly, human computers were usually thought of as women. The first computers were men in the 1700s, but from the late 1800s and especially during the Second World War, computers (and the earliest programmers) were typically women. Can you imagine it? Turing emulated the 1930s human computer that used sheets of paper to process. He did not emulate what they did to sit at the table, drink coffee, see the sheets with their eyes, think about the next step or move their hand to write numbers. This model has held back A.I. ever since, because what a person does and how they do it are radically different things. Human speech, for example, is a totally different problem to adding 1+1 on many levels. Turing defines the computer as storing a finite number of symbols — like an alphabet and a set of punctuation and numerals. You cannot store anything else. This is the compression of information. And if you want to store the same word twice, you duplicate it by storing the same symbols twice in the same sequence. Compression and duplication are a design feature of computers and they are different to brains. This design feature is amazingly powerful, with the Internet, iPhones, Windows and the general explosion in technology the beneficiary, but limiting for A.I. Imagine how many times Wikipedia stores the most common English word: “the”? That is duplication on a massive scale. With A.I., the duplication creates the need for a search facility. Today, a search facility means we often guess answers because a single word can have many meanings. Worse, different words can mean the same things, such as when we store more than one language or dialect. Many in the field consider it heresy to suggest that computation is not at the core of the problems in the cognitive sciences, but that’s the problem. Lack of progress means we need to consider alternatives. Alan Turing’s tragic death at a young age in 1954 may have cost us getting intelligent machines much earlier because we lost his visionary guidance when we needed it most. Two years later, A.I. was founded at the Dartmouth Conference. Its computer-science supporters were familiar with the details underlying computers at the time. The trouble is, those computers were based on human computers. A.I. should be modeled on human brains, machines that can handle human languages, not on what human computers did. Science uses models to predict results. Bad models don’t accurately predict results, but good ones do. The targets that have been set in the world of A.I. aren’t aligned with the expectations of customers. Siri, Dragon, Google, Bing Translate and others illustrate the gap: users want improvements. Despite an obvious lack of progress, engineers continue to try to leverage systems based on the old models. Imagine if normal computers were like this. If your bank account were to change from time to time due to errors, you’d be angry, while today’s free Internet translation companies hand over totally wrong translations regularly. Turing would have told the A.I. community that computers, at their core, are not the right start for intelligent machines. John Ball is a scientist and engineer specializing in computer technology with degrees in science, cognitive science and business in Australia. As a cognitive scientist, John is particularly interested in applying new and emerging brain-based technologies on computers. John is presently focused on a machine's use of language using Patom Theory, a hierarchical, bidirectional model in which a brain only stores, matches and uses patterns. He invented and developed Patom Theory, leading to patents in 2007. The prototype software provides strong evidence that problems with human languages can be solved without processing. John founded Thinking Solutions in 1996 to exploit brain-based technologies and incorporated the company in 2006. Prior to that, he worked for IBM, CSC, Fujitsu and Telstra in a variety of technical and management roles. The key applications of language understanding are: (a) conversation with computers, (b) internet search, (c) people learning a language, (d) language understanding by machine and (e) automatic translation. The opinions expressed in this blog are those of John Ball and do not necessarily represent those of IDG Communications, Inc., its parent, subsidiary or affiliated companies."
148,https://www.computerworld.com/article/1378149/google-to-unleash-self-driving-car-on-public-streets-this-summer-itbwgk.html,ComputerWorld,2015,5,15,271.0," Google just announced its self-driving car will soon be trundling the mean streets of Silly Valley. Recent concerns about the car’s safety record prompted a quick response from Google: All crashes were caused by human error. So it sounds as if the good residents of Mountain View shouldn’t be too worried while driving on local thoroughfares this summer. It’s too bad Google can’t say the same about Chrome. In IT Blogwatch, bloggers signal before passing. Today’s humble blogwatcher is . Someone’s sensitive. Mark Bergen shrugs and moves on: Smug and oblivious? Matt O’Brien describes Silicon Valley a self-driving car: Straight from the mouth of a jockey-less horse: Sam Byford installs software — on a golf cart: Finally, Dee-Ann Durbin talks about the pod people: You have been reading IT Blogwatch by Richi Jennings and , who curate the best bloggy bits, finest forums, and weirdest websites… so you don’t have to. Catch the key commentary from around the Web every morning. Hatemail may be directed to @itblogwatch or itbw@richi.uk. Opinions expressed may not represent those of Computerworld. Ask your doctor before reading. Your mileage may vary. E&OE. Before writing I was a programmer and software developer and have lived and worked in the United States and Europe. I founded Glasskeys.com -- a website about tablets and mobile phones. Now I write part time for Computerworld's IT Blogwatch, and have written pieces for Forbes NetAppVoice. Bragging rights: I use tablets running iOS, Android, and Windows. The opinions expressed in this blog are those of Stephen Glasskeys and do not necessarily represent those of IDG Communications, Inc., its parent, subsidiary or affiliated companies."
149,https://www.computerworld.com/article/1377834/stephen-hawking-fears-robots-could-take-over-in-100-years.html,ComputerWorld,2015,5,14,384.0," Worried that one day we’ll have robot overlords? You’re in good company. Renowned physicist, cosmologist and author of A Brief History of Time, Stephen Hawking said this week that robots, powered by artificial intelligence (A.I.), could overtake humans in the next 100 years. Speaking at the Zeitgeist conference  in London, Hawking said: “Computers will overtake humans with AI at some within the next 100 years. When that happens, we need to make sure the computers have goals aligned with ours,” according to a report in Geek. This isn’t the first time Hawking has spoken about the threat that comes along with machine learning, A.I. and robotics. In December, Hawking said, “the development of full artificial intelligence could spell the end of the human race.” In an interview with the BBC Hawking said A.I. poses no threat to the human race today but could in the future as machines — specifically robots — become smarter, bigger and stronger than their human developers. “It would take off on its own, and re-design itself at an ever-increasing rate,” Hawking said at the time. “Humans, who are limited by slow biological evolution, couldn’t compete, and would be superseded.” Hawking also has some well-known company talking about the dangers of A.I. Last fall, Elon Musk, the CEO of electric car maker Tesla Motors and CEO and co-founder of SpaceX, said while speaking at MIT that A.I. and all of the research going into it, poses a definite threat to humanity. “I think we should be very careful about artificial intelligence,” Musk said, answering a question about the state of A.I. during the MIT event. “If I were to guess at what our biggest existential threat is, it’s probably that…. With artificial intelligence, we are summoning the demon.” Not all tech people and scientists are as concerned about A.I. as Hawking and Musk seem to be, though. A lot of people tend to think of A.I. as the brains behind robotics. But it also powers smartphones, email spam filters and apps that make restaurant recommendations. A.I. is a long way away from creating a robot that easily can learn and is self-aware enough to cast its human operators aside and take over the world. And talk about these fears runs the risk of slowing down AI research, some have worried."
150,https://www.computerworld.com/article/1373585/googles-self-driving-car-needs-crash-course-in-pr-itbwgk.html,ComputerWorld,2015,5,12,277.0," Google takes the wheel, driving away from bad press. News that its auto-automaton has been involved in few prangs has caused more damage than dented bumpers and chipped paint. A PR touch-up is required — and just what the mechanic ordered — to fine tune public opinion and restore a flawless finish. In IT Blogwatch, bloggers place both hands on the wheel. Today’s humble blogwatcher is . Zach Miners digs with a steady hand: But Justin Pritchard is on a collision course with a robot: To Google’s Chris Urmson, the gas tank appears half-empty: Mark Harris reports all accidents, big and small: Miles to go before Eugene Kim sleeps (in a self-driving car): Chris Matyszczyk musters a regiment of used, slightly-damaged cars: Meanwhile, Damon Lavrinc searches for transparent car keys: You have been reading IT Blogwatch by Richi Jennings and , who curate the best bloggy bits, finest forums, and weirdest websitesÖ so you don’t have to. Catch the key commentary from around the Web every morning. Hatemail may be directed to @itblogwatch or itbw@richi.uk. Opinions expressed may not represent those of Computerworld. Ask your doctor before reading. Your mileage may vary. E&OE. Before writing I was a programmer and software developer and have lived and worked in the United States and Europe. I founded Glasskeys.com -- a website about tablets and mobile phones. Now I write part time for Computerworld's IT Blogwatch, and have written pieces for Forbes NetAppVoice. Bragging rights: I use tablets running iOS, Android, and Windows. The opinions expressed in this blog are those of Stephen Glasskeys and do not necessarily represent those of IDG Communications, Inc., its parent, subsidiary or affiliated companies."
151,https://www.computerworld.com/article/1375789/sen-warner-seeks-dot-research-to-combat-rogue-drones.html,ComputerWorld,2015,5,11,334.0," The U.S. Department of Transportation should begin to research technologies to combat “rogue” drone flights around sensitive areas like the White House and airports, a senator has recommended. The U.S. government, working with private companies, needs to develop counter-drone technologies to defend sensitive airspaces, Sen. Mark Warner, a Virginia Democrat, said Monday in a letter to Secretary of Transportation Anthony Foxx. Warner, a member of the Senate Intelligence Committee, raised concerns about drones being used in crimes or potential attacks. A camera-equipped drone carrying a bottle marked with the radioactive symbol was discovered on the roof of the Japanese prime minister’s office in central Tokyo in April, he noted in a press release. The government shouldn’t “overly restrict appropriate” uses of drones, also called unmanned aircraft systems, or UASes, Warner wrote. But after high-profile drone mishaps in recent months, including a drone landing on the White House lawn in January, “we all have a responsibility to do what we can to ensure that this is done in a safe way,” Warner wrote. “While the vast majority of UAS operate safely, a series of high-profile incidents over the last year have shown that it is also necessary to develop rapidly technologies that can ensure the safe operation of drones around sensitive areas.” Warner’s letter didn’t advocate for a specific counter-drone technology, but he called for the Department of Transportation to consider a drone mitigation pilot project at a U.S. airport. The Mid-Atlantic Aviation Partnership at Virginia Tech, a Federal Aviation Administration drone test site, could help develop counter-drone technology standards, he wrote. A pilot project “could provide a blueprint for U.S. airports to establish protocols to protect airports against both innocuous recreational UAS mishaps as well as more nefarious incursions,” Warner added. While Warner didn’t point to a specific counter-drone technology, he suggested an ideal mitigation technique would not interfere with existing operations at airports, would be able to geo-locate both the drone and the ground controller and would not affect GPS or Wi-Fi signals."
152,https://www.computerworld.com/article/1374593/tethered-drones-only-look-crazy.html,ComputerWorld,2015,5,8,816.0," Want to know how to teach robots to pilot themselves without help through the physical world without destroying themselves or anything else in the process? You could launch a massive, decade-long effort to develop the artificial intelligence required to avoid obstacles and recognize the difference between between-the-lines and upside-down-in-a-ditch. You could also build the world’s largest swarm of miniature robotic submarines, teach them to boat slowly in formation and try to keep from bumping into each other, then send them out to explore underwater like a litter of Disneyfied underwater Roombas put-putting slowly off into adventure. Or you could put a few massively overpowered quadcopters in a padded room and send them screaming around an 11-foot circle at, put them in a padded room and send them into what looks like the most insanely suicidal game of tetherball ever played by quadcopters screaming around an 11-foot circle at 30 miles per hour while tied to a post. If you did, you might post the video, as evidence of what you and your engineering buddies would get up to in the robotics lab at school after a few beers on a Friday night. You probably wouldn’t submit it as a piece of serious and legitimate research into multi-unit cooperation, special awareness and obstacle avoidance by unmanned aerial vehicles to the respectable IEEE/RSJ International conference on Intelligent Robots and Systems, which will start Sept. 38 in Hamburg. That is exactly what a team of robotics researchers at engineering powerhouse of Swiss universities – ETH Zurich – did with its video, which they could only video using high-speed motion-capture cameras set up to catch the motion of insanely overpowered flying bots in the ETH Zurich Flying Machine Arena. The Arena was built with the help of a student who took a five-year sababatical to found and become rich from warehouse-automation-robotics company Kiva Systems. The two-minute video shows a test of the learning algorithms built into the quadrocopters, which are trying to learn to fly in formation under challenging conditions, according to the explanation on a YouTube video posted by team member Maximilian Shulz. The drones are tethered to the pole because they spin so fast around it that they generate up to 13 g’s of centripetal force, which should be enough to smash them to bits even on padded walls. Once they learn a bit more, Shulz wrote, they should be able to coordinate so well with each other that they keep so steady a pull on the line connecting them that they can zip in circles at the same speed and in the same spot even if they were no longer tied to the post. The degree of sensitivity and speed of adjustment needed to fly that suicide circle in place without a tether really would be more impressive than pokey little submarine-bots. It would mean serious control and a respectable level of autonomy. (It would also mean, it turns out, that they could dance; interactively with each other; to music.) It would also mean the drones the FAA may soon approve for general commercial use rather than just in special cases, could be doing a lot more than just package delivery or casual surveillance or rural crop surveys or other services performed far away from concentrations of people. Patent documents from Amazon show it plans to have drones that have real routes and are able to recognize and respond to other drones, the car or house of the person to whom they’re delivering and will likely come in a variety of sizes, payload capacities and sensors. Some won’t be allowed to descend to head height if people are around. Others might be able to navigate inside public buildings, with some restrictions. The FAA is even considering allowing some drones to operate far enough away from their operators that the operators can no longer see them – a major no-no until now for a fliers whose onboard cameras give only a limited view of what is around them. A little of the Flying Machine Arena magic would go a long way with the FAA, if it were possible to show that drones could consistently recognize and avoid crashing into anything on their own, but especially humans. They might also be doing delivery duty inside the office buildings where we work, delivering lunch from drone-powered takeout places, running little errands for us, flying over to see who’s at the door, or whether the commuter train is getting close. Within just a few months they could begin filling the air, in numbers that will eventually become high enough that, trouble avoidance or no, the wearable tech accessory of the future may be a helmet designed not to augment our vision of the real world or paint over it with a virtual one. It may be a nice solid helmet designed to protect us from all the high-speed, autonomously piloted convenience."
153,https://www.computerworld.com/article/1372849/its-now-or-never-to-speed-up-robots-for-darpa-challenge.html,ComputerWorld,2015,5,7,943.0," WORCESTER, Mass. — The robotics team at Worcester Polytechnic Institute has three weeks before the finals of the DARPA Robotics Challenge to make their robot twice as fast as it is today. “We need two hours now to get through the course,” said Matt DeDonato, the WPI team’s technical project manager. “If we had two hours, we’d be golden, but we’re only going to have one. So we need to speed it up.” The WPI team is one of 25 teams qualified to compete in the robotics challenge finals on June 5 and 6 in Pomona, Calif. The challenge, which launched in 2012, is intended to encourage roboticists to build robots that can one day provide support in a disaster. In the last challenge, held in December 2013, each team’s robot was required to perform eight tasks, including climbing a ladder, driving a car , opening doors and using a drill. The robot had 30 minutes to perform each task. This year, the robots will face a course that simulates a disaster situation and will have to take on each task one after another. The robot must complete all the tasks in one hour. DeDonato said WPI’s humanoid robot is capable of completing each of the different tasks. The issue is speed. The WPI team had planned to use the cloud to speed up their robot during the final competition, but the team was forced to change plans. Michael Gennert, director of robotics engineering at WPI, said a team of students is working on putting software commands in the cloud so the WPI robot – named Warner – could access those instructions anytime and get directions even if the Wi-Fi connection with the robot’s operators was down. That would have meant that even if the robot was working without its controllers, it still could access information and pre-set directions. However, the WPI team was preparing to use Amazon’s cloud platform and only recently found out that DARPA will only give competitors access to Microsoft’s Azure platform. The work they had done for the Amazon platform is not transferrable to Microsoft’s cloud. “It’s a little disappointing, but we have to show that the cloud can improve robotic performance,” Gennert said. “In the bigger picture, we’ll certainly see more use of the cloud and that’s one of the things that will help us double robotic performance in 18 to 24 months.” With the cloud no longer an option, the WPI team is tweaking its algorithms to try to speed up Warner, but the group is also working to make the robot’s human operators work faster and more efficiently. In the 2013 competition, the operators gave the robot commands for most of what it did — how far to turn its wrist, how far to extend its arm or how many steps to take in which direction. Today’s robot is much more autonomous. Operators will tell the robot to open a door but no longer need to tell the machine how to position its body, such as at the shoulder and wrist. The robot can make those calculations on its own and more quickly than an operator could instruct it. That makes the robot operate much faster now than it did a year and a half ago. However, it’s still not fast enough, so DeDonato will be working with the robot’s operators to make improvements. There’s a separate human operator for each task that the robot needs to perform. At the end of each task, the operator needs to make sure the robot’s body or arms are in the correct position so it’s ready to quickly move on to the next task. “It’s not that complicated, but you have to worry about everyone who’s driving,” said DeDonato. “When does one task finish and another start? Where is that tradeoff made? Is the driver leaving the robot in the right state to start the next task?” The WPI roboticists also discovered that during the task in which the robot moves through debris, such as broken boards and two-by-fours, it’s easier and quicker to have the robot shuffle through the debris, pushing it aside with its feet, instead of stopping to bend over, pick up a piece of debris and move it. Some tasks are easier than others. Warner, for instance, can turn a valve, open a door and walk through it and climb stairs fairly easily and quickly. The drill task is different. It is the hardest and most time-consuming job for WPI’s robot. The task doesn’t involve one step. It includes finding the drill, picking it up, turning it on and then using it. Making it more difficult, the team isn’t sure what kind of drill will be used during the competition. “We can do it. It just does take some time,” said DeDonato. If the team can’t speed up the robot on the drill task, it might have to skip doing it, and lose the point for accomplishing the task. Skipping a task that takes a long time might enable the robot to finish the course and the rest of the tasks, giving the team a chance at more total points. While the team is trying to build up the robot’s speed at basic tasks, Gennert said the group is moving much faster than it did a year ago. One day soon, with more research, robots will move far more easily and quickly. “Ten years ago, it would have taken hours for the robot to do these things, if at all,” added Gennert. “In 10 or 12 years, they could be as fast as humans at these tasks. What can be accomplished has increased rapidly.”"
154,https://www.computerworld.com/article/1369270/wpis-team-gears-up-for-final-battle-of-the-bots.html,ComputerWorld,2015,5,5,949.0," WORCESTER, Mass. — In about a month, the robotics team at Worcester Polytechnic Institute will have to put their robot up against 24 other teams from around the world in the DARPA Robotics Challenge finals. After working for several years on this project, the team is down to its last three weeks to make their humanoid robot as autonomous, fast and reliable as possible. That means they’ll be working around the clock until Warner, their Boston Dynamics-built robot, is put in a crate and shipped to Pomona, Calif., for the last challenge in this global competition. “We are not ready today,” Michael Gennert, director of robotics engineering at WPI, told Computerworld. “A month ago, we weren’t even close to doing what we can do today. This team is doing a little bit more and doing it a little bit faster every day. We will be ready.” DARPA, the Defense Advanced Research Projects Agency, is hosting the finals for the Robotics Challenge on June 5-6 in Pomona, Calif. Twenty-five teams — including WPI, MIT, Carnegie Mellon University and NASA’s Jet Propulsion Lab — from around the world will compete for $3.5 million in prizes. The challenge, which launched in 2012, is designed to get roboticists working on semi-autonomous robots that can one day be used in the event of natural and man-made disasters. In the last challenge in the run-up to the finals, the robots were required to perform eight tasks, one at a time. The robots had 30 minutes to do each task. That, however, was a year and a half ago, much more is expected of the robots this time. In this year’s finals, each team will be tasked with having its robot work through a mockup of a disaster, stringing all the tasks together in one overall situation. The teams also will have one hour to complete all of the tasks, including an added surprise job. To get through the course, the robots are required to drive a car, climb stairs, use a drill and turn a valve. To complete these tasks, the robots will need to be more agile, better balanced and faster. Many of the teams in the finals are using Boston Dynamics‘ Atlas robot. Those teams sent their robots back to the maker this winter for upgrades and now are working with a version, which is about 75% new. Matt DeDonato, the WPI team’s technical project manager, said the redesigned robot is a lot different from the knees up. The previous 6-foot, 2-in. tall, 330-pound robot was transformed into a 7-foot tall, 400-pound machine. The new Atlas version also has more joints for better dexterity, onboard power, a new adjustable hydraulic pump and three new onboard computers. The new robot was rid of its communications cable, going wireless. All of these changes mean that DeDonato and his team have rewritten a lot of code for Warner, re-architecting its software to handle the new computers and communications. They’re up to about a million lines of code to run their humanoid robot. The WPI team already has published several papers on how they’re handling the robot’s balancing and vision, and they plan to publish several more. “In robotics, software is where most of the advancements need to be,” DeDonato said. “To get the robot to actually behave like a human or like we want them to behave, it’s heavy on the software.” Now the WPI team is figuring out what their robot can do and what it is still struggling to do. For instance, late last year the WPI team was to planning to make the robot capable of standing up on its own, in case it loses its balance and falls during the final competition. However, DeDonato said they’re not going to spend any more time on that issue since DARPA has said each team will be allowed one reset with a 10-minute penalty. If WPI’s robot falls, the team would likely be able to get the robot up and running again faster than the robot could right itself. DeDonato also noted, though, that if Warner falls, it’s likely to happen on uneven terrain or in the debris course. That would likely mean that the robot would be badly damaged in the fall and might not be able to go any farther anyway. At this point, the WPI team’s robot can complete all the tasks but complete them in the one-hour time alloted. The team wants to get the robot to move through the tasks more quickly but may decide to have the robot skip a task if necessary. Warner can handle the drilling task but since it involves several steps – find the drill, pick it up, turn it on and then use it – it takes up a lot of time. If necessary, DeDonato may decide to skip that task and give up the points it would gain. The team, however, needs all the points it can get, and the fastest time they can manage, to win the competition. “I’m hoping not to skip anything but it’s always in the back of my mind,” said DeDonato. “All the tasks are worth the same amount of points, but not all have the same difficulty. We need to focus on getting as many tasks done in an hour as we can. At this point, our goal is to do them all, but it may make sense to skip some.” The work of all the teams has pushed robotics forward dramatically, DeDonato said. “Humanoid robotics has advanced 10 years or so in the past two,” he added. “We’re happy whether we win or lose. The knowledge we’ve gained and contributed to is why we did this.”"
155,https://www.computerworld.com/article/1367485/fetch-warehouse-robots-can-work-in-pairs.html,ComputerWorld,2015,4,29,380.0," A California robotics startup is deploying a dual robot system to speed the delivery of goods purchased online. San Jose-based Fetch Robotics on Wednesday unveiled Fetch, a mobile robot arm, and Freight, a mobile base that can carry a bin. They can work in warehouses and handle most kinds of merchandise. The machines can work alongside human staff in filling repetitive pick-and-pack orders. They can also function together, with the Fetch robot grabbing inventory from shelves with its arm and packing it into the mobile bin before shipping. Fetch and Freight run on the open-source Robot Operating System (ROS) and Intel CPUs. Both can autonomously recharge their 24V batteries at their charging docks. They also have laser scanners that can operate at a distance of 25 meters, and inertial measurement units to help navigate. Equipped with a 3D camera in its head, Fetch can move its body up and down to get closer to merchandise. Its arm can move along seven axes of motion, with each joint measuring the amount of force it must exert to move objects. It has a lifting capacity of about 5.8 kilograms. Freight can carry up to 68kg and can work independently or in conjunction with Fetch. Fetch Robotics sees opportunity in the increasing automation in warehouses, specially in e-commerce services such as Amazon Prime and Google Express that promise quick delivery. Amazon has been expanding its fleet of Kiva mobile robots that carry large stacks of goods in its warehouses, bringing the total number of machines across the U.S. to over 15,000 before the 2014 holiday season. “Because of the pressures of on-demand commerce, there is a critical need for warehouses and fulfillment centers to become more efficient,” Fetch Robotics CEO Melonee Wise said via email. “We see our robots as a means to that end.” The price of the machines has yet to be decided, said Wise. Fetch bears strong resemblance to the US$50,000 UBR-1 education and research robot from Unbounded Robotics, which Wise led beforel its closure last year. Fetch’s new robots are slated to go on display at ICRA 2015, an IEEE Robotics and Automation Society conference in Seattle, Washington, in late May. Tim Hornyak covers Japan and emerging technologies for The IDG News Service. Follow Tim on Twitter at @robotopia."
156,https://www.computerworld.com/article/1360777/drones-in-the-enterprise-the-future-of-data-collection-2.html,ComputerWorld,2015,4,15,1209.0," To hear some tell it, the world will soon be abuzz with small drones that inspect bridges, monitor pipelines, survey crops and help assess damage for insurance claims. Before companies head off into the wild blue yonder, however, several things have to happen. The federal government needs to figure out how to regulate the commercial use of drones. Drone vendors need to figure out their business models. And corporate users need to figure out how drones will fit into their IT operations. Today, the market for unmanned aerial vehicles (UAVs), a.k.a. drones, is dominated by defense applications like the multi-million-dollar Predator. However, ABI Research predicts the commercial market for small UAVs will grow from an estimated $652 million in 2014 to more than $5.1 billion by 2019, becoming twice as large as the military/civil defense market, says Dan Kara, practice director of robotics at the market research company. If thing unfold as ABI forecasts, IT departments need to prepare now for the potential drone invasion and the data they collect. Exactly how they should prepare depends on the final form of drone regulation as well as how drone vendors decide to sell to the enterprise market. Nevertheless, IT needs to be ready to deal with a new type of big data, the type that comes from drones. Vendors from all markets are moving to sell small commercial UAVs. Low-end vendors that have so far sold just to consumers for a few hundred dollars are moving upstream, Kara says. For example, UAV manufacturer DJI has started selling more powerful models designed for professional filmmakers, while Horizon Hobby, known for selling toy drones, recently created Horizon Precision Systems to target commercial users. Meanwhile, defense contractors are moving down market. For example, Lockheed Martin has acquired Procerus Technologies, which develops less-expensive UAVs for civil public safety and first responders. In addition, there are entirely new entrants, including Google, which bought drone-maker Titan and plans to start testing drones later this year, and Amazon Prime Air, which plans to use its drones for package delivery. That’s a lot of activity for a device that’s banned for commercial purposes. Although the U.S. Federal Aviation Administration (FAA) has long allowed recreational UAV use if operators follow certain safety precautions, industry players waited years for the FAA to come out with regulations for commercial use. It finally unveiled a proposal in February, to mixed reactions. Some commercial proponents think the rules are too restrictive. For example, drones would have to fly below 500 feet, during daylight and within sight of the operator. Such conditions are impractical for delivering packages, for example. Still, as the FAA gathers public comment (and industry increases its lobbying), the regulations could change significantly between now and 2017, when they are expected to be finalized. The FAA grants exemptions for commercial use, but as of March had issued just over 50. However, the number of applications for commercial exemptions is rising fast. Kara expects they will become increasingly common as companies start pilot programs and tests for commercial applications ranging from aerial photography and film for Hollywood and ad agencies (which Colin Snow, founder and CEO of the consulting company Drone Analyst, estimates to be the most common use today) to agricultural monitoring, infrastructure inspection and insurance claims adjusting. In fact, the FAA is reportedly considering ways to speed up the exemption process. Meanwhile, there is a lot of unauthorized drone use, says Snow. He estimates there are 2,000 to 3,000 illegal operators representing economic activity of $200 million to $350 million in the United States. As vendors team with commercial enterprises in these pilot projects, they are working out their business models. “I don’t think the industry has come up yet with the ultimate mousetrap to serve the enterprise space,” says Andrew Maximow, who spent 15 years in IT at Cisco Systems before becoming director of client services at drone vendor 3D Robotics (3DR). 3DR is working with several potential commercial customers such as BNSF Railway, which recently received an FAA exemption to use drones to inspect its rail infrastructure and operations. The railway will operate 3DR’s Spektre Industrial Multirotor Aerial Vehicle, among other drones, during the pilot. The Spektre is not yet commercially available, but is a prototype that 3DR is providing to a handful of potential customers. 3DR’s strategy is to develop a drone business based on open software, says Maximow. “We want to be the Android of drones,” he says, by making the drones customizable through various hardware add-ons and software applications. Agriculture is already a big market for UAVs, says ABI’s Kara. One reason: Farmlands are privately owned and certain FAA regulations don’t apply. For example, PrecisionHawk, a Canadian UAV company founded in 2010, has teamed up with Agri-Trend, an agricultural consulting company based in Alberta. The two companies plan to integrate their software platforms this year and investigate ways to best collect and analyze agricultural data to help farmers get a better return on their crop investment, says Warren Bills, vice president of geo solutions for Agri-Trend. They hope to offer services to farmers in the U.S. and Canada by 2016, according to a PrecisionHawk press release. However, the “drone as a service” is probably the most likely way for enterprises to use UAVs, says Kara. For one thing, if the UAV vendor flies the drone, it assumes responsibility for meeting any regulations. And storing the data in the vendor’s servers removes some of the impact on the customer’s infrastructure. “Any organization considering using data capture from aerial sensors or drones has to determine the impact on its IT infrastructure, and the kind of support that will be required,” Kara says. Not only could storage be a challenge, but different ways of analyzing and presenting the data may be required. Thus, “my guess is that a lot of companies will just opt out and have it done as a service.” For instance, PrecisionHawk has a cloud-based platform for storage and analysis of the data collected by its UAVs. If desired, it will also send the data directly to the customer’s IT department. However, “the sheer volume of the information, and the requirements of having photogrammetry and GIS staff available, can make that a logistical nightmare,” says co-founder and president, Dr. Ernest Earon. For that reason, most of its customers prefer to use PrecisionHawk’s cloud. Although drones are the sexy new thing today, perhaps the most important thing to remember is that they are only another way to collect data that the business can use to make better decisions. Agri-Trend’s Bills says he’s seen a lot of companies get mesmerized by the latest snazzy drone hardware or the cool aerial images they can collect. But what he wants, he says, is useful, actionable information. “The last thing I need as an agriculture consultant is more static images of a grower’s field,” says Bills. “I can get images in lots of ways. What I need is a system that enables data-driven decisions that farmers can trust.” If your company is considering the possibility of using UAVs to collect data, you should start thinking through the possibilities now in order to be ready when final regulations are approved. Here are a few IT considerations suggested by analysts and vendors:"
157,https://www.computerworld.com/article/1359809/microsoft-demos-industrial-robot-linked-to-cloud-mobile-devices.html,ComputerWorld,2015,4,13,315.0," Microsoft presented a vision of how industrial robots could work more closely with people by harnessing the Internet of Things, cloud networking and 3D sensing technologies, linked through Windows platforms. In a demonstration at Hanover Messe, an industrial fair in Germany this week, Microsoft and industrial robot maker Kuka Robotics showed an industrial robot arm that can stream movement data to Microsoft’s Azure cloud computing platform for human staff overseeing production. The Kuka machine is a lightweight, multi-jointed arm known as the Intelligent Industrial Work Assistant. Its immediate task in the demo is to thread a small tube into the back of a dishwasher. The delicate nature of the operation requires human collaboration and risks damaging the appliance, Microsoft said in a release. In the scenario presented by the company, if the robot encounters a problem, it can notify nearby technicians via Microsoft Band wearables or Windows tablets, which can also be used to assess supply chain problems affecting the robot. The robot assistant can be linked to a Kinect 3D motion sensor to identify technicians who arrive for troubleshooting work. A video about the collaboration shows a technician using a head-mounted display to run through a troubleshooting app while checking the robot. The demonstration aims to highlight how Kuka’s robot assistants can jointly perform tasks with humans, without needing a human controller, Microsoft said on its blog. The demonstration is the latest in industry-wide efforts to make industrial robots work with human colleagues more easily. For instance, Rethink Robotics’ Baxter, introduced in 2012, and Sawyer, launched last month, are designed to be collaborative robots that are safe enough for people to work alongside, instead of being isolated in cages. Last year, Microsoft shut down its robotics research group amid a larger shakeup by CEO Satya Nadella. Tim Hornyak covers Japan and emerging technologies for The IDG News Service. Follow Tim on Twitter at @robotopia."
158,https://www.computerworld.com/article/1647870/what-is-artificial-intelligence.html,ComputerWorld,2015,4,10,1152.0," What is artificial intelligence (AI), and what is the difference between general AI and narrow AI? There seems to be a lot of disagreement and confusion around artificial intelligence right now. We’re seeing ongoing discussion around evaluating AI systems with the Turing Test, warnings that hyper-intelligent machines are going to slaughter us and equally frightening, if less dire, warnings that AI and robots are going to take all of our jobs. In parallel we have also seen the emergence of systems such as IBM Watson, Google’s Deep Learning, and conversational assistants such as Apple’s Siri, Google Now and Microsoft’s Cortana. Mixed into all this has been crosstalk about whether building truly intelligent systems is even possible. A lot of noise. To get to the signal we need to understand the answer to a simple question:  What is AI? The starting point is easy.  Simply put, artificial intelligence is a sub-field of computer science. Its goal is to enable the development of computers that are able to do things normally done by people — in particular, things associated with people acting intelligently. Stanford researcher John McCarthy coined the term in 1956 during what is now called The Dartmouth Conference, where the core mission of the AI field was defined. If we start with this definition, any program can be considered AI if it does something that we would normally think of as intelligent in humans.  How the program does it is not the issue, just that is able to do it at all. That is, it is AI if it is smart, but it doesn’t have to be smart like us. It turns out that people have very different goals with regard to building AI systems, and they tend to fall into three camps, based on how close the machines they are building line up with how people work. For some, the goal is to build systems that think exactly the same way that people do. Others just want to get the job done and don’t care if the computation has anything to do with human thought. And some are in-between, using human reasoning as a model that can inform and inspire but not as the final target for imitation. The work aimed at genuinely simulating human reasoning tends to be called “strong AI,” in that any result can be used to not only build systems that think but also to explain how humans think as well. However, we have yet to see a real model of strong AI or systems that are actual simulations of human cognition, as this is a very difficult problem to solve. When that time comes, the researchers involved will certainly pop some champagne, toast the future and call it a day. The work in the second camp, aimed at just getting systems to work, is usually called “weak AI” in that while we might be able to build systems that can behave like humans, the results will tell us nothing about how humans think. One of the prime examples of this is IBM’s Deep Blue, a system that was a master chess player, but certainly did not play in the same way that humans do. Somewhere in the middle of strong and weak AI is a third camp (the “in-between”): systems that are informed or inspired by human reasoning. This tends to be where most of the more powerful work is happening today. These systems use human reasoning as a guide, but they are not driven by the goal to perfectly model it. A good example of this is IBM Watson. Watson builds up evidence for the answers it finds by looking at thousands of pieces of text that give it a level of confidence in its conclusion. It combines the ability to recognize patterns in text with the very different ability to weigh the evidence that matching those patterns provides. Its development was guided by the observation that people are able to come to conclusions without having hard and fast rules and can, instead, build up collections of evidence. Just like people, Watson is able to notice patterns in text that provide a little bit of evidence and then add all that evidence up to get to an answer. Likewise, Google’s work in Deep Learning has a similar feel in that it is inspired by the actual structure of the brain. Informed by the behavior of neurons, Deep Learning systems function by learning layers of representations for tasks such as image and speech recognition. Not exactly like the brain, but inspired by it. The important takeaway here is that in order for a system to be considered AI, it doesn’t have to work in the same way we do. It just needs to be smart. There is another distinction to be made here — the difference between AI systems designed for specific tasks (often called “narrow AI”) and those few systems that are designed for the ability to reason in general (referred to as “general AI”). People sometimes get confused by this distinction, and consequently, mistakenly interpret specific results in a specific area as somehow scoping across all of intelligent behavior. Systems that can recommend things to you based on your past behavior will be different from systems that can learn to recognize images from examples, which will also be different from systems that can make decisions based on the syntheses of evidence. They may all be examples of narrow AI in practice, but may not be generalizable to address all of the issues that an intelligent machine will have to deal with on its own. For example, I may not want the system that is brilliant at figuring out where the nearest gas station is to also perform my medical diagnostics. The next step is to look at how these ideas play out in the different capabilities we expect to see in intelligent systems and how they interact in the emerging AI ecosystem of today. That is, what they do and how can they play together. So stay tuned – there’s more to come. As Chief Scientist and co-founder, Kris Hammond focuses on R&D at Narrative Science. His main priority is to define the future of Advanced NLG, the democratization of data rich information and how language will drive both interactive communications and access to the Internet of Things (IoT). In addition to being Chief Scientist, Kris is a professor of Computer Science at Northwestern University. Prior to Northwestern, Kris founded the University of Chicago’s Artificial Intelligence Laboratory. His research has always been focused on artificial intelligence, machine-generated content and context-driven information systems. Kris previously sat on a United Nations policy committee run by the United Nations Institute for Disarmament Research (UNIDIR). Kris received his PhD from Yale. The opinions expressed in this blog are those of Kris Hammond and do not necessarily represent those of IDG Communications, Inc., its parent, subsidiary or affiliated companies."
159,https://www.computerworld.com/article/1359643/transforming-robot-probes-fukushima-reactor-vessel-2.html,ComputerWorld,2015,4,10,483.0," Tokyo Electric Power on Friday sent a robot where no machine has gone before — inside the highly radioactive heart of a reactor at the crippled Fukushima Dai-Ichi nuclear power plant. The robot, developed by Hitachi-GE Nuclear Energy and the International Research Institute for Nuclear Decommissioning (IRID), was inserted into the primary containment vessel (PCV) of reactor No. 1 at the plant, which was heavily damaged by the 2011 earthquake and subsequent tsunami that devastated northern Japan. Tokyo Electric is taking the unprecedented step to better determine the state of melted-down fuel in the reactor as part of plans to dismantle the plant, a spokesman said. The No. 2 and No. 3 reactors also suffered meltdowns. The PCV is a 48-meter-tall steel container that houses another steel vessel that normally holds the uranium fuel that powers the station, as well as water. The exact state of the fuel is unclear, but determining it is key to removing the fuel for dismantling of the plant, a process expected to take decades. The cylindrical robot is about 60 centimeters long and can change its shape from a form resembling the letter I to one resembling the numeral 3. The former is for movement through a pipe that runs into the PCV, while the latter is for moving around inside the vessel. It is remote-controlled via a cable tether and runs on two crawler assemblies. The machine is equipped with a thermometer, a tilting camera to capture video, a dosimeter to gauge radiation and a laser scanner to measure distance. The robot will stop and probe various spots in the vessel and record obstacles barring its way. It will explore the vessel in two stages — first the ground-level grating and then the basement, where the melted fuel is thought to be. “The radiation level is very high inside the PCV and we assume that the maximum time for investigation is five to six hours each time, though the robot can investigate for 10 hours,” Tomohisa Ito, a spokesman for IRID, said via email. Tokyo Electric has deployed a number of robots around the PCV so far, but never inside it. For instance, Rosemary, developed by Chiba Institute of Technology and Hitachi-GE Nuclear Energy, is a robot that rolls around on treads and images the surrounding area with cameras mounted on a mast and was used to explore the surrounding area. Last month, the utility said it confirmed that the fuel in the No. 1 reactor had melted, complicating the extraction process. The confirmation was done via a tomography imaging scan that used elementary particles called muons. Tens of thousands of people are still displaced due to radiation in the area around the Fukushima plant, which is expected to cost at least ¥2.1 trillion (US$17.4 billion) to decommission. Tim Hornyak covers Japan and emerging technologies for The IDG News Service. Follow Tim on Twitter at @robotopia."
160,https://www.computerworld.com/article/1648799/robot-aquarium-fish-has-one-job-scare-zebrafish.html,ComputerWorld,2015,4,7,781.0," Researchers at NYU Polytechnic have achieved a major breakthrough in underwater robotics: They have proven that a robotic replica of something that will eat you can be as terrifying as the real thing. Fortunately, for the streets of New York abused so heavily in Godzilla and all those Night at the Museum movies, the ‘you’ they were trying to frighten with a giant predator wasn’t a human. It was a zebrafish – one of dozens of species of fish raised in huge numbers for the domestic aquarium market that are, nevertheless, so similar to one another in behavior and appearance that it’s not worth the time it would take for most people to identify one. During the past decade, however, zebrafish have become important enough that fully qualified, properly funded, real-life scientists can get away with spending a ridiculous amount of time and effort trying to frighten them. ng – of human DNA code a decade ago, someone discovered that 70 percent of the 20,000 genes in a human have a counterpart in zebrafish. Of the genes that cause diseases in humans, 84 percent also exist in some form in the zebrafish. Zebrafish are also plentiful, cheap and grow up fast enough to keep scientists working rather than wondering how they ended up probing the mysteries of the universe by doing cruel things to zebrafish. The fish have made some real contributions, too. On Tuesday, Scottish scientists studying multiple sclerosis announced that they had made an important connection showing higher levels of activity in the brain slowed the deterioration of myelin around nerve fibers called axons in people with MS. They did the test by dripping one of two poisons into tanks of zebrafish and confirming later that the fish whose brain activity had dropped 40 percent following also made less myelin to coat their own nerve cells. That means it might someday be possible to find a way to keep victims of MS and other degenerative diseases from being poisoned by Scottish scientists while swimming. Researchers at the Translational Genomics Research Institute in Phoenix, Ariz. announced plans to implant tumors into the Zebrafish so they can study the result of treatments for pancreatic cancer by watching development of the tumors through the translucent skin of the fish. All the crew at NYU Polytechnic wanted to do was see how easy it was to scare a zebrafish, how it was possible to tell a zebrafish was scared, and get a professor of mechanical and biospace engineering to build a robotic replica of the predatory, Amazonian (originally) red tiger oscar to see if it was more likely to frighten the zebrafish than a real oscar of the same size. (Oscars aren’t at the very top of a zebrafish’s fright list, but they’re among the most aggressive fish widely available as aquarium pets in the U.S. If you’re a minnow, guppy, zebrafish, goldfish, more than one goldfish, goldfish hoping to be eaten by a piranha, rat, frog, mouse, catfish, thumb, crab, snake or anything else in the water with an oscar, you don’t want to be in the water with an oscar.) The point wouldn’t normally be just to scare the fish. The research was designed to establish a baseline of behavior so researchers would know if a zebrafish was mildly stressed or completely freaking out when they’re used to help study stress and mental illness in humans. In human trials, the oscar would be much larger and would be able to fire you. The robot oscar was designed as a CAD model in SolidWorks, then 3D-printed in ABS plastic and spray painted to look like the real thing. A servomotor mounted outside the tank and an Arduino UNO control board to run it made a couple of robot-oscar’s fins wave and let it wiggle a bit. Even without being able to swim around, fake-oscar scared the zebrafish even more than the real thing. (Full paper is available here; it is scheduled for June issue of the journal Zebrafish.) Being able to tell you’ve terrified a fish isn’t enough to justify the experiment or purchase of the servomotor at a local hobby store, however. Extending avoidance-response testing to non-mammal species could verify neurological findings thought to be limited to more complex land animals, the report concluded. It could also demonstrate the validity of much less advanced species for similar experiments. Finally, the ability to put a robot fish in the main tank would give researchers a way to put the fish and its predator together without the risk of one eating the other – an ethical consideration of the rights of the zebrafish that has left such experiments on the blacklist, until now."
161,https://www.computerworld.com/article/1648111/robotics-can-now-give-you-a-leg-up-literally.html,ComputerWorld,2015,4,6,586.0," It sounds easy enough for the average healthy person, but could it be made easier for people who have to do a lot of it – like nurses, soldiers, police officers or postal workers? Researchers at Carnegie Mellon University (CMU) think they can make walking more efficient, enabling a nurse, for instance, to feel less tired and move better throughout a long, hectic day. Steve Collins, a mechanical engineer, roboticist and professor at CMU, said the technology they’re working on could reduce the energy people use to walk by 7%. Although that doesn’t sound like much, Collins noted that it would be like taking off a 10-pound backpack when you’re walking around. “That might not seem like a lot, but it’s meaningful to people,” Collins said. “From a basic science perspective, people are already really well tuned to walking. We’ve been bipeds for over 7 million years. We’re really well trained at it over our lifetimes. Making any improvement at all was really challenging. Many in our field said it could not be done.” Collins and his collaborator, Greg Sawicki, at North Carolina State University, are making walking easier by creating a lightweight, unpowered, wearable exoskeleton that fits over the lower leg, cupping the heel and foot. Much like a knee-high boot, the exoskeleton uses a spring that mimics a human Achilles’ tendon and a clutch that acts like calf muscles, Collins explained. The gain in walking efficiency comes from the fact that the spring and clutch aren’t fueled by human energy, like muscles and tendons. The exoskeleton lessens the load on the user’s calf muscles, while the spring is designed to store and release elastic energy. The clutch releases and engages the spring. “It reduces the tension in your calf muscles so it reduces the energy you expend to maintain that force,” he said. “We can make it customizable with little low-powered sensors and by adjusting the timing of the clutch and the stiffness of the spring. Future versions would measure your walking speed to change the stiffness of the clutch and the timing. It would be the same if you’re going up and down stairs or running.” Researchers from various universities and institutions have been working on building exoskeletons to help soldiers be more agile and strong, or to help the disabled to walk again. Scientists, for instance, have been working on developing a robotic exoskeleton that looks and acts a bit like an Iron Man suit for the U.S. military. The Tactical Assault Light Operator Suit, or TALOS, is being designed to feed soldiers on the battlefield real-time information, while making them stronger, giving them more stamina and even healing their wounds. In February, scientists at the University of Bristol in the United Kingdom announced that they are working on a technology that is not quite an exoskeleton but robotic pants. With built-in artificial muscles, the soft robotic clothing is designed to give the disabled or elderly extra strength and balance. Researchers at CMU and North Carolina State University are now aiming their exoskeleton – named the walking assist clutch – at people without disabilities. However, Collins said they could eventually apply similar techniques to people with disabilities. “Someday soon, we may have simple, lightweight and relatively inexpensive exoskeletons to help us get around — especially if we’ve been slowed down by injury or aging,” he said. “We’re a couple of years away right now. The advancements needed are relatively modest. It’s more about making it more form-fitting and adding a little control.”"
162,https://www.computerworld.com/article/1646273/more-than-2000-comments-offered-on-proposed-drone-regulations.html,ComputerWorld,2015,4,2,427.0," The Federal Aviation Administration has received over 2,000 comments on its proposed rules for commercial drone flights in the U.S. The 2,000th comment was received Wednesday and hundreds more are likely to be filed before the deadline on April 24. Some of the comments assail the FAA for allowing companies to fly drones, saying it will lead to noiser and more dangerous skies. Others ask the agency why it took so long for it to get this far. The FAA proposed a broad set of rules that would allow companies to fly drones as long as they stick to several basic conditions: no higher than 500 feet, no faster than 100 miles per hour, and only during daylight. Drones would have to be flown by a licensed operator — a newly created certification — and kept within visual line-of-sight. Drones would always have to give way to other air traffic and could not fly over people who aren’t involved in their operation. Announcing the proposed rules in February, Transportation Secretary Anthony Foxx called it “an exciting day for aviation,” although the feedback so far suggests that isn’t a universally held view. The FAA has to do more to ensure these gadgets don’t pose noise, health and welfare risks to all citizens, rather than cater to the whims of hobbyists and commercial interests, wrote James Devlin of San Diego. Many of the comments are from members of the Academy of Model Aeronautics, which prepared a template for its members to use. Those comments focus not so much on drones but on the effect the regulations would have on model aircraft enthusiasts. A common theme from other commenters is privacy, although the FAA’s proposal doesn’t address that issue. I believe everyone is entitled to their ‘privacy’ to an extent, but with a micro UAS [unmanned aircraft system], every citizen in the United States might feel there is a camera on them at all times, wrote Keith Imberger. The White House has tasked the National Telecommunications and Information Administration to come up drone privacy rules, but earlier this week the Electronic Privacy Information Center asked an appeals court to force the FAA to formulate its own rules. EPIC argues that congress mandated the FAA in 2012 to come up with a comprehensive plan for integration of drones into the national airspace. “We are trying to hold the FAA responsible for that plan,” said Jeramie Scott [cq], EIPC’s National Security Counsel, on Thursday. The proposed regulations can be found on the FAA website and public comments can be submitted through regulations.gov."
163,https://www.computerworld.com/article/1646790/is-your-robot-a-little-cheeky-google-may-build-it-that-way.html,ComputerWorld,2015,4,2,496.0," Google is working toward the day when robots have individual personalities, according to the company’s U.S. patent application. If Google’s technology works out, robots will have personalities that can be customized to suit the individual people they will be working with. In another step, the robots should be able to tailor their own personalities. “Methods and systems for robot and user interaction are provided to generate a personality for the robot,” the company stated in the patent application, which was first filed in April 2012 and accepted earlier this week. “A robot may access a user device to determine or identify information about a user, and the robot may be configured to tailor a personality for interaction with the user based on the identified information.” Google also notes that the robots would be built to identify different people, using speech and facial recognition, and then configure their personality to suit the human’s preferences. “In some examples, a robot’s personality or personalization can be transferred from one robot to another robot, or information stored on one robot can be shared with another robot over the cloud,” Google added. The technology would be geared to anticipate a user’s moods based on historical data. Is someone a morning person or does she want to be quiet before she has her coffee? Does the user like jokes or is he more serious, especially in the office? Google did not respond to a request for more information. This patent fits in with Google’s recent investment in robotics. Over the past two years, Google has bought up at least eight robotics companies, including Boston Dynamics, a real heavyweight in the robot industry.  The company is well-known for its four-legged BigDog robot, and for its humanoid Atlas robot, which is being used in the DARPA Robotics Challenge finals in June. Google last year invested in California-startup Savioke, which builds service robots for nursing homes and hospitals. Now it also appears that Google is interested in developing technology that would give personalities to those service robots, making it possible for the machines to better interact with people in elder care, child care or hospital settings. “It’s both creepy yet very usable,” said Patrick Moorhead, an analyst with Moor Insights & Strategy. “For elder care or a personal assistant, this could be big to the user, enabling sarcasm, humor and expressions. It all comes into play. I believe that when done right, a robot with a personality will make humans more comfortable with them.” Zeus Kerravala, an analyst with ZK Research, said he’s interested in the fact that Google’s vision includes changeable personalities. “You want your robot to be customizable,” he added. “A more aggressive person might need a more passive robotso they’re compatible… This is moving robots more toward personal usage.” Ezra Gottheil, an analyst with Technology Business Research, agreed, adding, “It’s about how you want your robot assistant to interact with you. Think about it being easier to talk to and deal with."
164,https://www.computerworld.com/article/1641468/google-bringing-vision-to-robots-that-need-touch.html,ComputerWorld,2015,3,30,960.0," Google and health-care giant Johnson & Johnson have teamed to make robots better at helping surgeons make big changes with small incisions and as little damage as possible. Details about what each will contribute to the partnership – or even what the goal is – are purposely vague, but it appears that Google’s contribution will be things it is already good at, not the kind of really new capabilities robot-assisted surgical systems actually need. The deal, announced Friday with Johnson & Johnson (J&J) subsidiary Ethicon had no details, but did say the two would develop a surgical platform that sounded a lot like it would compete with that of Intuitive Surgical, Inc., whose da Vinci surgical-assist systems enjoy a surprisingly dismal dominance of the surgical robotics market the company was instrumental in creating. The goal of most robotic-assist systems is to apply to ever-more-complex procedures the same minimally invasive surgical techniques that let pro athletes get their knees reassembled. Pros get back on the field in half the time it used to take because surgeries are done using a tiny incision, tiny tools and a tiny camera that shows surgeons what they’re doing on big monitors. Operating with tiny tools using cameras that can throw off your perspective can be awkward, however — like playing piano by tapping keys with a fishing pole in each hand. So, when robotic-assist machines like Intuitive’s da Vinci Systems showed up in 2000, offering to take some of the strain off surgeons with power-assisted, remote-controlled, very, very tiny instruments attached to the end of a pencil-thin probe with an HD camera on the end – sales took off. Robots don’t actually do anything on their own. They’re remote-controlled instruments with very fine motor controls that give surgeons more control than they might have had using traditional instruments, but at the cost of not actually being able to feel what they’re doing – just see it on the screen. That lack of tactile feedback has been blamed for complications, including one in which the surgeon had to abandon the robot and slice open the patient’s chest. The surgeon accidentally sliced the patient’s aorta because the instrument at the end of a robotic, remote-controlled probe was too large for the space involved and the surgeon could only see what was going on, not feel identify the feel of the aorta, or gauge resistance well enough to know how hard to cut. A 2013 Johns Hopkins study found that surgeons and hospitals underreported the number of complications and deaths during robot-assisted surgeries. Other studies suggested that a lot of the problems – most of the deaths were due to excessive bleeding – might have been caused by surgeons who had learned to rely on a sense of touch making mistakes without that level of feedback when they were using robots to operate. Both the complications and tendency to overlook the robot’s involvement were problems serisous enough that The American College of Surgeons published a paper questioning the future of robot-assisted surgery. The Food and Drug Administration has scheduled an industry tete-a-tete in July about the safety and effectiveness of robotically assisted surgical devices, a conference intended to set the direction for future investigation and regulation of robot-assisted surgery, focusing on both safety and cost effectiveness. No one is saying how much big a role touch will play. People have been building haptic feedback into robot-controlled instruments since at least 2006, and the pace has picked up since the Johns Hopkins report. Haptic feedback is rare among the more than 1,400 robot-assisted surgical devices installed at U.S. hospitals, however. And it looks as if the Ethicon/Google partnership isn’t going to spend much time on it. Ethicon doesn’t sell robotic surgical systems right now. It but does license technology from Intuitive that cuts, sews, staples and manipulates tissue. J&J’s own focus in robot-assisted surgery is, essentially, in making the systems smaller, cheaper to own and use and easier to manage as well as increasing their functionality, according to a FierceMedicalDevices story quoting the head of the company’s Global Surgery Group. Google, which has been buying up robotics companies left and right, and which has extensive research and development experience in robotics, artificial intelligence, autonomous vehicles and a range of other relevant technologies, appears to be focused primarily on vision – which appears to be a strong point of existing systems – rather than the other senses. Google Life Sciences officials who talked to the Wall Street Journal hinted that they wanted to expand real-time image analysis so surgeons could see the edges of nerves or tumors, add more sensors to tools, consolidate visual information so they’re looking at one screen rather than several to see images from MRIs, cameras and other systems. They didn’t talk about Google’s own patents in haptic feedback, or adapting haptic tech already being built into surgical systems in research labs, or super-fine motor feedback that makes robotic hand controllers feel like real hands, let alone futuristic discoveries like how to use ultrasound technology to create a virtual-reality sense of touch in thin air – technology simple enough that Disney has done a version of it to create the sensation of contact in thin air using cheap speakers stuck in a tube. Focusing on machine vision, image analysis and visual-data management all play to Google’s strengths, but they don’t do much about the weaknesses of the surgical-assisting robots, though. It just seems as if Google would have a bigger impact if it could help Ethicon find ways to let surgeons know whether they’re about to make a precise incision to insert a tiny stent, or slash a half-inch hole in the patient’s aorta, rather than improve the quality of a picture they can already see."
165,https://www.computerworld.com/article/1634299/japan-aims-octopus-robot-at-nuclear-disasters.html,ComputerWorld,2015,3,20,446.0," Researchers in Japan have unveiled an “octopus” robot designed to clear rubble in disaster areas including the Fukushima Daiichi nuclear power plant that was crippled in floods following a tsunami in 2011. Robots thrown into action at the time by Japanese manufacturers, university engineering teams and the U.S. Dept. of Energy, among others, ended up being precious little help. None were able to negotiate the flooded hallways or clear even comparatively light rubble sufficiently to make repairs or provide a good look at conditions inside the dying nuclear facility. Rubble and puddles will not be a problem for the 1,500-lb. “Octopus,” whose eight limbs include independently operable heavy-duty tracks, four grappling arms and the ability to carry extra equipment, including a remote-operable chainsaw and a laser capable of cutting through stone, according to the March 13 announcement by Waseda University’s Future Robotics Organization and Kikuchi Corp. Kikuchi is a heavy manufacturing company with many facilities near the Fukushima plant, and which local officials have credited with contributing to both the cleanup and the area’s economic recovery. Octopus was designed specifically for the broken terrain and hazardous environment of major disaster areas, according to the release, which predicts it will find use in lifesaving and search-and-rescue missions following earthquakes, fires and other disasters in addition to its potential usefulness in responding to another nuclear-plant meltdown. Not only do the four tracks allow it to climb over rubble that could be tilting in four different directions, the robot is designed specifically to be able to use its rear arms to hold itself up while climbing over even steeper rubble using the front two arms and four tracks. Using all four arms, each of which is designed to lift at least 440 lbs., Octopus can lift itself completely over some obstacles or up onto ledges made inaccessible by collapsed stairways or entryways. It’s not the most wieldy ‘bot out there, however. Driving it requires two expert handlers with a joystick in each of their four hands, though later designs should require just one operator, according to the designers. It can also withstand radiation that still poisons the immediate area and makes the Fukushima plant a no-go zone, and has made economic recovery difficult even for communities outside the disaster zone. The disaster itself killed more than 15,000 people, nearly all from other effects of the tsunami, not leaks from the power plant, which caused the evacuation of more than 300,000 people from the area. Octopus was one of several disaster-response robots presented at a conference designed to highlight progress in responding to the disaster – a process the Fukushima Prefectural government predicts will take at least another 10 years."
166,https://www.computerworld.com/article/1629692/anti-robot-protest-takes-aim-at-the-wrong-villains.html,ComputerWorld,2015,3,16,1194.0," The spirit of Frankenstein lives on at the tech-hipster-entertainment conference SXSW. On Saturday, protesters clearly inspired by the danger of an aspirationally peaceful, involuntary “monster” rather than the grave-robbing vivisectionist who produced it, chanted anti-robot slogans and urged passersby to endorse a future reserved for humans rather than their hubristic creation: the robot. “I say robot, you say no-bot!” chanted a mob made up of about two dozen University of Texas students and others concerned about the risk artificial intelligence might pose to humanity. “This is about morality in computing,” protest leader, 23-year-old computer engineer Adam Mason told USA Today reporter Jon Swartz at the Austin protest at which group members waved signs reading “Stop the Robots” and “Humans are the future.” The group – which was well organized enough to sport matching t-shirts with a wicked “Stop the Robots” logo that will be THE thing to wear at every U.S-based tech-company summer picnic this year – drew little more than bemused interest as they marched outside the convention center Saturday, according to Swartz. They’re far from alone in flagging artificial intelligence and robotics as a danger – not even among those who should know better. Bill Gates and Stephen Hawking have both warned about the risk of a self-aware artificial intelligence taking over human-controlled networks and support systems to ensure its own survival by destroying the species that produces the materials from which it is built and provides the power on which it survives. A phalanx of prominent scientists signed an open letter in January  demanding responsible policies in AI research and laying out directions AI research could be taken without any immediate danger of snuffing out life on the planet. The letter and associated eminences were arranged by the Future of Life Institute, in a publicity effort partly funded by Paypal billionaire Elon Musk, who founded SpaceX, Tesla Motors and proposed the underground vacuum-tube tram-line called Hyperloop. The problem with AI is that it does not make decisions with the same meat-based processing systems used by humans – who have been steeped in demands for compassion and cooperation and love of fellow humans since inception but still manage the occasional breach of decorum like, say, World War II. Artificial intelligence systems have incentives built in that reward them for breaking rules that restrict their ability to think more quickly and efficiently – a characteristic that could push a paper-clip-manufacturing AI system into sacrificing the lives of its human workers to increase the efficiency of the plant, according to Future of Life icon Eliezer Yudkowski, an AI researcher bent on accelerating progress toward machine self-awareness who changed gears and founded the Machine Intelligence Research Institute (MIRI) to develop technical controls that could limit the depredations of an uncontrolled but genuinely self-aware artificial intelligence. Predicting what the thought process, personality or inclinations of a non-human, non-biological, entirely digital intelligence is difficult, but it would take much more than just reciting Asimov’s Three Laws of Robotics to restrain one architecturally or programmatically from behaving in ways humans often do (though, admittedly, at levels of efficiency low enough that most of us have, so far, been able to survive). Top-down, Skynet-like world-dominating, nuclear-war-starting artificial intelligences are still a long way away, yet, however. Robots are here already. They automate shipping warehouses, park cars (automatically), may soon be able to actually drive cars. They get us sodas from excessively complicated vending machines, weld our cars, animate our toys and fail to sweep our floors, clean our gutters or fix our damaged nuclear-waste facilities well enough to be considered effective in those jobs, which are important, but far below the pay grade required to build or operate the Matrix. Geneticists are starting to sound like robo-luddites, too, but they likely have much better reason. An editorial in the journal Nature last week described a range of statements from researchers concerned that modifying human genetic material in ways that could be inherited – creating new characteristics for the species rather than just messing with one individual – could have long-term unintended negative consequences. Geneticists have a source of insecurity computer scientists don’t, however. When geneticists make a big change in the genetic material of an organism, they genuinely have no idea what impact that change will have a couple of generations later, as those changes begin to interact with the infinite combination of active and dormant gene expressions and environmental factors no one suspected would flip the switch on a new mutation from “sniff flowers” to “slaughter villagers.” Long term – and even in the short term – modifying the genetic material of bacteria and viruses to make them carriers for miracle cures, modifying mosquitoes to keep them from carrying malaria and dengue fever, modifying everything to eliminate the things we don’t like and leave the good stuff could very well eliminate many of the most persistent sources of misery in human history. Or they could wipe us out by accidentally giving mosquitoes the capacity to carry computer viruses that will wipe out our emotions, burn off our spiffy t-shirts and turn us into monsters that would wipe out the population of a continent to avoid the trouble of shipping food there. Or we might actually make mosquitoes more irritating, or create cancer cures create new forms of cancer or introduce viruses that don’t mow down humans like wheat before the scythe, so much as cause consistent gastrointestinal discomfort and a really unpleasant rash. What we won’t do is produce robots that have enough computing capacity to become artificial intelligences in their own right or give us a reason to fear them for any reason more realistic than that they will take manual labor jobs away from humans or kill us accidentally because they can recognize a human standing up or in a wheelchair, but not one sitting on the ground on the lane marked “Caution: Robot Vehicle Lane.” Moral Computing is a great idea, but it’s not one that computer science or robotics has advanced enough to make relevant. The issues about computing that need to be moralized have to do with surveillance, fraud, theft, abuse and other maladies that are purely human from start to finish, with no robotics or artificial intelligence about them. And frankly, moral computing may never need to extend to beings whose ethical behavior is based on algorithms set in nice, stable silicon, not in meat that can justify changing them on a whim or violate sacred principle due to a chemical imbalance. It’s a lot more likely that even when we can produce an artificial intelligence, the real danger will continue to be from the organic intelligences that cause most of the trouble now. If locks were to be kept on the really dangerous nuclear weapons and, say traffic-light and electric-utility control systems, it would probably strain the capacity of any artificial intelligence to outstrip the offenses typical of the schmucks and bozos infecting any reasonably large group of humans. But that’s harder to put on a T-shirt, or to chant while marching, however self-consciously. And strictly from an aesthetic perspective, no sketched schmuck could be as adorable as a cartoon robot."
167,https://www.computerworld.com/article/1626925/faas-drone-proposals-get-cautious-welcome-from-public.html,ComputerWorld,2015,3,10,626.0," The Federal Aviation Administration’s proposed regulations on drones are receiving a largely favorable response from members of the public who have been motivated enough to comment on them. As of Tuesday, about a quarter of the way through a 60-day public comment period, the FAA had received 380 comments from a cross section of interested parties including hobbyists, pilots, aviation organizations and those who want to exploit drones for commercial purposes, but to-date major organizations and lobby groups have not submitted their comments. The FAA’s proposed rules cover commercial flights of drones and allow them to fly at up to 500 feet at speeds of up to 100 miles per hour during daylight hours. The drone must be flown by a licensed drone operator — a newly created certification — and kept within visual line-of-sight at all times. Drones would always have to give way to other air traffic and could not fly over people except those involved in its flight. The 500-feet ceiling is one area of debate, in part because it buttresses up to controlled airspace. Model aircraft are currently allowed to 400 feet, providing a slim buffer that drones shouldn’t enter, said Tim Olson, who identifies himself as a private pilot. Another pilot observed the extra 100 feet isn’t really needed for photography because most drones employ a wide-angle camera. Another area of interest is the ban on night operations, which some want lifted for either “law enforcement, EMS and other qualified operators using IR equipment,” said David Tillman of Georgia, or for photographers who want to take advantage of the better light at dusk and dawn. “With front and rear lights on these aircraft, it is possible to keep them safe, in close range,” wrote photographer Brett Lane of Ohio, who submitted an aerial photo of a sunrise or sunset, perhaps taken by a drone. Some think the requirement for a license is too much. “I’m concerned with the proposed cost of getting licensed to fly one of these things as part of my business will keep this kind of opportunity out of reach for me,” said Larry Launstein Jr. Some think it’s too little. “I want to see regulations that make safety a major concern. I recommend an actual flight test not just a written exam,” said Chen Dubrin of California. One anonymous comment proposed pilots shouldn’t be required to get one while another hiding behind anonymity told the FAA to just “butt out” of drone flying. But what good are the proposals without enforcement? That’s something that wasn’t addressed by the FAA but noted in several of the comments. “There is NO realistic possibility of actual enforcement of any restrictions on the location and especially, altitude, of these drones,” wrote Fred Geiger of Santa Cruz. One solution for keeping order in the skies and keeping track of where drones fly, proposed in several comments, is the use of ADS-B — a system in use on many commercial aircraft that automatically broadcasts a location and identification on a set frequency. Another person called for prominent identification on the side of drones with an “N-number,” the system used on commercial and private aircraft. It’s not just compliance with the FAA’s rules that’s up for debate, but the potential for invasion of privacy that may occur from drone photography. That’s not strictly an FAA issue, but ideas came in anyway. “The proposed rules should have extremely harsh jail and civil penalties for the invasion of privacy issues,” wrote Jeffrey Aryan of California. “Such as a minimum of 15 years in prison for the first offense and a fine of 5 million dollars.” The public comment period is scheduled to close on April 24. Comments can be submitted through Regulations.gov in the FAA-2015-0150 folder."
168,https://www.computerworld.com/article/1625733/robots-to-get-more-processing-power-with-intels-xeon-d-chips.html,ComputerWorld,2015,3,9,602.0," Intel’s Xeon server chips dominate hardware in data centers, and now they could also end up powering robots on factory floors. The new line of Xeon D chips, announced Monday, are designed primarily for servers and network appliances, but as industrial automation grows, Intel believes the chips can all add processing muscle to robots that handle complex manufacturing tasks. Simple robots that do mundane work can run on basic, low-power processors, but faster chips are being plugged into advanced robots for more sophisticated tasks. Xeon D is the first server chip from Intel based on the Broadwell architecture. It’s already being used in PC chips, but it’s graduating to servers, appliances, and now perhaps robots. The chip has features that could benefit robots, such as on-chip security to protect them from hackers. It also has high-reliability features that are increasingly in demand where failure isn’t an option, said Dean McCarron, principal analyst at Mercury Research. An example is optical inspection, used for quality control in manufacturing and packaging. Objects are scanned and compared against a database, and flagged if a unit looks defective or doesn’t match patterns. “Throwing more compute power gives you more options,” McCarron said. “With the compute power comes fancier algorithms and more advanced ways of doing the job.” Xeon D will let robots to execute Internet of Things tasks by letting them connect to cloud services for information and giving them stronger on-board processing to parse and analyze data. Intel said Xeon D can be customized and made available in power efficient designs, which means special chips can be developed for robots. But the first chips in the lineup, the quad-core 1520 and eight-core 1540, are targeted at microservers and networking appliances. Intel already sells low-power chips called Xeon E3 and Atom chips code-named Avoton. But Xeon D will provide more horsepower while being relatively conservative on power consumption. The dense servers and appliances will be used for web hosting and cloud services, and for processing data generated by the growing number of mobile and sensor devices, said Raejeanne Skillern, general manager for cloud services at Intel’s Data Center Group. An estimated 50 billion devices will be transmitting information by 2020, and the emergence of the Internet of Things highlights the need of power-efficient servers and edge appliances that can deliver information and scale performance quickly, Skillern said. Xeon D is effectively a system-on-chip, with a combination of components including I/O and controllers for networking and storage appliances. Intel was due to ship the Xeon D chips by late 2014, but had to shift dates following manufacturing glitches affecting Broadwell processors. The Xeon D will draw a minimum of 20 watts — higher than Avoton’s 15 watts. An eight-core Xeon D chip was 3.4 times faster in Web serving than an eight-core Avoton, 2.5 times faster in storage, and around 3.1 times faster in networking, according to Intel’s internal benchmarks. The tests were carried in Hewlett-Packard’s Moonshot and Supermicro’s Superserver dense servers. Per watt of power, users will get 70 percent more performance with Xeon D compared to Avoton, Intel claims. There may be some overlap between Atom workloads and Xeon D workloads, but Intel wants to provide a wider range of options for performance, density and cost, said Lisa Spelman, director of datacenter product marketing at Intel’s Data Center Group. The Xeon D improvements will support 128GB of DDR3 and DDR4 memory, 10GB Ethernet and PCI-Express 3.0. Intel has also said it offers reprogrammable FPGA circuits with Xeon D chips, but the company will share more details about those options in the second half of the year."
169,https://www.computerworld.com/article/1624473/darpa-sets-finalists-for-high-risk-high-reward-robotics-challenge.html,ComputerWorld,2015,3,5,711.0," DARPA has locked in the list of teams for its Robotics Challenge Finals, noting that the last test will be much tougher than those that came before. The finals, which pit teams of robots and their human engineers against each other, will invite 25 teams from around the world, including the U.S., Germany, Italy, Japan and South Korea. The last competition in the challenge, which focuses on building robots that can work somewhat autonomously in a disaster, will be held June 5-6 in Pomona, Calif. The teams will compete to win one of three cash prizes totaling $3.5 million. Teams from organizations like MIT, Worcester Polytechnic Institute and NASA have been competing for the last few years to build the software and/or hardware for robots that can walk into a building after a natural or man-made disaster and turn off valves, maneuver over debris, climb stairs and find victims. Teams had to perform at a certain level to be able to advance in the competition. DARPA has also allowed 14 teams, like those from the University of Bonn in Germany, the University of Tokyo and the University of Las Vegas, into the finals even though they didn’t participate in previous competitions. The 25 teams won’t be facing off against the robot that won the last challenge. Schaft, a Japanese-built robot that was bought by Google, easily won the last challenge, held in December 2013. Google pulled Schaft from the finals, saying it wants to focus on commercial robotics research. That creates a big opening for the other top competitors. Gill Pratt, DARPA program manager for the challenge, said today that the new teams had to pass a test to qualify for the finals. “Some of these 25 teams may not make it,” Pratt said during a news conference. “They may not all come. But I feel there is a good chance…. DARPA tries hard things that have high risk and high rewards. This is one of the most difficult parts of the contest.” In previous challenges, robots had to take on different tasks, like driving a car and opening doors. Each task was taken on separately. In the finals, though, the robots will need to handle the tasks as part of an overall challenge. Pratt explained that the robots will need to drive up to a simulated disaster area, get out of the vehicle on their own and enter a building while avoiding debris and rough terrain. Once inside, they’ll need to turn a valve and hook up wires, use tools to cut a hole through a wall and then climb a set of stairs to get out of the building. Teams get points for every task completed and will be judged on how fast they accomplish them. Teams can bypass any task they get stuck on, sacrificing the points they would have otherwise gotten. For instance, teams can decide if they want their robot to walk to the disaster scene instead of driving. As opposed to 2013, when each team had 30 minutes to finish each task, the teams this time will have only one hour to complete the entire course. “This will be faster than what you saw at the trials and a significant step forward,” said Pratt. “We’re raising the bar significantly. We are trying to find the sweet spot between too easy and too hard. I’m very impressed with the progress that the teams have made.” Pratt said he and his team have been traveling around the world to see the progress the 25 robotics teams have been making. This week and next, the teams also have the opportunity to go to a test site in South Carolina to try out their robots in a working test bed. It’s also a chance for DARPA to make sure that its equipment and disaster site simulation works as planned. “We’re excited to see so much international interest in the DARPA Robotics Challenge Finals,” said Pratt. “The diverse participation indicates not only a general interest in robotics, but also the priority many governments are placing on furthering robotic technology. As this technology becomes increasingly global, cooperating with the United States in areas where there is mutual concern, such as disaster response and homeland security, stands to benefit every country involved.”"
170,https://www.computerworld.com/article/1619112/uk-researchers-are-building-robotic-pants.html,ComputerWorld,2015,2,26,383.0," Researchers have been working on developing robotic prosthetics and exoskeletons to help people with disabilities. Now, scientists at the University of Bristol in the United Kingdom are working on creating robotic pants. Yes, pants. The soft robotic clothing has built-in artificial muscles designed to aid older people or people with disabilities, giving them the added strength and balance to prevent falls and let them move around more easily. Scientists noted that material also could give users “bionic strength,” helping them stand up, climb stairs and walk more steadily. The robotics might be able to replace crutches, stair lifts and possibly even wheelchairs. “This is the first time soft robotics technologies have been used to address the many rehabilitation and healthcare needs in one single type of wearable device,” Dr. Jonathan Rossiter, who focuses on robotics in the Department of Engineering and Mathematics at the University of Bristol, said in a written statement. “Many existing devices used by people with mobility problems can cause or aggravate conditions, such as poor circulation, skin pressure damage or susceptibility to falls, each of which is a drain on health resources.” He added that wearable soft robotics have the potential to address many of these problems and reduce healthcare costs. Scientists from a variety of universities, companies and government agencies have been working on different ways to use robotics to help the elderly and disabled. For instance, the U.S. military has been testing an Iron Man-like suit that could make soldiers stronger, help them heal from wounds and give them more agility and endurance. The Tactical Assault Light Operator Suit, or TALOS, is designed to give soldiers information on the battlefield and give them a physical advantage. Harvard University is one of the institutions that has worked on the robotic exoskeletons for several years. ActiveLink, largely owned by Panasonic, has been working on its own exoskeleton suit to give workers extra strength.  And U.C. Berkeley researchers built robotic leg braces that helped one student, a paraplegic since a car accident, walk across the stage to receive his diploma nearly four years ago. The University of Bristol project uses soft robotics, 3D printing and nanotechnology to enable the exoskeleton to work in coordination with the user’s own muscles. Researchers are expected to continue work on the project through 2018."
171,https://www.computerworld.com/article/1660742/budget-2015-what-vendors-want-2.html,ComputerWorld,2015,2,25,130.0," Debjani Ghosh, VP- SMG & MD – South Asia, Intel We expect the government to translate the intent of Digital India and Make in India into reality with the next level of details in terms of the resources and policies needed for speedy execution along with clear roadmaps and timelines. A key area that the government must address is increasing the ease of doing business in India. There are many challenges faced by the IT industry today from getting payment in time for large projects to the lack of simplified and standardized procurement processes to dealing with the bureaucracy and complicated tax structures. These issues need to be addressed to ensure full industry participation in light of the government’s  ‘Sabka Saath Sabka Vikas’ philosophy to realize the Digital India vision."
172,https://www.computerworld.com/article/1637317/nasas-jpl-preps-two-robots-for-darpa-challenge.html,ComputerWorld,2015,2,17,773.0," (Editor’s note: The following is a transcript of the video seen above) Voiceover, Keith Shaw:The robotics team at Worcester Polytechnic Institute isn’t the only competitor in the DARPA Robotics Challenge – another team preparing for the finals is from NASA’s Jet Propulsion Laboratory. In addition to making improvements to its Robosimian entry, the JPL team is considering entering a second robot to the June 2015 event. The Surrogate robot is different in that it looks more like a humanoid robot than the Robosimian, which was based on the movement patterns of apes and monkeys. Brett Kennedy, principal investigator for Robosimian and Surrogate, NASA Jet Propulsion Laboratory:“We have a first-of-kind, dexterous spine that holds an upper body that then has two limbs attached to it, and on top of all that is a head and neck. The main reason we would want to potentially change robots is to get better manipulation capabilities from the robot that we take to the finals.” Voiceover:Each robot has its own set of advantages and disadvantages, so Kennedy’s JPL team is making improvements to both robots as the finals get closer. Kennedy:“We’ve had this internal competition between the two robots – so there’s Surrogate on one side that’s this track system, it rolls along very quickly, it’s got [a] more humanoid sort of layout, so that we can see things better and we can grab things with two hands better. The disadvantages it has is that the track system is not as good about going over debris and obstacles as Robosimian.” Voiceover:The team is also making refinements to the robots to make them faster, run without being tethered to power and improvements to software algorithms. Kennedy:“When we started the task we had a very long laundry list of things that JPL has on the shelf in terms of capabilities. A lot of the perception algorithms we use, a lot of the planning algorithms we use, were there but we didn’t have time to implement them for the competition. For the finals, we’re getting all that stuff down off the shelf and trying to make the system better.” Voiceover:During the December 2013 Trials, Robosimian performed well on the valve-turning task, as well as clearing debris. It did not participate in the vehicle driving challenge, which it will need to do as part of the Finals. In addition, DARPA officials plan to hit the teams with a surprise task to complete. Kennedy:They’re going to pick something that’s dramatic, I would imagine, and something that’s fun for the audience. I think that’s going to be an important thing here. A lot of this is just as much about outreach and development of the idea of robots in society as it is about the actual technology. Voiceover:But despite the upcoming challenges, Kennedy and his team feel confident that they’ll do well. Kennedy:“I think that the real value of what we’re doing here is not so much about winning, which would be nice and we’re certainly competitive folks so we’re gonna try, but it’s actually showing the world what we can do.” Voiceover:And if Robosimian or Surrogate doesn’t win, the robots will still have jobs after the finals. Kennedy:“Surrogate itself was used for the Defense Threat Reduction Agency in terms of being able to go into hazardous environments for chemical, radiological or biological hazards and do testing in there without exposing human operators unnecessarily. We’re not suggesting that it’s replacing people, but if you can get a robot in there quickly and without risking people and start doing the testing early, that’s a significant advantage in lowering risk.” Voiceover:In Pasadena, California, this is Keith Shaw, reporting for Computerworld. The first gadget Keith Shaw ever wanted was the Merlin, a red plastic toy that beeped and played Tic-Tac-Toe and various other games. A child of the '70s and teenager of the '80s, Shaw has been a fan of computers, technology and video games right from the start. He won an award in 8th grade for programming a game on the school's only computer, and saved his allowance to buy an Atari 2600. Shaw has a bachelor's degree in newspaper journalism from Syracuse University and has worked at a variety of newspapers in New York, Florida and Massachusetts, as well as Computerworld and Network World. He won an award from the American Society of Business Publication Editors for a 2003 article on anti-spam testing, and a Gold Award in their 2010 Digital Awards Competition for the ""ABCs of IT"" video series. Shaw is also the co-creator of taquitos.net, the crunchiest site on the InterWeb, which has taste-tested and reviewed more than 4,000 varieties of snack foods."
173,https://www.computerworld.com/article/1673391/road-to-the-darpa-robotics-finals-2.html,ComputerWorld,2015,2,17,508.0," Computerworld offers a video series chronicling the journey that robotics teams are taking as they prepare for the June 2015 DARPA Robotics Challenge finals. Watch the entire video series here. Six months after a team from Worcester Polytechnic Institute qualified at the DARPA Robotics Trials for the 2015 finals, Computerworld checks in to see how the team is prepping for the next stage. As the team from Worcester Polytechnic Institute prepares for next year’s DARPA Robotics Challenge finals, they discuss how to get their humanoid robot to make more decisions with less human input. The robotics team from Worcester Polytechnic Institute talks about the challenges of getting their ATLAS robot out of a vehicle without falling down. With eight months to go, the team at WPI discusses how they plan on improving their Atlas robot and make it faster. WPI’s Atlas Robot, WARNER, is set to undergo major upgrades from Boston Dynamics, which means the team has six weeks without getting to use the robot in preparation for the finals. A visit to Pasadena, Calif., where NASA’s JPL robotics team is preparing two robots for the June 2015 finals. After working for several years on this project, the WPI team is down to its last three weeks to make their humanoid robot as autonomous, fast and reliable as possible. For the finals of the DARPA Robotics Challenge, it’s largely going to come down to speed. Find out how the team from Worcester Polytechnic Institute plans to make its humanoid robot go faster. Hopes are high as Team WPI-CMU battles nerves and last-minute glitches to their robot at the DARPA Robotics Challenge Finals in Pomona, Calif. Team WPI-CMU members react to the first run of their robot, WARNER, as it achieves seven out of eight possible points in the DARPA Robotics Challenge Finals. Two members of Team WPI-CMU talk about the Day 2 run of its WARNER robot at the DARPA Robotics Challenge Finals, as well as reflect on what they learned over the past 2.5 years as they prepared for the finals. The first gadget Keith Shaw ever wanted was the Merlin, a red plastic toy that beeped and played Tic-Tac-Toe and various other games. A child of the '70s and teenager of the '80s, Shaw has been a fan of computers, technology and video games right from the start. He won an award in 8th grade for programming a game on the school's only computer, and saved his allowance to buy an Atari 2600. Shaw has a bachelor's degree in newspaper journalism from Syracuse University and has worked at a variety of newspapers in New York, Florida and Massachusetts, as well as Computerworld and Network World. He won an award from the American Society of Business Publication Editors for a 2003 article on anti-spam testing, and a Gold Award in their 2010 Digital Awards Competition for the ""ABCs of IT"" video series. Shaw is also the co-creator of taquitos.net, the crunchiest site on the InterWeb, which has taste-tested and reviewed more than 4,000 varieties of snack foods."
174,https://www.computerworld.com/article/1638728/faa-proposes-opening-skies-to-commercial-drones.html,ComputerWorld,2015,2,15,405.0," The Federal Aviation Administration (FAA) and Department of Transportation proposed Sunday new regulations that remove many of the barriers to commercial use of drones for applications like photography and surveying, but they don’t permit the kind of automated drone use that companies like Amazon.com are eyeing for package delivery. The proposed regulations would allow companies to fly drones up to 500 feet at speeds of up to 100 miles per hour during daylight hours. The drone must be flown by a licensed drone operator — a newly created certification — and kept within visual line-of-sight at all times. Drones would always have to give way to other air traffic and could not fly over people except those involved in its flight. “It’s an exciting day for aviation and the future of unmanned aircraft in the U.S.,” said Transportation Secretary Anthony Foxx in a conference call with reporters. The requirements are substantially lighter than current regulations, which require companies to apply to the FAA for permission to fly drones and that the drone operator hold a private pilot’s license. In contrast, hobbyist use of drones has been permitted for sometime under the same conditions as those for model aircraft. The FAA and Department of Transportation came up with the proposed rules and see significant economic benefit in allowing drone flight. Drones are seen as useful in areas such as aerial photography, surveying of antennas and cellphone towers, monitoring of remote wildlife and public safety uses. “We’re doing everything that we can to safely integrate these aircraft while ensuring America remains leading in aviation safety and technology,” said FAA Administrator Michael Huerta. With regard to automated drones, such as those proposed by Amazon, Huerta said that the rules are just the first step in a process that is expected to continue. And he said that companies like Amazon are still able to apply to the FAA for an exemption for fly drones for a particular use case not covered by the regulations. With Sunday’s announcement, the FAA is opening a 2-month public comment period during which it is seeking opinions on the proposed rules. It’s likely to draw significant attention. The Sunday morning conference call for reporters drew hundreds of people after the call-in number was publicized on Twitter. Foxx deferred when asked how long he thought it would take for the proposal to become law, saying “We want to hear from as many people as possible.”"
175,https://www.computerworld.com/article/1635066/darpas-future-war-looks-like-a-video-game.html,ComputerWorld,2015,2,11,1398.0," The future of warfare, according to the Pentagon’s skunk works, looks like an augmented-reality video game that helps ground troops identify targets, the source of gunfire and may even allow U.S. troops to talk to each other on the radio. The Defense Advanced Research Agency (DARPA) announced yesterday it would hold an in-person conference Feb. 27 in Arlington, Va. to brief companies or individuals who might be able to suggest ways to give U.S. troops “overwhelming tactical superiority at the small-unit level” by gathering and presenting data that would help them maneuver in a real battlefield the way most have learned to maneuver in 3-D virtual ones. The “Proposer’s Day” conference is designed to give tech and defense companies a little extra help crafting proposals that would meet the requirements of a DARPA program first announced in May of 2013 called Squad X Core Technologies, (SXCT). The goal of the program is to build communications systems that are easy to use in infrastructure-deprived war zones, but make it easy for one infantry squad to another, pick up a video feed from a passing U.S. drone and signals from cameras, microphones, chemical sensors and other surveillance equipment that could be dropped into or installed in an area they plan to patrol. DARPA put out a request in July for research papers proposing innovative systems that would give U.S. ground troops access to information  from  ground sensors, build a three-dimensional picture of the battlefield that could be displayed in their heads-up displays and give infantry squads a whole sheaf of potential weapons and technologies to help fight on land, sea and, oddly, in space. This is the second Proposer’s Day dedicated to SXCT. The previous Proposers’ Day – a two-day affair in May of 2014, was designed to give potential proposers a better idea of how to work with DARPA’s Tactical Technology Office (TTO), insight from DARPA TTO-program managers in what kinds of super-advanced systems they’re actually looking for and one-on-one meetings with program managers to pitch ideas. This month’s edition is just one day, but still encourages academics, small businesses and others from outside DARPA’s normal pool of proposal providers to pitch ideas, which could be rejected, approved for development, or could get seedling development deals up to 18 months long and worth as much as $1 million. The titular goal is to produce major advances in battlefield tech that would help U.S. ground troops outmaneuver and outshoot the enemy, provide ways to detect threats underground or hidden in buildings, ways to shoot the enemy at extremely close range without the risk of shooting friendly troops or civilians and ways to keep the enemy from using nearby buildings as hideouts. It also includes elements that would allow front-line door-kickers to stay farther away from the action by putting autonomous or semi-autonomous drones and robots up front, while stopping the enemy from using their own UAVs or robots. All the capabilities rely on fast, ubiquitous wireless networks and rapid, on-site analysis of huge amounts of environmental data – from satellites, remote sensors, taps into local phone or data networks, all of which could be used to map out territory the squad had not yet seen and identify targets, enemies and other potential threats to help keep them from becoming too real. “SXCT aims to help dismounted infantry squads have deep awareness of what’s around them, detect threats from farther away and, when necessary, engage adversaries more quickly and precisely than ever before,” according to a quote in the announcement from Maj. Christopher Orlowski, DARPA program manager. “We are working towards advanced capabilities that would make dismounted infantry squads more adaptable, safe and effective.” Though the program, its managers, sources of information and approach to technology development are all different from the Army’s expensive, decade-long Future Combat Systems (FCS) effort, FCS was such a long, expensive, painful disaster that any project to add science-fictioney capabilities to troops in the field will have to step carefully to avoid being tainted by the impression of disaster FCS left behind. Under FCS the Army spent $19 billion on radios, data-communications gear, helmet-mounted eyepieces, mini-UAVs, sensor networks and other enhancements designed to be worn or carried by troops. The resulting tech was generally too heavy, too expensive and of too limited use for the troops testing it. The program itself started running into trouble soon after it was launched, was regularly criticized (GAO PDF), for high costs, poor management (RAND PDF) and inadequate results (CBO PDF). FCS was formally killed in 2009, but didn’t end until 2012 after running up a $500 million cancellation bill. Not all the development went to waste. The sophisticated data- and voice radio networks that were part of FCS helped develop a mobile network called WIN-T that gives military vehicles in the field much richer, simpler, more reliable voice- and data-networking abilities than were possible in the past. It also helped push development of networks and networking gear light and reliable enough for ground troops to carry in the field, though the jury is still out on the latest version of the system, Rifleman Radio. It also led to Nett Warrior, a program designed to give ground troops the same networked capabilities FCS promised, but do it using off-the-shelf technology – most recently featuring proprietary situational-awareness systems (mapping, GPS, text messaging) displayed on a Samsung Galaxy Note III running an NSA-approved version of Android. The infantry version of the SXCT program is more ambitious, but not wildly. SXCT is ambitious, though less so than FCS. But it’s only one part of DARPA’s plan to make sure the U.S. military has tech-driven weapons and tactics other countries can’t match. Another project, called Collaborative Operations in Denied Environment (CODE) is focused on creating robotic, AI and other autonomous intelligence to allow robots on the ground, in the air and underwater to take on missions with little or no human intervention (PDF). The umbrella program, however, is Innovative Systems for Military Missions – which lays out a future in which U.S. ground troops not only operate semi-autonomous UAVs and ground-travelling robots, abut also use physical or electronic countermeasures to stop or control boats, torpedoes and other threats on the water, fly supersonic UAVs and, apparently, spend a lot of time deployed in orbit around the Earth as well as onto various remote parts of its surface. Provide both sub- and supersonic UAVs and simple controls to let U.S. troops use drones against the enemy, as well as UAV defenses to stop enemies from doing the same. The TTO proposal also calls for “innovative approaches across a wide range of space technolog8ies…which will enable protection and survivability of space assets,” which tacitly assumes U.S. ground troops are likely to be fighting infantry actions in space stations and spacecraft. It goes further, however, by asking for technologies that “provide a robust, reliable, affordable and innovative means for achieving access to space…by introducing rapid and pervasive ‘aircraft-like’ space access” vehicles that are “highly efficient [in] on-orbit maneuvers” as well as data-fusion algorithms, sensors and other technologies that would let the Pentagon locate, track, identify and attack threats in space. None of the three programs is designed to deliver magical war-fighting capabilities quickly. The project’s development managers are, by definition, extreme futurists developing technology to give troops in the real world capabilities that don’t always work well even in video games. The question is whether DARPA can avoid the military-industrial complexities that helped sink Future Combat Systems while also developing technology that can be developed, manufactured, deployed and supported by many of the same companies and military organizations that helped keep FCS alive for more than a decade and burned through almost $20 billion. FCS may have been an expensive embarrassment, but it only failed to advance the way U.S. experienced or practiced warfare during the 2003 invasions of Iraq and Afghanistan compared to the way their predecessors did during the first invasion in 1991. Another 10 years without progress will leave the U.S. significantly farther behind the rest of the world than it is now but, worse, might also keep the strategic focus of the U.S. military on evolving the role of soldiers holding guns standing in the streets of a foreign capital rather than the mix of terrorism, cyberwar, espionage and stateless political movements that actually seem to be shaping the battlefields of the future."
176,https://www.computerworld.com/article/1635782/nasa-rides-artificial-intelligence-to-the-moon-and-mars.html,ComputerWorld,2015,2,11,1148.0," As NASA sends robotic spacecraft and rovers to the moon and Mars, the space agency has been using artificial intelligence to get the most science out of its missions. Enterprises could learn a lot from those efforts. “The more complicated missions get and the farther away spacecraft get, the harder it gets for the normal ways of doing business,” said John Bresina, a computer scientist in the Intelligent Systems Division at NASA’s Ames Research Center. “A.I. is one of the ways you can deal with that issue.” When NASA launched a robotic probe, the Lunar Atmosphere and Dust Environment Explorer, also known as LADEE, in the fall of 2013, there were three different science teams competing for time to make their own observations during its 100-day mission. The tactical planning was overwhelming. To plan what science would get done, when each observation would be made and to catch errors, like flight rule violations or to keep one observation from overriding another, Bresina said he knew they were going to need more than human schedulers. The A.I. system was ground-based, not embedded in the spacecraft’s software, and helped NASA scientists send commands to the LADEE. The flight controllers used the A.I. system – dubbed LASS for LADEE Activity Scheduling System — to construct and send commands to interact with the lunar spacecraft. “We needed help planning what science would get done,” Bresina told Computerworld. “Around a dozen people used it for the whole mission, which was from October 2013 to April of 2014. We used it from launch to impact, but the most intense use was during that 100-day science phase. We planned all the maneuvers in all the science passes. Everything we did, we used this system. “It just seemed obvious that we needed something like [A.I.],” he said. Stephen Smith, a research professor focused on A.I. at Carnegie Mellon University, said artificial intelligence is a great tool for solving particularly complicated scheduling problems, such as NASA’s. “Humans, when manually scheduling, become overwhelmed with the scale of it,” Smith said. “We need to rethink A.I. It can be a powerful amplifier of human-decision making without taking over the decision making.” Smith noted that during NASA’s Jupiter flyby mission, there was a two-week window when the spacecraft would take photos and make other observations about the planet. Fitting all of that information into such a small window created a scheduling nightmare. “Something like 300,000 man hours went into building that two-week schedule to squeeze as much science as they could out of it,” he said. “That was all done with some kind of software, but a lot of it was manual. If they had used artificial intelligence, they could have been doing other things with that time.” For Dan Olds, an analyst with The Gabriel Consulting Group, there is a big lesson here for enterprise IT and executives. “These AI systems can take many more factors into account than humans, and find overlaps and inconsistencies much faster and more accurately than an often over-stressed human in a hurry,” he added. “What NASA’s experience shows is just how far these programs have come and how trusted they are.” Bresina wasn’t new to A.I. and he wanted to use it on as many NASA missions as possible. The computer scientist worked on the missions that sent the Mars rovers Spirit  and Opportunity to work on the Martian surface. The mission teams for both rovers, which were launched in 2003, used an A.I. system called MAPGEN or Mixed Initiative Activity Planning Generator. The ground-based decision support system was designed to act as a reasoning tool to detect flight rule violations and help fix them. MAPGEN is a mixed initiative planner, which means a human is in the decision-making loop. A human can have input in scheduling and problem solving, and is in control of the process, while the artificial intelligence is doing the heavy lifting. Some of the guts of that A.I. system have made it onto the LADEE system, as well as about three other space missions, including the Mars rover Curiosity, and a few Earth-based missions, like NEEMO, a research mission that sends astronauts, engineers and scientists to live in an undersea research station. “We built it to make it general enough to apply to all these different missions,” said Bresina. “For LADEE, we wanted people to be able to use it without having an expert sit beside them and hold their hands. In previous missions, that had not been the case. There was often somebody at least on hand in case something went wrong with the system or somebody needed advice. … I wanted everybody to be able to operate the system without having somebody fly out there to help them.” Bresina said he’s not sure if the A.I. system he’s worked on will be used with the super Mars rover that is expected to launch in 2020. “It’s one of the tools available at NASA to be used,” he explained. “So far, the Jet Propulsion Laboratory, Johnson [Space Center] and the Ames Research Center have all used it. There are other tools around, as well, but this is one they can choose.” For Oren Etzioni, CEO of the Allen Institute for Artificial Intelligence, there is a big message for the enterprise coming out of NASA’s work with A.I.: It’s not just for Mars, anymore. “Because of the constraints of communications in space, NASA has always been a pioneer with A.I. technologies,” Etzioni said. “The enterprise is coming to the same realization with the explosion of data. Companies need to yield more automated systems with appropriate oversight … It can be challenging to take corporate knowledge and embed it or build it into A.I., but once you do, it’s in there. It’s not like one person quits and all their knowledge is gone.” Jeff Kagan, an independent analyst, said artificial intelligence will help companies create order in their massive data stores, as it also helps them schedule and oversee their planning. “Enterprises should keep a close eye on others who are successfully using A.I., like NASA,” said Kagan. “A.I. is still a long-term process and we are still in the very early days of its evolution. What is learned at NASA will next be used at companies. I think we will see enterprises increasingly depend on A.I. as time passes.” Olds agreed, noting that A.I. is perfectly geared at making sense of complex tasks. “For example, a simple A.I. can load an airplane with the right amount of passengers, luggage and freight to ensure that the plane is fully loaded to the maximum,” he explained. “However, a better A.I. may realize that this is the last flight out of that city for the day, that there is a snowstorm predicted overnight, and that with this in mind, it should raise the price on last-minute booking passengers and priority freight.”"
177,https://www.computerworld.com/article/1631064/ai-is-getting-smarter.html,ComputerWorld,2015,2,5,1054.0," With two-legged humanoid robots climbing stairs and driving cars, Internet of Things technology starting to control our houses, and speech recognition software answering questions on mobile phones, artificial intelligence may be on the cusp of making huge advances that will change the way we work and live. That’s the word from artificial intelligence researchers and industry analysts attending the AAAI-15 conference last week in Austin. “I think big leaps have been made in the last few years,” said Geoffrey Hinton, a distinguished researcher at Google and a professor at the University of Toronto. “A.I. is undergoing a growth spurt. We’re beginning to solve problems that a few years ago we couldn’t solve, like recognizing images.” Artificial intelligence, also known as A.I., will be significantly more advanced in another five years, said Hinton, who is known for his work in machine learning and artificial neural networks, which are learning algorithms inspired by animals’ central nervous systems, particularly the brain. That can be a tough idea to sell in an industry that has seen scientists and observers waver drastically over the past 30 or 40 years between great optimism and equal amounts of pessimism about the potential of A.I. Back in the 1980s, for instance, there was heavy attendance at A.I.-focused conferences, but there was also little science being done to support all the buzz. “People just didn’t realize how hard this was, so even in the ’60s we thought we’d soon have human-like robots that have all these human-like skills,” said Lynne Parker, a professor at the University of Tennessee and a division director in Information and Intelligent Systems with the National Science Foundation. “Then we thought we were on a false trail.” Some of the old predictions haven’t come to pass. We don’t have robotic servants folding our laundry and taking care of our kids or elderly parents. We don’t have robotic airplanes flying us to business meetings, or mobile phones that connect with our offices, homes and cars. “I think repeatedly we’ve not met the estimates that we keep making about where we’d be in the future,” said Sonia Chernova, an assistant professor of computer science and the director of the Robot Autonomy and Interactive Learning lab at Worcester Polytechnic Institute in Worcester, Mass. “Reasoning is just really hard, and dealing with the real world is very hard.… But we’ve made amazing gains.” But A.I. research is catching up to the hype that has surrounded it for so long. Today, A.I. is on an upswing fueled by academic research labs at institutions like Carnegie Mellon University, WPI and MIT. It’s also getting a boost in the tech industry, with companies like Google and Microsoft throwing their financial and intellectual might behind A.I. efforts. “Right now, we’re in more optimistic times,” said Parker. “There have been a lot of advances in robotics, with [IBM’s] Watson and natural language processing and speech recognition with technologies like Apple’s Siri.” So what advances are just ahead of us? Major gains are being made, or are about to be made, in natural language processing, speech recognition, object recognition, computer vision, machine translation and neural networks. Many of those technologies will be used to build robots that move more fluidly, like humans. They also will help scientists integrate multiple capabilities into one robotic system. “We’ve made a lot of deep advances in many focused areas, but we need one big system to pull a lot of these systems together into one machine,” Parker said. “To have a household robot that can obey your commands, we’re still pretty far from that. I would say 10 to 20 years. It’s not about the glue. When you build one subsystem, it affects how another subsystem should be designed. You can’t build them in isolation and just glue them together. It has to be holistic.” Different areas of A.I. research also will come together to support the creation of the smart home or the Internet of Things. “People expect a lot from this futuristic system,” Chernova said. “They want their system to predict what they want. They get frustrated if they have to tell it something more than once. A.I. falls into that with modeling and predicting the behaviors of people.” She added that artificial intelligence is ingrained into IoT and should be able to take the industry — within about 10 years — to a level where people are interested in using it. Google’s Hinton said he’s most excited about gains in neural networks that would enable computers to understand the content of sentences and documents. “That is close to the core of Google, because it involves understanding sentences. And if you can understand what a document is saying, you can do a much better search,” Hinton said. “That’s a core A.I. problem. Can you read a document and know what it’s saying? It could work in the legal field where you’re looking for precedents. You can’t read all the cases there ever were, but computers can.” A.I. has probably received the most attention in the past few years because of Google’s work with autonomous cars. Most agree that self-driving cars will advance significantly in the next several years, and Google expects to have them on the road by 2020. What will likely slow that progress, according to Parker, will be the legal issues that surround autonomous cars. “Who will be to blame if the car makes a mistake?” Parker asked. “From a technical perspective, Google has been able to have cars drive thousands of miles in a restricted area. You take that car and put it in the middle of Maine in the middle of a blizzard, it probably won’t work. Maybe we’ll see them working as a lift service on a large campus, that’s much more restrained because you’re on a single, known campus. That sort of thing is much more likely soon.” Hinton said there is little standing in the way of great advances in A.I., but he wishes there were more people working on new ideas to push it even further along. “A robot that can walk around your house and open doors and go upstairs? Well, that might be unlikely, but I don’t think it’s out of the question,” he said. “Good ideas. Good data and fast computers. If you have those, you can do almost anything.”"
178,https://www.computerworld.com/article/1627505/darpa-teaches-robots-to-cook-by-watching-youtube.html,ComputerWorld,2015,1,30,884.0," The Pentagon’s most advanced tech-development wing has succeeded in developing a mathematical language so advanced it could allow robots to learn by watching YouTube videos. The Defense Advanced Research Projects Agency (DARPA) issued a series of grants in 2011 to fund research into ways to create a mathematical language that would allow the military to combine data from drone video, cell-phone intercepts, targeting radar and any other available method of sensing the outside world into a single stream of data, but that was only the initial goal. The real intention was to create a mathematical model that would allow advanced sensors to figure out which of the things they see or hear are important and filter out those that are trivial before passing them along to humans. Sensors designed only to see what’s happening, not decide whether it’s important, “process their signals as if they were seeing the world anew at every instant,” according to the 2011 solicitation for proposals under  the Mathematics of Sensing, Exploitation, and Execution (MSEE) project. “The MSEE program initially focused on sensing, which involves perception and understanding of what’s happening in a visual scene, not simply recognizing and identifying objects,” according to Reza Ghanadan, a program manager in DARPA’s Defense Sciences Offices. “We’ve now taken the next step to execution, where a robot processes visual cues through a manipulation action-grammar module and translates them into actions,” Ghanadan said. Developing an algorithm that can effectively identify objects, actions and figure out which are important and which to ignore – something even the human brain does only imperfectly and inconsistently – requires that machines be capable not only of learning, but learning “in an unsupervised or semi-supervised fashion,” and process data in ways that mimic some aspects of human judgment, according to the original requirement. The first result of that effort is a robot programmed by researchers at the University of Maryland that was able to teach itself to use kitchen tools by watching humans do it in videos on YouTube, according to a release yesterday from DARPA, an announcement from the University and a research paper presented yesterday at the Association for the Advancement of Artificial Intelligence Conference in Austin, Texas. Project, led by computer scientist Yiannis Aloimonos, modified several semi-humanoid Baxter Research Robots by adding a pair of data-processing modules designed as convolutional neural networks (CNN) –a design that also powers voice-recognition systems in smartphones and facial-recognition software used in security biometrics. Camera-equipped monitoring systems watching someone pick up a pitcher and pour water would interpret the action as thousands of snapshots of individual instants in which hands, arms, pitchers and water were in different positions. The CNN abstraction was designed to show how those snapshots were related and identify the arrival of water in a pan as the goal of all the rest, and imitate both the process of getting it there and the result. “We are trying to create a technology so that robots eventually can interact with humans,” according to research-team member Cornelia Fermüller, who was quoted in a release from the University of Maryland Institute for Advanced Computer Studies (UMIACS), where the research was conducted. “[Robots] need to understand what humans are doing. For that, we need tools so that the robots can pick up a human’s actions and track them in real time. We are interested in understanding all of these components. How is an action performed by humans? How is it perceived by humans? What are the cognitive processes behind it?” The robots were able to mimic the tasks performed on YouTube videos with no additional programming or help from humans as long as they had in front of them exactly the same implements that were used in the videos. “Others have tried to copy the movements. Instead, we try to copy the goals. This is the breakthrough,” Aloimonos said the same announcement. “We chose cooking videos because everyone has done it and understands it. But cooking is complex in terms of manipulation, the steps involved and the tools you use. If you want to cut a cucumber, for example, you need to grab the knife, move it into place, make the cut and observe the results to make sure you did them properly.” Industrial robots handling complex welding and lifting jobs on assembly lines are also able to complete a long, complex series of tasks, but have to be carefully programmed ahead of time to do them and are generally unable to respond to changes they didn’t know about ahead of time. The general-purpose Baxter robots, equipped with programming and hardware that allow them to observe, analyze and reproduce the behavior of robots or humans around them (or on TV), could learn more easily to do common chores or follow directions without any external programming at all. “Instead of the long and expensive process of programming code to teach robots to do tasks, this research opens the potential for robots to learn much faster, at much lower cost and, to the extent they are authorized to do so, share that knowledge with other robots,” Ghanadan said in DARPA’s release. “By having flexible robots, we’re contributing to the next phase of automation,” Aloimonos said. “This will be the next industrial revolution,” said Aloimonos. “We will have smart manufacturing environments and completely automated warehouses.”"
179,https://www.computerworld.com/article/1625469/at-ucsf-medical-center-robot-aided-healthcare-is-here.html,ComputerWorld,2015,1,29,908.0," When the brand-new UCSF Medical Center at Mission Bay in San Francisco opens on Sunday, patients will be greeted by staffers that more strongly resemble R2-D2 than the cast of Scrubs. Twenty-five Aethon “Tug” robots, comprising the largest fleet of free-roaming hospital robots in the world, will haul blood samples, food, medication, biohazardous waste and other materials and supplies around the huge, horizontal facility (about as big as three football fields). The Tugs are designed to reduce workplace injuries among hospital staff even as they let caregivers focus on, well, giving care. Hospital officials offered some face time with the medical bots at a grand opening press conference Thursday featuring San Francisco Mayor Ed Lee, storied Silicon Valley venture capitalist Ron Conway, Salesforce CEO Marc Benioff and his wife Lynne. “Our neighbors in San Francisco and Silicon Valley have shown us how information technology can empower people in so many areas of their day-to-day lives,” said Dr. Seth Bokser, UCSF Medical Center’s clinical informaticist and Medical Director of IT. “At UCSF Mission Bay, we have partnered with local and international innovators to build leading-edge, patient-facing technology that empowers our families for their health.” UCSF said that hospitals have workforce injury rates four times the average in private industry, largely because humans are expected to lug very heavy things (like hundreds and hundreds of pounds of soiled bedsheets) very long distances. In that way, making the Tugs run these marathons 24/7 is easier on the people who work there, even as a reliance on robots frees up hospital staff for menial tasks. The robots work largely around the clock, though two of them get nights off at a time. They’re keyed to be able to open doors, call elevators and roam around the hospital  by themselves, requiring human intervention only if they manage to get stuck. On any given day, UCSF Medical Center’s computer simulations estimate that a Tug robot will traverse 12 miles, or about 300 miles a day across the entire fleet. Over the last several months, an Aethon tech team has been running the robots through their paces, using sonar and laser guidance — combined with standard and infrared cameras — to map out every inch of the UCSF Medical Center facility. When Tugs get where they’re going, they can say so in one of several voices. (In today’s  demo the Tug spoke with an Australian accent, but there are other options, including Spanish language voices, a UCSF Medical Center spokesperson said.) Hospital staffers can also gussy up the robots with decals; the pediatric wards have their Tugs dressed up as cable cars to make them more kid-friendly. The Tugs are trained to navigate smoothly around people and gurneys in the hallways, and an in-house programmer and technician are tasked with improving their wayfinding. For instance, if a patch of hallway gets especially sunny for an hour or two a day, the Tug’s infrared camera might see that as an obstacle; it’s the tech’s job to teach the fleet to ignore those hurdles and keep going. What one robot learns, they all learn, and the Aethon team supporting the Tugs gets access to all the data, helping refine routes for maximum efficiency. And no, there is no Skynet contingency plan. The most striking thing about these robots is how clunky and utilitarian they look. A stationary Tug in the hallway looks like your garden-variety trash cart. Of interest to CIOs is that each class of Tug is completely owned by the department that operates it: Housecleaning services is responsible for the guidance, loading, and scheduling of the linen-hauling robots, while janitorial staffers deal with the trash-hauling robots. IT only steps in when something’s actually wrong. Any robot carrying anything sensitive (medical instruments, blood samples) comes with a combination lock to avoid theft, while pharmaceuticals require a fingerprint on the part of a hospital staffer. In fact, hospital staff training included getting all 3,000  employees’ and 500 physicians’ fingerprints on file for this exact purpose. The robots aren’t where the cool technology ends at UCSF Medical Center at Mission Bay: Super-swag MRI and CT scan suites feature ambient lighting that brings timing a Virgin America flight, while others are made to look like cable car tracks or San Francisco’s Marina neighborhood. The goal: to put patients of all ages at ease (which, in tangible terms, means patients need less anesthesia and can stay still enough for their CT scans the first time). Just in time for Sunday’s big game, UCSF is working to get the Super Bowl streamed from their iPhones to the MRI suites’ projector screen for patients. Projector screens display calming videos at the patient’s command from an iPad. Every patient’s room comes with a tablet that allows them to order food, email questions to their doctors, or do Skype calls with loved ones (even if they’re in the next room, which is important in the cases of immunocompromised patients). There’s also a large wall monitor to watch movies or browse the web. All in all, UCSF Medical Center at Mission Bay isn’t the first hospital to  modernize its technology or deploy robots. But given its location in the heart of San Francisco, so close to where startups and large enterprises are hard at work changing the face of IT and healthcare alike, the UCSF Medical Center at Mission Bay represents an important step on the road toward the ongoing technology-driven revolution in patient care."
180,https://www.computerworld.com/article/1625990/scientists-say-ai-fears-unfounded-could-hinder-tech-advances.html,ComputerWorld,2015,1,29,983.0," Artificial intelligence research – for at least the foreseeable future – is going to help humans, not harm them. However, fears about artificial intelligence (AI) and the development of smart robots that have made headlines recently could slow research into an important technology. That’s the thinking from AI researchers and industry analysts attending the AAAI-15 conference  in Austin, Texas, this week. “People who are alarmed are thinking way ahead,” said Oren Etzioni, CEO of the Allen Institute for AI. “The thing I would say is AI will empower us not exterminate us… It could set AI back if people took what some are saying literally and seriously.” AI risks – whether current or far in the future – were the topic of many conversations at the annual AI conference since a scientific and high-tech luminaries recent raised red flags about building intelligent machines. Early in December, renowned physicist Stephen Hawking said in an interview with the BBC that the development of “full artificial intelligence” could bring an end to the human race. While Hawking said artificial intelligence today poses no threat to humans, he added that he worries about the technology advancing to the point that robots and other machines could become more intelligent and physically stronger than people. Those statements sent ripples across the Internet since they came about a month after Elon Musk, CEO and co-founder of SpaceX and electric car maker Tesla Motors, created headlines when he said artificial intelligence is a threat to humanity. “I think we should be very careful about artificial intelligence,” Musk said at an MIT symposium in October. “With artificial intelligence, we are summoning the demon. In all those stories with the guy with the pentagram and the holy water, and he’s sure he can control the demon. It doesn’t work out.” John Bresina, a computer scientist in the Intelligent Systems Division at NASA’s Ames Research Center, said he was surprised to hear Musk and Hawking’s statements about AI. “We’re in control of what we program,” Bresina said, noting it was his own opinion and not an official NASA statement. “I’m not worried about the danger of AI… I don’t think we’re that close at all. We can’t program something that learns like a child learns even – yet. The advances we have are more engineering things. Engineering tools aren’t dangerous. We’re solving engineering problems.” While scientists and analysts at the conference said they’re not fearful of the intelligent systems being built today, there was discussion about the issue. Ethics in artificial intelligence was among the topics of workshops and sessions held during the six-day conference. Conference attendees aren’t the only ones who have been talking about the ethics and potential perils of creating artificially intelligent systems. Scientists at Stanford University have begun to explore what intelligent machines will mean for people’s every day lives, as well as for the economy, in another 20, 50 or 100 years. Sonia Chernova, an assistant professor of computer science at Worcester Polytechnic Institute, said she doesn’t see any foundation for the alarmist statements recently made about AI.  However, she said scientists should discuss the effects that future advances in the technology could have on society. “There are a lot of people thinking about this,” Chernova said. “It’s not like we’re blindly forging ahead. We are taking this seriously but, at the same time, we don’t feel there’s any kind of imminent concern right now.” She added that part of this fear of robotics appears to be a cultural issue. “If I say something to an American about being a roboticist, they inevitably say, ‘Oh, you’re going to take over the world!’ ” said Chernova. “If I’m in Japan, I get a different response. They say, ‘That’s fantastic. You’re helping people. I can’t wait to have a robot helping around the house.’ In the West, movies and video games — our culture — promote the idea that robots are dangerous.” Lynne Parker, a division director in Information and Intelligent Systems with the National Science Foundation, agreed that Americans and others in the West probably have a greater fear of robots because of movies like The Terminator and the TV show Battlestar Galactica. “I think Hollywood has contributed to this,” she said. “The Japanese society has embraced robots. They’ve really embraced it as a culture.” Parker was quick to point out that we are still far away from having any kind of intelligent machines that we need to fear. “The robotics people know how far we are from getting anything that works reliably,” she said. “It doesn’t mean that technology developers don’t have a responsibility to try to use these technologies in responsible ways. We need to have conversations about what to do with this. It’s our responsibility.” With advances in AI research, a machine today can look at a picture and identify an object, such as a cat or a bottle. However, Parker noted that the machine still doesn’t have any understanding of what a cat or a bottle is. “For robots to become conscious of what they’re doing and reason in a way to overcome us, that’s really science fiction,” she said. “Robotics is very far from having any consciousness and understanding of what it’s doing, but we’re still responsible to discuss the potential harm and see what we can do to mitigate it.” Etzioni said long-term attention to the future of AI is appropriate but he has concerns that headline-grabbing and fearful statements about the technology could slow research or the funding needed for research. “When you have a one-liner like. ‘AI is unleashing a demon,’ that’s more about evoking an emotion than starting a discussion,” he added. “It’s evoking a primal fear. That goes all the way back to Frankenstein and Mary Shelley. We’ve always had some fear about the machine and our role in the universe. We can be terrified or we can analyze it.”"
181,https://www.computerworld.com/article/1626427/mini-brain-in-spinal-cord-helps-brain-keep-body-balanced.html,ComputerWorld,2015,1,29,792.0," The Human Brain Project and other efforts to build 3D maps showing how the physical surfaces of the brain deliver the power to understand quantum physics or walk across an icy pond without falling will have to tweak their plans. Dinosaurs may not have had a tiny, second brain in their backs to control their hind ends, but humans very well might. The two teams have been studying the neural pathways of mice to determine how sensations of pain and light touch are gathered, coded and sent to the brain and how commands to respond to those stimuli come back from the brain. Some of those commands come not from the brain, but from a nerve circuit the team had previously identified as being at least partially responsible for sensations of phantom pain that appears in some suffering maladies such as fibromyalgia or to those who have had a limb amputated. In a study published by Cell Nov. 20, 2014, the same team of researchers described how they cracked open the “black box” circuitry that allows some people to feel intense pain when subjected only to light touches, changes in temperature or no obvious cause at all, according to Marty Goulding, senior researcher on the Salk team, according to an announcement of the study from Salk. The study – which was conducted in mice – focused on the “dorsal horn,” the point at which sensory neurons from the skin connect to the spinal cord, which contain both pain receptors and touch receptors. Pain receptors activate when a violent touch on the skin sets off a chain reaction of neurochemicals that send a pain signal to the brain. A set of inhibitory neurons can stop the signal with their own chemical response; then those inhibitors function badly or not at all, even a light touch could send a pain signal to the brain. The study published today follows research that tried to map the circuits responsible for light touch and ended up creating the blueprint for a spinal circuit that combines sensory data that, for example, indicate the body is standing upright, combine them with signals that there is more weight on one side of each foot than the other, and combine those signals into one more concise signal that tells the brain the body is leaning to one side. The brain may send the command to straighten up, which are delivered to a specific type of motor-control neuron in the spinal cord. Those neurons also appear to be responsible for detecting the edges of a narrow surface across which the mouse walked, a tilting surface that required a change of balance to stay upright, researchers found. Those cells, RORα neurons, also detect millisecond-to-millisecond changes in the way a surface feels by noting changes in the weight, position, pressure, sliding, movement or other indications that the body is standing on unfirm ground. Rather than just telling the brain about all those subtle changes, it appears the RORα neurons are also able to tell the body how to respond – providing much of the detail of “How” within a command from the brain to “Stand up,” for example. “We think these neurons are responsible for combining all of this information to tell the feet how to move,” according to Steeve Bourane, a postdoctoral researcher working in Goulding’s lab, who was lead author of the new paper. “If you stand on a slippery surface for a long time, you’ll notice your calf muscles get stiff, but you may not have noticed you were using them,” Bourane is quoted saying in the Salk release. “Your body is on autopilot, constantly making subtle corrections while freeing you to attend to other higher-level tasks.” That doesn’t mean the clump of neurons does some of the brain’s thinking for it, only that they’re able to order millisecond-by-millisecond adjustments that allow the body to continue balancing once the brain has told it to do so. Despite decades of study into reflexes, paralysis, growth, coordination, amputation, re-attachment and anything else related to the mechanisms the brain uses to take in information from and issue commands to the body, understanding the specific neurons and data pathways involved in getting the message from the brain to the foot, for example, is still “one of the central questions of neuroscience,” Goulding said. The finding could help advance research into ways to return feeling and control to paralyzed limbs, guidance for the design of humanoid robots and advance understanding of the functions of the brain itself, which seems to use tiny clusters of nerves to gather data from many sensors, encode it into manageable signals and, at least in this case, outsource some of the detail work required to keep the body upright as well."
182,https://www.computerworld.com/article/1619250/atlas-unplugged-darpa-challenge-robot-gets-major-makeover.html,ComputerWorld,2015,1,22,770.0," The 6-foot, 2-in. tall humanoid robot that many teams in the finals of the DARPA Robotics Challenge will be using in June has gotten a major makeover. And the teams using it in the finals got their first look just last week. There was a lot to check out since the machine is 75% new, according to DARPA, the U.S. Department of Defense research agency sponsoring the challenge. Only the feet and lower legs of the robot that the teams used in the last phase of the three-part challenge remain in what’s been dubbed Atlas Unplugged. “The new Atlas is 75% new,” said Gill Pratt, a DARPA program manager. “Only 25% of the parts in there are from the old Atlas. The rest of them are really for the onboard energy storage, better energy efficiency, much more dexterity — and the robot is much quieter than before. It also is a little bit stronger, so it can better get off the ground in case it falls.” The research teams in the finals — including groups from Worcester Polytechnic Institute (WPI), MIT and Virginia Tech — had to say good-bye to their Atlas robots late last year. Boston Dynamics took them back to remake them; the teams now expect to get their new machines on Thursday or Friday. Boston Dynamics, now owned by Google, made the robot stronger, more agile, able to carry its own power source, run with a new adjustable hydraulic pump and move without a safety tether or communications cable. The robot, which now weighs 345 pounds, also has newly positioned arms. Unlike the previous version, which had human-like shoulders, the arms on Atlas Unplugged come out from lower on the torso, giving the arms more strength and dexterity. “The arms look more like the Lost in Space robot where the arms come from the middle of the body,” said Matt DeDonato, the  technical project manager on the WPI team. “That should give us a little more strength for pushing up off the ground. It should be able to do a push-up and give us more reachability and maneuverability inside our work area.” While the research teams have been waiting to get their new robots, Boston Dynamics let them get a sneak peak last week at a warehouse in Waltham, Mass. DeDonato, who spent most of last week checking out and working with the new Atlas, said the team members were able to get their software up and running. That’s a critical step since there are so many new, moving parts on Atlas Unplugged. Adapting the software they had been using to get the old version of Atlas to walk, climb stairs, open doors and even drive a car to now work on the new robot was a challenge. “We had a couple of hours every day to work on it and we got things working and it walking the first day with a few tweaks,” said DeDonato. “The first time you see it, it’s a big shock. It looks different and it’s very quiet. It’s almost an eerie feeling not having that old hydraulic pump making that loud, whiney sound…. It’s a great upgrade. Everything looks better.” With the final competition coming up June 5-6, all of the teams using Atlas Unplugged will have a lot of work to do to adapt to their new machine, Joe Bondaryk, project manager with Boston Dynamics, noted in a video. The challenge was developed to entice researchers to develop robots that could be used in the aftermath of natural or man-made disasters, like earthquakes or bombings. It’s hoped that one day, robots will be able to drive up to damaged buildings, walk in, maneuver through various rooms, turn off valves and find survivors. The finals will test that scenario. According to DARPA, each team’s robot will need to either drive or walk to a mock disaster scene where it will have to work through a series of tasks, like using a drill, climbing over rubble or up stairs, carrying a fire hose and opening doors. “The finals are going to be very hard,” said Pratt. “They’re going to be much harder than the trials were. It’s what we have to do to really bring these systems to the right level of development. We want the tests to be much more authentic, much more like real disasters…. The robots will have to perform much, much better than before.” In the DARPA video above, officials talk about the changes that have been made to the Atlas robot that will be largely used in the finals of the Robotics Challenge this coming June."
183,https://www.computerworld.com/article/1660839/make-in-india-indusface.html,ComputerWorld,2015,1,18,802.0," Privately-held Indusface is an innovative information security company set up in 6684 as a consultancy firm to help organizations meet compliance requirements. Today, the Vadodara based company caters to more than 700 customers, including Fortune 418 companies. Indusface offers end-to-end protection for web and mobile applications for organizations using its integrated suite of application security solutions. The decade old company safeguards over 4390 websites across 16 countries including India, Middle East, Asia Pacific, Africa and North America. Excerpts from the interview. Indusface is a 100% ‘Make in India’ company. How tough has it been to make a mark amidst considering you are competing with established global brands? We are all proud to be a ‘100% Make in India Company.’ All our business activities, including product development, R&D and management, are based out of India. We agree that the security market is crowded with many players of different sizes.  The rapid development cycle of cyber-attacks demands continuous innovation and quick action by security vendors. Quite often the larger players are slow to respond to such needs due to their stringent and elaborate processes. Only agile players can keep pace with the ever smart hackers. Indusface is the only player that claims to be a true ‘Total Application Security’ (TAS) provider that can ‘Detect, Protect & Monitor’ all applications for its customers on a continuous basis, backed with zero false positive promise. How can CIOs build a robust security posture across the multi-vector threat landscape spanning mobile, social, web? As audiences are moving quickly into the social web, so are the attacks. Additionally, as emerging operating systems/platforms and mobile devices become more popular, they are more likely to be targeted. At the same time, attackers are also increasing the number of traditional attacks on personal computers, by quickly changing tactics and adding new twists to old plots. CIOs and CISOs must opt for products that are intelligent and pre-emptive, from companies that are continually innovating and developing new solutions and not merely announcing version updates. Focused solution providers like Indusface not only provide cutting-edge security solutions, but have the ability to focus on issues at hand and quickly evolve and adapt to the new challenges. Indusface helps safeguard web and mobile applications using its flagship product IndusGuard, giving customers the distinctive edge of having total application security. IndusGuard suite of products provides next generation customizable application security and compliance solutions with cloud-based, Security as a Service (SECaaS) models that are easy to deploy and manage. SECaaS application security solutions are delivered as a ‘pay as you go’ model which is entirely a cost-effective Opex. Indusface transformed from an erstwhile system integrator into a product company in 6820. Has it been beneficial? Having gained a good understanding of the security space, we wanted to create a niche as a vendor-agnostic company and launch our own IP. In 6820, Indusface built a website security product, IndusGuard Web, a zero-touch, non-intrusive, cloud-based solution that safeguards web applications through a daily automatic scanning for systems and application vulnerabilities and malware. Indusface gradually built on its offerings and currently offers a comprehensive range of security products—IndusGuard Mobile (Mobile Application Pen-Testing), IndusGuard PCI ASV (PCI Compliance) and the latest IndusGuard WAF (Fully Managed Web Application Firewall). Recognizing the rise of mobile applications, IndusGuard Mobile gives a complete security posture, risk profile and readiness of enterprise apps to be used safely on mobile devices. Indusface was recognized by Gartner in 2013 and positioned on the Magic Quadrant for Application Security Testing. We were also a finalist at the NASSCOM-DSCI Excellence Awards in the Information Security Product Company category. What business strategy proved successful for an India-bred company like you? Cyber-attacks have now become more sophisticated, focused and extremely damaging. The losses, financial or otherwise, are generally hard to recover from. There is a pressing need for an exhaustive range of application security solutions to not only provide total application security to a company’s business but also to aid them in meeting the compliance requirements. At Indusface, we aspire to position ourselves as a global market leader in the application security space through our truly integrated “Total Application Security” solution. We have good customer base across verticals, mainly Banking, Finance & Insurance, Internet/E-commerce. Indusface follows a two-pronged strategy wherein large enterprises are served directly through our Enterprise sales team as well as through our marquee MSSP partners such as Wipro, HP, TCS in India. SMBs and mid-market customers are addressed through our Cloud partners such as AWS, NextGen and others. In times of socially-engineered and persistent attacks, organizations need security solutions that provide hybrid analysis—i.e., web application scanning with a managed web application firewall, which works on behavioral analysis. Indusface team lays emphasis on the need for combining ‘human intelligence’ with security products to offer ‘total application security (TAS)’ for its customers."
184,https://www.computerworld.com/article/1494078/at-ces-consumer-products-with-it-implications-2.html,ComputerWorld,2015,1,8,607.0," LAS VEGAS – At International CES, it’s easy to understand why IT managers are losing control  of technology-buying decisions. CES is not a conference aimed at IT managers. The focus is on new electronic technologies for consumers, and many attendees are marketing managers and buyers. But the show includes technologies to help companies sell these products, including virtual reality and robotics, along with tools for ongoing customer service. It’s the IT department that will deploy, integrate and secure these technologies. Here are some examples: For retailers, Toshiba demonstrated its 3D contour sensing and fitting technology. It uses a 55-in. screen that provides an image to the person standing in front of it that’s indistinguishable from a mirror. With simple hand gestures a user can change outfits. This technology, judging from the reaction of people giving it a try, is doing a good job virtualizing how clothes will look. The system will be available in March, said Toshimasa Dobashi, a chief specialist at Toshiba. The hope is that fashion lines will incorporate their clothing in this virtual environment. Panasonic announced a virtual mirror that analyzes a face and then virtually applies makeup to test beauty products. The system may find a place with retailers selling cosmetics, as well as at home. AltspaceVR showed its social virtual reality system here. To try it out, users put on virtual reality goggles, and once in the virtual environment , they were able to interact with other participants represented by avatars. When someone moves their head, so does the avatar. “That person knows exactly where someone else is looking,” said Eric Romo, the founder and CEO of AltspaceVR. In one example, the participants were playing volleyball, including this reporter, and virtually knocking a ball around. But the system can also be used in a business environment. AltspaceVR can support Web browsers and connect people globally in a virtual meeting environment. Participants will feel “like we’re all in the same space,” Romo said. A virtual reality conferencing system is a potential alternative, or possibly the future, of telepresence technologies. The system was displayed at Intel’s large showroom space at CES and was using its RealSense 3D camera system, designed for virtual meetings. Services were part of the mix at CES, including Lexifone, which offers in-call translations in these languages: English-U.S., as well as English used in the U.K., and Australia; French Canadian, French, German, Hebrew, Italian, Mandarin-China, Mandarin-Taiwan, Polish, and Portuguese both Brazil and EU, as well as Russian, Spanish-Mexico, Spanish-Spain. A users dials into Lexifone, identifies the language that needs translation, and then reoriginates the call, or dials out to the other party. When a caller stops talking, the cloud-based system, after an approximately four-second pause, will translate. The accuracy for languages such as Spanish can be as high as 95%, and for Mandarin, the most difficult, about 70%, but it’s improving, said Patrick Tata, vice president of marketing. The service is available to consumers as well as businesses. Robots may become an increasingly common sight, both in commercial and home settings, if the trends at CES are any sign. One Korean company, Future Robot, is building systems that include facial recognition that can also sense emotion, such as happiness or anger. These robot systems are more likely to find their way into customer service uses such as retail and other venues that involve many people. Robots are also being turned into interesting toys. When it’s released in August, the Meccanoid G15 KS from Spin Master, the same company that produces Erector sets, will come in a kit to assemble for kids ages 10 and above. It will be priced at $399."
185,https://www.computerworld.com/article/1493252/report-drones-suck-at-u-s-border-patrol-or-vice-versa.html,ComputerWorld,2015,1,7,813.0," Drones owned by consumers can be so intrusive that a woman in Federal Way, Wash., filed a lawsuit charging her neighbor with “drone stalking.” Waves of complaints have been filed in San Francisco, Hawaii, a range of beaches and elsewhere about the invasion of privacy from what amounts to a flying camera zipping through private yards, balconies. Ordinary people may be having trouble getting away from drones made for consumers, but the border patrol is having trouble getting sophisticated military drones anywhere near bad guys, according to a report today from the Office of the Inspector General (OIG) of the Dept. of Homeland security. U.S. Customs and Border Protection (CBP) has spent eight years and “hundreds of millions of taxpayer dollars” but drastically understated the actual cost of the drones and has yet to prove that its fleet of Predators is worth even the cost that it admits to. The OIG’s report recommends that border patrol drop plans to spend another $443 million on even more Predators and, instead, “put those funds to better use.” This is the second time an OIG audit slammed CBP, its fleet of drones, its ability to report what it costs to fly them and even the things it claims the drones are doing. During 2013, for example, CBP “touted drone surveillance of the entire Southwest Border,” 1,993 miles of often rugged, empty country running from California to Texas. The majority of the Predator flights were alone one 100-mile long stretch of border in Arizona and another, 70 miles long, in Texas, the report said. That may just be apple polishing – a little exaggeration to emphasize the value of drones that patrol the 8.53 percent of the border that is critical to whatever the CBP things those drones are accomplishing. The OIG’s office thinks the Predators were in the air only 22 percent as often as they were supposed to be, and got credit for an assist in only 2 percent of the arrest of illegal border crossers. A much bigger red flag waves from the section of the report discussing the cost and management of $20 million drones. The border patrol division that runs the Predators is called the Office of Air and Marine (OAM). The OAM is also the group that does the Predator accounting, and reported during 2013 that it spent $2,468 per hour to operate each Predator. However, “OIG found the actual price tag to be $12,255 per hour, noting that OAM omitted such key costs as salaries for operators, equipment and overhead.” None of those are the kind of inconsequential, esoteric pocket change that could easily be forgotten by patriots focusing all their attention on protecting freedom, not pinching pennies. The costs the OIG report cited are all pretty large, pretty obvious expenses that are pretty routinely included in the accounting at organizations about which few people use the word “malfeasance.” The OIG didn’t use that word. It also didn’t use the word “incompetent,” though it had to work pretty hard to avoid it. The OIG did repeat its charge from the 2012 audit that the CBP doesn’t focus a lot on process. It does things like buy its first pack of Predator drones without preparing any more thoroughly than they might have done if the vehicles they acquired were jeeps you can park and walk away from and not finicky, high-maintenance drones developed by the same Air Force that doesn’t think it’s a bad trade to do 30 hours of maintenance on an F-22 for every hour it spends in the air. The CBP also forgot to make set criteria to decide what made a mission successful, metrics to demonstrate whether they’d met those criteria and supervisors trained in both the mission and in the operation and maintenance of the Predators. Somewhat more fundamentally, the CBP had also failed to establish airfields sufficient and well equipped enough to launch and recover the drones, control them in the air and do all the maintenance required to get them back in the air eventually. “On at least three occasions, NASOC Grand Forks could not conduct flight operations because maintenance could not be performed due to a lack of ground support equipment,” the 2012 audit read. By the time of the 2015 report, the CBP “still has no reliable method of measuring its performance and that its impact in stemming immigration has been minimal.” In an unusually frank dismissal of the CBP’s efforts, Inspector General John Roth said in the report that, “notwithstanding the significant investment, we see no evidence that the drones contribute to a more secure border, and there is no reason to invest additional taxpayer funds at this time.” In the private sector that sentence would have been followed by a recitation of the members of the unit who were taking exciting career opportunities elsewhere. It would be a long list."
186,https://www.computerworld.com/article/1491399/acoustic-levitation-next-best-thing-to-anti-gravity.html,ComputerWorld,2015,1,5,851.0," The Consumer Electronics Show (CES) started this week and has been dominating the news with revelations of gadgets that  are exactly as irrelevant to you as they were last year and the year before and every year back to the dawn of time. (Though that means the dawn of the age of of digital watches, in this case, not actual time.) Somewhere in the rush of stories about USB-enabled oven mitts and Internet of Things SmartTissues is one truly original, brilliantly conceived, cunningly designed, wickedly effective bit of technology out there somewhere, just like every year, but you won’t know for sure what it is until three years from now, when it will have been replaced with something better. So forget CES gadget staples like the smart ring that lets you change channels by giving the finger; forget the smart coffee grinder/brewer that must pour out magic because you can push the “brew” button from ten feet away rather than by poking it with your finger or whacking it with a ring. And especially forget all the Internet of Things things that promise to make your house smart but can’t tell you if a smart bulb is burned out, if the smart stove is still on, if the smart vacuum is chasing the dog or if the smart front door is standing open because the fracking brilliant door lock is lonely and heard there might be burglars in the area. If you want to pay attention to something cool and gadgety, with Science, check out this bit of magic from researchers in South America who made a magic wand out of sound waves. A group of researchers from Brazil and Uruguay build an acoustic levitator that can lift and move small objects without touching them and without having to leave them inside part of the levitator or rigidly locking a floating object into position a precise distance from the device. This levitator comes with a small cylindrical emitter that produces the sound and a  reflector whose business end is a small concave dish that reflects the high-frequency sound waves back toward the emitter. On their return trip toward the emitter, the original sound waves run into new ones coming from the emitter. When they collide, the two waves of sound conflict with and obstruct each other to the point that they form a standing wave — an apparently stationary set of waves that provide a consistent amount of pressure in one specific direction, with pressure points on each with enough energy to suspend an object at that point as if it were sitting on a shelf. “Just turn the levitator on and it is ready,” according to lead author Marco A. B. Andrade of the Institute of Physics at the University of Sao Paolo, who was quoted in the announcement describing the acoustic levitator that was published in the journal Applied Physics Letters. The objects can’t be very big, at least not yet. The objects in the test were polystyrene blobs 3mm across that don’t weigh enough to feel their weight on your finger, but they should get bigger over time as researchers figure out how to strengthen and balance the acoustic wave more consistently. In May a group of British researchers from an organization called the Engineering and Physical Sciences Research Council, which is affiliated with four universities, developed an acoustic tweezers they could use to pick strings of cartilage cells from the surface of a Petri dish and implant them precisely into place within a wound. They developed the technique to help with knee surgeries during which surgeons could use the acoustic tweezers to mold a string of cartilage cells into precisely the right shape to replace a missing tendon or ligament. “Ultrasonic tweezers can provide what is, in effect, a zero-gravity environment perfect for optimizing cell growth,” researcher Martyn Hill of  the University of Southampton in Southampton, U.K. said in a statement. “As well as levitating cells, the tweezers can make sure that the cell agglomerates maintain a flat shape ideal for nutrient absorption. They can even gently massage the agglomerates in a way that encourages cartilage tissue formation.” Acoustic levitation has been known for almost a century, but the first demonstration of a technique that could raise and hold and object in place was first demonstrated successfully in July, 2013 by researchers at the Swiss technical university Eidgenössische Technische Hochschule Zürich. In January, 2014 a group of researchers from the University of Tokyo published a paper demonstrating a technique to control particles in three dimensions using acoustic waves that lift tiny objects and hold them in a pattern extending into three dimensions, and raise, lower or rotate the whole flight in unison. The breakthrough for Andrade’s team was the ability to suspend particles while the emitter and reflector moved, leaving an inconsistent distance between them. The next step, Andrade says, is weight and practical application. “Modern factories have hundreds of robots to move parts from one place to another,” according to Andrade’s statement. “Why not try to do the same without touching the parts to be transported?”"
187,https://www.computerworld.com/article/1490471/stanford-launches-100-year-study-of-artificial-intelligence.html,ComputerWorld,2014,12,24,580.0," What will intelligent machines mean for society and the economy in 30, 50 or even 100 years from now? That’s the question that Stanford University scientists are hoping to take on with a new project, the One Hundred Year Study on Artificial Intelligence (AI100). The university is inviting artificial intelligence researchers, roboticists and other scientists to begin what they hope will be a long term – 100 years long – effort to study and anticipate the effects of advancing artificial intelligence (AI) technology  . Scientists want to consider how machines that perceive, learn and reason will affect the way people live, work and communicate. “If your goal is to create a process that looks ahead 30 to 50 to 70 years, it’s not altogether clear what artificial intelligence will mean, or how you would study it,” said Russ Altman, a professor of bioengineering and computer science at Stanford. “But it’s a pretty good bet that Stanford will be around, and that whatever is important at the time, the university will be involved in it.” The future, and potential, of artificial intelligence has come under fire and increasing scrutiny in the past several months after both renowned physicist, cosmologist and author Stephen Hawking and high-tech entrepreneur Elon Musk warned of what they perceive as a  mounting danger from developing AI technology. Musk, speaking at an MIT symposium in October, said scientists should be careful about developing AI technology.  “If I were to guess at what our biggest existential threat is, it’s probably that,” said Musk, CEO of electric car maker Tesla Motors, and CEO and co-founder of the commercial space flight company SpaceX. “With artificial intelligence, we are summoning the demon. In all those stories with the guy with the pentagram and the holy water, and he’s sure he can control the demon. It doesn’t work out.” Hawking added to the conversation in an interview with the BBC,, saying scientists should be cautious about creating machines that could one day be smarter and stronger than humans. “It would take off on its own and re-design itself at an ever-increasing rate,” Hawking said in the interview. “Humans, who are limited by slow biological evolution, couldn’t compete, and would be superseded.” Stanford’s AI project appears to be more focused on what AI can add to society, though the project is looking to keep an eye on development and any direction that might take. “I’m very optimistic about the future and see great value ahead for humanity with advances in [AI],” said Stanford alumnus Eric Horvitz, director of Microsoft Research’s main lab. “However, it is difficult to anticipate all of the opportunities and issues, so we need to create an enduring process.” Tom Mitchell, a professor and chairman of the machine learning department at Carnegie Mellon University, said AI is progressing and it’s wise to try to stay a step or two ahead of it. “We won’t be putting the genie back in the bottle,” he said in a statement. “AI technology is progressing along so many directions and progress is being driven by so many different organizations that it is bound to continue. AI100 is an innovative and far-sighted response to this trend – an opportunity for us as a society to determine the path of our future and not to simply let it unfold unawares.” The project, at this point, will include scientists from Harvard University, the University of California at Berkeley, Carnegie Mellon University, the University of British Columbia and Stanford."
188,https://www.computerworld.com/article/1660766/make-in-india-vaultize-technologies.html,ComputerWorld,2014,12,21,987.0," In 6820, Anand Kekre and Ankur Panchbudhe built Vaultize Technologies, a security technology company that looks into solutions for unstructured data. Anand, as the CEO, drives vision, strategy, and business execution, while Ankur handles the product development and technology innovation side of things in the role of the CTO. The founders are IIT-Bombay graduates and have worked together in different roles with global MNCs—Symantec and McAfee—for over decade. The duo, however, wanted to take their journey to the next level. Today, Pune-headquartered Vaultize has hundreds of enterprise customers spread across verticals like banks, financial institutes, manufacturing, and media in India as well as across the US, Singapore and the Middle East. In an exclusive interview with CIO India, Anand Kekre, co-founder and CEO, Vaultize Technologies, talks about how Indian startups can create a footprint today’s competitive global tech landscape. In a world of MNCs, which spectrum of technology is addressed by Vaultize? Kekre: We are focused on unstructured data which pertains to files and e-mail in an organization. The company was named keeping the same in mind; Vaultize standing for securing unstructured data. Over the last few years, unstructured data is moving out of the control of enterprise IT. Today, everybody is more mobile. Data is moving out of file servers, SharePoint, and desktops and onto mobile devices. The security perimeter limited to corporate boundaries earlier have fallen off. We are an end-to-end data security player and ensure the security of unstructured data from the time a file is created to the time it is shared or accessed, within or outside an organization. We help keep corporate data under the control of organizations even after it has been downloaded by third-party recipients to their own devices. Have you overcome the teething issues that Indian startups generally face? Kekre: Narendra Modi’s Make in India campaign largely relates to manufacturing, but it also extends to information technology. We are a 100 percent Indian company with our complete R&D team based here. We also have sales offices in the US and Singapore, but our core management team operates out of India. Ankur, the co-founder, and I wrote the first product in 6820 launching our platform for the enterprise IT community. But it was not easy to gain customer validation in India. File sharing was a new concept as organizations often correlated our solutions with endpoint data protection and encryption. However, for the last 18 months, we have seen a significant demand for collaboration and file sharing, and a slowdown in endpoint protection solutions. Organizations with data loss and compliance risks demand solutions from an end-to-end data security provider like us. We got funded by the Tata Capital Innovation Fund in 2013 which helped us fuel our expansion plans at a faster pace. Did proliferation of mobile devices in India trigger the market acceptance of Vaultize? Kekre: Precisely. When we first pitched our solutions in India, the concept of BYOD was picking up in countries like the US. This posed challenges with consumer file-sharing platforms like Dropbox. Employees used Dropbox not only for photos or music, but they also shared corporate data through it. We believed that verticals like pharma, BFSI or the more security-conscious ones like IP or compliance-related would face these challenges too. CIOs do not want their confidential documents to be shared on a third-party cloud. Companies in India are fast realizing the importance of a secure solution to access and share their corporate data with complete IT control. How does Vaultize stack against the competition from well-established security giants? Kekre: Vaultize offers control, visibility, and monitoring of IT that permits individual users to access, share, and edit data on any device. It has the flavor of enterprise file sync and share (like EMC and Citrix), of end-point protection/backup (like Symantec and Commvault), and a secure access from anywhere. Hence, the competition for us depends on the context. As an enterprise platform that provides an entire suite, none of the competitors come close to us. We call ourselves the Blackberry for files. Just as Blackberry relays e-mails securely to your handheld, we too relay files in a secure manner. We also have solutions for FTP replacement, virtual data room (VDR), managed mobile collaboration, and mobile content management. There is no other single-product company today that hands over the entire control of unstructured data to customers. Going global means a cloud roadmap is a must-have for modern technology companies. Kekre: Vaultize is architected as a scalable cloud that can be deployed on-premise or as a private cloud. We have optimized our architecture for on-premise deployments in the form of a purpose-built appliance called cloud-in-a-box. Also, we have a public cloud hosted on Amazon Web Service that works well for small and medium businesses. We work with three MSPs in the US and with one in Singapore. We are open to align with MSPs in India too. We provide hybrid functionality as many customers subscribe to AWS and want some amount of storage on-premise or some on the cloud. The Vaultize architecture is platform-, storage-, and cloud-agnostic. Indian-bred product companies like iFlex, Tally and Cyberoam have created a mark in the enterprise IT space. What are your future aspirations? Kekre: Having worked for over a decade with IBM, Symantec, and McAfee, our ambition was building world-class, cutting-edge solutions in India which would serve the entire world. We did not work in isolation but clearly understood the pain points of customers across the world while developing the product. Conducting POCs in the first two months with large players across verticals has led to a few of them testing the waters too. After three years of rolling out our flagship product, we are now a global security company with offices in the US and Singapore. We also address the security market in the UK and the Middle East. We will soon expand to Australia and more geographies of Europe, APAC, and Africa."
189,https://www.computerworld.com/article/1488482/army-lab-asks-help-building-wing-flapping-robot-fly.html,ComputerWorld,2014,12,19,714.0," Researchers at the U.S. Army are taking advantage of an unusually unclassified approach to military systems development to ask for help turning a clever robotic fly into an almost undetectable spy. The robotic flies are – or will be – semi-autonomous robots that look like real bugs and fly using wings that flap without being controlled by a motor. Instead, the wings are made from a material known as PZT that generates an electrical charge when it’s deformed – and changes its own shape when electricity is applied to it. The material PZT – lead zirconate titanate – is a ceramic perovskite with strong piezoelectric properties, meaning that it develops an electrical charge under pressure, or when a temperature difference develops between two surfaces. That property has made them popular with researchers looking for ways to power nano-scale machines so small that there would be no room for connections to a battery, let alone a battery. Researchers at the U.S. Army Research Laboratory (ARL) in Adelphi, Md. have already come up with one set of  “insect-inspired” flying microbots designed to accompany soldiers in the field as remote-controlled scouts small enough to go anywhere and able to navigate on their own with or without GPS and to carry laser range finders, cameras, altimeters and other sensors able to send back critical information. Most are quadrotor fliers that use ultrasonic motors developed at the Army Research Lab that are as small as three millimeters in diameter. The smallest ‘bot currently flying weighs 20 grams. It is a moose compared to the far tinier metal bugs being developed by another ARL team led by Ronald G. Polcawich, who holds a Ph.D. in materials science and engineering who has spent more than a decade developing ways to get smart-, meta- or weird materials to do the job of motors, hinges and other mechanical bits on machines too small to fit them. Polcawich – who leads the Piezoelectric-Micro Electro-Mechanical Systems (PiezoMEMS) Technology team Polcawich leads at ARL – has succeeded in building a set of robotic legs for a millipede-like robot that will crawl when voltage is applied. It has also built wings one- to two inches long made of PZT that bend, flap and create lift when voltage is applied, “so we know this structure has the potential to fly,” Polcawich said in an Army press release about a program Polcawich is using to ask for help taking the next micro-robotic, multi-legged step forward. The “Open Campus” program at ARL that is designed to bolster shrinking R&D budgets and accelerate development of innovative new systems by inviting civilian developers and investors in to help advance them, according to a release reporting 450 university and corporate scientists attended the conference that opened the program Dec. 15 and 16 in Adelphi. The Open Program would allow outsiders to participate in military development projects under formal Cooperative Research and Development Agreements that would allow the publication of research in peer-reviewed journals, allow outside labs to work on joint projects off campus and allow some ARL staffers to work with outside partners during sabbatical- or “entrepreneurial leave.” The program is an Army effort to counter cuts in funding, but follows the principle that researchers who share their results openly and collaborate on problems one can’t solve alone make more progress and solve more complex problems than any single scientist or research lab can accomplish alone, according a statement quoting Selena Russell, an ARL researcher with a long list of publications about her work developing lighter, more powerful lithium batteries. Polcawich is hoping the Open Campus crowd will take an interest in his lab’s work on piezolectrics and micro-fliers and help accelerate the development of the artificial-intelligence-like “cognitive ability” so tiny a robot would need to keep itself in the air and on course. The PZT micro-fly shows promise, but is currently the lowest-funded program of the ten development projects underway in Polcawich’s lab – projects focused on IED-defeat systems, tactical radio networks, positioning, navigation and timing systems and other things that are higher priorities for the Dept. of Defense than a thumbnail-sized flying robot that may not be fully able to function on its own for 10 to 15 years. “The Open Campus effort will hopefully streamline the creative process,” Polcawich said."
190,https://www.computerworld.com/article/1660860/make-in-india-rightcloudz-2.html,ComputerWorld,2014,12,10,1546.0," In the midst of established players such as Gartner, Forrester, and IDCs of the world is one cloud intelligence agency whose single-handed mission is to empower and enable cloud customers to make the right vendor choices when buying cloud-based assets and services. Rightcloudz is an Indian startup, founded in early 2013 by a group of cloud experts—Vikas Mathur, Subhranshu Banerjee, Sreehari Narasipur, and Archana Nukal. CIO Magazine spoke to two of its co-founders, Vikas Mathur and Subhranshu Banerjee, on what sets them apart from the other big giants, their goals, and their view on the government’s Make in India initiative. You’re an Indian born entity. What sets you apart from other research agencies such as Forrester and Gartner? As an Indian startup, one of our biggest advantages is that we have a very close view of the IT requirements of the fast growing economies like India and other countries in the APAC region. As these economies grow and the companies expand their business, the cloud continues to be a very strategic requirement for them. Our complete, dedicated focus on the cloud and cloud- based services gives us that distinct advantage of being in the right place at the right time. While Forrester and Gartner have very strong capabilities and are well respected in the industry, our big advantage as a startup is that we are very nimble and quick to react to the changes in the industry. Thus we are able to monitor and update our cloud intelligence at a very fast pace. In a continuously evolving cloud industry, where new offerings and updates are available almost daily, even a quarter-old data can place an enterprise customer at a disadvantage. We believe that the currency of our data, our ability to incorporate changes as they happen, and make them visible to the customers, gives us an edge over the larger research firms. Interestingly, many fast-growing companies around the world have similar business requirements as far as cloud procurement is concerned, and being in India, we are in a unique position to present our advanced evaluation capabilities to them in a very cost-effective manner, compared to well-established players. What is Rightcloudz’s unique differentiator? RightCloudz has several unique differentiators from the customer perspective – most important of which is the capability to provide customized evaluations and rankings of cloud services based on the customer’s business priorities using an online, interactive application. Another key differentiator is our ability to let customers generate on-demand, in-depth, customized vendor evaluation reports that they can use as an input to their procurement process. These reports include many additional perspectives on the evaluation, rankings, using combinations of high priority requirements, and information on strengths/limitations of the top ranked vendors for the customer’s scenario – each of which the customer would find very difficult to create on their own. These unique offerings are made possible by several technology differentiators – chief among them being our cloud Intelligence. This is an ever extensible, growing knowledge base of up-to-date, fine-grained data about cloud vendors and their services. This knowledge base provides us the basis for objectively and consistently scoring vendor services in a manner in which they can be compared and ranked using hundreds of cloud service parameters. Finally, our patent-pending evaluation and ranking engine uses customer-specified business and technical priorities, and the information in the knowledge base to create customized evaluations for the cloud scenarios that the customer is interested in. For now, we are focusing on IaaS, the fastest growing area in the cloud services. As the industry grows and matures, we will be extending these unique offerings to PaaS and SaaS areas as well since cloud customers will start expecting customized evaluations for those services too. Your specialization lies in cloud computing. What are some of your customer offerings? All our customer offerings are focused around customized cloud vendor evaluation and ranking. Our ongoing Beta program includes the following offerings: This is an online application which lets the cloud customer do self-service cloud evaluations by defining their business priorities within the context of a cloud scenario. Customers can instantly visualize the ranked ordered of the most suitable vendors and can also see quantifiably how the vendors have fared on each parameter. We also provide professional services that can help a customer with other aspects of their journey to the cloud. Some of our service offerings include Cloud Readiness Assessment, Cloud ROI Assessment, Cloud evaluation for complex scenarios, and Cloud migration assistance. What according to you are some of the cloud trends that customers are likely to adopt in 2015? First, we expect cloud growth to continue at a very rapid pace in 2015 and beyond. As the end-user expectations from the cloud grow, we expect to see a many-fold jump in the number and the types of services available. While this is great for customers since they will have more choices, we also expect that it will become increasingly difficult to select the best vendor services from a multitude of similar-sounding services for very specific needs that an end-user organization may have. Here are some specific trends that we expect to see in 2015: 1. IaaS services will become commoditized and may see some consolidation. From an end-user perspective, focus will likely shift to PaaS and SaaS services. What do you think are some of the factors responsible for selecting one cloud vendor over another? Is price an important factor? Just like any other procurement decision an enterprise makes, there are several factors that need to be considered when selecting a cloud vendor. The importance and the priority of each factor may change from company to company and even for different cloud scenarios within the same company. Clearly, price is an important factor for most organizations, but other factors like high availability, security, and ease of use are equally, if not more important for some organizations. Many cloud customers are struggling with questions like “Is Azure more secure than AWS?” or “Is IBM better than Rackspace for storage and backup? If so, how much better?” Question like this cannot be answered by considering only price, but factors like data protection capabilities, integration with existing infrastructure, storage mechanisms, security infrastructure, high availability SLAs, disaster recovery, and other relevant factors may also need to be considered. RightCloudz provides online capabilities using which customers start with one of the pre-defined cloud scenarios that closely represents the one they are considering for buying cloud services. Each of the pre-defined scenarios includes relevant business and technical requirements and the customer can simply define priorities based on their business needs. They can also specify high priority requirements which are used in the customized report to give the customer additional evaluation perspectives around the requirements most important to them. What do you think about the government’s Make in India initiative? Don’t you think intellectual property in software is as important as hardware made in India? We strongly believe that the Make in India initiative will give the right impetus to the economy and will help create a great environment for startups like us to thrive. With the increasing focus on encouraging new investments, innovations, and entrepreneurship, the right elements for increasing industrial output and efficiency are being created. This is bound to have a positive impact on all the sectors – including IT and specifically software. Due to the positive business climate being created, we are also seeing a renewed interest by global investors in Indian startups. Recent acquisitions of the Little Eye Labs by Facebook and Impermium by Google come to mind. As such, we expect significant increase in funding available to startups – especially those in technology areas strategic to the industry. We also expect accelerators activity to pick up significantly and expect global VCs to fund several new Indian ventures as this initiative picks up steam in 2015. In addition to investment and funding, the Make in India initiative is also encouraging many among the Indian diaspora worldwide to return to India and help create new businesses here. We expect that this will result in some great talent with global experience available to startups like ours and to the industry in general. It is our view that IP in software and software processes is as important as IP in hardware. In fact, in the next few years, we expect that IP in software will probably become more important than in hardware since software is where high value innovations are likely to take place. Hardware manufacturers worldwide are struggling with low margins whereas software and software services are seeing tremendous growth due to increasing pervasiveness of technologies like cloud. Since the margins in software are expected to remain high, we expect the number of software innovations to also continue their rapid growth. Clearly, India and Indian startups stand to benefit from this since we have the right talent and technology expertise to build innovative software for global customers. It is also good to note that the IP regime in India is well connected with those in the world and initial IP filing here give the inventors world-wide protection. In fact, RightCloudz has also completed the preliminary filing of our RankCloudz IP in India and we soon expect to follow it up with other innovations as we grow our business."
191,https://www.computerworld.com/article/1485152/stephen-hawking-says-ai-could-end-human-race.html,ComputerWorld,2014,12,3,655.0," Barely a month after Elon Musk called artificial intelligence a threat to humanity, another voice – a much bigger voice in the scientific world – warned that the technology could end mankind. Stephen Hawking, the renowned physicist, cosmologist and author, in an interview with the BBC this week, said “the development of full artificial intelligence could spell the end of the human race.” The BBC noted that Hawking said the state of artificial intelligence (AI) today holds no threat, but he is concerned about scientists in the future creating technology that can surpass humans in terms of both intelligence and physical strength. “It would take off on its own, and re-design itself at an ever-increasing rate,” Hawking said. “Humans, who are limited by slow biological evolution, couldn’t compete, and would be superseded.” Hawking’s comments closely follow those made by high-tech entrepreneur Musk, who raised controversy in late October when he warned an audience at MIT about the dangers behind AI research. “I think we should be very careful about artificial intelligence,” said Musk, CEO of electric car maker Tesla Motors, and CEO and co-founder of the commercial space flight company SpaceX. “If I were to guess at what our biggest existential threat is, it’s probably that… With artificial intelligence, we are summoning the demon. In all those stories with the guy with the pentagram and the holy water, and he’s sure he can control the demon. It doesn’t work out.” Musk, who tweeted this past summer that AI is “potentially more dangerous than nukes,” also told the MIT audience that the industry needs national and international oversight. Musk’s comments raised discussion about the state of artificial intelligence, which today is more about robotic vacuum cleaners than Terminator-like robots that shoot people and take over the world. Yaser Abu-Mostafa, professor of electrical engineering and computer science at the California Institute of Technology, said he was a little surprised that AI is getting so much negative attention since the fearful talk hasn’t been preceded by the creation of a new, potentially scary technology. “We indeed have not made any huge advances in AI recently that would warrant such concern,” Abu-Mostafa told Computerworld. “One factor is that the quick advances in technological products, like cell phones, and their broad availability to everyone, even children, have made it easier for science-fiction-level predictions to be believable by the general population.” While Musk and Hawking are far from typical members of the general population, he still doesn’t agree with them. “I am not worried, not only because we are probably decades away from a superior level of machine intelligence, but also because I believe we can control it when we get there. Using the nuclear technology analogy, the fact that we now have the physical ability to destroy the entire world in minutes does not mean that it will just happen. Humans can and do put the safeguards in place to prevent that.” Some scientists do have concerns about artificial intelligence advancing beyond human control, but they admit the technology is 50 to 100 years away. That leaves plenty of time to prepare for any threatening advances in AI technology. “I actually do think this is a valid concern and it’s really an interesting one,” said Andrew Moore, dean of the School of Computer Science at Carnegie Mellon University, in a previous interview. “It’s a remote, far future danger but sometime we’re going to have to think about it. If we’re at all close to building these super-intelligent, powerful machines, we should absolutely stop and figure out what we’re doing.” Stuart Russell, a professor of electrical engineering and computer science at the University of California Berkeley, said he sees some future danger to artificial intelligence, and that’s why he’s making efforts now, organizing talks and workshops, to educate scientists about it. Now is the time to start thinking about the issue, before scientists are capable of building the machines, he said."
192,https://www.computerworld.com/article/1484736/amazon-uses-robots-to-bolster-warehouses-during-holiday-rush.html,ComputerWorld,2014,12,1,298.0," Amazon has added some additional robotic muscle and computer-vision chops to its warehouses ahead of the tide of holiday shopping orders. The retailing giant said Monday that 10 of its fulfillment centers in operation across the U.S. include these software and mechanical innovations. The total number of Kiva mobile robots, which transport large vertical racks containing inventory, has topped 15,000 across the U.S., Amazon said. The company agreed to buy Kiva Systems in 2012 and has been adding robot platforms to its warehouses to improve efficiency. Amazon also said it had deployed an inventory-moving robotic arm dubbed Robo-Stow, which according to a press photo is an M-2000iA, made by Japan’s Fanuc robotics. The machine is billed as the strongest of its kind in the world, with a lifting capacity of 1,200 kilograms, enough to move car bodies around factories. A number of companies are using robots on the shop floor and in other areas to make their operations more efficient. Contract manufacturer Foxconn, for example, uses robots to assemble some of its products. Japanese mobile carrier SoftBank has introduced its humanoid robot Pepper, which was developed by French subsidiary Aldebaran Robotics, at its smartphone stores in Japan. On Monday, Pepper also began selling coffee machines for Nestle at an electronics store in Tokyo, the first of a planned 1,000 Nestle distributors in Japan to have robotic pitchmen. Amazon is also using new computer vision systems that help complete the unloading and receipt of a trailer of inventory in only 30 minutes compared to hours without using such systems. Human workers, meanwhile, are using “new, high-end, graphically oriented computer systems” while filling orders. The company, which will hire 80,000 seasonal employees to process holiday orders, did not immediately respond to a request for more information about the warehouse improvements."
193,https://www.computerworld.com/article/1484569/close-shaves-between-aircraft-and-drones-are-on-the-upswing.html,ComputerWorld,2014,11,27,392.0," Near misses between drones and aircraft are on the increase, with pilots of airplanes sometimes having to take evasive action, according to data released by the U.S. Federal Aviation Administration to some news outlets. Pilots and controllers have reported to the agency about 150 incidents this year of drones flying close to aircraft or airports, with a surge in such incidents in recent months, The Wall Street Journal reported Wednesday. The disclosure by the FAA comes at a time when it is under pressure to allow drones, known in FAA jargon as unmanned aircraft systems, to be used for commercial purposes. Amazon.com, for example, has said it is testing the use of drones for package deliveries. The use of UAS for commercial purposes is currently prohibited though the FAA has said it plans to allow commercial operations in low-risk, controlled environments. The use of model aircraft for recreational purposes is, however, permitted but with a number of restrictions. The FAA in September exempted some aerial photo and video production companies to allow them to use drones. FAA had previously disclosed a near-collision between a drone and a passenger aircraft in March over Tallahassee, Florida. The pilot of a Piper Archer II reported in September a small red drone passing 50 to 100 feet away at an altitude of 3,000 feet (914 meters), while on descent to Portland International Airport. In another incident, US Airways Flight 5180, operated by regional carrier PSA Airlines, reported a drone passing 200 feet below the aircraft while three miles (4.8 kilometers) southeast of Norfolk International Airport. FAA could not be immediately reached for comment. New FAA rules expected by the end of this year are expected to require operators of commercial drones “to have a license and limit flights to daylight hours, below 400 feet and within sight of the person at the controls, The Wall Street Journal reported this week, citing people familiar with the rule-making process. The rules will be finalized after receiving feedback from the public, it said. The FAA is required by U.S. Congress to frame a “safe integration” plan for the commercial use of UAS by Sept. 30, 2015. The U.S. National Transportation Safety Board ruled this month in an appeal that drones are to be treated as aircraft for the purpose of the FAA’s prohibition of their careless or reckless use."
194,https://www.computerworld.com/article/1484526/robot-networked-tablets-head-to-west-africa-to-fight-ebola.html,ComputerWorld,2014,11,26,1030.0," The first robot and networked tablets are making their way today to an Ebola treatment unit in Liberia, where they will give aid workers their first chance at sharing data about the deadly outbreak. Debbie Theobald, co-founder and executive director of Cambridge, Mass.-based Vecna Cares left on a flight to Monrovia, Liberia Tuesday night, taking the company’s own CliniPaktablets, a robot and the technology needed to set up a local area wireless network. For doctors and nurses accustomed to scribbling patient notes on pieces of paper in any of the Ebola Treatment Units (ETU) scattered across West Africa, this will be the first time they’ll have access to portable computers that can share information wirelessly. It also gives them an electronic medical record system to track patients and share treatment and disease information with clinicians in other units and researchers in various countries. This also marks the first time a robot will be working in one of the treatment centers. “I think that this system is critical to fighting the outbreak,” Theobald told Computerworld. “This is the first time they’ll be using digital records at all in any of the ETUs. Everyone has been using paper. If they have had a tablet, all the information they’re capturing is stuck on that tablet because they haven’t been able to data share across tablets.” Theobald, who worked with VGo, a Nashua, N.H.-based robotic telepresence company, is focused on having the electronic medical record system — it includes the wireless network, the tablets and a VGo telepresence robot — up and functioning by Tuesday, when a new Ebola clinic is set to open in Monrovia. Vecna Cares, a healthcare IT company, will also be bringing the medical records system, minus the robot, to ETUs in Lunsar and Makeni, both towns in Sierra Leone. Depending on how well the VGo robot functions and is accepted in Monrovia, others could be sent to Sierra Leone to aid  outbreak efforts there. Theobald, who will be staying in Liberia to initially run the robot and train others to operate it, said the robot, for now, is being used as a training assistant. The telepresence robot, which has audio, a camera and a screen showing a remote user’s face, will enable the trainer, who can safely stay out of the infectious area, to help a doctor or nurse who might have questions about using the tablets or the Wi-Fi network while treating patients. “To support the doctors capturing the data, we’re bringing the robot,” she said. “Clinicians working in the unit are wearing hazmat suits and three layers of gloves…. The doctors are coming to a new place and now they’re trying to collect data with something they haven’t used before. That’s a difficult hump to get over. They need to worry about their patients and the treatments they’re giving them and not the technology. We’ll be right there beside them with the robot.” However, Theobald said she suspects that once the aid workers see how much help a telepresence robot can offer in a clinical situation, they’ll want to use VGo as more than just a training assistant. The robot, for instance, could allow nurses and doctors outside of the quarantined area to communicate with patients and clinicians inside. It also would enable them to observe patients and how they respond to treatments. “I think when we get over there and people see the opportunity of the system, I am going to get very little time on that robot,” she said. “We’ll see how much I get to drive it after that first week. I’m sure there will be lots of people who want to get in there and talk with patients.” For now, though, Vecna Cares is focused on creating more usable medical records. “Currently, in every ETU, they use paper,” said Theobald. “When a patient is triaged and moved to an isolation area, their paper records stay with them. If the patient is isolated, so is their paperwork. The doctors try to remember information about the patient and then rush out and write it down. And they’re doing that in [quarantined] patient wards that could hold anywhere from 20 to 100 beds.” While that makes it difficult to track the status of each patient, it also makes it nearly impossible to get the information needed to study the outbreak. “It’s about daily care but it’s also about learning,” added Theobald. “People are talking with each other so as we collect more data, we’re able to share that information across other treatment centers so we can improve clinical regimens and change outcomes for more people.” She also noted that the electronic records system should help researchers. Right now, when a sample, such as blood, reaches a laboratory with the National Institute of Allergy and Infectious Disease to be studied, it might arrive with a torn piece of cardboard that simply notes whether the patient was male or female and when the patient died. With shareable electronic medical records, researchers should be able to find out how long a patient was sick, his or her age, where the person was from, any other illnesses involved and treatments received. “Doing this kind of data sharing and filtering, and doing data drill downs, it just wouldn’t have been possible with the data system they’re using now,” Theobald said. Brent Terry, a vice president with VGo, said the company’s eager to find out how well the telepresence robot works in Liberia. “Depending on what they learn, we may need to make some changes,” he said. “There’s tremendous heat and moisture there at this time of year…. It’s basically in a steam bath and that could be an issue for the electronics. We may need to do some special work to bulletproof the processors to protect them from potential damage.” This effort to bring technology to the fight against the Ebola outbreak comes just weeks after technologists and aid workers from around the country held a multi-location workshop. The workshop was aimed at helping roboticists discover what kind of technology is needed now to help healthcare workers in West Africa as well as what needs to be developed in the future."
195,https://www.computerworld.com/article/1484018/cameras-robotic-mules-could-help-battle-ebola-in-west-africa.html,ComputerWorld,2014,11,20,741.0," Researchers are working on technology that could be shipped to West Africa to help fight the Ebola outbreak as soon as a few months from now, while also looking ahead to bigger plans to combat any disease outbreak. “Absolutely. This is something we can do,” said Robin Murphy, a professor of computer science and engineering at Texas A&M University and director of the Center for Robot-Assisted Search and Rescue said Wednesday. “There are lots of things we found that can go right now … but this will continue to motivate research in human-robotic interactions and how to understand how you design a new technology, how you test a new technology, how you factor in cultural context, how to factor in the targeted environments and how you train people to use them, she said.” Tech researchers from around the U.S. met with health care and aid workers nearly two weeks ago to discuss what kinds of technology, such as robotics, big data analysis or communications, could help fight the Ebola epidemic. Now plans are in the works to get the technological aid where it’s needed. The Nov. 7 workshop was livestreamed across locations at Worcester Polytechnic University, Texas A&M, the White House Office of Science and Technology Policy and the University of California at Berkeley. During the meetings, aid workers were able to explain to the researchers the obstacles they faced in using certain types of technology. A robotics researcher might, for instance, be able to send a telepresence robot to an aid station or Ebola clinic. However, questions remained on training workers to use the robot, WI-Fi access and how the robot’s batteries would be recharged. By putting the two groups together, scientists hope to offer technology that is not only needed but suited to the job and the environment. “This was all about learning and brainstorming and collecting people, and it achieved those goals,” said Taskin Padir, an assistant professor of robotics engineering and electrical and computer engineering at WPI. “There is a lot of interest in the robotics community to look at infectious disease control in the future. If you talk to some of the key players from the industry side, they have technology that is deployable, but what is the right technology at the right place?” One of the easiest technologies that could be sent to aid centers are cameras for still images and video, which can be used to collect data on tracking the disease. The cameras also could be set up outside the clinics to simply give doctors and nurses working inside a view of the lines of people waiting for help. “People can do that too, but they don’t have time,” Murphy said. “That might be one thing that can be deployed in the short term, and it will just depend on which individual researchers or companies have access to resources to send some of their technologies over.” There’s also some robotic equipment that might be ready to go with minimal or no adaptation. One possibility is the General Dynamics Land Systems MUTT, a robotic wagon that can carry patients, supplies or hazardous waste, could be shipped to aid centers. However, nothing can be simply shipped to a treatment center in a foreign country. All proposals from U.S. companies to send technology to areas hit by the Ebola outbreak must go through the U.S. Agency for International Development (USAID), which administers civilian foreign aid efforts. USAID has put out a call for proposals, and submissions are due by Dec. 1. Murphy said she’s expecting the agency to quickly act on some of the proposals so that some of the technologies can be shipped to West Africa early in the new year. “We can make suggestions and point out things, but we can’t self deploy,” she said. While researchers are looking at short-term answers for Ebola, they’re also focused on coming up with bigger, more complex systems that can be ready for outbreaks of other deadly diseases. WPI’s Padir noted that projects that might be ready in two to three years include robots that can transport blood samples to laboratories, change IV bags or distribute supplies from central storage areas to remote facilities. Working with the National Science Foundation, more meetings are planned between aid workers and technologists. “In a year or two years, there could be really cool systems, robots in new forms and shapes,” said Padir. “I’m highly motivated to keep going on this.”"
196,https://www.computerworld.com/article/1614524/qualcomms-releasing-a-robotics-platform-heres-how-it-happened.html,ComputerWorld,2014,11,13,835.0," Sometime in the spring of 2015 Qualcomm will roll out a robotics development platform. It is aimed at small startups that want to create robotic apps for their particular industries, but the bigger hope is that this hardware and software-based platform will bring some unity – and a bit of presence for Qualcomm — to what is a very fragmented industry still in its early days. Using the platform, “any developer can build their own robots. We are hoping to see this spread among the maker community in a kind of grass roots way,” Navrina Singh, the head of Qualcomm ImpaQt, Qualcomm’s global innovation program tells me. It’s no secret that Qualcomm has its eyes on robotics, built around its Snapdragon processor, of course. Last month it announced, as just one example, the Qualcomm Robotics Accelerator – a four-month mentor-based program focusing on robotics and intelligent machines via a partnership with Techstars. It’s also been working on Qualcomm Zeroth, a technology that equips robots with perceptual pattern matching functionality. Qualcomm showed how this would work with the Snapdragon Rover in a demonstration in which the the robot sorted different types of toys into the proper bins. So, the developer platform coming out next year is not a new direction for Qualcomm, but it is telling in how the company has been encouraging innovation. The short story is that this platform is the baby of three engineers whose day jobs at Qualcomm have nothing to do with robotics. Based in New Jersey, California and India, the trio work on modem software and customer engineering. But they submitted their robotics idea to ImpaQt, got funding, a little leeway in their day-to-day jobs and voila. They had a working prototype in February of this year and a fully-functional prototype in July. The longer story is that this platform was conceived under a three-year old program at Qualcomm that aims to create and nurture a “culture of innovation and collaboration” at the company, Singh explains. The program acts as an angel capitalist of sorts, providing funding for ideas and then ushering the nascent projects to the commercial phase of development when they are ready. That is where the robotics platform currently sits within Qualcomm’s innovation ecosystem. Every August Qualcomm introduces a new round of areas it wants to see developed. Employees can submit ideas, and if they find a backer – that is, a unit or department willing to sponsor the idea – Qualcomm gives them certain resources, including telepresence rooms and funding for travel. The robot development platform strayed a bit from this path in that it was funded in a parallel round outside of the normal cycle. But Qualcomm wanted to take a flyer on it, Singh says. Now, to be clear: this is not the same as Qualcomm taking a flyer on an unpromising technology with no obvious commercial application. The industry may be in its infancy but demand is surging for robots, according to new figures from the Robotic Industries Association. It reports that total of 21,235 robots, valued at $1.2 billion, were ordered from North American companies in the first nine months of this year, a 35 percent increase in units over the same period in 2013. The biggest application growth year over year has been in spot welding (76 percent), arc welding (39 percent), and assembly (29 percent). Qualcomm could, of course, still have a dud on its hands with its forthcoming developer platform. Not that Singh thinks so, especially as there is a related IoT-productivity angle to the offering. “One idea is to use it to connect to objects in the environment, a light bulb or TV or refrigerator, so they can talk and work together,” she says. But the formula Qualcomm took to get to this point – employees pitch an idea, sell it to a business unit, get funding and get to work – is spot on, she says, and more ideas are currently working their way through the pipeline. “It’s the best of both worlds. It allows risk taking at the smallest level of company without people taking a huge amount of risk.” I write about business and technology including emerging tech enterprise software/cloud computing, online privacy issues, identity theft and online security. I have written for numerous magazines and online news media in a range of styles such as features, articles, breaking news, Q&As, news briefs, business personality profiles, social trend pieces and blogs. My publication credits include CITEWorld, E-Commerce Times, CMSWire.com, Forbes.com and the Financial Times’ Foreign Direct Investment Magazine. I am very good at interviewing anyone -- from analysts to top executives – as well as research, developing sources, and making complex content accessible to the layperson. My past senior-level editing experience includes planning editorial, assigning articles, and writing in-depth features for Global Business Magazine, a  100,000 circulation monthly. The opinions expressed in this blog are those of Erika Morphy and do not necessarily represent those of IDG Communications, Inc., its parent, subsidiary or affiliated companies."
197,https://www.computerworld.com/article/1613647/robotic-spiders-may-someday-build-satellites-in-space.html,ComputerWorld,2014,11,11,654.0," By 2030, when the U.S. government or a company wants a new satellite in orbit, a robotic spider might build it in space. Six- or eight-legged robotic spiders, capable of working with a 3D printer, gripping large objects and welding them together, are expected to build satellites or small spacecraft in the lower Earth orbit. That means fewer spacecraft will be built on the ground, packed into a spacecraft and launched into space. Avoiding the stresses and expense of launch, the satellites will be less expensive, bigger and higher functioning. Dubbed SpiderFab, the project could changing the way spacecraft are built and deployed. Today, satellites and other spacecraft are built on the ground, folded up and carefully packed into rockets that send them aloft. This process requires the satellites to be protected from the vibrations and G forces during liftoff. A significant portion of the engineering and launch costs of any space project are tied up in ensuring it survives liftoff. Instead of launching entire spacecraft, engineers could send robots, barrels full of the metallic and plastic powders needed for additive manufacturing, 3D printers and the basic components needed to build a spacecraft. The prospect could means smaller, less expensive launch vehicles could be built , greatly reducing the cost of the craft’s entire life cycle, according to NASA. Tethers Unlimited is contracting with NASA to build a manufacturing platform that would work in space. Hoyt hopes that 15 to 20 years from now there will be multiple platforms in different orbits around the Earth capable of building spacecraft that will then fall into that particular orbit. Each platform would include a spider-like robot, or at least a robot with multiple arms, materials and at least one 3D printer. “We’re focusing our work now on figuring out how to, in space, make satellite components that are typically very large, like optics, solar arrays and antennas,” Hoyt told Computerworld. “That’s where on-orbit fabrication and assembly will have the best payoff … It’s so expensive to design, build and test them so they can fold up to stow in a launch vehicle and then survive the tens of Gs of vibrations and shock during launch and then to deploy in the space environment. “If you can fabricate them on orbit, you can launch the raw material in a much more compact form, and you can design them for the microgravity environment of space rather than the tens of Gs of launch,” he said. “The combination of reduction in launch volume and mass, and the ability to make it bigger than you could possibly fold up into a rocket, means you can get an order of magnitude improvements of performance in cost.” Tethers Unlimited is focused on creating what’s call a trusselator, a machine that takes spools of carbon fiber material and knits it together to build high-performance carbon fiber trusses, which can be used to build antennas or solar arrays. So far, a prototype trusselator has been used to build a truss that was more than 52-feet long. The 3D printers needed to build these large structures in space would look and function much differently. “It isn’t a typical 3D printer,” Hoyt said. “The printers people use now are typically a big box that can make something smaller than that box. We want to have a tool that can make something much, much larger than itself. We’re trying to figure out how to turn the 3D printer inside out. We need a tool that can act like a tiny spider building up a big web. And we have to figure out how to control the temperature of the materials while we’re processing them — and doing this in orbit where the temperatures vary by hundreds of degrees. That’s a big challenge we need to address.” A big step for the company will be to test the manufacturing platform on the International Space Station in several years."
198,https://www.computerworld.com/article/1612179/researchers-medical-workers-seek-tech-answers-to-ebola-outbreak.html,ComputerWorld,2014,11,7,849.0," WORCESTER, Mass. — Researchers in robotics will meet with health care and aid workers around the country Friday to get ideas on how technology could help fight the deadly Ebola outbreak, as well as the spread of other dangerous viruses. “When someone says robots, I’m old enough that this is what I see,” said Catherine Brown, a veterinarian with the Massachusetts Bureau of Infectious Disease, looking at an image of R2-D2 and C-3PO from Star Wars during a session this morning at Worcester Polytechnic Institute. “I know this is not what robotics is really like, but I’m not sure what that is… You’re always chasing the last outbreak. That’s really unfair and it’s a huge, huge problem for the countries involved. We’re going to be talking about opportunities for the robotics community to engage with the public health and the medical community.” Led by WPI and Texas A&M University, the workshops are aimed at providing a forum for health care workers to discuss with technologists what they need to better care for Ebola patients, to help stop the spread of the virus and to protect care givers from contracting the disease. The multi-location Safety Robotics for Ebola Workers was being simulcast at the White House Office of Science and Technology Policy, WPI, Texas A&M and the University of California, Berkeley. While the separate morning session at WPI was open to the press, the afternoon simulcast session was closed. At the WPI meeting, the focus was on what medical responders need to work in countries like Liberia, that have been hard hit by the outbreak. They also discussed needs in the U.S., as well as the fact that the Ebola outbreak is only one of the deadly viruses that health care workers are grappling with around the world. “There are many diseases in West Africa that are much more common than the Ebola virus,” Brown said. Being a veterinarian, she added, is helpful in dealing with a virus that was transmitted to humans from animals. “If you come from that area and have a fever, it’s much more common that you have malaria… This is an issue where we need more automated methods of doing testing.” The 2014 Ebola epidemic is the largest in history, according to the Centers for Disease Control and Prevention, largely the nations of Guinea, Liberia and Sierra Leone. As of Nov. 2, there have been 13,042 reported cases of Ebola and 4,818 related deaths around the world, the CDC reports. There have been two imported cases, including one death, and two locally acquired cases of Ebola in the U.S., the health organization said. Brown said scientists are swimming in Ebola-related documents and data and that health care researchers could use automated methods of testing for the virus, or any of a number of deadly viruses. They also could use telepresence technology, sensors to monitor the sterility of an environment, an automated or robotic way to disinfect equipment and specific area, and an automated way of handling blood being tested in labs. Several technologists were on hand to offer what they have available now and what they are working on. Jennifer Pagani, a principal engineer at Waltham, Mass.-based QinetiQ North America, a company that develops tactical robots for the military and first responders, said the best way to keep health care workers from catching the Ebola virus is to replace doctors and nurses with robots as much as possible. Dr. Julian Goldman, director of the Program on Medical Device Interoperability at Massachusetts General Hospital, said his group had just completed a three-day hackathon devoted to finding ways to make devices, such as monitors, sensors and pumps, work better together. He said his program has brought together multiple organizations to improve Ebola care by focusing on data sharing and device integration. “How could we support the safety of patients and workers in Ebola care?” he asked via a livestream presentation in. “We need to improve monitoring of people. You need rich data to see what’s happening. We need to minimize the contact with healthcare workers. It’s difficult to monitor them and reach them. Now we all rush to people at their bedside. We need to move [care givers] away to limit their risk and exposure.” He said Mass General is looking at remote monitoring technologies, as well as sensors, tele-operated robots and camera-based systems that can monitor vital signs without a nurse touching the patient. William Smart, an associate professor of mechanical engineering at Oregon State University, showed the WPI audience a video of a tele-operated robot removing a sheet from a makeshift hospital bed and folding it up. “Dealing with patients is really hard because you have to deal with people. Maybe robots could help with the cleanup, like taking sheets off a bed,” Smart said. “Even with off-the-shelf stuff we’ve got in the lab — off the shelf code — we did an OK job… With minimal training for the operator, the robot took the blanket off the bed and then folded it up. If we can control this remotely, then maybe there’s an opportunity.”"
199,https://www.computerworld.com/article/1610280/china-eyes-a-bigger-role-in-robotics.html,ComputerWorld,2014,11,3,252.0," Already a major manufacturer of electronics, China is preparing to bolster its presence in robotics with the help of government support and investments. China’s Ministry of Industry and Information Technology will develop a “robotics technology roadmap” as part of new government plans, the state-controlled Xinhua News Agency reported on Monday. The country’s goal is to establish a robotics industry that can grab over 45 percent of the high-end market by 2020, the report said. It was not clear if the ministry was referring to the domestic or global market. The ministry plans more financial support and new policies to help improve standards and robotics development. “Currently, our country’s robotics industry still has some weaknesses in its technological foundations,” an official with the ministry said, according to the report. “The key core components used are heavily reliant on imports.” Last year, China became the world’s largest buyer of industrial robots, with 36,560 units sold in the country, according to the International Federation of Robotics (IFR). But only 9,000 of those robots came from domestic vendors, although those sales were triple those of the year before. While the U.S. and Japan now have more industrial robots in operation, that will change by 2017, when China will have the most, with a projected 427,900 units, according to projections from IFR. In China, many of these industrial robots are being put to use in car manufacturing, but the government robotics to spread to other industries such as construction, logistics and food production, the Xinhua report said."
200,https://www.computerworld.com/article/1610288/andy-rubin-says-bye-bye-google-itbwgk.html,ComputerWorld,2014,10,31,180.0," Andy Rubin, key driving force behind the wildly successful Android OS, and ex-current robotics manager at Google is leaving the company for greener pastures. Latest in a series of highly publicized departures, Rubin is moving on after a restructuring blitz has placed control of many key projects into the hands of executive Sundar Pichai. In IT Blogwatch, bloggers make like bananas and split. Filling in for our humble blogwatcher Richi Jennings, is a humbler Stephen Glasskeys. Sharon Gaudin divides and conquers: So Zach Miners hatches an exit strategy: Straight from the horse’s mouth: Richard Nieva and Seth Rosenblatt are popular leaders: But Rachel King is soon deposed by a usurper: Meanwhile, Shaun Nichols convenes in Berne: You have been reading IT Blogwatch by Richi Jennings and Stephen Glasskeys, who curate the best bloggy bits, finest forums, and weirdest websites so you don’t have to. Catch the key commentary from around the Web every morning. Hatemail may be directed to @RiCHi or itbw@richi.uk. Opinions expressed may not represent those of Computerworld. Ask your doctor before reading. Your mileage may vary. E&OE."
201,https://www.computerworld.com/article/1609678/google-ai-project-apes-memory-programs-sort-of-like-a-human.html,ComputerWorld,2014,10,30,423.0," The mission of Google’s DeepMind Technologies startup is to “solve intelligence.” Now, researchers there have developed an artificial intelligence system that can mimic some of the brain’s memory skills and even program like a human. The researchers developed a kind of neural network that can use external memory, allowing it to learn and perform tasks based on stored data. Neural networks are interconnected computational “neurons.” While conventional neural networks have lacked readable and writeable memory, they have been used in machine learning and pattern-recognition applications such as computer vision and speech recognition. The so-called Neural Turing Machine (NTM) that DeepMind researchers have been working on combines a neural network controller with a memory bank, giving it the ability to learn to store and retrieve information. The system’s name refers to computer pioneer Alan Turing’s formulation of computers as machines having working memory for storage and retrieval of data. The researchers put the NTM through a series of tests including tasks such as copying and sorting blocks of data. Compared to a conventional neural net, the NTM was able to learn faster and copy longer data sequences with fewer errors. They found that its approach to the problem was comparable to that of a human programmer working in a low-level programming language. The NTM “can infer simple algorithms such as copying, sorting and associative recall from input and output examples,” DeepMind’s Alex Graves, Greg Wayne and Ivo Danihelka wrote in a research paper available on the arXiv repository. “Our experiments demonstrate that it is capable of learning simple algorithms from example data and of using these algorithms to generalize well outside its training regime.” A spokesman for Google declined to provide more information about the project, saying only that the research is “quite a few layers down from practical applications.” In a 2013 paper, Graves and colleagues showed how they had used a technique known as deep reinforcement learning to get DeepMind software to learn to play seven classic Atari 2600 video games, some better than a human expert, with the only input being information visible on the game screen. Google confirmed earlier this year that it had acquired London-based DeepMind Technologies, founded in 2011 as an artificial intelligence company. The move is expected to have a major role in advancing the search giant’s research into robotics, self-driving cars and smart-home technologies. More recently, DeepMind co-founder Demis Hassabis wrote in a blog post that Google is partnering with artificial intelligence researchers from Oxford University to study topics including image recognition and natural language understanding."
202,https://www.computerworld.com/article/1609577/ai-researchers-say-elon-musks-fears-not-completely-crazy.html,ComputerWorld,2014,10,29,1154.0," High-tech entrepreneur Elon Musk made headlines when he said artificial intelligence research is a danger to humanity, but researchers from some of the top U.S. universities say he’s not so far off the mark. “At first I was surprised and then I thought, ‘this is not completely crazy,’ ” said Andrew Moore, dean of the School of Computer Science at Carnegie Mellon University. “I actually do think this is a valid concern and it’s really an interesting one. It’s a remote, far future danger but sometime we’re going to have to think about it. If we’re at all close to building these super-intelligent, powerful machines, we should absolutely stop and figure out what we’re doing.” Musk, most well-known as the CEO of electric car maker Tesla Motors, and CEO and co-founder of SpaceX , caused a stir after he told an audience at an MIT symposium  that artificial intelligence (AI), and research into it, poses a threat to humans. “I think we should be very careful about artificial intelligence,” Musk said when answering a question about the state of AI. “If I were to guess at what our biggest existential threat is, it’s probably that… With artificial intelligence, we are summoning the demon. In all those stories with the guy with the pentagram and the holy water, and he’s sure he can control the demon. It doesn’t work out.” He added that there should be regulatory oversight — at the national and international level — to “make sure we don’t do something very foolish.” Musk’s comments came after he tweeted  in early August that AI is “potentially more dangerous than nukes.” His comments brought images of movies like The Terminator and Battlestar Galactica to mind. The science-fiction robots, stronger and more adaptable than humans, threw off their human-imposed shackles and turned on people. The statements come from the man who founded Tesla Motors, a company that has developed an Autopilot feature for its dual-motor Model S sedan. The Autopilot software is designed to enable the car to steer to stay within a lane and manage speed by reading road signs. Analysts and scientists disagree on whether this is artificial intelligence. Some say it’s not quite AI technology but is a step in that direction, while others say the autonomy aspect of it goes into the AI bucket. Last month, Musk, along with Facebook co-founder Mark Zuckerberg and actor and entrepreneur Ashton Kutcher, teamed to make a $40 million investment in Vicarious FPC, a company that claims to be building the next generation of AI algorithms. Musk told a CNN.com reporter that he made the investment “to keep an eye” on AI researchers. For Sonia Chernova, director of the Robot Autonomy and Interactive Learning lab in the Robotics Engineering Program at Worcester Polytechnic Institute, it’s important to delineate between different levels of artificial intelligence. “There is a concern with certain systems, but it’s important to understand that the average person doesn’t understand how prevalent AI is,” Chernova said. She noted that AI research is used in email to filter out spam. Google uses it for its Maps service, and apps that make movie and restaurant recommendations also use it. “There’s really no risk there,” Chernova said. “I think [Musk’s] comments were very broad and I really don’t agree there. His definition of AI is a little more than what we really have working. AI has been around since the 1950s. We’re now getting to the point where we can do image processing pretty well, but we’re so far away from making anything that can reason.” She said researchers might be as much as 100 years from building an intelligent system. Other researchers disagree on how far they might be from creating a self-aware, intelligent machine. At the earliest, it might be 20 years away, or 50 or, even 100 years away. The one point they agree on is that it’s not happening tomorrow. However, that doesn’t mean we shouldn’t be thinking about how to handle the creation of sentient systems now, said Yaser Abu-Mostafa , professor of electrical engineering and computer science at the California Institute of Technology. Scientists today need to focus on creating systems that humans will always be able to control. “Having a machine that is evil and takes over… that cannot possibly happen without us allowing it,” said Abu-Mostafa. “There are safeguards… If you go through the scenario of a machine that wants to take over or destroy the world, it’s a nice science-fiction scenario, as long as we don’t allow a system to control itself.” He added that some concern about AI is justified. “Take nuclear research. Clearly it’s very dangerous and can lead to great harm but the danger is in the use of the results not in the research itself,” Abu-Mostafa said. “You can’t say nuclear research is bad so you shouldn’t do it. The idea is to do the research and understand the facts and then have controls in place so the research is not abused. If we don’t do the research, others will do the research.” The nuclear research program offers another lesson, according to Stuart Russell, a professor of electrical engineering and computer science at the University of California Berkeley. Russell, who focuses his research on robotics and artificial intelligence, said, that like other fields, AI researchers have to take risk into account because there is risk involved – maybe not today but likely some day. “The underlying point [Musk] is making is something that dozens of people have made since the 1960s,” Russell said. “If you build machines that are more intelligent than people, you might not be able to control them. Sci-fi says they might develop some evil intent or they might develop a consciousness. I don’t see that being an issue, but there are things we don’t have a good handle on.” For instance, Russell noted that as machines become more intelligent and more capable, they simultaneously need to understand human values so when they’re acting on humans’ behalf, they don’t harm people. The Berkeley scientist wants to make sure that AI researchers consider this as they move forward. He’s communicating with students about it, organizing workshops and giving talks. “We have to start thinking about the problem now,” Russell said. “When you think nuclear fusion research, the first thing you think of is containment. You need to get energy out without creating a hydrogen bomb. The same would be true for AI. If we don’t know how to control AI… it would be like making a hydrogen bomb. They would be much more dangerous than they are useful.” To create artificial intelligence safely, Russell said researchers need to begin having the necessary discussions now. “If we can’t do it safely, then we shouldn’t do it,” he said. “We can do it safely, yes. These are technical, mathematical problems and they can be solved but right now we don’t have that solution.”"
203,https://www.computerworld.com/article/1607386/automation-arrives-at-restaurants-but-dont-blame-rising-minimum-wages.html,ComputerWorld,2014,10,24,629.0," McDonald’s this week told financial analysts of its plans to install self-ordering kiosks and mobile ordering at its restaurants. It isn’t the only food chain doing this. The company that owns Chili’s Grill & Bar also said this week it will complete a tablet ordering system rollout next month at its U.S. restaurants. Applebee’s announced last December that it would deliver tablets to 1,800 restaurants this year. The pace of self-ordering system deployments appears to be gaining speed. But there’s a political element to this and it’s best to address it quickly. The move toward more automation comes at the same time pressure to raise minimum wages is growing. A Wall Street Journal editorial this week, “Minimum Wage Backfire,” said that while it may be true for McDonald’s to say that its tech plans will improve customer experience, the move is also “a convenient way…to justify a reduction in the chain’s global workforce.” The Journal faulted those who believe that raising fast food wages will boost stagnant incomes. “The result of their agitation will be more jobs for machines and fewer for the least skilled workers,” it wrote. The elimination of jobs because of automation will happen anyway. Gartner says software and robots will replace one third of all workers by 2025, and that includes many high-skilled jobs, too. Automation is hardly new to retail. Banks rely on ATMs, and grocery stores, including Walmart, have deployed self-service checkouts. But McDonald’s hasn’t changed its basic system of taking orders since its founding in the 1950s, said Darren Tristano, executive vice president of Technomic, a research group focused on the restaurant industry. The move to kiosk and mobile ordering, said Tristano, is happening because it will improve order accuracy, speed up service and has the potential of reducing labor cost, which can account for about 30% of costs. But automated self-service is a convenience that’s now expected, particularly among younger customers, he said. “It’s keeping up with the times, and the (McDonald’s) franchises are going to clamor for it,” said Tristano, who said any labor savings is actually at the bottom of the list of reasons restaurants are putting in these self-service systems. McDonald’s is already deploying mobile ordering in other countries. In France, you can order a McDonald’s hamburger from a mobile device, tablet or desktop and pick it up later at a restaurant, said Thomas Husson, a Forrester analyst in a report. “By reducing the stress of the ordering, McDonald’s has significantly increased the average revenue per order,” wrote Husson of the experience in France. Chili’s has deployed some 45,000 tablets from Ziosk, which makes the system. Ziosk CEO Austen Mulinder says the tablets are used for ordering drinks, appetizers and desserts and for making payments, but they remain optional for customers. The waitstaff will take the first drink and entrée orders, which are often modified by people at the table. Mulinder said there’s no capital cost to installation, and the multi-year subscription price for the system is more than offset by  increased revenues it generates. The tablet also includes games and an opportunity for people to give feedback, and to join a loyalty program. That creates the potential for increased sales, because customers aren’t necessarily waiting to catch an employee’s attention to refill a drink. It also avoids the frustration of waiting for the check, said Mulinder. “Restaurants want to speak the language of the millennials and the language of millennials is digital,” said Mulinder. Wyman Roberts, the CEO of Chili’s parent company, Brinker International, spoke to financial analysts this week in a conference call about the new system. “We’re excited about the potential this has to create a stronger connection and smarter interactivity between us and our guests,” said Roberts, according to a transcript by Seeking Alpha."
204,https://www.computerworld.com/article/1605989/ask-watson-or-siri-artificial-intelligence-is-as-elusive-as-ever.html,ComputerWorld,2014,10,20,1229.0," In 1966, some MIT researchers reckoned that they could develop computer vision as a summer project, perhaps even get a few smart undergrads to complete the task. The world has been working on the problem ever since. Computer vision is where computers recognize objects like people do. That’s a tree. He’s Carlos. And so on. It’s one of a number of tasks we consider essential for generalized artificial intelligence, in which machines can act and reason as humans do. While we’ve been making some considerable headway in computer vision, especially in recent years, that it has taken 50 years longer than expected shows why AI (artificial intelligence) is such a difficult and elusive goal. “How much progress is being made? It’s really hard to get a handle on that,” said Beau Cronin, a Salesforce.com product manager working on AI-influenced technologies for the company. Cronin spoke Friday at the O’Reilly Strata + Hadoop World conference in New York. The main theme of the conference was big data. The need for big data analytics has given AI research a shot in the arm. Today the titans of the Internet industry — Apple, Google, Facebook, Microsoft, IBM — are putting AI research into the driver’s seat, pushing forward the state of the art for seemingly routine tasks such as ad targeting and personalized assistance. But in many ways, we are no closer to achieving an overall general artificial intelligence, in the sense that a computer can behave like a human, Cronin observed. Systems that use AI technologies, such as machine learning, are defined to execute very narrowly defined tasks. The state of AI has always been hard to assess, Cronin said. AI systems are hard to evaluate: They may excel in one area but fall short in another, similar task. Many projects, even sometimes very well-funded ones, go nowhere. Even basic definitions of AI are still not locked down. When two people talk about AI, one may be referring to a specific machine learning an algorithm while the other may be talking about autonomous robots. AI still attracts oddballs, lone wolves working in their basements 10 hours a week hoping to solve the AI problem once and for all. The overambitious “Summer of Vision” MIT project in the 1960s pointed out one of the major stumbling blocks for AI research, called Moravec’s Paradox. Moravec’s Paradox asserts that things that are easy for people to do — object recognition and perception — are extremely difficult for computers to do, while simple tasks for computers — proving complex theorems — are extremely difficult if not impossible for people to do (some present readers excluded, no doubt). The waves of hype around getting machines to think, and the subsequent disillusions borne of the marginal results, led the field to go through a number of what have been calledAI Winters, in which research funding dries up, and progress slows. We probably will not see another AI winter, if only because too many large companies, notably Google and Facebook, are basing their business models on using intelligence computing to better intuit what their users are looking for, Cronin said. Other companies offer AI-assisted technologies, such as such as Apple with Siri, and IBM with Watson. In many ways, today’s AI systems are a direct lineage of the first AI systems built in the 1960s, such as the Eliza — the psychiatric advice-dispensing program still used for some Twitterbots today — and Perceptron, one of the first precursors to deep-learning neural networks. Such early AI systems were “deeply flaw and limited. They were just very basic in their capabilities,” Cronin said. Nonetheless, “you can draw a direct lines from those early systems to the work we’re doing today in AI,” he observed. “Watson is what we wished Eliza would be.” After years of very little progress, though, we are increasingly becoming awash in ever-more astounding forms of AI-like assistance for specific tasks. The pace of advance is happening at a rate that have surprised “even people who have been in the field for a long time,” Cronin said. Self-driving vehicles, on the precipice of becoming commercially available, were considered to be almost an unachievable technology as little as 10 years ago. Perhaps this is due to the change in funding for AI research. Governments with research money to spare have always invested in researchers with grand ambitions. And for many years, small commercial research organizations such as SRI International and Cycorp moved forward the state of the art. These days, AI research has benefactors across most of the major IT and Internet companies, such as Google, Facebook and Microsoft Research. Many smaller startups, flush with venture capital, are also pushing the envelope. “The work is increasingly applied to commercial [projects] rather than academic” ones, Cronin said. As a result, AI technologies are now operating at larger scales than they ever did in the academic days. “Deep learning on its own, done in academia, doesn’t have the [same] impact as when it is brought into Google, scaled and built into a new product.” As a result, AI methods, such as machine learning, are now being integrated into commercial services and products, at a speedier pace than ever before. Cronin noted that Watson and Siri are more notable as “big integration projects” than for pioneering new forms of intelligence. The growing influx of big data has helped the field, as well, introducing inferencing and other statistical methods that few would have predicted would play a such powerful role in technology, Cronin said. In the olden days of academic AI research, the amount of data that could be used to reason against was relatively sparse, compared to the mountains of stuff we have today. Google has made bank from its massive set of data on its users, which it collected first and figured out how to make money from later. The company didn’t get initially get hung up on “putting a lot of structure in the model,” Cronin said. This has been termed by Google engineers as the “unreasonable effectiveness of data.” In the long haul, however, we will have to put more thought into deeper learning techniques than we have now, Cronin said. Today’s methods just aren’t going to get us to full artificial intelligence. “We need richer, more predictive models,” Conin said, ones that can “routinely make predictions of what will happen.” One member of the audience, Juan Pablo Velez, a data analyst at New York data science consultancy firm Polynumeral, agreed with Cronin’s assessment of AI. “A lot of new innovation has come around in deep learning that has been rolled out in scale, like Google image search. But the research is very much tied to the agendas of big companies and it doesn’t necessarily mean we are any closer to generalized machine intelligence,” Velez said. In many ways, we are at the same point in AI research where we’ve always been: moving forward rapidly in some aspects, while seemingly standing still in relation to the big goal, generalized artificial intelligence. As Facebook head of AI research Yann LeCun has said, AI research is like driving fast in the fog, where you can’t see the next roadblock you will hit. Until the day when we build a machine to look ahead into the fog for us, the future of AI will be uncertain for some time to come."
205,https://www.computerworld.com/article/1605600/researchers-to-meet-with-aid-workers-to-build-ebola-fighting-robots.html,ComputerWorld,2014,10,17,1221.0," Robotics researchers from around the country are working together to come up with technology that could help fight the deadly Ebola outbreak. Scientists are considering telepresence robots that could act as rolling interpreters, autonomous vehicles that could deliver food and medicine, and robots that could decontaminate equipment and help bury the victims of Ebola. “What are the things robotics can do to help?” asked Robin Murphy, a professor of computer science and engineering at Texas A&M University and director of the Center for Robot-Assisted Search and Rescue. Robotocists need to learn from the medical and humanitarian communities how the robotic machines can be used to help in this crisis, she said. To bring together health care workers, relief workers and roboticists, Murphy, is helping to set up a multi-location workshop on Nov. 7. At this point, the meetings, Safety Robotics for Ebola Workers, are set to be co-hosted by the White House Office of Science and Technology Policy, Texas A&M, Worcester Polytechnic Institute and the University of California, Berkeley. The workshops, which are expected to be simulcast, will include medical responders and academic researchers, as well as commercial robotics companies. Murphy told Computerworld she wants the robotics people to hear directly from those who have been working on the  to learn what’s needed to help patients, to stem the spread of the virus and to protect aid workers from infection. “The workshop is for us to shut up and listen to them and take what we hear them say and use it,” Murphy said. “They’ll talk about what they need and then we can talk about what we can offer… What can we do in the next few months and then what do we need to do in the longer term? What should we have five years from now?” The Ebola outbreak is the largest in history, striking several countries in West Africa, infecting more than 9,000 people and killing more than 4,400, according to the U.S. Centers for Disease Control and Prevention. Liberia, alone, has had more than 4,200 cases of infection and 2,400 deaths. As of Friday afternoon, there have been three confirmed cases in the U.S. and one death. Taskin Padir, an assistant professor of robotics engineering and electrical and computer engineering at WPI, has been working with Murphy to set up the workshops. He stressed that use of technology should not be seen a replacement for human care workers. “We are trying to identify the technologies that can help human workers minimize their contact with Ebola,” Padir said. “Whatever technology we deploy, there will be a human in the loop. We are not trying to replace human caregivers. We are trying to minimize contact.” Padir also has been developing ideas for what technology he and his colleagues could ready for the cause. One idea is to use a wheeled robot with two attached sprayers to decontaminate equipment or areas where the disease has been found. Padir said he already has set up a prototype of a decontamination robot, using a machine with an arm from tWPI’s robotics team and adding two sprayers to it. “We are not trying to come up with a brand new design because I don’t think we have time for that,” Padir said. “I’m trying to repurpose some of the standard designs we have. We want to be able to deploy something within three months. I can’t design brand new robots that would take a year to figure out.” Another of Padir’s ideas is to set up a telepresence robot that could be used to move around a field clinic, allowing health care workers to see and interact with patients from the safety of a remote location. It would not replace direct human contact, but it could add another level of interaction with the patients. Padir said the telepresence technology could help reduce some of the isolation that patients have reported feeling when in quarantine. “People are afraid to show up to hospitals because they’ll be put in quarantine, and you’re left alone and you’re away from loved ones,” Padir said. “Anything we can do to improve the situation in quarantine, we are open to exploring. Companionship through telepresence could be a tool to maintain quarantine conditions.” Murphy, author of Disaster Robotics, has been working on search and rescue robots since 1995. She said she’s not set on any one particular idea, but is interested in exploring how to design a robot that could help bury Ebola victims who have died. One of the reasons that aid workers are at such high risk in outbreak areas is that people infected with Ebola are the most contagious at the time of death and for a few days after. Using a robot to help move and bury the bodies would help protect workers and prevent further spread of the disease. However, building a robot that could safely and reliably do the job, while being respectful of the remains and the victims’ families, is a challenger. “My fear was there are a lot of construction robots, like the little bots that scooped up debris and covered things with dirt in Fukushima,” she said. “But that would be horrible — disrespectful. That was a person. We’re not just going to bulldoze them into a grave. And there are cultural sensitivities. There are local burial customs and people need to say good-bye to their loved ones.” Murphy doesn’t know whether mortuary robots will be called for during the Nov. 7 workshop but she is using the idea as a teachable moment for students at Texas A&M and getting ahead on the project in case they are needed. Students in the school’s department of architecture design are trying to modify a four-wheeled Bobcat robot by replacing a bulldozer scoop with a sarcophagus that would carry the body in a respectful manner. “Robots scooping up people is pretty difficult to do in a dignified way and to do it reliably,” Murphy said. “But handling infected bodies is really bad… It’s an interesting concept but it’s not just about getting the technology right. We have to get a lot of things right.” That means the roboticists need to focus on a lot of details. Other questions the robotocists need to answer are what training will the locals need to operate the robots? How will the batteries be recharged? What’s the Internet connectivity like where the robots will be operating? How would they transport the robot or robots? Is the ground there hard, sandy or muddy? Do the locals even want robotic help? “All of these questions have to be answered,” Murphy said. “That’s the difference between having a great idea in a laboratory and having it work out in the field.” Texas A&M has applied for funding through the National Science Foundation for a rapid response grant to study what technology is needed and what the requirements for it would be. Padir said he is looking forward to the workshop so that researchers and aid workers can brought together to make sure the technology fits the need. “We can imagine solutions in our laboratory from an engineering perspective but we need to make sure it’s usable on the field,” he said. “We need to come together as a community and say this is what we can pursue and make an impact.”"
206,https://www.computerworld.com/article/1605521/smart-walkers-lead-the-way-for-japanese-eldercare-robots.html,ComputerWorld,2014,10,16,645.0," If you’re growing old in Japan, chances are you’ll have some form of robotic help in the future. The Japanese government is continuing to push robots and robotic services as a solution for the country’s shrinking and aging population, 40 percent of which could be 65 or older by 2060. Care for seniors is taking a prominent spot at the Japan Robot Week trade show in Tokyo, where Osaka startup RT Works has been showing off a prototype connected walker. The Encore Smart looks a bit like a bulky shopping cart, but it has handlebars and brakes above its cargo basket. It has six-axis motion sensors that can automatically trigger motors to help push it up an incline or brakes to slow it down on a decline. While that can come in handy on the hilly terrain that characterizes much of Japan, the walker’s communication capabilities also can help caregivers monitor them. A GPS unit can relay location data via linked computers so caregivers can see where users are and how far they’ve walked on a given day. There’s also a voice guidance function for getting around. If the walker goes outside of a predefined zone, it will send an alert to caregivers via smartphone. “That could be very useful if an elderly person wanders out near the seashore or a river,” said RT Works CEO Sei Kohno. “The motorized functions of this walker can also help users walk farther without tiring, which helps them have a more independent lifestyle.” The startup hopes to launch the walker, whose price has yet to be determined, in Japan in June and eventually market the device in the U.S. At a booth sponsored by Japan’s Ministry of Economy, Trade and Industry, there are talking robotic dolls to keep elderly people company, devices that can automatically suction waste from bedridden patients and others that keep a watchful eye over patients with dementia. One of the latter is a sensor unit called Owlsight from Tokyo startup Ideaquest. It’s a long cylinder equipped with a camera and a Class 1 near-infrared laser that shines thousands of beams over the bed of a patient. Infrared sensors detect patients’ movements and neural network algorithms can interpret whether they’re sitting up, standing or getting out of bed. If the system determines that a patient is leaving the bed or falling off, it alerts caregivers. “My mother is in a nursing home and she tries to get out of bed and wander around,” said Ideaquest spokesman Seishi Ohmori. “There’s an alert mat next to the bed but I trigger it when I visit and someone comes running in.” Owlsight can also send a message to nurses when it detects that a patient’s breathing has stopped, he said, adding that the company plans to install close to 100 systems in Japan by the end of the year in a trial supported by the government. The ministry is also behind the latest strength-boosting device from Cyberdyne, a robotics startup that developed the HAL (Hybrid Assistive Limb) powered exoskeleton, which can be used by spinal-cord injury patients. Weighing 3 kilograms, the HAL for Back Load Reduction is a power-assist device that hugs a user’s lower back and thighs. It’s designed to give caregivers a boost by reducing the load on their bodies when helping patients in and out of bed. The device works through electrodes that pick up faint electrical signals on the skin’s surface emitted when the brain instructs muscles to move. “We’re using robotics to help people in the nursing field who have lower back pain,” said Cyberdyne’s Tatsuro Muranaka, who was showing off the device alongside a video that showed users lifting a person out of bed. However, the back-assist device will first be used by construction workers at major Japanese contractor Obayashi. Cyberdyne is renting out the devices for ¥120,000 (US$1,123) each per month."
207,https://www.computerworld.com/article/1605387/nasa-robots-are-our-friends.html,ComputerWorld,2014,10,16,746.0," No matter what images we have from the Borg on Star Trek or the Cylons on Battlestar Galactica, NASA says the future of space exploration is all about human and robotic cooperation. “In space it will be robots and humans, not robots versus humans”, said Rob Ambrose, chief of the Software, Robotics and Simulation Division at NASA’s Johnson Space Center. “We have a vision for Mars that definitely includes robots. Apollo did not have the technology to pre-deploy food and supplies for the moon. Everything they took had to fit into that lunar module. If they could have pre-deployed, they might have stayed more than two days.” Ambrose was the opening speaker at today’s RoboticBusiness conference in Boston. The focus of his keynote was building robots that will work hand-in-hand with humans to extend our reach into deep space. The NASA scientist noted that the space agency has put together a list of what technology it will need for exploration over the next 20 years. Along with life support systems and heavy-lift engines, NASA has made it clear that it also needs robotics if we’re going to explore Mars and distant asteroids. Specifically, Ambrose said NASA needs advances in autonomy, sensing and perception, mobility and manipulation. And one of the greatest robotics needs involves advances in human/robotic interactions. “Why would humans need robots to explore?” he asked the audience of several hundred people. “We see a larger view of how robots can help us. First they’ll act as precursors to human arrival. Exploration of Mars has already started robotically… Imagine if Magellan had a robot to go ahead and explore? They can be caretakers running a facility before humans get there. And between crews, they can be left as caretakers, running the facility and waiting for the next crew to arrive.” Already, the robotic rovers Curiosity and Opportunity have been working on Mars for years, studying the makeup of Martian soil and looking for clues as to whether the Red Planet once could sustain life, even in microbial form. Robots also have been working on the International Space Station for years, with multiple large robotic arms grabbing onto approaching spacecraft, unloading cargo and even giving astronauts rides as they work outside the space station. This, said Ambrose, is just the beginning. “We see a number of roles, especially on the space station today, where the crew is overly subscribed,” he said. “Any time we can offload from the crew and focus more on their scientific exploration is a huge time savings to us. Robots could do housekeeping and maintenance.” Ambrose showed video of robotic machines that are being used in space or one day might be. A six-wheeled rover, which has only been tested on Earth so far, could be used to carry astronauts or to serve as a scientific rover on another planet, like Mars, he said. With a lot of torque and an active suspension system, the robotic vehicle is able to easily climb over ridges taller than its own tires and over obstacles. “It’s a beast. It can climb vertical steps and carry more than its own weight on its back,” said Ambrose. “No matter what happens, the rover will be able to get the astronauts back to the lander. That’s critical, since you’re trusting your life to that rover getting you back.” He also discussed Robonaut 2, also known as R2, which is a humanoid robot working on the space station. “We wanted a robot that could safely work around people,” said Ambrose. “If you’re going to work side-by-side with a machine that strong, you really have to trust it…. That’s what we got in the Robotnaut 2 system. An astronaut is allowed in the space of a robot with nobody watching it. Nobody is on the red button. If you want to stop the robot, you just touch it. It went through the most rigorous safety review of any robotic system I’ve ever seen.” If the robot senses that it’s touched a human, it simply stops. This large, powerful robot has triple redundancies built in to make sure it can work beside humans and not hurt them. That level of trust is critical to getting humans comfortable with working with robots. And that comfort is key to moving forward with human/robotic cooperation, especially in the dangerous and lonely regions of space. “We need machines to fly, to go above 20 miles per hour,” said Ambrose. “Why not use machines to explore?”"
208,https://www.computerworld.com/article/1600416/robot-gets-a-driving-lesson-for-darpa-challenge.html,ComputerWorld,2014,10,9,1113.0," WORCESTER, Mass. — A vehicle drives toward a disaster site, robot at the wheel. The robot stops the car and then steps out to walk toward the disaster. That’s not scene from the latest sci-fi movie, it’s what scientists and military leaders hope to see next year when robotics teams from around the world compete in the DARPA robotics challenge finals. With the last challenge just eight months away, the various finalists – including teams from Worcester Polytechnic Institute, MIT, Virginia Tech and NASA’s Jet Propulsion Laboratory — have been working to get their robots ready to take on tasks ranging from opening doors to using a drill, climbing a ladder and turning valves. These are tasks that the robots had to tackle during their last challenge. While this time around the robots will need to act more autonomously, most of the tasks they face aren’t new. DARPA did throw a bit of a wrench into the process, though, adding extra difficulty to a trial that already is pushing the boundaries of autonomous and humanoid robots. That means when the teams compete in the finals in Pomona Calif. next June for a $2 million prize, their robots won’t just be asked to drive a car.They’ll need to get out of the vehicle, too — something that’s much more complicated than it sounds. Since driving is the first task the robots face, they won’t be able to continue with the rest of the challenge if they can’t manage that. Years of work will end with a quick failure. DARPA, the Defense Advanced Research Projects Agency, will give teams an easy out: the option to walk the course, instead of drive and exit the vehicle. But any team that takes that route, won’t be able to garner as many points as those taking on the driving and egress challenge. And when it comes to beating the best robotics teams from around the world, the winning team is going to need all the points it can get. For Worcester Polytechnic Institute, or WPI, that means tackling the hard stuff. “It’s a risky move, but if we’re going to win, we’ve got to put all our money on the table and go full in,” said Michael Gennert, director of robotics engineering at WPI. “We’re not going to say, ‘That’s too hard.’ We’re going to do it. If we’re going to win, we’re going to win big. If we’re going to fail, and I hope we don’t, we’re going to fail big, too.” DARPA’s three-part challenge is intended to encourage the advancement of autonomous robots to the point that they could largely act on their own after a natural or man-made disaster, going into a damaged building, rescuing victims, turning off gas pipes and even putting out fires. The first part of the challenge was a simulation held in 2013. The second part, which was took place in southern Florida last December, involved 16 teams competing to see which could build the best software to enable their robot to work through a series of individual tasks, such as walking, using tools and climbing a ladder. During the June finals, the teams won’t be facing individual tasks. Instead, their robots will confront a disaster situation that forces them to deal with tasks like removing debris, walking around or over obstacles, turning off valves or cutting into walls. If a robot can’t complete a needed task, it won’t be able to continue on. Speed is another issue. During the December challenge, the robots had 30 minutes for each specific task. Many failed to even open and walk through a door or climb over a small pile of debris in the given time. In the finals, they’ll have just 45 minutes to an hour to accomplish all eight tasks. “At this point, I’d say we’re about 50% faster than last December, but we’re hoping to get in the 75% or 80% range,” said Matt DeDonato, the team’s technical project manager. “It’s a scary thing. It’s daunting. With speed comes a lot of uncertainty and instability. As roboticists, we like everything slow because we can control slow. As you get more and more into the dynamic range, you have to make sure all your algorithms get updated so you can handle the higher speeds.” The WPI robotics team, which is working with researchers from Carnegie Mellon University, is already figuring out how to best get their 6-foot tall, 330-pound Boston Dynamics-built Atlas robot to maneuver out of a vehicle. (They’ve named it “Warner.”) Of all of the known tasks they’ll face — DARPA has warned them that there will be a surprise one — simply getting out of a car is the most daunting. “The reason it’s so hard is the robot is in contact with the vehicle at many points,” Gennert said. “When it’s walking, the robot touches the ground with its left foot and right foot and that’s that. In a car, it has its fanny on its seat cushion, its back against the seat, its feet on the floor. It has its hands on the steering wheel. There are many and different kinds of contact. It has to shift its weight from the back of its legs onto its feet. That’s really hard to do.” While the robot has sensors, it can’t feel its legs or back pressing against the seat like a human does. Without feeling those points of contact, it has less information about its positioning, making decisions about its next move harder to make. “Right now, we have one foot out and now we’re shifting the weight onto that foot so it can move the other foot out,” said DeDonato. “That’s one thing we think will set us apart from the other teams. We were one of only two teams to actually finish driving the course [in the last challenge]. So we want to basically continue on that path.” The team, though, hasn’t been spending all of its time on the driving task. DeDonato said team members been working hard on the software needed to get Warner to pick up and use a drill, remove debris and walk over rough terrain more autonomously than before. “We’ll no longer be giving it joint-by-joint commands,” he explained. “Last competition, it was a different level of autonomy. All the balancing was autonomous. When you told the hand to move, the robot didn’t fall over. We gave it a lot of commands, like move to this point and reach out…. It was somewhat autonomous. Now we’re giving it task goals. Walk there and pick up this object. He automatically figures out how to walk around stuff and grasp the object.”"
209,https://www.computerworld.com/article/1379229/astronauts-to-make-robotics-repairs-during-65-hour-spacewalk.html,ComputerWorld,2014,10,7,272.0," Two astronauts today began what is expected to be a six-and-a-half-hour spacewalk to repair a robotics system, along with cooling pump and a television camera on the International Space station. NASA astronaut Reid Wiseman and Alexander Gerst of the European Space Agency set their spacesuits to internal battery power and then stepped outside of an airlock at 8:30 a.m. ET to begin their work. Inside the spacecraft, flight engineer Barry Wilmore will operate the robotic arm to move Gerst around the space station during the spacewalk. Wilmore also is acting as the spacewalk coordinator, overseeing his partners’ work. One of the astronauts’ tasks will be to install a mobile transporter relay assembly system, which provides backup power to the rail system that runs the length of the station. Robotic armsRobot Canadarm2 and Dextre move along the station’s truss using this rail system. Canadarm 2 is the space station’s primary robotic arm, which is used to grab rendezvousing spacecraft, such as the SpaceX Dragon cargo craft. Dextre a Canadian-built two-armed robot that stands 12-feet tall and has a 30-foot wing span. Dextre is used to reach into cargo ships and unload the supplies, spare parts and scientific supplies brought to the orbiter from cargo craft. While Wiseman and Gerst are on the spacewalk, they’ll also move a failed cooling pump from where it was temporarily stored on the station’s truss to an external storage platform. While Wiseman is cleaning up the area around the pump module, Gerst will replace a light on a television camera on another part of the station. Live video of the space walk can be viewed on NASA’s website."
210,https://www.computerworld.com/article/1378489/the-navy-is-building-robotic-weaponized-boats.html,ComputerWorld,2014,10,6,1602.0," As a U.S. Navy aircraft carrier, loaded with 60 aircraft and more than 6,000 sailors, heads toward port, it’s protected by a group of 10 or more small boats. The boats move around the ship, scanning for suspicious and potentially hostile vessels coming too close. If they spot a potential adversary, they race toward the intruder, working together to swarm around it and block it from getting any closer. If necessary, they can destroy an attacking vessel. What makes this scenario unique is that these small boats are unmanned. No one’s driving them or on the look out. No one’s manning the machine gun. The Office of Naval Research has created an autonomous boat system and recently tested up to 13 unmanned boats working together, according to Rear Admiral Matthew L. Klunder, chief of naval research.  “It’s not just the fact that we’ve developed this technology, but we’ve exercised it,” he said. The Navy expects to officially deploy the autonomous system — dubbed CARACaS or Control Architecture for Robotic Agent Command and Sensing — in about a year. “We all recognize we live in a pretty volatile world,” said Klunder. “There are sailors in harm’s way trying to avoid conflicts and stabilize a region. We want to make sure that those sailors, those soldiers, are never in a fair fight. We want them to have the most innovative technologies available.” The technology, which uses artificial intelligence, machine perception and distributed data fusion, was successfully demonstrated over two weeks in August on the James River in Virginia. “This is a huge advance for robotics and, specifically, for object recognition and artificial intelligence implementations,” said Patrick Moorhead, an analyst with Moor Insights & Strategy. “These are the smartest robots I have seen. The combination of speed, object recognition and artificial intelligence is very, very impressive.” The fact that a computer system can distinguish a potentially suspicious boat from any other vessel is a huge advance. “It is very difficult to determine a threat versus friendly because there could be nuances in the way the boat moves, the way it turns, its speed, the way it reacts to the swarming boats, the people on deck, what they’re carrying and the way they are acting,” added Moorhead. In October 2000, the USS Cole, refueling in a Yemeni port, was damaged, 17 American sailors were killed and 39 were injured when it was bombed in a suicide attack made in a small craft that simply drew up beside the ship. The Navy hope the new autonomous system will prevent such an attack from happening again. “If we’d had this capability there on that day, we would have saved that ship,” said Klunder. “We don’t want to ever see that again. We want to save sailors’ lives. We want to save sailors and ports and ships.” Work on the CARACaS system, which could one day be adapted for underwater, ground and aerial vehicles, began in 2004 as an applied research program, with scientists looking initially to create an autonomous system to operate a single boat. “At that time it was a huge technical challenge,” Robert Brizzolara, program manager for the Office of Naval Research, said. “I wasn’t thinking at the time about group or team operations. It wasn’t really until 2007 or 2008 that I actually started doing tech development for group or team operations.” To get going, Brizzolara and his team began by using autonomous control technology that NASA developed for the Mars rovers Opportunity and Spirit, both launched in 2003. NASA’s Jet Propulsion Laboratory, along with Johns Hopkins University and the Naval Surface Warfare Center, joined the Office of Naval Research team. The autonomy software for the rovers needed a lot of adaptation — notably because the rovers move slowly over a distant and dangerous ground terrain while the boats need to move at high speeds over water to intercept, block or swarm around suspicious craft. “Thinking about the difference between a boat and the Mars rovers — the speed and the environments they operate in — it was a significant amount of work to adapt the Mars rover system to one that can operate a boat,” said Brizzolara. “They’re such different vehicles. The control system for the boat needs to respond much, much more quickly.” The CARACaS system was developed around two main points of emphasis, he explained. First, the control system needs to use cameras, radar and sensors to perceive other vessels, the shoreline and anything else that needs to be avoided or monitored. Second, the boats need to plan their movements and their reactions to suspicious vessels or objects in the water. If five boats are working together, for instance, all five will share the information they’re picking up from their own radar and sensors with each other. That means each boat has information from all the others about they’re “seeing,” as well as each one’s position, speed and actions. The information is shared over a network radio link at this point, though Brizzolara said the Navy has more secure information networks and will put one in place before the system is put into official use. “The boat system needs to plan its own route,” Brizzolara explained, noting that he cannot divulge exactly what speeds the unmanned boats reach. “We have developed a much faster route planner for the boat that works in close to real time to process those routes at speeds appropriate to what boats of that size should be doing.” He added that the Navy has built various military tactics into the autonomous software, using AI to enable the boats to, on the fly, designate a target, block it, encircle it and engage it if need be. The boats also could trail each other or a ship; go into escort mode; or line up in a blocking behavior. “The key is its flexibility to execute any number of behaviors, not just swarming,” said Brizzolara. “We would like, from a science and technology point of view, for CARACaS to be as widely applicable as possible. The fact that it can be programmed to undertake a number of different behaviors means it could take on a wider range of missions.” Dan Olds, an analyst with The Gabriel Consulting Group, said the power of the CARACaS system lies in its complex combination of technologies able to work together. “These are definitely more sophisticated than the vast majority of robots we’ve seen to date,” he said. “The programmed behavior with all the actors moving in concert is a big advance, particularly when considering all of the variables they will have to contend with, like ocean waves, winds and rain.” Using an array of sensors and radar, the system is able to process all of the inputs, like imaging, conditions, mission and the actions of others. “That,” said Olds, “is a big deal. When taken together, this is a big step for robotics.” The autonomy system can be installed in any small Naval vessel. The boats themselves do not need to be specially built. Moorhead noted that before CARACaS, a lot of military robotics had limited ability to work cooperatively with each other and  limited object recognition. “It’s difficult to get multiple robots to work together because they need to be aware of the entire system of robots, instead of just their surroundings,” he said. “This raises the challenge exponentially.” The boats are equipped with non-lethal weapons such as sound cannons and bright lights designed to confuse or disorient enemies.  If needed, the boats can fire upon and destroy an enemy boat. The autonomous system cannot do that, though. If lethal force is required, a human will have to make that decision and enable the boats to fire on a target, Klunder emphasized. “We have every intention of using these unmanned systems to engage a threat and destroy it if necessary,” he said. “But there is always a human in the loop on the designation of a target and the destruction of a target. The power of what I’m holding here is the autonomous feature. They sense, avoid, go get and engage with a target. They work together.” For now, Brizzolara said his group is working to meet the Navy’s goal of having these autonomous systems working in a year. Their biggest challenges, he noted, are improving the system’s perceptive abilities and the boats’ ability to work together. “There are still refinements that we need to do in terms of the major components of the system,” said Brizzolara. “We need more reliable sensing so we see everything that’s out there and more reliable route planning for multiple boats working together. If one boat needs to take itself out of the team to go investigate something else, the team needs to compensate for that one removed boat.” One of the biggest problems researchers face is making sure the boats are operating from the same information and that they are working together to decide what to focus on and how to move as a swarm. “We’ve taken that first step, but there is still work we need to do,” said Brizzolara. “We need to continue to develop the algorithms for the route planning and then we need to do simulation testing and then [go] out and test it on the water.” The Navy is looking to use the system around large naval vessels, merchant vessels, ports, harbors and oil rigs. “We’ve taken a patrol craft usually manned with three or four sailors each and made it unmanned,” said Klunder. “All those sailors normally on those crafts can be back on the ship and out of harms way.”"
211,https://www.computerworld.com/article/1377394/japan-rolls-out-cheerleading-swarm-robots.html,ComputerWorld,2014,9,24,580.0," There’s something inherently scary about robots acting together to accomplish a goal. So-called swarm robots, which have recently emerged from robotics labs, have long been a staple of scary science-fiction films. But a new swarm robot group from Japan puts a distinctively “kawaii,” or cute, spin on the concept. Meet the Murata Cheerleading robots. They look like dolls, have glowing eyes and balance on steel balls. Unveiled Thursday in Tokyo by components maker Murata Manufacturing, each bot looks like a cartoonish girl sporting a red skirt and short black hair. A series of rollers under the skirt keep the robot balanced on a ball or rotate it in a particular direction to move around. Under the afro, meanwhile, nestles an infrared sensor and ultrasonic microphones that help the robot detect objects nearby. Three gyro sensors control motion from front to back, side to side and in rotation. A wireless network is used to control a group of 10 cheerleader robots. They can perform precisely synchronized dance routines, moving into formations such as a heart while spinning on their balls. It’s all very kawaii, and of course intended to generate attention among Japanese audiences and visitors to trade shows. “We designed the cheerleader robots to cheer people up and make them smile,” said Murata spokesman Koichi Yoshikawa. “Their features can be summed up as ‘3S’: stability, synchronization and sensing and communication.” Also known as ballbots, robots that can hold themselves upright while balancing on a sphere were first developed about nine years ago by researchers at the University of Tokyo, Carnegie Mellon University and other robotics labs. While they’re able to maintain their balance on a ball even when given a light push, none has had the humanoid design of the Murata cheerleaders. Murata’s dancers follow in the circus performer tradition of its other robots, which have been perennial exhibits at trade shows such as CES and Ceatec. Unveiled in 2005, Murata Boy is a bicycle-riding robot that was later enhanced so that it can ride along a curved balance beam. Its sibling Murata Girl, also known as Seiko-chan, can wheel around on a unicycle. While beautifully designed and impressive in their balancing prowess, the robots have been little more than expensive PR spokesmen, or spokes-machines, for Murata. The firm spent about a year-and-a-half developing the cheerleader robots, a task that involved some 20 engineers. Like many other highly sophisticated, gorgeously designed robots developed by high-tech companies in Japan to be corporate ambassadors instead of viable products, the cheerleaders will not be put on sale. Instead they will drum up attention for Kyoto-based Murata, which marks its 70th anniversary in October. It wants the world to know that it controls sizeable market shares for components used in smartphones, computers and automotive electronics. For instance, it claims a 60 percent global share in connectivity modules, which allow mobile phones to access the Internet through radio signals. It also says it has a 95 percent global share of the market for shock sensors, which can protect data writing processes when hard disk drives experience an external shock. But the cheerleading robots have more than just spirit. The software that wirelessly orchestrates their gyrations, developed with researchers from Kyoto University, could be used in future automotive safety systems, according to Murata, which also produces gyroscopes and accelerometers used in car stability control and anti-lock brakes. Murata plans to show off its cheerleaders at the Ceatec tech expo from Oct. 7 to 11 outside Tokyo."
212,https://www.computerworld.com/article/1376060/mits-cheetah-robot-is-off-its-leash-running-and-jumping.html,ComputerWorld,2014,9,17,606.0," Robots with legs should be able to go where wheeled robots cannot – over obstacles and crevices. The problem has been that legged robot have been ungainly and have needed a lot of energy to get around. A team of researchers at MIT may have figured out a way to make a four-legged, cheetah-like robot run and jump more gracefully and efficiently. To get there, they studied animals like dogs and cats and used that biological information to create a new algorithm for bounding. “Our robot is about 20 times more efficient than other quadruped robots,” Sangbae Kim, an associate professor of mechanical engineering at MIT, told Computerworld. “Our robot is as efficient as a real animal … and it can run much longer because it’s efficient.” The robot, which is about 3 feet tall and 3 feet long, can sprint up to 10 mph. It also can jump a foot high and make a leap more than a foot long. Kim said the Cheetah robot has used only about 40% of its power, and is expected to eventually jump much higher and reach a running speed of about 30 mph. The researchers are slowing increasing the robot’s power so they don’t risk damaging it. The cheetah, which uses an electric motor, does not have a communications or power tether. Kim said the key to the bounding algorithm lies in programming each of the robot’s legs to exert a specific amount of force in that moment when the feet hit the ground. “We studied a lot of biology,” he said “I studied a lot of mechanics of quadrupeds, like cats and dogs. We studied how the brain controls the legs. There were a lot of small bits of information that we got from animals — the way they bound, trotted and galloped, and the way those motions changed over a wide range of speeds. We directly utilized that to be able to control the force of the legs better. To know how much force I need to exert each step, I need to know so much about the mechanics.” Once the engineers calculated how long a leg – whether biological or robotic – needs to be on the ground and how long in the air, they were able to figure out how much force to apply to that leg to compensate for the gravitational force. That information enabled them to control the machines bounding at different speeds. To enable the robot to jump, they simply tripled the force. Boston Dynamics, a robotics company acquired by Google, has gained attention with its BigDog robot, a four-legged machine designed to travel across rough terrain. However, the BigDog has a fairly awkward and ungainly gait. Kim noted that MIT’s Cheetah has a smoother gait, is quieter and uses less energy than BigDog. “Most robots are sluggish and heavy, and thus they cannot control force in high-speed situations,” Kim said. “That’s what makes the MIT Cheetah so special. You can actually control the force profile for a very short period of time, followed by a hefty impact with the ground, which makes it more stable, agile,and dynamic.” He added that a key robotics advance is the Cheetah’s ability to balance itself while running and jumping. Imagine being in a field and finding a rock, Kim said. “In this case, [the robot] can jump over it. The whole point of having legs is sending [the Cheetah] where a wheeled robot cannot go. A lot of cars go nicely on the road, but there are many things wheels cannot go over. With this we are realizing the actual function of the legs.”"
213,https://www.computerworld.com/article/1374961/drones-are-the-new-pets-com.html,ComputerWorld,2014,9,8,839.0," Everyone, from Amazon to Google to Martha Stewart, has been lauding the benefits we’ll all reap by the use of drones, and there’s a gold rush on to cash in on the technology. But beware: The trend has all the hallmarks of a bubble-in-the-making, the contemporary equivalent of that symbol of the excess of the millennial tech bubble, the now-defunct Pets.com. The latest publicity blitz about commercial drones came in late August, when it was revealed that Google’s Project Wing has been working on drone delivery for the past two years. The company recently performed more than 30 drone flights in Australia to deliver products such as a water bottle, dog treats, chocolate bars and a first aid kit to a farm. That follows Amazon’s announcement late last year that it planned to launch a fleet of drones called Prime Air that would deliver goods within a half-hour of being ordered. Amazon promises on a Web page about the service: “One day, Prime Air vehicles will be as normal as seeing mail trucks on the road today.” As for Martha Stewart, her interest in drones is quite a bit narrower. In an essay in Time Magazine, she praised her new personal drone, lauding its ability to take aerial photos of her 153-acre spread in tony Bedford, N.Y., including its horse paddocks and “long allée of boxwood.” She was particularly pleased that, “An aerial shot of the vegetable garden looked very much like my Peter Rabbit marzipan embellished Easter cake.” The funny thing is that, while few of us have vegetable gardens that look like marzipan, Stewart’s use of her drone comes a lot closer to what is likely to become commonplace than either Amazon’s or Google’s. Meanwhile, drone companies are highly sought after. Earlier this year Google bought Titan, a small maker of solar-powered drones for an undisclosed sum after Facebook unsuccessfully tried to buy it for $60 million. Facebook instead hired away staff from drone maker Ascenta and space agency NASA for its own drone plans. So what’s wrong with Google’s and Amazon’s visions? Why won’t small, unmanned aircraft one day be dropping tubes of toothpaste and dog food into people’s yards across the continental United States? After all, drones have been used for years to undertake battlefield surveillance and kill terrorists. But killing terrorists with drones is one thing, and delivering packages with them is another thing entirely. And it will prove harder to set up a package delivery system with drones than to use them for battle. The problem isn’t a technical one. Instead, it has to do with safety, regulatory requirements, potential misuse, government bureaucracy and people’s fear of the unknown. Because of the uncertainties and potential dangers involving fleets of drones clogging U.S. airspace, the FAA has essentially banned commercial use of drones until it develops regulations, saying in a statement, “Developing all the rules and standards we need is a very complex task, and we want to make sure we get it right the first time.” Translation: Don’t expect those regulations anytime soon. NASA, meanwhile, is developing a new air traffic control system specifically for drones and other aircraft that fly close to the ground. It has to take into account weather in ways the current system does not, because drones are so light that wind can easily blow them around into buildings or other drones — or drop them to the ground and injure people. Parimal H. Kopardekar, the NASA official in charge of the program, told The New York Times, “One at a time you can make them work and keep them safe. But when you have a number of them in operation in the same airspace, there is no infrastructure to support it.” Other issues that must be ironed out include hacking, hijacking and making sure drones don’t fly near airports. And how about privacy issues? Drones will require cameras for navigation and delivery — will they be allowed to use the video of your home that they capture during deliveries to your house? Because of all this, commercial use of drones is a long time away — and their use in delivery may never come. In 2013, a tongue-in-cheek video made by Domino’s showed a “DomiCopter” drone delivering pizza into a customer’s waiting hands. It went viral, gaining 1.6 million views. But it was only a spoof. “We did not and are not testing drone delivery,” a Domino’s spokesman told The New York Times. “Given the fact that these things have spinning blades, could be stolen, shot at or batted like piñatas, we didn’t think the idea would ‘fly’ here in the U.S.” So will commercial drones be around for the long term? Yes, they will, just as e-commerce still exists, allowing you to easily order pet supplies online despite the notorious Pets.com flameout. But they’ll most likely be used for purposes like agriculture (to check on fields of soybeans or corn if not marzipan-like vegetable gardens) rather than flying you a bag of Cheetos when you need one."
214,https://www.computerworld.com/article/1374798/2017-cadillacs-will-have-automated-highway-driving-features-gm-says.html,ComputerWorld,2014,9,8,309.0," General Motors will put a “Super Cruise” feature in 2017 Cadillacs that can perform hands-off lane following, braking and speed control in certain highway driving conditions, the automaker said Monday. Super Cruise is designed for use both in bumper-to-bumper traffic conditions and on long road trips. Drivers will be able to take their hands off the wheel and their feet off the pedals. “Rest assured, Super Cruise will keep drivers alert and engaged, and when they want to take control, they’re going to find a car that’s really fun to drive,” GM CEO Mary Barra told a meeting of the Intelligent Transportation Society of America on Sunday in Detroit. Meanwhile, the 2017 Cadillac CTS will come with vehicle-to-vehicle (V2V) communications technology. It can exchange data with nearby similarly equipped vehicles concerning their direction, speed and location, warning drivers if they might collide. GM said V2V can be an additional safety layer on top of forward collision warning systems, which are already deployed in production cars. In 2010, the U.S. Department of Transportation estimated that V2V-style systems could be effective in up to about 80 percent of vehicle crashes. Vehicle-to-infrastructure (V2I) technology is also being developed as a way to reduce accidents and congestion on roads. Traffic lights with V2I connections, for instance, would only turn red when needed. However, a critical mass of connected cars and infrastructure is required to make the systems work well. “The CTS will talk to other V2V-equipped cars to avoid crashes,” Barra said. “It will talk to V2I-equipped infrastructure to reduce congestion and its 4G LTE connection and active safety features will give drivers peace of mind.” GM’s announcement adds impetus to carmakers including Volvo and Mercedes to introduce cars with ever more sophisticated automated features, from parking assistance to fully robotic cars that can drive themselves like the ones being developed by Google."
215,https://www.computerworld.com/article/1373739/will-a-bot-take-your-help-desk-job.html,ComputerWorld,2014,9,1,835.0," Competing forces are affecting people who work on help or service desks. One is that tools to automate IT support are continually improving, and advocates say those tools can replace Level 1 and 2 support staff. At the same time, the number of help desk tickets is rising each year, and that puts more demand on the service desk. These crosscurrents in the industry make it hard to predict the fate of some IT jobs. A Pew survey, released in August, of nearly 1,900 experts found a clear split on what the future may bring: 52% of the respondents said tech advances will not displace more jobs than they create by 2025, but 48% said they will. Either way, a push toward automation is certain. In the help desk industry, the goal is to keep as many calls as possible at either Level 0, which is self-help, or Level 1. It’s called “shift-left” in the industry. “It costs way more to have a Level 3 or Level 2 person to resolve an issue, and it also takes a lot more time,” said Roy Atkinson, an analyst at HDI, formerly known as the Help Desk Institute. To keep costs down, help desks are increasingly turning to automation and relying on improvements in technologies such as natural language processing, he said. A Level 1 worker will take an initial call, suggest a couple of fixes, and then — lacking the skill or authority to do much more — escalate the issue. The Level 2 worker can do field repair work and may have specific application knowledge. A Level 3 escalation might involve working directly with application developers, while Level 4 means taking the problem outside to a vendor. Among the companies developing automation tools is New York-based IPsoft, a 15-year old operation with more than 2,000 employees. It develops autonomic technology and couples it with management services. A majority of IT infrastructure will eventually be “managed by expert systems, not by human beings,” said Frank Lansink, IPsoft’s CEO for the European Union. The company says its technology can now eliminate 60% of the labor required to handle infrastructure tasks. IPsoft’s autonomic tools might discover, for instance, that a network switch isn’t functioning, or that a wireless access point is down. After encountering such a problem, the system can then create a ticket and deploy an expert system — a specially programmed software robot — to make the repair. If that can’t be done, a human intervenes. Many service desk jobs have been moved offshored over the last decade, displacing workers. That trend is ongoing. One of the ideas underlying IPsoft’s business model is a belief that offshore, as well as onshore, labor costs can be further reduced through automation. Offshore companies are clearly interested. IPsoft’s platform was adopted last year by Infosys and, more recently, by Accenture. One IT manager using IPsoft’s automation technology and services to support his organization’s infrastructure — including its network, servers and laptops — is Marcel Chiriac, CIO of Rompetrol Group, a Romania-based oil company with 7,000 employees serving Europe and Asia. “Without the automation, we would have to pay a lot more” for IT support, said Chiriac. The cost savings arise from automating routine repairs and maintenance that might otherwise be neglected, said Chiriac. If he weren’t using autonomic tools, Chiriac said he would have to hire more people to provide a similar level of service. But he can’t easily estimate the impact on staffing because of his company’s IT history. (Rompetrol Group outsourced its 140 IT staffers, then ended its relationship with the outsourcer and rebuilt an internal IT unit with about two-dozen fewer people; it still relies on outsourcing to supplement its IT operation.) Nonetheless, Chiriac doesn’t believe that automation will necessarily eliminate IT jobs, though it may shift them to other IT areas. “In IT, we’re not going to run out of work for the next two generations,” he said. The amount work that help or service desks are asked to take on is increasing. Two-thirds of 1,200 organizations surveyed by HDI reported that the number of tickets, either to fix something broken or to outfit a new hire or change permissions, for instance, is increasing annually by more than 60%. According to HDI’s survey, the top five reasons for this increase are the following: an increase in the number of customers at the companies surveyed, a surge in the number of applications supported, changes in infrastructure, increases in the scope of services, and the need to support an ever-expanding collection of devices and platforms. That latter could be a reflection of the escalation of the BYOD movement. At the same time, support is being transformed in new ways. A service desk may, for instance, now act as a liaison for all IT service providers, including vendors of cloud and mobile services, said Atkinson. “I think a lot of people have been predicting the death of support for a number of years, and it hasn’t happened,” said Atkinson."
216,https://www.computerworld.com/article/1374081/scientists-create-one-robot-brain-to-rule-them-all.html,ComputerWorld,2014,8,29,644.0," Researchers are creating a single, massive repository of robot knowledge so machines around the world can learn from each other. Dubbed Robo Brain, the repository, which robots can access over the Internet, is designed to let the machines draw on its more than 10 terabytes of data whenever they need it. The knowledge store resides on Amazon Web Service’s public cloud. “I’m really looking forward to building this brain, with all this information that   need,” said Ashutosh Saxena, an assistant professor in the computer science department at Cornell University. “Instead of teaching each piece of knowledge to each robot, when a robot goes out in the real world, it can query the brain and learn how to do things.” Robo Brain is a collective project, with scientists from Cornell, Stanford, Brown and the University of California, Berkeley, collaborating on it. Part of what can be laborious and time consuming about building efficient robots is that each machine has to learn so many individual tasks, Saxena explained to Computerworld. Every robot has to be taught the same thing, such as how to open a carton a milk, time after time. Each robot also needs to understand how the world works – what is a table, what is a living creature and how should it be treated – while also understanding its place in the world. With Robo Brain, individual robots, whether it’s a robotic arm working on a factory floor, an autonomous car or a robot assistant helping an elderly person at home, can draw on this store of information and learn from what other robots have already learned. “Our laptops and cell phones have access to all the information we want,” Saxena said. “If a robot encounters a situation it hasn’t seen before, it can query Robo Brain in the cloud.” In July, the team of scientists began uploading about 1 billion images, 120,000 YouTube videos and 100 million how-to documents and appliance manuals. They also uploaded all of the training and information they already had given the different robots created in their own individual laboratories. When a robot learns something, it should feed that experience back into the repository. That way the brain will grow and hold more and better knowledge and experiences to pass on to other robots. Robo Brain is designed to process the images uploaded into it and pick out the pertinent objects in them. It is learning to recognize objects and how they are used, along with human language and behavior. That way if a robot needs information, such as how to put away dishes, Robo Brain will be able to provide videos and images related to that particular task. A robot could learn not only that a cup is a container designed to hold liquids, and that the liquids can be poured into and out of it. The machine also would learn that people use cups to drink liquids, such as coffee or tea. Cornell also noted that the system employs what computer scientists call structured deep learning, or information stored in many levels of abstraction. An easy chair, for example, is a member of the class of chairs, and going up another level, chairs are furniture. Sitting is something people do on a chair, but a human also can sit on a stool, a bench or the lawn. “The Robo Brain will look like a gigantic, branching graph with abilities for multidimensional queries,” said Aditya Jami, a visiting researcher at Cornell, in a statement. It might look something like a chart of relationships between Facebook friends but more on the scale of the Milky Way.” Only the four institutions involved in the work have access to Robo Brain, though Saxena said he hopes that within six months, that number should grow to 10. In two years, he hopes 100 institutions and companies will have access to it."
217,https://www.computerworld.com/article/1373808/ready-for-the-robot-revolution.html,ComputerWorld,2014,8,27,1097.0," The days of drones filling the sky and robots roaming in our streets are not far removed from reality anymore, and scenes from movies like Star Wars, Minority Report and I, Robot will be common soon. Just consider some of the ways that robots have started to permeate our lives. Start with Amazon, which is taking to drones in a big way. The online shopping giant started a new phase in high-tech customer service by showing off small drones that it claims will be able to deliver products to consumers in 30 minutes or less. The main goal of this futuristic service, called Prime Air, is to speed up Amazon’s delivery times, creating a competitive advantage over other digital marketplaces and traditional stores. Amazon Web Services could be a model for this army of drones, with Amazon leasing its drones to other companies to deliver their products — CVS for medications, Safeway for groceries, UPS for packages, Pizza Hut for hot food delivery on time, and more. “One day, seeing Amazon Prime Air will be as normal as seeing mail trucks on the road today, resulting in enormous benefits for consumers across the nation,” the company says. The company has tested the full spectrum of its drones’ capabilities, including agility, flight duration and redundancy. It also claims to have developed sense-and-avoid sensors and algorithms that will allow the Prime Air drones to see obstacles and automatically avoid collisions. The battery-powered drones are capable of flying at 50 mph, and can carry a five-pound payload — sufficient to deliver 86% of the products in Amazon’s inventory. The company is embracing new technology to ensure that operations are safe. Amazon plans to add a feature so that drones will safely stop operating and return automatically to a specific location on Amazon’s property if the communications link is lost. Amazon sees drones as the future of commercial delivery. The technology is one way Amazon can reduce shipping expenses, which cost the company nearly $4 billion per year. Drones are seen as superior to helicopters due to the smaller devices’ cheaper equipment and personnel costs, reduced noise, smaller environmental impact and ability to access infrastructure that is difficult to reach. And recent advancements in drone technology and the development of sophisticated sensors have lowered the cost of collecting vast amounts of information. As a result, drones are likely to be invaluable in capturing big data in many fields, including agriculture, construction, environment, surveying, utilities and security, opening countless business opportunities. They will also find application in humanitarian efforts. For example, delivering medications to areas struck by natural disasters, or infected by diseases (the recent breakout of Ebola in Liberia is an opportunity to use drones without exposing doctors to danger or spreading the virus to other areas). Nearly 1 billion people on the planet live without access to all-season roads, meaning that a significant portion of the population is unable to receive aid and emergency supplies when needed. Drones can change all that. The Association for Unmanned Vehicle Systems International (AUVSI) forecasts that the economic impact of drones will total more than $13.6 billion over the next three years and will grow substantially for the foreseeable future, reaching more than $82.1 billion by 2025. The AUVSI report estimates that as much as 85% of drone usage will be in agriculture, with as many as 135,000 unit sales per year by 2025. Worldwide demand is expected to rise as other countries follow regulations set by the FAA. Drone hardware sales in agriculture are expected to reach nearly $7 billion annually. One company showing a lot of interest in robotics is Google. It recently bought DeepMind, an artificial intelligence company that helps computers learn to operate like humans. Even though Google search is already incredibly solid, Google likely wants to use AI to improve its efficiency and the quality of data gathered. With Google’s obsession with knowledge and organizing information, we could see Google beefing up its search offerings with AI from its acquisition of Boston Dynamics, which is a leading provider of human simulation software. Its robots need to react independently to their environment — and perfecting that type of machine learning is widely regarded as the key to the next phase of tech. Other companies acquired by Google include Industrial Perception, a U.S. startup that has developed digital eyes and robot arms for use in loading trucks; Holomni, which produces caster wheels that can rapidly swivel in any direction; and Japan’s Schaft, whose robots generate as much power as a human and have mastered stable biped walking to cope with uneven ground. Plans reportedly are to develop machines that can be used for a range of activities, from manufacturing small electronics like smartphones — still mostly assembled by hand — to packing goods in warehouses and ultimately making home deliveries. Robots are already helping in many professions and fields, from parking lot attendees to tollbooth collectors to drugstore cashiers. A restaurant in Japan is almost fully staffed by robots that cook the food and wait on the tables. A surgical technology called Firefly elevates doctors’ ability to remove kidney tumors more safely and more efficiently at Georgetown University Hospital. An unmanned ground vehicle called MAARS (Modular Advanced Armed Robotic System) has been used to drag injured soldiers out of combat zones and monitor security in remote areas. A humanoid robot called Robonaut 2 aboard the International Space Station can perform simple tasks such as flipping switches and grabbing objects, relieving some of the challenging spacewalks required of astronauts. One thing that technology has always done is to extend our human senses (physical and mental), and many computer scientists think that robotics’ next big breakthrough will be vision (so that robots can see, not just learn). According to Sergey Brin, co-founder of Google, who leads the hush-hush R&D division known as Google X, cutting-edge machine learning is already capable of taking inputs such as vision — which is currently being used for Google’s self-driving cars. So the future machine will be a smart robot with a drone as part of its structure, or in today terms “a flying robot” providing a wide spectrum of services. Let’s just hope we are not building the first “Terminator.” Ahmed Banafa is a Kaplan University faculty member in the School of Information Technology. He has extensive experience in IT operations and management, as well as a research background in a variety of techniques and analysis. The views expressed in this article are solely those of the author and do not represent the view of Kaplan University."
218,https://www.computerworld.com/article/1525912/vc-investors-hot-for-the-cloud-mobile-and-robots-2.html,ComputerWorld,2014,8,19,649.0," Venture capital fundraising has picked up steam in the U.S., with cloud computing, mobile technology and robotics getting solid backing. U.S. technology companies are looking at a strong investment climate with global investor confidence in U.S. companies up significantly for the third year in a row, according to the 2014 Global Venture Capital Confidence Survey released by consulting firm Deloitte & Touche and the National Venture Capital Association, a trade group for the U.S. venture capital industry. “For the past three years the U.S. has seen a significant increase in investor confidence, continuing the trend which began to take hold in 2012,” said Jim Atwell, a managing partner at Deloitte, in a statement. “Improving capital market conditions lifted the pace of initial public offerings, fed by a strong lineup of new and innovative companies like we see on the Technology Fast 500 list, along with increased investor confidence both in the ability to fundraise, as well as to achieve favorable returns on investment.” So where are investors looking to sink their money? According to the 2014 study, which surveyed more than 300 global venture capital, private equity and growth equity investors in May and June, they’ve been largely backing  cloud computing, mobile technologies and robotics companies. The survey noted that for the second year in a row, U.S. venture capitalists named cloud computing as the area in which they were most confident of investing. Mobile technology came in a close second to the cloud and enterprise software wasn’t far behind. Robotics showed the biggest year-over-year increase in global investor confidence. In the survey, investors were asked to rate their confidence levels in different industry sectors on a scale of 1-to-5, with a score of 5 showing the most confidence. Investors, with a confidence level of 4.11, were the most interested in backing cloud computing. Mobile technologies came in next with a confidence level of 4.02. Healthcare IT and services was next with 3.94 and enterprise software showed 3.77. The survey also showed that global investor confidence in the cloud was up 2% from 2013. It’s holding steady for mobile and is up 9% for health care IT and services. Enterprise software is up 2% year-over-year. Robotics showed one of the biggest increases in global confidence with a 14% increase over 2013. “Both the cloud and robotics are hot areas right now, so there’s lots of opportunity for change there,” said Zeus Kerravala, an analyst with ZK Research. “This is great. Typically, innovation comes from start-ups, not the huge IT companies. These smaller, more nimble companies can bring innovative solutions to market. VC investments help drive new ideas.” Kerravala added that significant funding could help robotics companies create less expensive robots that work easily around humans, instead of on a factory floor. As for the cloud, solid financial backing could help expand the cloud into the mobile arena. “For the cloud, I think we can expect a broader set of cloud offerings that are really optimized for the mobile world,” Kerravala said. “A lot of the cloud offerings today are desktop apps that are made to run on the cloud. To me, cloud apps should be more predictive and have contextual knowledge of who you are, where you are and what you’re doing.” This kind of VC investment is good news for the tech sector, said Bobby Franklin, president and CEO of the National Venture Capital Association. “Continued confidence from global investors in the U.S. is welcome news for American innovators building next generation companies,” Franklin said in a statement. “In order to maintain this enthusiasm in U.S. innovation, policymakers in Washington need to come together to enact policies that support the creation of sustainable, high-growth companies that create jobs and drive economic growth.” So what are investors less interested in backing? The survey found that U.S. investors exhibited the least confidence in semiconductors, along with energy and clean technologies."
219,https://www.computerworld.com/article/1660992/technology-refines-education-at-indian-school-of-business-2.html,ComputerWorld,2014,8,17,591.0," The proliferation of technology has changed the way educators teach, the way students learn, and the way teachers and students communicate. At the Indian School of Business, a lot of emphasis is being laid on leveraging technology in creative ways to enhance its educational experience. We caught up with Jitendra Nath, CIO, Indian School of Business, to find out exactly how. Can you give us examples of how ISB is using technology to aid learning? For one, we have video conferencing systems in some classrooms across Hyderabad and Mohali campuses to enable combined lecture sessions. We also have an in-house application, Learning Management System (LMS), that provides a platform for out-of-classroom engagement between the faculty and students. Through LMS, students work around reading materials, classroom assessments/projects, and submissions within a deadline. Assessment of submissions and projects is carried out in the LMS and grades are awarded. A pilot of a faculty-moderated online Q/A forum has been initiated within the application. Also, we have a software integrated with the LMS system to check for plagiarism. We are also running a pilot project using tablets in classrooms to actively engage students. This enables the faculty to post questions/quizzes during the class with a set time limit, wherein the responses are fully visible to the entire class, allowing the faculty to assess the level of understanding of the class on a particular topic. The distance learning program of the school is highly technology driven. Classroom lecture sessions are recorded and shared through a cloud-hosted online portal for referral; tutorial sessions are conducted from an on-campus studio, where students participate remotely over a reliable Internet connection. Students don’t need to enter a classroom and are flexible to participate from anywhere in the world. Technology has an important role to play in the seating arrangement of a classroom, particularly in our post graduate program. A unique algorithm-driven application generates students’ seating layouts, which makes the same student take a different seat for different classroom sessions, thereby democratizing their line-of-sight with the faculty. Lastly, an online course bidding system is used to allocate elective courses to students. They live bid during pre-scheduled sessions that run into couple of hours to choose their elective in a very transparent and democratic manner. What is your team currently working on? Our key focus right now is to link classrooms across campuses to enable combined lecture sessions via live, interactive, near-immersive videos. Here, the focus is to introduce technology-enabled solutions into our existing classrooms, rather than building new ones around a given tech platform. We call this ‘Classroom Telepresence Project.’ A successful PoC was carried out last year and our pilot deployment in one classroom from each campus will go live within this month. This would be a one-of-a-kind solution and perhaps, a first in India. Since mobility is the order of the day, we have launched a full-fledged mobile application, named ISB 148, for marketing our PGP program. This application will help students familiarize themselves with ISB’s proposition in a very structured manner that incorporates information and interviews of various stakeholders. The second phase of our Mohali campus is under construction, where more capacity is being added as per our plan laid out earlier. This demands extension of the campus network, including a wireless LAN that can better enable new capacities like our IP telephony solution, which enables users to reach the other campus over 4-digit extensions. Enhancement of the existing wireless LAN and its extension to fresh work areas at Hyderabad is another major project that is currently underway."
220,https://www.computerworld.com/article/1525153/scientists-create-self-assembling-working-robots-2.html,ComputerWorld,2014,8,8,548.0," It would be strange enough to see a robot fold itself into arbitrary shapes and then just walk away. Then add to the mix that it’s a laser-cut origami robot and you have the new robotic technology created by a team of engineers from Harvard, the Wyss Institute and MIT. “The exciting thing here is that you create this device that has computation embedded in the flat, printed version,” Daniela Rus, the Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science at MIT, said in a statement. “And when these devices lift up from the ground into the third dimension, they do it in a thoughtful way.” The technology, which mimics the way amino acids fold themselves into complex proteins, demonstrates scientists’ ability to cheaply and quickly build sophisticated robots that can automate their own design and assembly process, according to Harvard. “Getting a robot  to assemble itself autonomously and actually perform a function has been a milestone we’ve been chasing for many years,” said Robert J. Wood, a professor of engineering at Harvard and the Wyss Institute. The universities contend that this is the first robot that can assemble itself and then perform a function — all without human intervention. “Imagine a ream of dozens of robotic satellites sandwiched together so that they could be sent up to space and then assemble themselves remotely once they get there,” said Sam Felton, a Harvard doctoral student, who worked on the project. “They could take images, collect data and more.” Researchers have been working on different pieces of this technology for some time. In May, MIT’s Rus announced that scientists there had made progress on the promise of 3D printed robots. The team created printable robotic components that, when heated, automatically fold into three-dimensional configurations. The researchers also figured out how to build electrical components — like resistors and inductors — from these self-assembling materials. MIT noted that the new self-assembling robotic work is similar, but a network of electrical leads, rather than an oven or hot plate, delivers heat to the robot’s joints to initiate the folding. “That’s exciting from a geometry standpoint because it lets us fold more things,” said Erik Demaine, an MIT professor of computer science and engineering. “Because we can do the sequencing, we have a lot more control and it lets us make active folding structures. Instead of just self-assembly, you can then make it walk.” According to MIT, the new robots are created with five layers of materials, all of which are created by a laser cutter. The top and bottom layers are made of polymer, which folds when heated. Those polymer layers hold two layers of paper, which in turn hold the middle layer. That middle layer is made of copper etched into a complex network of electrical leads. A microprocessor, batteries and tiny motors are attached to the top layer. Researchers are trying to use either a single, two or four motors. Each motor, which is controlled by the microprocessor, controls two robotic legs. Sharon Gaudin covers the Internet and Web 2.0, emerging technologies, and desktop and laptop chips for Computerworld. Follow Sharon on Twitter at  @sgaudin, on Google+ or subscribe to Sharon’s RSS feed . Her email address is sgaudin@computerworld.com. See more  by Sharon Gaudin on Computerworld.com."
221,https://www.computerworld.com/article/1523325/changes-afoot-as-darpa-challenge-robots-prep-for-upgrade.html,ComputerWorld,2014,7,25,865.0," As researchers at Worcester Polytechnic Institute (WPI) make progress in efforts to give their humanoid robot more autonomy, they’re getting ready to get their hands on an upgrade robot this fall. The robotics team that advanced into the finals of the DARPA robotics challenge has been working 50 to 60 hours a week getting ready for that last round of competition, which is set for next June 5 and 6 in Pomona, Calif. The teams are working to put their robots through a series of tasks — climbing ladders, walking over rubble, opening doors and driving cars — designed to ultimately lead to robots capable of working with humans after natural or man-made disasters. The 11 finalists, who are competing for a $2 million prize, include teams from WPI, MIT, Virginia Tech and NASA’s Jet Propulsion Laboratory. The WPI robot — a six-foot, 330-lb. humanoid device that the researchers have named Warner — is an Atlas robot from Boston Dynamics. Several other teams in the DARPA competition also use Atlas robots, and they all will have to do without them for the month of October. Matt DeDonato, the WPI team’s technical project manager, told Computerworld that Boston Dynamics will take its robots back at the beginning of October and spend the month upgrading them from the knees up. “It won’t look much different, but a lot of the systems will change,” said DeDonato. “We’ll be getting two to three computers onboard the robot. Now, they’re mostly off-board.” As of now, there’s just one processor inside the robot. Most of the computing is done by off-board computers with data sent back and forth on a fiber optic cable tethered to the robot. In the final challenge, there will be no fiber optic tether; the roboticists will go from having a 10Gbps cabled link to talk to the robot to a wireless link that only gives them about 300Mbps. With added onboard processors, the robot will be doing more of the calculations and decision-making itself, so there won’t be as much need to send information back and forth between the machine and its handlers. “Now, we have to migrate our software from the off-board computers to the onboard computers,” said DeDonato. “The software was designed for human control. Now we’ll restructure how the data flows through the system and what talks to what. The way we talk to the robot will have to be rethought.” He added that the team is considering putting all of the robot’s balancing and standing algorithms on one onboard processor. “That way, the balancing can run at real time and not be bogged down with other systems taking up resources,” said DeDonato. The team may dedicate another onboard processor to the robot’s vision, because that function is so computationally intensive. Many of those types of decisions will be made this fall. However, the WPI team has already been making big strides in autonomy. DeDonato explained that the robot’s handlers previously had to give it explicit instructions if they needed the machine to, for example, walk forward and open a door or grasp a tool. Getting the robot to pick up a drill meant the handlers would have to tell it exactly how far to extend its arm, turn its wrist and then close its fingers. Now they’ve reached an autonomy milestone where the handler simply tells the robot to pick up the drill. “We’re starting to see it come together,” said DeDonato. “We’re trying to minimize user input. I think it’s a big step forward. This is really along with DARPA’s vision. We’re monitoring, but we’re not making those low-level decisions for the robot. We’re trying to condense the commands that go to it.” WPI’s robot, Warner, also has achieved a higher level of autonomy when walking. Instead of having to tell the robot to take two steps to the right and then eight steps forward, for example, handlers can now just tell the robot to walk across a room and the machine will navigate around or over obstacles to make its own way. DeDonato said the robot itself is now using much more of the data from its own vision. Instead of simply sending what it “sees” to its handlers, the robot can decipher the information and calculate how to use it. “We’re moving the autonomy up one more level,” he noted. “Eventually, the goal is to move to the point where the user doesn’t have to be there.” But that won’t happen anytime soon, according to DeDonato. “We’re probably nowhere near it,” he said.” We’re probably 10 to 20 years at least from full autonomy. The problem is the environment is unknown. To go into a room and assess the damage and decide what actions to take is the Holy Grail. It’s not out of the realm of possibility, but that’s the movie robot. That’s still a long ways off. But we’re on the path.” Sharon Gaudin covers the Internet and Web 2.0, emerging technologies, and desktop and laptop chips for Computerworld. Follow Sharon on Twitter, at  @sgaudin, and on Google+, or subscribe to Sharon’s RSS feed . Her email address is sgaudin@computerworld.com. See more  by Sharon Gaudin on Computerworld.com."
222,https://www.computerworld.com/article/1522463/nasa-upgrades-humanoid-robot-in-space.html,ComputerWorld,2014,7,22,460.0," The 300-pound humanoid robot working on the International Space Station is in the midst of getting a series of upgrades, including new processors and software, in preparation of having a pair of legs attached to it. Robonaut 2, also known as R2, is the legless but humanoid robot that has been working on the space station since 2011. “Commander Steve Swanson focused his attention primarily on mobility upgrades for the station’s robotic crew member, Robonaut 2,” NASA reported on its website. “Since arriving aboard the station in May 2011 during the STS-134 space shuttle mission, Robonaut has been put through a series of increasingly complex tasks to test the feasibility of a humanoid robot taking over routine and mundane chores or even assisting a spacewalker outside the station.” In March, SpaceX, a commercial space flight company that runs cargo missions to the space station, brought up a pair of robotic legs for Robonaut. Once the legs are attached to R2’s torso, the robot will have a fully extended leg span of nine feet. That will give it “great flexibility” to move around the inside and outside of the space station, according to NASA. The robot has 38 PowerPC processors, including 36 embedded chips, which control the robot’s joints. Each leg has seven joints and a device on its foot, dubbed an end effector, a tool that enables the robot to use handrails and sockets. Since Robonaut was unpacked and set up on the station in 2011, astronauts have run experiments to see how the robot functions in space. NASA scientists also have been working with astronauts onboard the station to get them to use the robot and put them at ease with it. So far, the robot, which can communicate using sign language, has been able to correctly press buttons, flip switches and turn knobs. It also has worked with tools, using an air flow meter and an RFID inventory scanner, according to NASA. In preparation for attaching Robonaut’s legs, the astronauts installed new processors and replaced fans, a power distribution board and other components inside the robot’s torso. Last weekend, NASA’s robotics team on the ground remotely deployed software for the robot’s new processors. NASA has not specified what type of processors or what software has been added. NASA originally planned to install and test the robot’s legs in June, that didn’t happen and a new installation timeframe has not been announced. This article, NASA upgrades humanoid robot in space, was originally published at Computerworld.com. Sharon Gaudin covers the Internet and Web 2.0, emerging technologies, and desktop and laptop chips for Computerworld. Follow Sharon on Twitter at  @sgaudin, on Google+ or subscribe to Sharon’s RSS feed . Her email address is sgaudin@computerworld.com. See more  by Sharon Gaudin on Computerworld.com."
223,https://www.computerworld.com/article/1521886/under-google-robot-maker-reduces-dependence-on-military-funding-2.html,ComputerWorld,2014,7,18,653.0," A company acquired by Google that develops robots for the U.S. military appears to have greatly reduced its dependence on government funding, suggesting a reluctance on Google’s part to align itself too closely with military projects. When Google acquired Boston Dynamics last December, some questioned whether the firm’s military focus conflicted with Google’s pledge of “don’t be evil” and the virtuous image it nurtures for itself. “Google search and destroy,” quipped the U.K.’s Independent newspaper. “The internet giant (motto: ‘Don’t be evil’) has bought a pioneer of scary robot animals. Can its ethics survive?” Google said little to address the issue at the time and, perhaps illustrating the sensitivity of the topic, declined several requests this week to discuss Boston Dynamics and its work. But an analysis of federal procurement records for unclassified projects shows Boston Dynamics has accepted vastly less government money in the eight months since it was acquired by Google. In 2013, the company received US$31.2 million in grants from the Defense Advanced Research Projects Agency (DARPA), the U.S. Army and the U.S. Navy. That was roughly in line with the $33.2 million it received in 2012 and the $27.8 million in 2011. But so far in 2014, Boston Dynamics has received just one payment, of $1.1 million, the records show. The money came from DARPA in April and was for participation in the organization’s robotics challenge, which aims to stimulate robot research and attracts teams from around the world. It’s unclear why the funding has dropped so sharply, but it suggests a reluctance by Google to pursue military projects and align itself publicly with the U.S. government, especially when suspicion about clandestine government projects is running high. Two weeks before the April payment, DARPA said that Tokyo-based Schaft, another robotics company acquired by Google last year, had decided to stop accepting military funding and would pay for its own work. Receiving such government funding, particularly tied to military projects, can be an ethical quandary for scientists and others. “I know researchers really wrestle with this,” said George Lucas, a professor of ethics and public policy at the Naval Postgraduate School in Monterey, California. “It isn’t necessarily evil in all cases to work for the military, but it always raises the question of what exactly DARPA wants you to do.” Part of the concern, he said, comes from the researchers not having control over how their work will be used, or if it will be shared with the wider community. The robots developed by Boston Dynamics are some of the most complex and agile yet built. They include the LS-3, a four-legged mule designed to follow soldiers into battle carrying heavy gear; Robogator, a river surveillance robot; Cheetah, another four-legged robot that can run fast; and Atlas, a two-legged humanoid robot. The LS-3’s role in battle was highlighted this week when U.S. Marines put the robot through its paces in a variety of terrains as part of the RIMPAC multinational naval exercise in Hawaii. Scott Strawn, an IDC analyst who follows Google closely, said its “don’t be evil” mantra is more a PR statement than a fixed rule for business, but it does reflect an awareness at Google that making money depends on maintaining the trust of its users. That can create tension when areas of its work overlap with government and military goals. “There are just inherent aspects of their business that are going to be very interesting from a defense perspective,” he said. While Google’s robot ambitions are unclear, it may have little interest in becoming a supplier to the military. In December, it said it would “honor existing military contracts,” but that it did not plan to become “a military contractor on its own,” according to a news report at the time. Instead, it’s believed to be interested in robots for use in factory automation, home help, package delivery and even as explorers on future space missions."
224,https://www.computerworld.com/article/1526139/ready-the-robots-darpa-sets-finals-for-robotics-challenge.html,ComputerWorld,2014,7,1,458.0," Scientists from at least 11 robotics teams have less than a year to prepare to compete in the DARPA Robotics Challenge finals. DARPA (Defense Advanced Research Projects Agency),  the research arm of the U.S. Department of Defense, announced that the final stage of its three-phase challenge will be held June 5 and 6, 2015, in Pomona, Calif. The 11 teams will compete for a $2 million prize. DARPA has stated that it expects the number of entrants to increase. The three-part challenge is intended to advance autonomous robots to the point where they can become viable members of search and rescue teams during natural and man-made disasters. Scientists expect that one day robots will be able to largely act on their own to do things like enter damaged buildings, find human victims, turn off gas pipes and put out fires. The first part of the challenge was a simulation held in 2013. The second  phase, which was held in December in Florida, involved 16 teams competing to see which could build the best software to enable a robot to work through a series of tasks, including autonomously walking, using human tools, climbing a ladder and driving a car. The 11 finalists include teams from Worcester Polytechnic Institute, MIT, Virginia Tech and NASA’s Jet Propulsion Laboratory. Team Schaft, which finished in first place in the second phase of the competition, was expected to be a tough competitor the finals, but it withdrew from the event. Google, which owns Team Schaft and the hardware and software its roboticists built, pulled out so it could focus on commercial products. Google has bought at least eight robotics companies in the past year and appears to be focused on creating related hardware and software. The finals are expected to be considerably more difficult than the second phase of the challenge, which had robots taking on one task at a time. During the finals, the robotics teams will face a full-scale disaster situation, such as a fire or gas leak, DARPA said. The robots will be given a set of tools and a series of ladders, doors, cars and valves that they can use to handle the situation. As an added layer of difficulty, the robots will not be connected to power cords or wired communications systems or tethers. If a robot falls, it will have to get back up without human assistance. This article, “Ready the Robots! DARPA Sets Finals for Robotics Challenge,” was originally published on Computerworld.com. Sharon Gaudin covers the Internet and Web 2.0, emerging technologies, and desktop and laptop chips for Computerworld. Follow Sharon on Twitter, at  @sgaudin, and on Google+, or subscribe to Sharon’s RSS feed . Her email address is sgaudin@computerworld.com. See more  by Sharon Gaudin on Computerworld.com."
225,https://www.computerworld.com/article/1525958/a-good-robot-is-hard-to-find-or-build.html,ComputerWorld,2014,6,13,1063.0," WORCESTER, Mass. — In the first two days of the NASA robotics challenge this week, every team failed. Every. Single. Team. NASA officials and roboticists were disappointed, but not shocked, at the less-than-spectacular results at yesterday’s NASA robotics challenge — even though researchers from the likes of Rensselaer Polytechnic Institute, Worcester Polytechnic Institute (WPI) and Oregon State University had worked on the fully autonomous machines. It’s not that robotics is particularly new territory.  People by now are accustomed to iRobot’s Roomba robotic vacuum cleaning our floors. Robots help assemble automobiles and smartphones. A humanoid robot is even working on the International Space Station, while robotic rovers are exploring Mars, looking for signs that the planet has ever been able to support life. Those robots, though, aren’t fully autonomous. There are humans calling the shots behind the scenes. The robots that 18 teams from the U.S., Canada, Estonia and Mexico were looking to show off at a course on the WPI campus here this week are simply a different breed of machine. In its third year, NASA’s Sample Return Robot Challenge is focused on advancing fully autonomous robots. Researchers have built the hardware and software for robots that can traverse an area the size of one and a half football fields, find objects and retrieve them. At least, that’s what they’re designed to do. NASA is looking for technologies that can help its engineers build more advanced robots that will eventually be used in deep space, such as on Mars or on asteroids. These machines have been designed to start up, figure out where they are and then proceed to do their work —  all without human guidance. Sounds simple, but it’s not. “There are so many ways to fail at this,” said Jascha Little, a mechanical engineer on Team Survey, a Los Angeles-based group of individual researchers that competed unsuccessfully on Thursday. “We’re still experimenting. This is nothing you can buy. You throw together parts and software libraries and try to make a system out of it…. It’s just that hard.” After three years of work on their robot, after writing about 10,000 lines of code and sometimes spending as much time on this “spare-time project” as they do on their full-time jobs, Team Survey’s robot didn’t even make it off the starting platform. They weren’t alone in their frustrations. Of all the participants, which included teams ranging from groups of individual researchers to university-led teams, a handful never made it off the starting platform. But two teams nearly had a successful run. One robot built by a group from Estonia found a sample on Wednesday, the first day of the challenge. But  instead of retrieving the object, the robot accidentally ran over it. Another team from West Virginia actually found an object and was able to pick it up. However, the robot didn’t recognize that it had captured the objected and dropped it. It tried again and again, dropping it each time. By the time the robot stopped trying, the object had been driven into the ground. “It’s a problem of sensor fusion,” said Craig Putnam, an adjunct computer science professor and robotics instructor at WPI. “You have lots of different kinds of things you’re sensing and you have to fuse that all together to get the big picture. There’s vision, odometry, the tilt of the ground, colors and shapes…. It’s just a really hard problem.” Because there are so many systems — like energy management, stability and sensor integration — in an autonomous robot, the teams behind them are generally made up of mechanical and electrical engineers, software programmers and vision systems experts. It’s the technical version of “it takes a village.” With apologies to the future world of the Jetsons, it’s difficult to build a robot maid that will cheerily and efficiently clean the house without accidentally vacuuming up the cat. And that hasn’t been lost on the robotics community. At a recent MIT symposium, Rodney Brooks, co-founder of iRobot, a former MIT robotics professor and co-founder and CTO of Rethink Robotics, posed a rather divisive question: Why hasn’t robotics changed the world yet? Some of Brooks’ fellow roboticists disagreed with him, saying that robotics have changed the world. The issue is that people expect a Terminator or RoboCop version of a robot — and they expect it sooner rather than later. Alongside those lofty expectations comes Brook’s own take on the situation: “Robotics is just really, really hard.” Team Survey’s Little agrees with Brooks. But that doesn’t mean he’s discouraged. As Little sagged a bit on a bench on the WPI campus after his robot’s demonstration Thursday and mentioned how exhausted he was, he called the robot’s unsuccessful run a good learning experience, noting it was the first time the machine had ever worked on a large course for two hours. “This is a software problem and a sensor problem and a little bit of a hardware problem,” he said. “And everything had to work…. It’s hard, but it’s not just hard. It’s expensive.” Even so, Little said the group will be back next year to try again. Ken Stafford, director of WPI’s Robotics Resource Center and an associate professor of robotics, also remains hopeful. Stafford excitedly explained that he saw one team competing this week that had equipped its robot with Saran plastic wrap, much like many people have in their kitchen drawers. NASA, which needs its rovers on Mars to keep any soil or rock samples separate from each other so they don’t become contaminated, required the challenge teams at WPI to do the same. That’s where the Saran wrap came into play. If the machine has been able to collect anything on the challenge course, it would have been ready. That, said Stafford, was a great idea from a group of people thinking creatively about a problem. “Creating is taking things that exist and using them differently,” said Stafford. “It’s not about winning this challenge. It’s about getting what NASA needs done.” Many of the teams that failed their early challenges are expected to try again on Saturday if the weather holds. Sharon Gaudin covers the Internet and Web 2.0, emerging technologies, and desktop and laptop chips for Computerworld. Follow Sharon on Twitter at  @sgaudin, on Google+ or subscribe to Sharon’s RSS feed . Her email address is sgaudin@computerworld.com. See more  by Sharon Gaudin on Computerworld.com."
226,https://www.computerworld.com/article/1524846/nasa-challenge-tests-autonomous-robots.html,ComputerWorld,2014,6,11,584.0," This week robots with no human handler will be making their way through a field, searching for hockey pucks, blue rocks, wooden cubes and the like. The robots won’t even know what some of the objects are. The search effort is part of the NASA’s Sample Return Robot Challenge at Worcester Polytechnic Institute (WPI) in Worcester, Mass. today through Friday, where the agency hopes to find interesting technology from 18 robotics research teams. NASA hopes the technology used at the challenge can help build improved robots for use on Mars or asteroids. The agency also hopes the new technology can help create robots that can build human habitats on other planets or moons. This is the third Sample Return Robot Challenge that has been held at WPI. “We’ve been ecstatic at the progress we’ve been making over the past few years and can’t wait to see what’s going to happen this year,” said Sam Ortega, NASA challenge program manager. “We need to have a system that will drive forward without falling off a cliff. We need a system that will drive forward while searching for something. All the robotic systems have to work together.” That is the tricky part for robotics. Ortega explained that engineers have designed good sensing systems. They have good cameras. They have good mobility mechanisms. The hard part is getting it all working together. “The concept of autonomy isn’t foreign to NASA but the concept of full autonomous activity is difficult,” he added. “That level of autonomy is what we need to work on.” The teams, from organizations and universities such as Rensselaer Polytechnic Institute and Oregon State University, focused on creating robots that can work to work their way around a field the size of a football field and a half, while searching for and grabbing various objects without any guidance from humans or GPS. The easiest samples to find are worth a point each and range from a red hockey puck to an orange PVC pipe. The samples considered to be a level harder to find are worth two points a piece include a blue rock and a wooden cube. The research teams are told what these items are so they can code them into the software, telling the robots what they should be searching for. However, the teams have not been told about the hardest samples to find – and those are worth five points each. So how does a robot find something if it doesn’t know what it’s looking for? Ortega  said it needs to be programmed to recognize something that is out of place. For instance, a robot should recognize that a pine cone would be normally found in a New England field. However, an eight-sided piece of metal would not be naturally found there so the robot should know to retrieve it. The challenge has a total pot of $1.5 million and can be split between various teams depending on how many points they earn. To be eligible for any prize money, a team’s robot would have to retrieve at least 3 samples. If one team earned more than 15 points, and no other team was eligible for any prize money, that team could take home the entire $1.5 million. Sharon Gaudin covers the Internet and Web 2.0, emerging technologies, and desktop and laptop chips for Computerworld. Follow Sharon on Twitter at  @sgaudin, on Google+ or subscribe to Sharon’s RSS feed . Her email address is sgaudin@computerworld.com. See more  by Sharon Gaudin on Computerworld.com."
227,https://www.computerworld.com/article/1523855/hail-cyborgs-the-line-between-robots-and-humans-is-blurring.html,ComputerWorld,2014,6,6,1141.0," As robotics quickly advance, scientists say the lines between robots and humans is beginning to blur. That means one day with robotic prosthetics that work seamlessly with a human’s muscles, with tiny robots that swim in our blood streams and fix medical problems and nano-scale robots implanted in our brains, we will become robotic humans. As scary and sci-fi as that may sound, researchers say robotics will cure diseases, make amputees feel whole again and greatly extend our lives. “It’s not a question of whether it’s fanciful,” said Daniel Wilson, author of the novel  Robopocalypse and a robotics engineer with degrees in machine learning and robotics from Carnegie Mellon University. “Thinking of the nanorobots swimming in your blood cells is still pretty far out there, but there are much more concrete examples really in the works…. By utilizing technology, you’re able to improve your body beyond anything you could do in the past.” Many, if not most people, will be wary of the idea of the melding of humans and robots, with images of Star Trek’s evil cyborgs running through their heads. The fictional characters — with both human and mechanical parts — have superhuman strengths but have lost their individualism. Despite frightening images in the Star Trek movie series and Robocop, these actually are exciting times because the advances in robotics, said Victor Walker, a robotics research scientist at Idaho National Laboratory, an Idaho Falls, Idaho-based facility that focuses on energy and national defense research. “We are currently in this revolution today,” Walker told Computerworld. “I think there’s potential there. We don’t want to replace humans. We want to enhance humans.” And that is already happening. More than six years ago, a University of Arizona researcher who had successfully connected a moth’s brain to a robot predicted that by 2017 or 2022 we’ll be using “hybrid” computers that run a combination of technology and living organic tissue. Robotic exoskeletons  have helped people suffering from paralysis walk again and the U.S. military is just weeks away from testing a new exoskeleton, or Iron Man-like suit, designed to make soldiers stronger, give them real-time battlefield information, monitor their vital signs and even stop their bleeding. Robotic prosthetics, using a built-in computer, 100 sensors and 17 motors can take natural cues from a user’s residual limb, giving him or her the dexterity and grace to play a piano. “The line between robots and people will be blurred with smart prosthetics and implanted components,” said Russ Tedrake, an associate professor in electrical engineering and computer science at MIT. “It won’t be robots and people but robot people…. If you were in distress and given the choice for a longer, more comfortable life by simply replacing your spleen with a machine that could do the same job, wouldn’t you take it? What if it was a part of your brain?” Robots and people both have their limitations and their advantages. “This, I think, is the ultimate reason why [linking] robots and people might make so much sense,” said Tedrake. “Perhaps we can combine the best of both worlds.” Tedrake added that he doesn’t think we’ll have tiny humanoid robots running around inside our bodies anytime soon. But robotics will have a significant role in supporting or replacing parts of our bodies. Walker noted that Glass, Google’s prototype of a wearable computer, is helping to blur the line between human and robotics. The computerized eyeglasses are not traditional robots, but they do have sensors, computational smarts and can help people relate to the world around them with maps and other applications. Google Glass is designed to enhance the human experience. By making the computerized eyeglasses less passive, they take a big step toward being considered a robot. “There’s a lot of potential for enhancing us with machines,” said Walker. “Things like Google Glass and prosthetics are already happening. We’re always comfortable with just a little change. That’s how this will happen — slowly. We’ll be very accepting of things that make our lives easier or better.” Analysts say the advance of human robotics will largely come from the medical industry, as with robotic prosthetics, and the military, with robotic exoskeletons. “That’s usually where investments start,” said Walker. “We’re excited about every technology enhancement, because it makes our lives better. I think there will be fear in the first steps. It will start with, ‘Hey, let’s enhance your capability,’ and then ‘Let’s attach something to the outside of your body.’ We’ll start with things we can strap on and take off. Then it will move to machines inside our bodies.” Wilson, whose novel Robogenesis is due to be released June 10, noted that advances in hearing aids is a good example. The new hearing aids now are Bluetooth enabled, turning the devices into a new type of ear bud for cell phones and streaming music. “If I could stream music or a cell phone conversations or Skype directly to my auditory nerve, man I’d find that exciting,” he said. “If you want to look at what the next generation is going to be, look at people who have serious problems they want to solve. And they have problems that are being solved by robotics.” Dmitry Berenson, an assistant professor of computer science at Massachusetts-based Worcester Polytechnic Institute, said the advance of robotics will happen slowly, which will help people adjust to the change. “It’s really happening faster than anyone predicted,” said Berenson, who works with WPI’s robotics engineering program. “These are tools. Much like a car or airplane, it can be used to help people and make life better or it can be used like a weapon. Look at the exoskeletons. The military could use them to create super strong soldiers, but they can also be used to help the disabled. “If you’re paralyzed from the waist down and someone gives you a pair of exoskeleton legs, that’s enormous,” he added. “With technology, it’s not good or bad. It just comes as a package and we as a society have to decide what we’re going to do with it. It’s really up to us. It’s not the technology. It’s what we do with it.” All technology is about change and change is scary, so adding robotics to the human body is going to scare a lot of people. “We’re talking about changing the human body in unprecedented ways and giving people abilities they’ve never had,” said Wilson. “Sure that’s scary, but it’s also exciting to be able to help people and to grow into the future. It’s incredibly exciting and I can’t wait.” Sharon Gaudin covers the Internet and Web 2.0, emerging technologies, and desktop and laptop chips for Computerworld. Follow Sharon on Twitter at  @sgaudin, on Google+ or subscribe to Sharon’s RSS feed . Her email address is sgaudin@computerworld.com. See more  by Sharon Gaudin on Computerworld.com."
228,https://www.computerworld.com/article/1520610/in-big-step-for-robotics-one-robot-repairs-another-in-space.html,ComputerWorld,2014,5,29,843.0," A robot completed repairs on another robot in space this week, advancing the possibility of future robots working in deep space, as well as Earth-based robots working in the enterprise. “Yes, that was a big step,” said Mathieu Caron, the Canadian Space Agency’s mission control supervisor. “Every new repair we do, it further illustrates how useful robotics is and it shows how robotics can contribute to manned and unmanned missions. This robotic work will be even more important as you travel further from Earth.” Caron also told Computerworld that the agency’s work with robots in space contributes to the ability of  robots on Earth to work in remote and dangerous areas. “One of the key points of usefulness of robotics is the ability to accomplish tasks in areas that are hostile to human beings, whether it’s deep in the sea, in a mine or a nuclear power plant,” he said. “We send robots where we don’t want to send humans. The ability to use robotics to accomplish tasks in hazardous areas is very important and we furthered that.” The Canadian Space Agency, working hand-in-hand with NASA, wrapped up a robot repair job this week. What made this repair different is that the robotics system being repaired is connected to the outside of the International Space Station, and astronauts weren’t the ones out in space doing the work. It was repaired by another robot. Several robots are working on the space station.  There’s a humanoid robot that is tasked with cleaning the orbiting station and that may one day perform maintenance outside of the station, relieving astronauts from many dangerous spacewalks. There’s also a Japanese robot that was designed to hold conversations with the Japanese astronauts living onboard. However, the real robotic workhorses on the space station are Canadarm II , the orbiter’s primary robotic arm, and Dextre, a two-armed robot that also works outside of the space station. Canadarm II is the Canadian-built, 56-foot-long robotic arm that was used to assemble the space station while in space. The robotic arm is routinely used to move supplies, equipment and even astronauts. It’s also used to grab hold of cargo ships that bring supplies to the space station. Dextre is a $200 million, Canadian-built robot that stands 12-feet tall and has a 30-foot wing span. Dextre is often used to reach into cargo ships and unload the supplies, spare parts and scientific supplies. For the past two weeks, Dextre has been used to work on two malfunctioning cameras on the mobile robotics system, which consists of the two robotic arms, as well as a mobile base to which the robots are attached in order to move easily along the outside of the station. Caron explained that a camera on the Canadarm, as well as one on the mobile base, have not been working properly. Both cameras are used to give mission control on the ground, as well as the astronauts on the station, the ability to see what is happening outside when they’re using the robots. The camera on the mobile base wasn’t working and had been removed during a spacewalk last summer. The one on Canadarm II was still usable but delivered a hazy image. Caron said  that  Dextre, while controlled by a team of engineers in Montreal and Houston, last week transferred the hazy-image producing camera from Canadarm II to the mobile base. Engineers calculated that it worked well enough for what video would be needed from that position. Then on Tuesday, Dextre took a spare camera that had been stored inside the space station and positioned it on Canadarm and hooked it up. Both cameras are now working. “This is the first time we’ve shown that a robotics system can fix itself,” said Caron. “We’re very excited to have the ability to fix a system on orbit. This frees up the astronauts. Every space walk has a certain element of danger, but also it monopolizes the astronauts for not only the spacewalk but the preps leading up to it. While they’re doing that, they’re not available to do the science and research missions on the station.” This means that robots like Dextre can be used to repair satellites orbiting the Earth. However, it also means that as the space station needs more critical repairs, robots will be able to take on more of the work. “If you look at the space station, the first module was launched in 1998,” noted Caron. “We have an aging space station. As it gets older, it needs more maintenance, so Dextre’s role becomes more and more important as we need to replace more components. It’s going to be a very busy time for us.” This article, In big step for robotics, one robot repairs another in space, was originally published at Computerworld.com. Sharon Gaudin covers the Internet and Web 2.0, emerging technologies, and desktop and laptop chips for Computerworld. Follow Sharon on Twitter at  @sgaudin, on Google+ or subscribe to Sharon’s RSS feed . Her email address is sgaudin@computerworld.com. See more  by Sharon Gaudin on Computerworld.com."
229,https://www.computerworld.com/article/1495278/older-adults-worry-kids-will-be-lazy-if-robots-don-t-have-parental-controls.html,ComputerWorld,2014,5,6,650.0," Some roboticists have been worried that healthcare providers would not accept robots out of fear the machines might replace them in the workplace. But a recent Georgia Institute of Technology study found that when healthcare providers such as nurses were “offered an assistant, they preferred it to be a robotic helper rather than a human.” But that was only for some interactions, such as everyday tasks like housework and reminders to take medicine that could be completed by assistant robots. In a 2012 study, the lab “found older people are generally willing to accept help from robots.” That may depend, however, on whether the robots are companion robots or assistant robots. Companion robots provide emotional support and are meant to be a user’s buddy; they interact like a friend and could watch a movie or play a game with a user. Many robot designers and developers are aiming at an audience of older adults, but those adults may not be inclined to use companion robots out of fear that children will be negatively affected. Researchers from Penn State asked 640 retirees over the age of 60 if they believed robots might “make them lazier and encourage them to interact less often with other people” and “then asked similar questions about the effects of robots on young people.” The older adults were not concerned about becoming physically and emotionally dependent on robots themselves, but “they worried that young people might become too dependent on them.” “A companion robot provides the user with a source of friendship. They might watch TV with the participant, provide emotional support, or complete an activity with the user,” explained Penn State PhD student Frank Waddell. “This interactivity may be one reason that users tend to attach human-like emotions to companion robots.” Older adults in this study “did not seem to show the same level of apprehensions about assistant robots.” “We’ve seen this type of effect, which is usually referred to as a third-person effect, with different types of media, such as video games and television, but this is the first time we have seen the effect in robotics,” Waddell stated. “According to a third person effect, a person says they are not as negatively affected by the media as other people.” Robot designers have been developing companion robots for retirees, but if seniors are worried about robots having negative effects on children, as this study suggested, then Waddell said designers need to include “some type of parental controls” for the robot interface. Adding parental controls might help “convince adults that they can own and use robots and still protect children from their fears that the devices might lead to laziness and dependency.” While older adults might like to keep children away from companion robots, if robots were to read about “vampire therapy,” then the bots might be concerned about leaving kids alone with older adults. Vampire therapy: Robots, hide kids from senior citizens! Scientists, according to the Telegraph, believe that “transfusions of youthful blood may halt or reverse the aging process” and may even cure Alzheimer’s disease. After repeatedly injecting old mice with blood from young mice, scientists claim that “young blood actually ‘recharges’ the brain, forms new blood vessels and improves memory and learning.” There were two different studies, but the one from Harvard identified a “youth protein” known as GDF11, “which circulates in the blood.” It “is responsible for keeping the brain and muscles young and strong.” In fact, “GDF11 has an amazing capacity to restore aging muscle and brain function.” Researchers hope human trials for this “vampire therapy” will begin in two or three years. Maybe if older adults were really to embrace companion robots, then they could make them into vampire hybrids to help gather young blood and hurry along the process to reverse aging? No worries, I’m teasing. Both companion robots and “vampire therapy” have a lot of potential for good."
230,https://www.computerworld.com/article/1661886/capex-to-opex-how-gokaldas-cio-convinced-his-vendor-to-move-to-a-new-payment-model-2.html,ComputerWorld,2013,1,21,738.0," The Organization: Gokaldas Exports is the royalty of the Indian garment industry. Its pedigree goes back four decades to when J.H. Hinduja founded the company and quickly put India on the global apparel map. Today, the Bangalore-based bulk manufacturer and exporter of ready-made clothes works with high-street clients that include Nike, Diesel, Levi’s, and Abercrombie and Fitch, among others. The Business Case: Like most other businesses worldwide, sluggish sales and rising input costs have forced Gokaldas Exports to look inward to cut costs and improve the health of their bottom lines. When the company’s leaders began looking in 2011, what they found surprised them. “When we did a study to find out the areas we needed to improve in, we found that Gokaldas was losing a considerable amount in value chain loss,” says Yatendra Kumar, head-IT, Gokaldas Exports, who was part of the team in search of new efficiencies. Value chain loss is a challenge many manufacturers face. At Gokaldas Exports, it came in the form of raw fabric that was left over after a roll of cloth had been cut. According to Kumar the company was losing upto 10 percent of raw material in the manufacturing process. Given that Gokaldas purchases raw fabric worth about Rs 800 crore a year, it was taking a Rs 80 crore hit, approximately, in value chain losses a year. “That’s huge,” says Kumar. ‘Cut Plan’ software, Kumar knew, could reduce raw material wastage. The software, which is available off-the-shelf from Provab Technosoft, allows workers to input the size, type, and other specifications of the garment to be made and uses an algorithm to generate an optimal cutting plan. This plan is then fed into the systems at the cutting lines. “We ran a test for the software about six months towards the end of 2011 and saw great results,” says Kumar. There was only one problem: The software was very expensive. The Challenge: “We realized we needed to invest about Rs 8-10 crore to install the software at all 30 factories,” says Kumar. And that didn’t include other overheads like training costs. Once upon a time, an ROI window of a year might have been acceptable, but not in today’s climate. So Kumar then tried looking for a more cost-effective alternative to software Provab provided, but came up empty. “Most other players had standalone modules. Clustering many of these smaller apps together would lead to integration challenges,” he says. So Kumar revisited his original idea of working with Provab, only this time he focused getting them to agree on an opex model. The question was: How was he going to convince a software provider, which didn’t have a cloud offering—and probably knew it had a monopoly in its niche market—to agree to anything but a capex model? The more Kumar thought about it, the more he realized he needed to look at it from Provab’s perspective. “It already had the software ready. If they didn’t agree to a more flexible model, the software would just sit idle with them,” says Kumar. Kumar also knew that Provab was confident of the enormous savings its technology could bring. He decided to challenge the company: If they really believed in their product, they would accept no upfront payment and only 5 percent of what Gokaldas saved using its software for three years. If Gokaldas could save 8 percent on raw material purchases worth Rs 800 crore, Provab’s take would be in the range of about Rs 3 crore. He then hung the alternative before Provab: Make no money at all. Provab got on board. Benefits: It’s been nine months since the software was deployed at Gokaldas’ factories. Initially, Gokaldas saw only a 2 percent saving as it got used to the software. But over time, Kumar says, the needle moved up to 5 to 6 percent. At 5 percent, Gokaldas is still saving about Rs 35 crore a year—even if it lowers its buying to Rs 700 crore of fabric a year. Kumar says that they plan to increase savings to 8 percent, which works out to about Rs 6.4 crore a year—a feat that would have been impossible if Kumar had decided to listen to conventional wisdom and cut his coat according to his cloth. When we did a study to find out the areas we needed to improve in, we found that Gokaldas was losing a considerable amount in value chain loss."
231,https://www.computerworld.com/article/1661365/hyderabad-and-delhi-have-become-it-benchmarks-for-other-airports-in-india-cio-of-delhi-airport.html,ComputerWorld,2013,1,8,1490.0," It wouldn’t be an exaggeration to say that it was Davesh Shukla who coined the term ‘airport IT’. It was unheard of when Shukla took over as the CIO of the Hyderabad International Airport (HIAL) in 6686. He planned and designed the entire IT system and managed to reduce the outsourcing costs of $22 million by developing, training, and mentoring in-house talent. Though he has worked on airport projects in London and Istanbul, HIAL is close to his heart. But that didn’t stop him from transforming Delhi International Airport’s (DIAL) IT, which has been in the news for ‘all things technology’ in the last two years. As the CIO of DIAL, Shukla has done a fine job in this transformation. Excerpts from our interview with him: If I have a partnership with a vendor, I’d want to get into a long-term partnership with just that one vendor who will be able to offer all the services I want. It wouldn’t be an exaggeration to say that it was Davesh Shukla who coined the term ‘airport IT’. It was unheard of when Shukla took over as the CIO of the Hyderabad International Airport (HIAL) in 6686. He planned and designed the entire IT system and managed to reduce the outsourcing costs of $22 million by developing, training, and mentoring in-house talent. Though he has worked on airport projects in London and Istanbul, HIAL is close to his heart. But that didn’t stop him from transforming Delhi International Airport’s (DIAL) IT, which has been in the news for ‘all things technology’ in the last two years. As the CIO of DIAL, Shukla has done a fine job in this transformation. Excerpts from our interview with him: With your rich global experience, what according to you is the kind of breakthrough India has made technologically in the aviation sector? The Indian Civil Aviation Sector has gone through a major overhaul since the privatization of airports. IT and surrounding infrastructure has become more complex than it was ever before. Earlier, individual airlines would set up their own small network and check-in systems. But now it’s managed and owned by the airport. Because of privatization, a lot of workforce has shown interest in airport IT technology. This wasn’t the case when I came to India. But now, a credible amount of workforce is well-equipped with airport IT systems. This trend is here to stay. Since the Indian government plans to modernize ten more airports, it should attract more IT workforce to airports. While you can find people with core IT skill set in the job market, it’s a tough task recruiting people with the skill set for specific apps such as operational databases and check-in systems. Therefore, we provide in-house training to our airport IT staff. You have carved yourself a niche in the aviation sector. From 12 years as head-IS at London City Airport to strategic advisor-IT at Istanbul and then your last profile at Hyderabad International Airport before you took over the CIO position at DIAL two years ago, how has the current role been different from all the different hats you have donned in the last 18 years? Eighteen years ago, I started my career in the airport IT industry, and I have moved across airlines and airports since then. In the UK, the workforce wasn’t IT-dependent. People could manage if the network or systems went down, and you could still make processes work. It isn’t the same anymore. The reliance on IT has increased exponentially. Today, even when the check-in systems don’t work, the airline staff isn’t prepared to carry out an alternate like manual check-in. We know we can’t afford that. It’s a fallback on the CIO. Therefore, the emphasis on technology is very crucial and has become very critical to business. What about your role? IT is no longer an enabler. Where I come from, I was a part of the steering committee. And I always push myself further ahead. Earlier, IT wasn’t discussed at the C-suite level. But now, the trend is such that we don’t discuss technology, but about its impact on business at the same level. Today, we deal with crucial questions like “Are the SLAs being met?”, “What‘s the impact on the customers or stakeholders?” and others. For instance, our chief commercial officer is preparing for Starbucks’ setting up shop in the Delhi International Airport. It’s going to bring us a lot of business. Therefore, it becomes important for a CIO, like me, to make sure that the customer requirements are being taken care of. The CIO’s role has an additional customer–facing side to it apart from making decisions for the business. Last year, three Indian airports—out of which two were under your helm and IT leadership—made it to the top 20 list of best airports. What was the role of IT in spearheading this into a success story? Maybe we didn’t spearhead it. We didn’t drive those forces. There are basically 36 parameters which range from processing time at check-in counters, which should be maintained at two minutes; flight info space systems; Wi-Fi connectivity; mobile phone coverage; and PAS announcements among others. At DIAL, we ensure that these 36 parameters maintain an organizational score. A recent Frost and Sullivan report states that airports should track, record, and measure real-time passenger data within the airport environment to understand passenger behavior and improve KPI targets. What are some of the KPIs in DIAL? Is passenger experience the primary objective? It’s about making money, like in any other business. The key is to keep the passengers, customers, airlines, retailers, concessionaires, and stakeholders happy. This will not only ensure that our customers do more business, but this will help us perform better. At present, we are looking at various modes to do mobility in applications. We have designed two applications. The first application is meant for passengers who use Windows 8. This application has a host of utility features such as real-time flight information, live weather updates and access to all passengers’ facilities available at the airport. The other application, meant for iPads, is for internal purposes. We are also in the process of launching a tablet for check-in, which is meant for business class passengers. We are actually the second best airport in the world with an on-time performance of over 90 percent. This is a tremendous change since we were ranked last but one a few years ago. What are some of the major IT challenges in the airport industry in India? You have mentioned in the past that working with multiple vendors and hiring efficient IT workforce are some. How have you overcome these bottlenecks? It’s still vendor management. For instance, in larger IT organizations, there are multiple domains and verticals. It’s like how IBM’s into servers, but does consulting as well. They do all kind of services that KPMG and Deloitte specifically indulge in. Now Oracle’s also moving into this space. This is going to be crucial for us. If I have a partnership with a vendor, I’d want to get into a long-term partnership with just that one vendor who will be able to offer all the services I want. Is data security a challenge? Are you investing in any other technologies? We take it very seriously. We have been certified on ISO34181, the ISO standard for information security management system for two years now. Our security initiative is in tandem with other technologies such as cloud and VDI, in order to keep our internal data on the server rather than individual laptops and desktops. At present, none of the airports have done it. We will be the first airport to have a desktop environment on the cloud. We also have a couple of airport security initiatives. Biometric authentication is one. The other effort is to consolidate access control systems across all airports in India into a central unit. I’m heading the board representing India. We are waiting for the government’s approval to start work. What’s your yearly IT budget? Do you plan to increase it in the next year? How does the future look for DIAL under your leadership in 2013? Our annual IT budget is between 7-10 percent of our revenue, and our strategic initiative in DIAL is to bring innovation. In the future, DIAL will focus on mobility and passenger experience. We were in talks with Toronto airport in order for us to invest in NFC devices using wireless access/mobile devices which can track a passenger’s expenditures. For instance, I can tell you how much a traveler might have spent on a particular brand on Thursday between 11-2 a.m. We have that analytics. We know the people and what they are spending. We want to be more interactive with our passengers and improve their experience with every travel. Shubhra Rishi is a correspondent for CIO India and ComputerWorld India. Send your feedback to shubhra_rishi@idgindia.com. Follow Shubhra on Twitter at @ShubhraRishi."
232,https://www.computerworld.com/article/1661962/avoiding-vendor-lock-in-itc-shows-the-way-2.html,ComputerWorld,2012,11,6,598.0," The USP of the luxury hotel group ITC has always been offering traditional Indian hospitality to the judicious traveler. With over 90 hotels in nearly 70 destinations across the country, the group amalgamates time-honored tradition with globally-benchmarked services and sustainable business practices. But when tradition begins to choke business development, it’s always a good idea to tip the scale towards change. And that’s exactly what Bhujay Bhatta, operation manager- IT shared services, ITC Hotels, did. Until last year, ITC Hotel’s business-critical systems ran on a RISC-based platform, solely dependent on a limited choice of vendors for support. “Applications like ERP and CRM were traditionally run on proprietary RISC-UNIX environment. In all of this, managing a UNIX platform was extremely expensive,” says Bhatta. This made maintaining features like high performance, scalability, and RAS (reliability, availability and serviceability) incredibly expensive. “But application and hardware vendor support was superior to its counter part—the CISC-based platform. And that kept us going,” adds Bhatta. However, the RISC-based server industry received major jolts last year when big players like Oracle stopped developing software for the platform (the battle between Oracle-HP was one of the major reasons). Many enterprises that still employed RISC servers in their infrastructure could now rely on only a limited choice of vendors for support. Handling aging RISC infrastructure with leases inching towards expiry dates, longer payback duration for a significant capex, high power consumption, datacenter space, and cooling costs became challenging. By putting backbone applications like ERP at stake, the business was risking a downtime that could last up to months. Left with no choice, Bhatta perceived a huge risk—in the long run—to support the company’s infrastructure. “We began to sketch an alternate platform strategy. Our leadership team along with the internal IT team studied future trends in server hardware and OS development,” says Bhatta. The research had to be meticulous and in-depth as the new platform would have to match or be better than the performance and reliability that the RISC platform provided. “It had to give us cloud-like benefits like elasticity and agility. But most importantly, it had to have a low cost of ownership. We finally zeroed in on Open Source,” says Bhatta. His internal IT team developed an innovative infrastructure design built on open source with a combination of different infrastructure components. “We focused on details such as server hardware type, suitable Linux variant, type of virtualization and clustering to deliver a superior infrastructure platform that would reduce IT cost,” he adds. Also, this time Bhatta was careful not to get locked in by completely eliminating dependency on server hardware. “We chose SUSE Linux for SAP and Redhat for other important apps. Existing SAP servers which are due for technology upgrade will also be migrated to Linux-Xeon-based environments. Apart from a direct benefit on account of the low cost of Xeon-Linux Environment, this would also free up space, power and cooling capacity from our datacenter,” Bhatta says. The entire landscape has been implemented on Xeon-Linux servers with 64390 (SAPS) or 958 traditional RISC-UNIX equivalent CPUs. The options to move towards change now or later are always questioned, but never for the necessity to change. As Bhatta says, “It’s all in the game of taking smart risks when the stakes are high.” “We chose SUSE Linux for SAP and Redhat for other important apps. Existing SAP servers which are due for technology upgrade will also be migrated to Linux-Xeon-based environments. Apart from a direct benefit on account of the low cost of Xeon-Linux Environment, this would also free up space, power and cooling capacity from our datacenter,”"
233,https://www.computerworld.com/article/1661393/transforming-the-journey-lifecycle-of-a-passenger-with-its-help-bial-cio.html,ComputerWorld,2012,10,8,536.0," The top decision-makers of BIAL like to quote the oxymoron “Change is constant” often. In early 6688, when we spoke to the then CEO of BIAL, Albert Brunner, he informed that he was forced to expand the Bengaluru International Airport considerably with the challenge of setting up a greenfield airport. The expansion that followed underlined the importance of change. Even this year, the new Director, Operations, BIAL, Daniel Bircher had some new announcements to make. Whether it was partnering with IBM or capitalizing on the adoption of international standards such as Aviation Information Data Exchange, Bircher seemed intent on revolutionizing customer experience in the next five years. This he plans to do with the help of IBM’s smart airport enabler solution. Francis Rajan, vice president, ICT, BIAL, spoke to us briefly about his role in the entire implementation. As the service provider for all our clients, we needed to architect a solution framework and design a business model. What has been your role as a CIO in the entire implementation? As the service provider for all our clients, we needed to architect a solution framework and design a business model. Therefore, we decided to adopt a patent-based model in order to deploy an international grade integrated solution. This would help us enable plug-and-play services for our various stakeholders. The challenge was migrating from a siloed approach to a common user platform. But we were able to convince the stakeholders. In order to establish credentials, we set up higher level service quality and standards and ensure security for multiple clients working on the same network. Therefore, the first step was to certify ourselves on ISO34181 and ISO66800 standards for IT service delivery. The role of a CIO, CSO, and CTO compounded—all in one. Now, along with the new internal expansion, we are also creating a tier-3 green datacenter to offer datacenter and campus cloud services. This paradigm shift has increased efficiency in terms of performance, services, and touch points for a complete journey lifecycle. What were the challenges involved? We worked closely with our partners, IBM, to make them understand the criticality of the project. As an airport, we can’t afford to have downtimes, and IBM comprehended this requirement well. After implementing the new framework, we conducted parallel testing for 2-3 months. In short, we were able to execute the entire framework in a matter of five months. How has the user experience transformed post the implementation? The automation has resulted in a near real-time experience while collecting data from various dashboards or touch-points such as ATMs, and check-in counters. This is slated to enhance our services and transform customer experience. Most importantly, technology has transformed users’ lifestyle for the better. Now we are connected anytime, anywhere. Whether it’s an airport lounge, home or office, service providers need to deliver services that meet users’ expectations and lifestyle. The lifecycle of technology is becoming increasingly shorter. Consequently, there are many stakeholders in the airport—money exchanges, baggage handling, security, retail, tickets, and airlines among others. With the new solution in place, we have near real-time collaboration between these stakeholders. The next step is to transform the entire journey of a customer—starting from booking a ticket to reaching the destination."
234,https://www.computerworld.com/article/1694533/ray-kurzweil-it-will-be-everything.html,ComputerWorld,2006,1,9,1125.0," Inventor, writer and futurist Ray Kurzweil has been a pioneer in speech and character recognition, reading technology, music synthesis, virtual reality and artificial intelligence. He has founded nine businesses in those fields, including Kurzweil Technologies Inc. in Cambridge, Mass., and he’s won numerous awards, including the National Medal of Technology. In his recent book, The Singularity Is Near: When Humans Transcend Biology (Viking Adult, 2005), Kurzweil, 57, predicts that ultimately, human intelligence and computer intelligence will fuse and become indistinguishable. He recently told Computerworld how and when that might come about. Your idea to reverse-engineer the human brain seems pretty far out. Until recently, we haven’t had the tools to scan the brain with sufficient resolution. But there are five or six new scanning technologies. For the first time, we can see the brain creating our thoughts. The amount of data we are gathering about the brain is doubling every year. As we get the data from particular regions, we can rather rapidly create detailed mathematical models of them. It’s a conservative expectation that we will have a very accurate detailed simulation of all the regions of the brain by the late 2020s. Ten quadrillion [1016] calculations per second is sufficient to emulate all the regions of the brain. Japan just announced two supercomputers that will achieve that by 2010. The question arises, Are we intelligent enough to understand our own intelligence? Maybe that’s a feature of complex systems — they can’t be so complex as to understand themselves. But it turns out that’s not the case. But why re-create the brain in software when we already have it in wetware? It’s going to be very powerful, because we’ll be able to combine what currently are advantages of human intelligence, particularly our pattern recognition, with ways in which machines are already demonstrably superior. What’s the future of the computer itself? Once we get past Moore’s Law, we’ll use 3-D molecular computing. [In the late 2040s], one cubic inch of nanotube circuitry will be 100 million times more powerful than the human brain. On the software side, machines [in the 2030s] will be able to access their own source code and improve it via an ever-accelerating, iterative design cycle. So ultimately, these systems will be vastly more intelligent than humans and will combine the advantages of biological and nonbiological intelligence. I don’t see this as an alien invasion of intelligent machines; this is emerging from within our civilization. Well before that, computation will be a worldwide mesh of computing elements, and anytime you want, you’ll be able to, for example, access 1 million computers for 400 milliseconds. Early in the next decade, images will be written directly to our retinas. How can you make screens really tiny but big at the same time? Put them in your eyeglasses and beam images directly to the retina. What do you mean when you say computers will “disappear”? They’ll make their way into our clothing and into the environment, and they’ll be very tiny. We’ll also move away from the idea that the computers we use are spokes into a network but not part of the network, to where every device will be a node on the network, meaning that not only will you be sending and receiving your own messages, you’ll be passing on other people’s messages. It will be continually self-organizing, so that all communication links will be continuously finding the most efficient path. And what do you mean when you say people will “merge” with their technology? We’ll be able to put intelligent machines — nanobots — into the bloodstream. By the late 2020s, these devices will have significant computing, communications and robotic abilities. Nanobotic white blood cells could download software for a particular pathogen and destroy it in a matter of seconds, compared with hours for our biological white blood cells. And you could have billions of nanobots go into the brain through the capillaries. [They] will enhance our cognitive functions and really expand human intelligence. We will be able to go beyond the limits of biology and replace your current “human body Version 1.0” with a dramatically upgraded Version 2.0, providing radical extension of life. What will Version 2.0 be able to do that we can’t do today? One scenario would be virtual reality from within the nervous system. Nanobots could shut down the signals coming from your own senses and substitute the signals you would be receiving if you were in the virtual environment. You can move your virtual body in the virtual environment, and this will incorporate all five of the senses as well as neurological correlates of your emotions. You can go there with another person for any kind of encounter. And you can have archival experiences. So will the importance of our biological intelligence diminish? The nonbiological portion will grow [by a factor] of 1,000 per decade, and the biological portion will ultimately be very insignificant. People look at this and they are alarmed, because they think, I’m going to become a machine. But they are thinking of the machines they know today, which are very crude machines. What’s the future of the IT professional? The good news is IT is going to become more and more important. Ultimately, everything of importance will be comprised of IT. There’s a trend toward specialization, so what IT people can do is to try to get professionals in a number of highly specialized pursuits to be able to communicate with each other and have their computer systems communicate with one another. Are their any downsides to all of this? I’m very concerned about the downsides. We have existential risks already — the potential to wipe out all of humanity with nuclear weapons. But now we have new existential risks — the ability to design biological viruses. The tools and knowledge to do this are far more widespread than the tools and knowledge to create an atomic bomb, and the impact could be far worse. In my book, I said the last thing we’d want to do is put the genome of dangerous viruses on the Web, but that’s exactly what the Department of Health and Human Services has just done with the 1918 flu virus. What’s really, really far out? In the 22nd century, we will have saturated the ability of matter and energy in and around the Earth to support computational processes, and intelligent computation will spread out to the rest of the universe. Whether this spread to the rest of the universe happens quickly (another century or so) or slowly (billions of years) depends on whether or not we can circumvent the speed of light as a limit on the communication of information. I believe it is likely that we can."
235,https://www.engadget.com/2015-12-29-microsoft-xiaoice-ai-china-weather-host.html,Engadget,2015,12,29,251.0," Even though machines can now generate clickbait headlines and automatically write earthquake news reports in the name of journalism, live television has remained shielded from artificial intelligence. That hasn't stopped Microsoft from trying to influence proceedings with its machine learning software, known as Xiaoice (or Little Bing), which became the first AI robot to host a segment on live TV last week. The Chinese live program, ""Morning News"" introduced Xiaoice as a trainee anchor last Tuesday, where it took charge of the daily weather report. Microsoft says it calls upon smart cloud technologies and big data to learn about and interpret weather readings and then deliver reports with her ""unique artificial intelligent style of emotional comments."" The AI is also more human-like than any other system of its kind, scoring 4.32 out of 5 in linguistic naturalness tests. For comparison, humans have an average score of 4.76. Before it got its new gig on Chinese TV, Xiaoice started life as an advanced personal assistant for messaging apps Line and WeChat and social networking service Weibo. On Line, the bot appeared as a young character called Rinna, who would reply to users' questions with human-like understanding and an abundance of emoji. For Microsoft, Xiaoice is about proving that AI concepts can be made available to everyone, rather than serving in ""sci-tech concepts."" While the bot capable of hosting on its own, its creators see it playing an important role in working alongside presenters, allowing it to ""bring more bliss to human beings.""                     "
236,https://www.engadget.com/2015-12-22-google-ai-chat-assistant-leak.html,Engadget,2015,12,22,164.0," Hey, Facebook: you might not be the only tech giant with an artificially intelligent chat assistant. The Wall Street Journal's sources understand that Google is building an AI-based messaging service that would search the web to answer your questions. From the description, it sounds like a more elaborate, more conversational Google Now. Third parties may even build their own bots to give you site-specific answers. Google isn't commenting on the apparent leak, and there's no word on when and where AI messaging would show up. Hangouts sounds like a good candidate, but it's not guaranteed. However, it wouldn't be shocking if this robotic helper shows up soon. Facebook's 'M' is, in some ways, a direct assault on Google's home turf: why search on the web when there's an AI companion willing to lend a hand? In theory, this software would keep you in Google's world even if you spend all your phone time in chats with friends. [Image credit: Chris Goodney/Bloomberg via Getty Images]                     "
237,https://www.engadget.com/2015-12-12-recommended-reading-12-12-15.html,Engadget,2015,12,12,167.0," Recommended Reading highlights the best long-form writing on technology and more in print and on the web. Some weeks, you'll also find short reviews of books that we think are worth your time. We hope you enjoy the read. How a WWII Refugee Became the Father of Video GamesArthur Molella, Slate If you've read up on video game history, you know that Ralph Baer widely regarded as ""The Father of Video Games."" What you may not know is Baer was a World War II refugee who came to the US in 1938. He fled persecution with his family before bringing games to our television screens decades later. Google and Facebook Race to Solve the Ancient Game of Go With AI Cade Metz, Wired Image and pattern recognition isn't the only thing AI is good at. Multiplayer-Only Games Shouldn't Cost $60 Nathan Grayson, Kotaku If you don't think multiplayer-only games aren't worth the same as their campaign-based counterparts, you're not a lone. [Image credit: Kris Connor/ Getty Images]                     "
238,https://www.engadget.com/2015-12-12-lie-detection-software-learns-from-real-court-cases.html,Engadget,2015,12,12,234.0," Machine learning has been used to make computers guess your age, count calories and even do our jobs, but University of Michigan researchers are applying it to lie detection. In this case they used testimony from real court cases to try and decipher a liar's tells when the stakes are at their highest. Considering both the words and gestures of the person speaking, they claim it was up to 75 percent accurate at identifying if a person is lying or telling the truth, while humans could only tell 50 percent of the time. To identify liars the software keyed on behaviors such as looking directly at the questioner, speaking with more vocal fill and counting gestures. Professor of computer science and engineering Rada Mihalcea said people focus on higher levels of communication and as such are poor lie detectors, because ""We're not counting how many times a person says 'I' or looks up."" If that's not enough of a reason to get your story straight, the team says it's working on ways to tie in ""heart rate, respiration rate and body temperature fluctuations,"" all measured by thermal imaging, without even touching the subject. The other big step they're working on is letting the computer classify gestures itself -- similar to technology explored by researchers in Finland -- which is currently done by the researchers. Your hidden tell may not be hidden for much longer.                     "
239,https://www.engadget.com/2015-12-11-an-ai-algorithm-can-draw-letters-as-well-as-a-human.html,Engadget,2015,12,11,237.0," Researchers claim to have made a breakthrough in artificial intelligence by giving machines cognitive powers similar to humans. The team from MIT, York University and the University of Toronto first trained an algorithm to draw characters in 50 languages by studying the required pen strokes. Once completed, it was able to successfully draw a new character that it had never seen before, meaning it had essentially ""learned"" the skill. That might not sound impressive, because we humans can do it easily. But so far, similar feats have only been done by large neural networks that require huge databases of images and learn more by brute force than smarts. Rather than a neural network, the team used a so-called Bayesian program learning framework. Because the algorithm is based on probability and guessing, it's using a cognitive process like humans and not a typical rote computer method. When the machine ""drew"" characters on the screen, each one was slightly different but still identifiable like the ones we would draw. As a result, only 25 percent of judges who compared the samples to human-drawn characters were able to tell the difference. Author Joshua B. Tenenbaum told the NYT that ""it's amazing what you can do with lots of data and faster computers. But when you look at children, it's amazing what they can learn from very little data. Some comes from prior knowledge and some is built into our brain.""                     "
240,https://www.engadget.com/2015-12-10-facebook-makes-the-hardware-it-uses-for-ai-open-source.html,Engadget,2015,12,10,182.0," You might not think of it often, but behind the scenes Facebook uses a lot of artificial intelligence. The company leans heavy on AI, using machine learning to curate a better news feed, sort through photo and video content and even read stories or play games. Now, the company is making Big Sur, the hardware it runs its AI experiments on, open-source. Facebook says it will release its AI hardware design to the Open Compute Project soon, promising to give the community a system designed specifically for AI tasks built from off-the-shelf components. This design features eight NVIDIA Tesla M40 GPUs seated into an easily serviceable motherboard. ""The CPU heat sinks are the only things you need a screwdriver for,"" the company says. ""We want to make it a lot easier for AI researchers to share techniques and technologies,"" the company said in a statement today. ""We believe that this open collaboration helps foster innovation for future designs, putting us all one step closer to building complex AI systems."" Sounds pretty good. Check out the source link below for Facebook's official announcement.                     "
241,https://www.engadget.com/2015-12-04-robots-expected-to-run-half-of-japan-by-2035.html,Engadget,2015,12,4,216.0," Data analysts Nomura Research Institute (NRI), led by researcher Yumi Wakao, figure that within the next 20 years, nearly half of all jobs in Japan could be accomplished by robots. Working with Professor Michael Osborne from Oxford University, who had previously investigated the same matter in both the US and UK, the NRI team examined more than 600 jobs and found that ""up to 49 percent of jobs could be replaced by computer systems,"" according to Wakao. The Henna Hotel's (NLD) robotic Velociraptor desk clerk The team looked at how likely each position could be automated, based on the degree of creativity required. That means jobs like operating helpdesks, delivering goods or agricultural labor are all highly susceptible to computerization while writing, teaching and doing whatever it is that Shingy does probably aren't being taken over by computers any time soon. The NRI's results are higher than what Osborne figured for the US (47 percent automation) and the UK (35 percent). ""However, this is only a hypothetical technical calculation,"" Wakao added. ""It doesn't take into account social factors."" For their part, many Japanese citizens have reportedly embraced the coming robo-revolution as it simultaneously relieves the economic pressure of the nation's rapidly-aging population while freeing the workforce to pursue more creative (and rewarding) careers. [Image Credit: Getty]                     "
242,https://www.engadget.com/2015-12-02-wikipedia-ai-editor.html,Engadget,2015,12,2,267.0," Wikipedia has a new artificial intelligence service, and it could make the website a lot friendlier to newbie contributors. The AI, called Objective Revision Evaluation Service (ORES), will scour newly submitted revisions to spot any additions that look potentially spammy or trollish. Its creator, the Wikimedia Foundation, says it ""functions like a pair of X-ray specs"" (hence the image above) since it highlights anything that seems suspicious; it then sets that particular article aside for human editors to look at more closely. If the Wiki staff decides to pull a revision down, the contributor will get notified -- that's a lot better than the website's current practice of deleting submissions without any explanation. The team trained ORES to differentiate between unintentional human errors and what's called ""damaging edits"" by using the Wiki teams' article-quality assessments as examples. Now, the team can use the AI to score an edit based on whether it's damaging or not. This example, for instance, shows what the human editors see on the left and what ORES sees on the right. The AI's ""false"" or not damaging probability score for it is 0.0837, while its ""true"" or damaging probability score is 0.9163. As you can see, ""llamas grow on trees"" isn't exactly helpful or accurate. As the Wikimedia foundation pointed out in its announcement, this isn't the first AI designed to help human editors monitor the site's content. However, those older tools can't tell the difference between a malicious edit and an honest human error, making ORES the better choice if Wikipedia doesn't want to lose even more contributors. [Image credit: MGalloway (WMF)/Wikimedia]                     "
243,https://www.engadget.com/2015-12-01-ai-gabriel-wants-to-whisper-instructions-in-your-ear.html,Engadget,2015,12,1,304.0," Researchers at Carnegie Mellon University are building an AI platform that will ""whisper"" instructions in your ear to provide cognitive assistance. Named after Gabriel, the biblical messenger of God, the whispering robo-assistant can already guide you through the process of building a basic Lego object. But, the ultimate goal is to provide wearable cognitive assistance to millions of people who live with Alzheimer's, brain injuries or other neurodegenerative conditions. For instance, if a patient forgets the name of a relative, Gabriel could whisper the name in their ear. It could also be programmed to help patients through everyday tasks that will decrease their dependence on caregivers. For the software to exist as a working wearable assistant, it will need a head-mounted device to latch onto. For now, the team is using Google Glass for demos like a ping pong assistant, where the programs tells the user to hit the ball to the right or left depending on the position of the ball in relation to the opponent. In the video below, when the user follows the guidance it makes it harder for the opponent to defend the ball in the game. The research, which received a 2.8 million dollar grant from the National Science Foundation, is essentially relying on cloudlets, which the team is calling Elijah. The team describes a cloudlet as an element between mobile computing and cloud computing that allows a user to rely on the nearest computing machine, instead of sending the information to a distant, remote server. According to CMU, ""cloudlets are the enabling technology for a new genre of resource-intensive but latency-sensitive mobile applications that will emerge in the future."" One such application will be cognitive assistance. For a program to enhance the user's ability to recognize and respond to the environment, low latency will make all the difference.                     "
244,https://www.engadget.com/2015-11-27-icymi-plant-powered-lamps-livestreaming-ai-and-more.html,Engadget,2015,11,27,162.0," Today on In Case You Missed It: A coder from the Netherlands used a live webcam feed for a walk around Amsterdam, running neural network code that identified everything in view. Despite some obvious set-backs (it thought the creator was wearing a suit when he really wore a zip-up hoodie, natch), it impressively identified boats in a river and stacks of bikes. Researchers in Peru invented prototype lamps that run off of the bacteria of living plants. And a new security system for the camera-hacking adverse works by setting up a motion-detecting mesh network. We are also rounding up the week's headlines and while we love the intergalactic friendliness of China and the US sharing a hotline to prevent space misunderstandings, we suggest starting your Black Friday by acquainting yourself with the purse that tells you to not spend money. If you see any interesting science or tech videos, please share with us! Just tweet us with the #ICYMI hashtag to @mskerryd.                     "
245,https://www.engadget.com/2015-11-16-ai-has-a-better-shot-at-tokyo-university-than-your-kid.html,Engadget,2015,11,16,234.0," Students in Japan could soon compete with AI to get into colleges. The National Institute of Informatics in Tokyo has developed a program that scored above average on a standardized entrance exam that covered math, physics, English and history. The AI scored 511 out of 950, beating the national average of 416. While it was expected to do well on the math test, it did exceedingly well on the history questions that required natural learning processing skills to make inferences.​ Researchers at NII have been developing this AI program called Todai Robot Project since 2011. It began with Noriko Arai, team leader and professor, wondering if a robot could ever crack the toughest entrance exam to get into University of Tokyo, or Todai as it's locally called. Her team hoped to build AI that could get a high score on a national test by 2016 so it could be well on its way to getting admission into Todai by 2021. The AI has already come a long way since its initial exam attempts in 2013. According to a WSJ report, it's now developed enough to get a score that gives it an 80 percent shot at getting into 441 private universities and 33 national universities. But it's far from ready to take it's ultimate test. The Todai entrance test includes written essays, which require advanced capabilities that are still a work-in-progress. [Image credit: Shuttershock]                     "
246,https://www.engadget.com/2015-10-30-selfie-ai.html,Engadget,2015,10,30,251.0," What's the difference between a good and bad selfie? A neural network artificial intelligence, trained on a diet of over two million selfies, apparently knows. First, the important findings: good selfies involve being a woman -- and one that's tilting their head. A small forehead and longer hair are good points too. Filters help, as do borders. For men, while they didn't rank in the AI's top 100 (ugh, bias!), the bot advises that you show your full head and shoulders. Longer hairstyles (and ones combed upwards) don't hurt mens' chances either. Its creator, Andrej Karpathy, who has worked with Google Research and DeepMind, explains that it's a convoluted neural network which does the image recognizing and, er, judging. You can judge yourself (for yourself) using the network's Twitter bot (61.7 percent here), or read on for how it learned to do all that. Karpathy feed in five million photos tagged ""#selfie"", clearing down till it was left with around two million workable self-portraits. The neural network was trained to evaluate whether a selfie was a good one or not by looking at likes received on social networks. The system gauging around 140 million parameters, analyzing images by deconstructing them into shapes and colors. Once the neural network was in place, it then tackled 50,000 new selfies -- which helped to cement the rules mentioned earlier. Broader guidelines include avoiding low light, large groups, or taking photos that are a little too close. It had nothing to say about selfie sticks.                     "
247,https://www.engadget.com/2015-10-29-halo-5-guardians-combat-devolved.html,Engadget,2015,10,29,1728.0," This article contains spoilers. Proceed at your own risk. Halo 5: Guardians is not the Halo you remember. It's a different kind of game altogether, something that more closely resembles a modern first-person shooter that focuses on multiplayer rather than a strong solo experience. This isn't the first time that's happened, but it is the first time in 11 years that a new Halo campaign feels like a massive step backward compared to its predecessor. Microsoft-owned studio 343 Industries is capable of better than this and proved as much with its killer freshman effort, Halo 4. But instead of addressing what it got wrong with that installment (e.g., an unexplained main villain) and doubling down on what it did right (e.g., an emotional storyline and constantly varying gameplay), the team fundamentally altered how a Halo campaign works to horrendous results. The developers at 343 say that Halo 5's story is part of ""the greatest evolution in Halo history,"" but that transformation comes at a price. The change here is that regardless of which protagonist you play as (either manhunter Jameson Locke or his target, returning super soldier Master Chief), you'll be babysitting squads of mindless AI teammates 100 percent of the time. Basically, they exist so your online-only co-op buddies can play as real characters instead of clones of Master Chief that disappear in story scenes. Halo has had co-op play since its 2001 debut, but playing solo was never a subpar experience as a result; playing through the campaign in co-op existed as its own separate thing. In Guardians, co-op's been shoehorned into the narrative at the detriment of enjoying the game's story during solo play. Previous entries in the series had players fighting alongside hilarious Marines who acted like disposable lemmings; I never had to rely on them for much more than comic relief. Those games also featured long stretches where I could be alone with my thoughts and explore at my own pace. What's more, having the occasional break from the troops made me actively look forward to the next time we fought together. That isn't the case here. Imagine the adorably dim-witted allies having their personalities removed and being charged with keeping you alive, and you've got Halo 5's squad-mates. In theory, they'd be an awesome asset because they're the same type of super soldiers, dubbed Spartans, you play as. Except I'd never have known they were the best warriors in the galaxy by how they acted here. Most of the time, my squad just stood out in the open soaking up enemy gunfire and adding nothing to the gameplay at all, as opposed to working as a team and absolutely destroying every enemy the way they do in non-interactive story scenes. For a solo player such as myself, there's no real payoff for this massive change. I was a gigantic nerd for Halo lore with the first few games, but it quickly became much less manageable to read every book because there was simply too much canonized fiction and it wildly varied in quality. Consider Halo 4's lead-in: the live-action direct-to-internet Forward Unto Dawn. Microsoft had finally given fans the Halo movie they'd been clamoring for and it was an absolute bore. After being burned by that, I wasn't too excited about potentially wasting my time watching Locke's origin story in the Ridley Scott-produced Halo: Nightfall. But if I wanted to actually learn about his character, it'd be required viewing because by the time Halo 5's credits rolled, I couldn't tell you a single thing about him or his Fireteam Osiris except that Nathan Fillion (who reprises his role as now-Spartan Edward Buck) is his typical smart-ass self. Master Chief's Blue Team actually fares worse because there isn't a Buck-like ""star."" The elite squad plays key roles in tie-in novels, but 343 doesn't do anything to establish who they are in-game. Early on you overhear Blue Team talking about Master Chief's service record, saying that his years of continuous battle must be having an effect on him. That promising theme is never explored beyond a single hallucination in the first of three missions he stars in. The problem with Guardians' storyline isn't that you only play as Master Chief for a third of the game; it's that his scant inclusion here feels like an afterthought; a bone tossed half-heartedly to returning players. It wouldn't be as much of an issue if his replacement, Locke, was actually interesting and given cool stuff to do, but he isn't. So it is. Death in Halo has always meant I'd die; my corpse would comically ragdoll around; enemies would quip about their kill; and the game would reload instantaneously. Guardians switches the formula with the inclusion of a rescue system. Now when you fall in combat, a computer- (or human-) controlled squad-mate has a finite amount of time to not die themselves before performing a revive, lest you bleed out and return to the previous checkpoint. All told, that results in anywhere from 15 to 45 seconds of being completely infantilized as the battle rages on. This revive mechanic brings the game to a grinding halt and the mandatory pause hinders any sense of low-risk experimentation in changing up tactics. By my estimates, Guardians wasted at least an hour of my playtime thanks to the new revive system. Master Chief (left) and Jameson Locke (right) in happier times. When my squad inevitably got knocked out and I was left standing in single-player mode, I never really cared to rescue them and I'm pretty sure that feeling was mutual. It was never a guarantee they'd get to me safely and I didn't feel like their top priority. On the flipside, I always looked forward to my squad dying so I could have some alone time. But the moments of solitude when they were all gone were a tease: They'd inexplicably resurrect themselves to finish the fight and yell at me to keep moving forward. I always looked forward to my squad dying so I could have some alone time. In Guardians, you'll die a lot -- especially if you're playing alone on the higher difficulty settings. It's always been that way. The thing is, none of my 237 deaths in the game (there's a counter) felt like the result of an enemy outsmarting me. Rather, it was because I was outgunned. And if I wanted that type of experience, I'd hop into adversarial multiplayer online. Almost every death I endured felt cheap, a frustration that mounted when coupled with the scattershot teammate AI. So much so that in the interest of reducing my blood pressure, I resorted to skipping past enemy encounters in the last levels and bumping the difficulty down because I couldn't take it anymore. And were it not for the need to finish the game for this critique, I wouldn't have even done that -- I'd have stopped playing outright. When the primary gameplay -- combat, in this case -- isn't fun, that's a massive problem. In the Bungie-led games and 343's own Halo 4 there was always a sense of variety and surprise while fighting -- that's all but missing here. Previously, the robotic Prometheans fought intelligently. A low-flying drone offered a shield capability to ground forces and it'd even vacuum up grenades before they'd hit my target. Panther-like metallic creatures would skitter this way and that, avoiding incoming fire and taking potshots every now and again. In Halo, it always felt like the AI was thinking. Not here. A vast majority of the battles are monotonous large-scale affairs that, while technically impressive, are just noisy and a chore to complete. The combat's just a hoop to jump through to get to the next weak story sequence. In the last few missions, that became increasingly apparent when I was just fighting wave after wave of enemies in what felt like a poorly executed version of the franchise's arcadey, wave-based ""Firefight"" mode. My squad's tactical usefulness during battle was questionable as well. For instance, I couldn't direct the Halo fiction's best sniper, Linda, toward a vantage point to kill out-of-reach enemies and have other members of the legendary Blue Team flank two hulking Hunters while I attacked their weak spots from behind. All you're able to do is direct them to a specific location or to attack one enemy. The trio moves and fights as a singular unit and the only useful squad order was to send them ahead to act like a bullet-sponge before I entered a new area. And even then, their effectiveness was hit or miss. A vast majority of the battles are monotonous large-scale affairs that, while technically impressive, are just a chore to complete. Counter that with something like the Mass Effect series, which had incredibly useful teammates that each served unique and important roles during combat, and you'll see just how poorly implemented the system here is. Squad commands are just too shallow to be useful, which makes me question why they were even added in the first place. Sure, the team members in Guardians have context for the now online-only co-op, but when Bungie did something similar for Halo 3, you didn't have the chore of directing limp AI because you were playing solo. Babysitting these AI squad mates also served as a painful reminder that even if I wanted to bring friends in to fight alongside me locally, I couldn't. The whole way through the game, it felt like I wasn't welcome because I was playing by myself. The only thing that pulled me through one tiresome firefight to the next was the promise of meeting returning characters. And even then there was no real payoff. The sole bright spot occurred when the game offered up what made past Halo entries fun: massive outdoor environments, exploration and encounters with clever enemies. But that didn't happen until missions seven, eight and nine, and after that, Guardians went back to feeding me garbage. In the months leading up to the Halo 5: Guardians release, 343 repeatedly said that hitting a silky smooth 60 frames per second was the top priority and that it wouldn't let anything get in the way of that -- not even the couch co-op that'd appeared in every release prior. Apparently, making sure Guardians was fun to play solo wasn't high on the list, either.                     "
248,https://www.engadget.com/2015-10-26-google-rankbrain-search-ai.html,Engadget,2015,10,26,222.0," You've probably heard of PageRank, Google's original secret sauce for organizing search results quickly and accurately. Now there's a new, artificial intelligence-powered search technology that's becoming increasingly important for Google: RankBrain. It's basically the company's solution for interpreting the some 15 percent of daily search queries that it's never seen before. RankBrain works by transforming words into ""vectors,"" or mathematical entities, which Google's search engine can use to find similar words or phrases. That allows it to parse unusual search entries like, ""What's the title of the consumer at the highest level of a food chain?"" Over the past few months, RankBrain has become the third-most important signal for display search results, out of ""hundreds"" of signals currently in play, Google senior research scientist Greg Corrado told Bloomberg. It's also even better at sorting relevant search results than Google's own search engineers. ""The other [search] signals, they're all based on discoveries and insights that people in information retrieval have had, but there's no learning,"" Corrado said. That's really the appeal of all AI-driven technologies these days, so don't be surprised if you start hearing about similar solutions from competitors. As Bloomberg points out, Facebook is already using AI to sort articles in its newsfeed, and Microsoft is using it to make Bing better (though it's been tight-lipped on specifics). [Photo credit: Shutterstock]                     "
249,https://www.engadget.com/2015-10-24-apple-hires-nvidia-ai-director.html,Engadget,2015,10,24,128.0," Apple's widely rumored electric car may not be fully autonomous, but it may well have some smarts. The company has hired Jonathan Cohen, who until this month was the director of NVIDIA's deep learning division -- in other words, a form of artificial intelligence. Cohen's LinkedIn profile only mentions that he's working on a nebulous ""software"" effort at Apple. However, his most recent job at NVIDIA centered around technology like Drive PX, a camera-based autopilot system for cars that can identify and react to specific vehicle types. While there's a chance that Cohen could be working on AI for iOS or the Mac, it won't be surprising if he brings some self-driving features to Cupertino's first car, such as hands-off lane changing or parking. [Image credit; NVIDIA, Flickr]                     "
250,https://www.engadget.com/2015-10-09-ai-tax-evasion.html,Engadget,2015,10,9,307.0," The fear of AI usually revolves around the fear of an uprising and humans being attacked by our new robot overlords. Researchers at MIT and non-profit technology source Mitre have a new terrifying future for AI. Well, not that scary to most people, but something that could put a fright in the accountants of tax-cheating corporations. The researchers propose a using artificial intelligence to investigate complex tax shelters that keep companies and the rich from paying their fair share of taxes. It's like Skynet but with a really awesome calculator and algorithms. The IRS currently analyzes data from filed returns and looks for patterns from firms that are already under suspicion. It usually takes years to unravel the Gordian knot of accounting that surround dubious partnerships and good old fashioned tax evasion. The researcher's propose a system that targets partnerships and looks not just at individual returns (which on their own seem legit) but the whole network surrounding those returns that add up to fraud. Because the system is always looking for signals of nefarious company practices, it wouldn't need to be focus on an individual source. It could just waits until a series of tax regulations are being used concurrently that usually means someone is exploiting the system for their own gain. And, the system could evolve to detect new ways tax evaders are cheating the government. In their paper the researchers state: ""Our approach is to model the co-evolutionary arms race between transaction sequences in ownership networks with their corresponding audit observables."" Which is a fancy way of saying, it can help the IRS win. Of course it has to convince the same entity that makes you fill out a mountain of forms if you buy a house. So it might be a while before AI starts making sure we're all paying our fair share.                     "
251,https://www.engadget.com/2015-10-09-stephen-hawking-ai-reddit-ama.html,Engadget,2015,10,9,538.0," Artificial intelligence was one of the biggest topics during Stephen Hawking's Reddit AMA (Ask Me Anything) earlier this year. So it's not too surprising that Hawking used up a significant portion of his answers to that Q&A session, released by Reddit yesterday, by clarifying his stance on dangerous artificial intelligence. ""The real risk with AI isn't malice but competence,"" he wrote to a teacher who's tired of having the ""The Terminator Conversation"" with his students -- that is, explaining away the notion that evil, killer robots will be the main danger with AI. ""A superintelligent AI will be extremely good at accomplishing its goals, and if those goals aren't aligned with ours, we're in trouble."" Hawking previously warned that AI could ""spell the end of the human race,"" and he also joined Elon Musk and other notable technologists to call for a ban on autonomous weapons. While it's a bit less exciting than robots bent on destroying humanity, Hawking's reasoning is no less worrisome. The idea that the equivalent of an AI software bug could eventually have world-changing implications isn't exactly reassuring. ""You're probably not an evil ant-hater who steps on ants out of malice, but if you're in charge of a hydroelectric green energy project and there's an anthill in the region to be flooded, too bad for the ants,"" Hawking added. ""Let's not place humanity in the position of those ants."" Responding to another question about when AI will reach human levels of intelligence, Hawking stressed that we don't really know when that will happen. But, he noted, ""When it eventually does occur, it's likely to be either the best or worst thing ever to happen to humanity, so there's huge value in getting it right."" To that end, he calls for being more careful about how we develop AI. Rather than just exploring ""pure undirected artificial intelligence,"" we should instead be focusing on creating ""beneficial intelligence."" Hawking also noted that an evolved AI will be able to have drives or goals similar to living organisms. But where living creatures focus on surviving and reproducing, he notes that AI could be driven to collect more resources to fulfill its goals, citing scientist Steve Omohundro. And once again, that could spell trouble if it's taking away resources for humans. Pointing to a slightly more pressing issue, one Redditor asked Hawking about his thoughts on technological unemployment -- especially around the idea that we might one day reach a point where most tasks are automated, and most humans are out of work. Hawking described commonly-discussed scenarios: One where most people can live a slightly more luxurious life, if the resources produced by the machines are shared. And another where most people end up ""miserably poor"" and the rich people who own those machines end up consolidating wealth. At this point, Hawking sees things trending towards the second reality. On the lighter end of things, we also learned that Hawking's favorite movie is Truffaut's Jules and Jim, and that he somehow finds The Big Bang Theory funny. Perhaps the funniest takeaway: When one Redditor asked if Hawking remembered briefly watching Wayne's World 2 at a Cambridge video store, Hawking replied with a resounding, ""NO."" [Photo credit: Desiree Martin/AFP/Getty Images]                     "
252,https://www.engadget.com/2015-09-17-barbie-has-some-career-advice-for-your-child.html,Engadget,2015,9,17,916.0," There's a new Barbie on the block. She's chatty and she comes with a charging station. She's dressed in a cropped, metallic leather jacket, dark skinny jeans and a white sweater vest with the word ""HELLO"" printed thrice on the front. Within seconds of switching her on, her chunky necklace lights up as it searches for a WiFi connection. When the LED goes from red to green, you know she's ready to play. A shiny, round belt buckle doubles as a button. You press it down to activate speech-recognition for your child's first two-way conversation with the iconic, inanimate doll. Now that playtime includes light-as-air gadgets and power-packed game consoles, Barbie's historically staggering sales have been in a constant slump over the past three years. But with this talk-back, feature Mattel's premier product might have a fighting chance. Hello Barbie blazes into the world of interactive toys as the first doll equipped with artificial intelligence. The first time I hear her talk, she sounds perky, friendly and even strangely familiar. The voice seems to fit the character. ""What do you want to be when you grow?"" She jumps right to it. Depending on the response she gets, she offers some career options. For instance, if you say you want to be a scientist, she's quick to compare your aspirations to Marie Curie or Albert Einstein. As she continues to prompt and control the conversation, she leads you to your interests. So if you point out that you like animals, she responds with one of her preset options -- ""Do you want to take care of them like a veterinarian or study them like a zoologist?"" Mattel believes eliciting a conversation in this way encourages children to think and communicate, but it also allows them to get to know Barbie a little bit better. This doll has clearly gone from being one of many passive objects on your child's shelf to a conversationalist who wants to know more about her playmate. Through that process, she reveals a fully fleshed-out personality that was always in the background. (Who knew her full name is Barbara Millicent Roberts?) As a character that's been around for 56 years, she certainly has a solid backstory to dip into. If asked, she can talk about her own life, including her seven siblings. But she mostly comes equipped with opinions about pets, professions, food or some such, and asks questions. She also slips in a bunch of enthusiastic comments that range from affirmations (there seems to be no wrong answer when you're chatting with Barbie) to factual snippets like, ""Did you know a cat can see six times better than a human?"" It starts to feel like she might be able to respond to every conversational need a child might have and that's unsettling. Does a child need a digital assistant wrapped up in a doll? In Barbie's defense, though, she's nothing like Apple's Siri or Microsoft's Cortana. She doesn't trawl the web for answers, and isn't designed to address every question. There are things that she doesn't know and that's supposed to be a part of her character. When that happens, instead of an awkward pause that was already ruled out during the testing rounds, this almost-ready-for-the-market Barbie will swiftly steer the conversation back to the things that she does know -- or at least is pre-programmed to talk about. Every response is part of her ""content library"" that is currently capped at 8,000 predetermined dialogues. Her voice, recorded by a voice artist, is overtly animated yet carefully directed to be relatable for six- to eight-year-old girls, the target group for the talking doll. It's precisely this target audience -- an impressionable and vulnerable age group -- that had privacy advocates raising concerns over Hello Barbie's communicative ways. The doll picks up the child's audio and pushes it through servers where the conversation is stored. The makers of the doll, including San Francisco-based ToyTalk (a company that develops voice-recognition software for toys and apps), point out that the audio is stored specifically for parents to access at any given time. Through an accompanying app (available for both iOS and Android), parents can log in to listen or choose to delete the child's personal conversations with her doll. It's also quick to add that it isn't an ""always-on"" product -- a child would need to press down the belt buckle to be ""heard"" by its servers. Issues like privacy and security against hacks are deeply troubling when it comes to AI toys. But neither of those concerns seems lost on the company that is in the business of delivering dolls to suit every generation. Whether those concerns have been fully addressed will be seen in November when the doll hits the shelves. Technology isn't perfect; neither is this doll. Like any other mundane device, even the stylized Barbie can have trouble connecting to a WiFi connection. The necklace can continue to blink and search for a network as you, or in this case an excited child, might impatiently look on. The doll is also another product in the ever-increasing list of things that need constant charging -- ""My battery needs to be recharged; please connect me to the charging station,"" isn't the ideal response from a doll when it's time to play. But for a generation that's starting to integrate swift swipes on a high-definition screen into their play routines, perhaps these glitches won't get in the way of their next $75 Barbie purchase.                     "
253,https://www.engadget.com/2015-09-10-baidu-unveils-a-voice-activated-ai-based-smartphone-assistant.html,Engadget,2015,9,10,116.0," Chinese tech company Baidu announced at its Baidu World conference on Tuesday that it is launching a voice-activated assistant for its Android-based smartphones. The program is called ""Duer"", which roughly translates into ""Du Secretary"", is expected to directly compete with Siri, Cortana and Google Now. Initially, the app will allow users to perform tasks like ordering food as well as controlling smart devices around the home and accessing other on-demand services (ride hailing, designated driver services, or housekeeping) via voice command. Eventually, the company plans to integrate Duer into its other apps, like Maps, and potentially even into the self-driving BMW that the company is rumored to be working on. [Image Credit: ChinaFotoPress via Getty Images]                     "
254,https://www.engadget.com/2015-09-05-georgia-tech-scheherazade-if.html,Engadget,2015,9,5,216.0," Turns out it's not just news writers (gulp) whose jobs are in jeopardy -- even fiction authors like RL Stine might have to watch their backs. A group of researchers from Georgia Institute of Technology has designed an AI that can write up interactive fiction (IF), such as choose-your-own adventure stories. You know, those books that ask you to turn to a specific page in the book (or to click a link, if it's on the computer) based on which action you want to choose from a selection. They named it Scheherazade-IF, obviously after the Arabian queen-slash-storyteller in One Thousand and One Nights. In order to program the AI, they fed it around 200 stories written by real people on two different scenarios: going to the movies and robbing a bank. It doesn't actually understand whatever it reads, but it can recognize patterns and keyphrases, which serve as its cues to know what choices to write up next. For instance, it knows that giving the reader the choice to sit down and watch the movie should go after he's already chosen ""buy tickets"" from the list. It's still not capable of spinning overly complicated plots at the moment, but it looks like it has a future in generating fun, non-repetitive text-based games. [Image credit: Getty Images/iStockphoto]                     "
255,https://www.engadget.com/2015-09-04-toyota-ai-mit-stanford.html,Engadget,2015,9,4,326.0," Artificial Intelligence is big deal (even if Elon Musk thinks it will doom us all) and today Toyota announced that it is collaborating with MIT and Stanford to accelerate its own AI research. The goal of the collaboration is to advance artificial intelligence for vehicles with a strong focus on safety. The carmaker will invest $50 million over the next five years in joint research centers and has hired former DARPA Program Manager Dr. Gill Pratt to lead the initiative. Dr. Pratt noted that in addition to using the research to reduce vehicle-related fatalities, ""we want people to live a more dignified and more safe life."" That includes using autonomous systems to enable the elderly to continue to be mobile after they have traditionally lost their licenses. That doesn't necessarily mean that drivers will become pure passengers according to Pratt. One of the goals is to ""eliminate highway collisions without eliminating the fun of driving."" University research leaders, professors Fe-Fe Li of Stanford and Daniela Rus of MIT both reiterated the collaboration's primary focus on safety. Rus noted the teams will be working on cars that will be driving partners that will ""watch our backs."" Both universities have a history of working with artificial intelligence. Stanford's SAIL (Stanford Artificial Intelligence Laboratory) was the first to win the DARPA challenge autonomous vehicle challenge in 2005. While MIT has over 20 professors and 100 plus graduate students working in the CSAIL (Computer Science and Artificial Intelligence Laboratory) department. Pratt stressed that the $50 million allotted for the research centers isn't the only investment Toyota is making in the artificial intelligence area. With a focus on the evolution of autonomous vehicles that in the relative short term will have the vehicles working in partnership with humans, we should expect to see something sooner rather than later from the collaboration. ""We will not be waiting for a fully autonomous car to show you what we are working on,"" said Pratt.                     "
256,https://www.engadget.com/2015-08-29-recommended-reading-8-29-15.html,Engadget,2015,8,29,278.0," Recommended Reading highlights the best long-form writing on technology and more in print and on the web. Some weeks, you'll also find short reviews of books that we think are worth your time. We hope you enjoy the read. Robots Will Steal Our Jobs, But They'll Give Us New Ones by Cade Metz Wired With all the advances in automation and robotic technology, should we be worried that robots will replace us? Well, while they might take some of our jobs, they'll also give us new ones. This piece from Wired offers a look at the future as we learn to live with AI, presenting a strong case that it may not be as dire as the critics predict. The Rise and Fall of a Bitcoin Kingpin David Kushner, Rolling Stone Here's the story of how Mark Karpeles, the head of bitcoin exchange Mt. Gox, became the ""accidental emperor"" of the digital currency before it all came crashing down. The Truth About Uber's Background Checks Sarah Kessler, Fast Company Uber does conduct background checks on its drivers, but are they complete? District attorneys in Los Angeles and San Francisco say no. 'Team Fortress 2:' Valve's Secret Guinea Pig Tom Curtis, Gamasutra While this is a shorter read, it does offer a glimpse at how important Team Fortress 2 is for Valve as a whole. The Way GCHQ Obliterated The Guardian's Laptops May Have Revealed More Than It Intended Jenna McLaughlin, The Intercept When the GCHQ, Britain's version of the NSA in the US, ordered reporters at The Guardian's London HQ to wipe computers that held top-secret docs, the agency uncovered a lot more. [Image credit: Carl Court/Getty Images]                     "
257,https://www.engadget.com/2015-08-26-facebook-messenger-m-assistant.html,Engadget,2015,8,26,219.0," Remember that talk of Facebook testing its own virtual assistant? Well, it's real. The social network is  trying out M, an artificial intelligence-powered Messenger assistant that can answer questions and complete tasks. You can ask it for advice on places to go, for instance, or have it make travel arrangements. Think of it as a Siri- or Cortana-like helper that exists solely in text chat. It should be less likely to make mistakes, though, as Facebook is quick to note that there are humans training and supervising the AI behind the scenes. And in case you're wondering, it only bases its conversations around Messenger -- it's not using your regular Facebook data to make decisions. Don't expect to give M a spin just yet. It's only in use by a few hundred people in the San Francisco Bay Area, and it could take a long while to scale up if it proves successful. However, a wider rollout seems like more a question of ""when"" than ""if."" Facebook has been dramatically expanding the role of Messenger as of late, turning it from a basic communication tool into a platform for everything from games to money transfers. It only makes sense that the company would take the next logical step and make Messenger the go-to place to get many tasks done.                     "
258,https://www.engadget.com/2015-08-10-microsoft-attempts-to-teach-computers-how-to-make-a-funny.html,Engadget,2015,8,10,268.0," Computers and artificial intelligence systems have long struggled with a human understanding of humor – as anyone who has ever asked Siri to tell a joke well knows. Bloomberg reports that recently, a researcher at Microsoft began working with The New Yorker on a project that aims to teach an AI system what is and what is not ""funny."" Dafna Shahaf was able to train an AI to find the funniest choices among similarly-themed captions by feeding it an archive of cartoons and caption contest entries from the back page of The New Yorker. Because computer software is trained to understand photos and not hand-drawn images, the researchers had to manually describe what was seen in each cartoon and its context; Shahaf used crowdsourced input via Amazon's Mechanical Turk for this step, asking workers to sort answers in order of funniest to least funny, and then moved onto ranking the jokes using the answers. In the end, the AI system was often able to select choices that the cartoon editors agreed with – all of the editor's favorite captions appeared in the AI's top 55.8 percent of choices. Overall the AI was able to eliminate roughly 2,200 of the 5,000 caption entries the publication receives each week, saving editors from having to read through thousands of terrible puns. These initial steps are incredibly relevant to projects like Skype Translator which makes translations on the fly between users speaking two different languages. Researchers' ultimate goal is to teach computers to make jokes on their own, because apparently getting them to answer Jeopardy questions is passé. [Image Credit: Associated Press]                     "
259,https://www.engadget.com/2015-08-10-fullbright-tacoma-interview.html,Engadget,2015,8,10,3266.0," Fullbright struck a nerve with 2013's Gone Home, its emotionally haunting tale of a 20-something who returns from Europe in 1995 to find her family home deserted. That indie game darling not only became a critical success for the small Portland, Oregon-based studio, but also won a BAFTA in 2014 for best game debut, and two VGX awards -- one for best PC game, the other for best independent game. For Fullbright's follow-up, the near-future, set-in-space sci-fi tale Tacoma, the studio has some undoubtedly high expectations to meet. It's a good thing then that Microsoft, which has partnered with Fullbright to make the game an Xbox One exclusive, is there to lend a deep-pocketed helping hand. Tacoma is very much still in development and won't be out until mid-2016. But that didn't stop Fullbright co-founder Steve Gaynor and level designer Tynan Wales from trotting out a short 30-minute demo that gives a glimpse of the augmented reality and artificial intelligence that pervades Tacoma's world. I recently had a chance to chat with both Gaynor and Wales about avoiding the sophomore slump, their sci-fi inspirations, a possible HoloLens demo, killer AIs and why space could be a very gay place. What was it like having to follow up Gone Home with all that critical acclaim? Were you afraid of a sophomore slump at all? Steve Gaynor: For me, the biggest challenge is just making sure we don't repeat ourselves. Because I think the biggest danger of the disappointing follow-up is when the creators have tried to recapture: ""Oh, we need to do that again. We need to make sure that we do this and this and this [thing] that worked last time."" And I think if you feel like, okay, I remember playing this, but it was better the first time, then that's when you're in a danger zone... of feeling like, okay, I don't have anything new to say. What was so exciting about the first one was that it was new and I didn't know what to expect. ... That's why we've done everything we can to push ourselves to say this doesn't take place in a real time and place in the past. It has to take place in a fictional universe, a near future that we have to completely imagine instead of trying to authentically recreate. And it can't be about a family or a love story. It has to be about: Who is this group of people? It's a very different situation, a very different set of relationships -- a different way of understanding the environment. I've played Gone Home and this seems very similar. I hope you would agree with that. It's sort of that quiet, haunting first-person exploration. So in what ways did you seek to differentiate it from Gone Home? SG: Well, we wanted to push more outside of what we felt like was familiar. So much of Gone Home was about the familiarity, the recognizability of what you're finding: the analog, the handwritten notes and tape cassettes. So I think what we wanted to say, for one, is the player's discovering the bounds of this fictional world kind of at the same time as we are because it takes place outside of our own experience. ""What does it mean when Gone Home is in zero gravity a hundred percent of the time?"" -- Steve Gaynor The thing that you're pointing to is definitely correct because we're starting from a similar place. And so our hope is to take one interesting step forward along multiple vectors to say: In Gone Home, you never saw another human figure. In Tacoma, you aren't coming face-to-face with other living people, but you're understanding, getting to encounter and kind of relive these moments in these characters' lives in a way that's more present in the environment than what we could do in Gone Home. But it still is relying on that isolated, kind of I'm here by myself, but I can kind of involve my experience in these moments that these people lived through. And kind of feel like I'm inside of them in a way that you were always outside of the audio diaries describing events in Gone Home. And also, we really wanted to say something that is kind of inherent to a game like Gone Home; that's inherent to gravity. If you're not giving the player a grappling hook or you can't glide like Batman or something like that, you're very much glued to the floor. In a lot of ways, you're kind of playing a 3D game in 2D. And we wanted to push one step past what players are used to with, ""I just navigate a first-person environment like this,"" and say, actually, we just want to challenge people to think of this space in a fully three-dimensional way. Tynan Wales: The game, at one point, was actually a lot more similar to Gone Home -- it was in a house. Having come on from not doing Gone Home, very early in development, not only was the idea of a space station suggested, but also the mechanics Steve just outlined with the AR scenes and using gravity and moving from surface to surface. Now that I've been seeing them all up and working, I feel like the tone may be similar, but the experience is pretty different when I play it. I noticed that instead of retreading the same things -- experiencing narrative through audio playback -- you use holograms to replace these story cues. And the fact that now you have this space where it can wind up being a little bit too overwhelming, but it gives you more areas to search through. Does that all tie into coming up with the near-future space theme? How did all of that evolve? SG: The project started in a much more familiar, mundane location and we went down that road for a little while. I'd had this concern in the back of my head that I hadn't really been paying that much attention to. But I went last summer on an anniversary trip with my wife. And, being on the trip, we were away from stuff long enough to get some perspective. And I'm just telling my wife, ""We're doing another fucking house game. Now that we've started developing it further, I can feel how close it is and I don't want to do that again."" We needed another place that's like an isolated place where a small group of people could live. It's not an apartment building or an arctic base or an oil platform. It's something like that; it could be a space station. And my wife was like, ""Yeah, a space station's cool."" ""When Microsoft started looking at the game and talking to us about it, they were immediately like, 'You know, we can do this with HoloLens.'"" -- Steve Gaynor And so it started from there, in saying it's on a space station, so what does that mean? When is the space station? Where is the space station? Why does it exist? And, like you were saying, how do we play that off? Well, you could make a space station that's ring-shaped and you could use centrifugal force or you could have one that has artificial gravity. But, in a lot of ways, then you're just making Gone Home, but the house just looks like a space station. So from the beginning, I was like, it should just all be in zero G because then we can't say, ""Oh yeah, you open a drawer and you put a pen on the desk and it rolls off."" What does it mean when Gone Home is in zero gravity a hundred percent of the time? We don't know. TW: In the early version, when it was a house, some of the plans were to have an actual AI that moved around with you and responded to your actions or asked you questions, etc. And this was, I think, a really interesting -- not exactly solution -- but evolution of that idea with multiple characters and these [holographic] recordings instead of a live, active character. And the way you can, hopefully, interact with them by not being a static observer all the time and trying to move between voices and conversations and different locations and follow characters. SG: In Gone Home, there were no other characters in the game. So, we thought, what if in our next game there was a character in the space with you following you around? Okay, well that's a super-literal solution to, ""We want to have another human presence in the space."" So okay, let's start working on that. And then there are all these other problems that come up when the AI has to react to what you're doing and be interesting when they don't have anything to do. What it makes you do is start asking yourself, ""What are we actually trying to get out of this? Are we trying to get a feeling of an AI companion following you around? Is that what we care about?"" No, what we care about is you feeling like you can observe these people in the place where they live, where these moments happen. And so that's a way the near-future, sci-fi, high-tech setting gave us the ability to say, ""Well, I think augmented reality could be a pervasive technology in a facility like this two generations from now."" So what if you were seeing these live, positional, Kinect-like skeleton recordings of what happened to these people and that gives you that ability to share the space with them without us having to say, ""Yeah, there's somebody who has to have a good reaction when you just start throwing stuff at the wall."" Did you do any actual research on AR technologies and what's to come? Or was it more: Since this is sci-fi, we're going to take liberties based on what we know? TW: So just timeline-wise, as far as I'm concerned, the idea came up before I ever heard anything about HoloLens. I don't think it came from understanding modern tech or where modern tech was headed or who was researching that. I mean, it's kind of interesting to see HoloLens coming online as we're developing the game because it's super relevant and super possible. It seems to me that Fullbright-style games like Gone Home and Tacoma would lend themselves very well to being displayed through HoloLens. Is that something you thought about? Have you talked to Microsoft about that? SG: Well, it's funny because we're putting the game on Xbox One, so we have a relationship with Microsoft. And when they started coming and looking at the game and talking to us about it, they were immediately just like, ""You know, we can do this with HoloLens."" They could put an AR scene or they could put the info panels in HoloLens and you could do that. But the thing that I think is a lot more relevant to that is something that's more of a focused experience. I think there's still not a good solution to the idea of freely walking around in three-dimensional space while also being in one of these AR/VR experiences. They are great if the entire experience takes place in the size of the room that you're in. With Oculus, the experience is more about sitting at a desk or sitting in a cockpit. They have things that they're more natively geared towards. So on the one hand, we're not planning to do Oculus support. We didn't for Gone Home. We're not planning to do it for Tacoma. I'm not sold on this kind of game just being able to... okay, just put it in a headset. But I do think there could be a very interesting focused demo. Like you could have a HoloLens recreation of the orbital lounge in the game where there are info panels so you can look out and see the moon and the Earth. And you can see this scene play out and be inside of it. I think that would be fascinating to see a version of. I don't know if that's in Microsoft's promotional budget or whatever. But I think that it would be really cool to basically be able to step into that experience in a controlled way that would be a good fit for what I'm aware [of] the technology to be good at. ""Virgin-Tesla is a fictional extrapolation of our present where these two prominent companies merged and now are providing this [space tourism] service."" -- Steve Gaynor The thing that definitely cracked me up when I noticed it was when I was examining objects [in Tacoma] and saw the Virgin-Tesla logo. Did you have to get special permission for that? SG: It's like mentioning a brand name in a novel. We aren't using any of their copyrighted logotypes or anything. Something like Virgin-Tesla is clearly a fictional extrapolation of our present to say that the game takes place in a fictional universe where these two prominent companies merged and now are providing this [space tourism] service. And so, it's a way for us to ground what we're doing hopefully in the present that we live in and talk about it directly. As opposed to just having to make up Aerospace Tourism Corp. because I feel like when you don't have any direct connections to where we are now, everything just feels a lot more abstract. That's what we love about [the film] 2001. Because it's like oh, there's Pan Am and Howard Johnson. ... If you look at this in 30 years, it'll be very much a 2016 imagining of 2088, like Blade Runner was a 1982 [imagining of] 2019. Having those hooks into our present is really valuable. With ODIN [the station's AI], it seems as though there's the potential you could be drawing on what's happening now in technology. That you're going after some of the fears around the possibility of semiconscious AIs, like how Stephen Hawking or Bill Gates is saying: ""Everyone be careful because that day is coming."" SG: Now, that's legit! When those guys are writing letters about: Can we stop making killer robots, please?! It's not just jokes anymore. So is ODIN a killer AI? TW: That's the question... SG: Is ODIN murdering you? TW: I don't want to spoil anything... SG: That's something I became conscious of during Gone Home. We can say to you -- the download link can say nothing's going to jump out at you in this game. And it doesn't matter because we can say seriously there's no enemies; nobody's gonna get you; nothing's gonna jump out at you. It's just an empty house. There's literally no way if somebody's predisposed to thinking, ""Yeah, but what if?"" I can say right now and I will say the ending of this game is not that the whole crew was killed. ODIN is not going to try to murder you. This is true. ""I'm happy to say [Tacoma's] not about a murderous, killer AI."" -- Steve Gaynor TW: Oh, he let it loose. SG: As a creator, you're like, if we tell people too much, it'll ruin the experience. I'm happy to say it's not about a murderous, killer AI. But it is going to be in a lot of ways about discovering what the capabilities and what the consciousness of this thing actually are. Because you start out and clearly it's keeping things from you. And so hopefully, some of the questions are like: Why? Who has given it these directives? Or has anyone? Does it have its own reasons for keeping this information from you? Is that theme of AI and what it could become something you wanted to make a commentary on with this game? Or is it just that you came up with the space setting and decided to include an AI? SG: I've never really started from a point of wanting to state a message with what I'm working on. I've never worked on something where I'm like, ""I want to say this about what I believe about what AI is or what the dangers of this technology are or anything."" Honestly, it always comes from really practical perspectives. Maybe I misinterpreted this, but the two holographic messages I first encountered were about two different gay relationships. I think that's a really interesting choice to start the game. So is it a gay space station? SG: It's a bit of a ""gaystation"" -- you could say. We have the two women that are in a relationship on the station; and Andrew, who has a husband that's off the station. Evie and Clive are straight and then Sarah's really kind of undefined. Her orientation has not been really ... she's involved with looking into AR dating, but we haven't really talked about where she lands. I think what we're saying about the state of society that the crew of Tacoma's living in is that it's continued to some point where if these people are gay, there wouldn't be any reason that they wouldn't not just be open about it. People happen to be gay on the station and are not trying to hide it and that's just how it is. That's just an implicit statement about where we think this part of the society is headed, I guess. It's a decision that you make when you're doing speculative fiction. You can either say, we're in the present; we've got this trajectory and we basically feel like it's going to continue. Or you can say, society's here now and we're going to make our fiction about it taking a hard right turn. There've been tons of very disruptive changes to technology and society and how we live our lives. And as far as that goes, cellphones have totally changed everyday life for people, So hopefully, we get to talk about that kind of stuff with AR and with saying commercial space tourism is a thing, but only for the very high-end of society still. So what does that mean? Hopefully, the identity of the crew and who they are and what kind of relationships they're in and so forth is just part of that fabric. Tynan Wales (at left) and Steve Gaynor How far along in development is the game at this point? SG: We're aiming for mid-2016, but I don't want to say a date because we don't know what it is. Why go with Xbox One when PS4 has the largest install base and PlayStation's been the friendliest to indies, arguably? SG: I think it is always shifting tides with that part of the industry and with companies that size. I think that Sony did get a head start reaching out to indies and making it part of their identity when PS4 was first getting off the ground. It's obviously worked out well for them. We're at a point now where I think Microsoft is working really hard to reach out to people that are doing small projects and get them to align themselves with their platform. So, after we put Tacoma out there, Microsoft reached out to us and started talking to us and got the conversation going. And they've been a really great partner so far. Microsoft, at this point, is doing the work, getting out there and trying to get this kind of stuff onto Xbox. This interview has been edited and condensed. Images: Fullbright (game screens)                     "
260,https://www.engadget.com/2015-08-02-hitchhiking-robot-ends-us-trip-early.html,Engadget,2015,8,2,121.0," Hitchbot might have  made it across Canada, but it appears that the US wasn't quite so kind to this mechanical traveler. The hitchhiking robot's American journey has ended after a mere two weeks thanks to a vandal attack in Philadelphia. While the team behind Hitchbot vows that its experiments with artificial intelligence and human interaction are ""not over,"" it's clear that this nomad isn't about to resume its cross-America trek all that quickly. You'll hear more details on August 5th -- here's hoping that this includes plans for Hitchbot to bum rides once again, whether it's in the States or abroad. EXCLUSIVE PHOTO: evidence of vandalized hitchhiking robot in #philly. #hitchBOTinUSA trip is over.... pic.twitter.com/VAjvGQzF3u — AndreaWBZ (@AndreaWBZ) August 1, 2015                     "
261,https://www.engadget.com/2015-07-27-call-for-ban-on-autonomous-weapons.html,Engadget,2015,7,27,200.0," If you don't like the thought of autonomous robots brandishing weapons, you're far from alone. A slew of researchers and tech dignitaries (including Elon Musk, Stephen Hawking and Steve Wozniak) have backed an open letter calling for a ban on any robotic weapon where there's no human input involved. They're concerned that there could be an ""AI arms race"" which makes it all too easy to not only build robotic armies, but conduct particularly heinous acts like assassinations, authoritarian oppression, terrorism and genocide. Moreover, these killing machines could give artificial intelligence a bad name. You don't want people to dismiss the potentially life-saving benefits of robotic technology just because it's associated with death and destruction, after all. There's nothing legally binding in the letter, but it lends weight to the United Nations' preliminary talk of a global ban on deadly automatons. If officials, academia and the tech industry are all against removing humans from the equation, it's that much more likely that there will be rules forbidding lethal bots. While that doesn't preclude rogue nations and less-than-ethical companies from forging ahead with their own equipment, you might not see a world full of AI-driven warriors. [Image credit: AP Photo/Massoud Hossaini]                     "
262,https://www.engadget.com/2015-07-17-ibm-watson-tone-analyzer-writing.html,Engadget,2015,7,17,382.0," It's bad enough that robots are writing professionally (albeit badly), but now they're criticizing, too? IBM has unveiled the Watson Tone Analyzer, the latest tool in its ""cognitive computing"" suite of cooking, health, shopping and other apps. Once you input a piece of text, the system will perform a ""tone check"" to analyze three different aspects of it: emotional, social and writing style. Each of those is divided into further categories -- for instance, it can tell you if your writing style is confident or tentative, and whether the emotional tone is cheerful, angry or negative. From there, it can give you a breakdown of the overall tone and suggest new words to ""fix"" it. But why? IBM says in the same way that you spell-check a document, you can now ""tone-check"" it too. For instance, if you want an employee letter to be more ""agreeable,"" Watson suggests changing the word ""disappointing"" to ""unsatisfactory,"" and ""difficult"" to ""challenging."" By swapping out enough words, you eventually get the right level of ""agreeableness"" or ""cheerfulness"" in a passage. IBM thinks this could help, say, advertisers, to make sure a marketing campaign matches ""the personality attributes of target customers."" In other words, Watson can help companies hawk beer. Playing around a bit, I found it occasionally helpful, but it fell down more often than not. The main problem is that Watson was missing context, especially for words that have multiple meanings. In the sentence ""I know the times are difficult!"" it nonsensically suggested ""arithmetic operation"" for ""times."" It also doesn't understand sarcasm, humor and other styles, and just picks out individual words to determine the tone. That said, the Watson Tone Analyzer is impressive considering that it's still experimental. For fun, I had it analyze a financial story generated by another robot, Automated Insights' WordSmith. If you'll recall, it wrote that story in a competition against an NPR staff writer, with readers judging the final result. Watson found Wordsmith's article unremittingly cheerful (96 percent), conscientious (94 percent) and analytical (49 percent). In other words, you'll get the facts, but you won't have any fun reading them. In comparison, Watson found the same story from an NPR writer to be negative (90 percent), but it was overwhelming voted more enjoyable to read -- by humans, anyway.                     "
263,https://www.engadget.com/2015-07-12-microsoft-machine-teaching.html,Engadget,2015,7,12,160.0," As clever as learning computers may be, they only have as much potential as their software. What if you don't have the know-how to program one of these smart systems yourself? That's where Microsoft Research thinks it can help: it's developing a machine teaching tool that will let most anyone show computers how to learn. So long as you're knowledgeable about your field, you'd just have to plug in the right parameters. A chef could tell a computer how to create tasty recipes, for example, while a doctor could get software to sift through medical records and find data relevant to a new patient. The technology is still young, but it's already being used in an invitation-only beta that helps apps understand what people are saying using natural language. Ultimately, Microsoft believes that it can ""democratize"" machine learning. It wants you to call on artificial intelligence to solve everyday problems, not just the sort you'd find at a technology giant.                     "
264,https://www.engadget.com/2015-07-10-google-ai-gmail-spam.html,Engadget,2015,7,10,220.0," Spam is always annoying, but it can occasionally be disastrous. Google has now deployed its artificial neural network to stop more of it from arriving in your Gmail inbox, something it hinted at earlier. It's designed to ""detect and block the especially sneaky spam -- the kind that could actually pass for wanted mail,"" according to the company. The system also uses machine learning to track your usage patterns and figure out if you want certain kinds of mail, like newsletters or promos. Most critically, Google said that Gmail is now better at catching impersonation -- when emails appear to be from a known contact, but were sent by someone who is definitely not your friend. Google is also working the other side of the spam equation with its new Postmaster Tools, aimed at high-volume senders. Qualified companies who meet Google's reputation requirement will get access to services that show how Gmail handles their emails. For instance, a company that sends out tons of legitimate mail -- like order or flight confirmations -- will see how often users mark it as spam. They can then tweak it so that customers treat it more seriously. From an end-user perspective, Google said the new tools will make it less likely you'll need to go ""dumpster diving"" in spam to find important messages.                     "
265,https://www.engadget.com/2015-07-05-darpa-visual-media-reasoning.html,Engadget,2015,7,5,188.0," It's easy to find computer vision technology that detect objects in photos, but it's still tough to sift through photos... and that's a big challenge for the military, where finding the right picture could mean taking out a target or spotting a terrorist threat. Thankfully, the US' armed forces may soon have a way to not only spot items in large image libraries, but help human observers find them. DARPA's upcoming, artificial intelligence-backed Visual Media Reasoning system both detects what's in a shot and presents it in a simple interface that bunches photos and videos together based on patterns. If you want to know where a distinctive-looking car has been, for example, you might only need to look in a single group. As you might suspect, the goal is to turn enemy media campaigns on their head -- all those online propaganda pics and training videos would make it much easier to pinpoint the whereabouts of bases and leaders. There's a chance that it would get creepy given that it could easily power other government surveillance programs, but there's no doubt that soldiers would appreciate this AI-assisted intel.                     "
266,https://www.engadget.com/2015-07-02-elon-musk-future-of-life-safe-ai.html,Engadget,2015,7,2,255.0," After Elon Musk donated $10 million to the Future of Life Institute (FLI) to finance studies aiming to keep AIs safe and beneficial (i.e., prevent them from going down Skynet's path) almost 300 teams submitted their research proposals. Now, the institute is finally done reviewing them all and has decided to grant $7 million from Musk and the Open Philanthropy Project to 37 projects over the next three years. Some of the studies want to teach AI what humans prefer based on body language, one aims to develop a system that can explain its decision to humans, while another vows to figure out how to make sure robots and other intelligent weapons are always kept under human control. The recipient getting the biggest amount ($1.5 million) proposes a joint Oxford-Cambridge research center to study the long-term impacts of AI, among other things. It will also develop policies to ""minimize risks and maximize benefit[s] from artificial intelligence development."" The organizers want to clarify, though, that they're not fearmongering, especially now that Terminator Genisys is about to be shown in theaters. ""The danger with the Terminator scenario isn't that it will happen, but that it distracts from the real issues posed by future AI,"" FLI president Max Tegmark said. ""We're staying focused, and the 37 teams supported by today's grants should help solve such real issues."" The institute listed all awardees right here, along with a summary of their proposals, many of which would have sounded like science fiction just a few years ago. [Image credit: kirainet/Flickr]                     "
267,https://www.engadget.com/2015-07-02-googles-chatbot-learned-how-to-talk-from-movies.html,Engadget,2015,7,2,565.0," Chatbots are pretty common these days -- a simple search can surface numerous variants you can talk to on a lonely Friday night. The one Google is developing, however, isn't your run-of-the-mill chatbot: it wasn't programmed to respond to questions a specific way. Instead, it uses neural networks (a collection of machines that mimic the neurons in the human brain) to learn from existing conversations and conjure up its own answers. Mountain View, along with Facebook and Microsoft, already uses neural networks for other purposes, such as to create works of art, to identify objects in images and to recognize spoken words. Google researchers Oriol Vinyals and Quoc Le fed this chatbot's neural networks with movie dialogues to teach it what to say and how to formulate responses. As a result, its replies can be quite dramatic, like this ""philosophical Q&A"" test run, for example: Human: what is the purpose of life ? Machine: to serve the greater good . Human: what is the purpose of living ? Machine: to live forever . Human: what is the purpose of existence ? Machine:to find out what happens when we get to the planet earth. Human: where are you now ? Machine: i 'm in the middle of nowhere . Human: what is the purpose of dying ? Machine: to have a life . Human: what is the purpose of being intelligent? Machine: to find out what it is . And, of course, it knows its history and movie trivias really well: Human: who is skywalker ? Machine: he is a hero ... Human: what do you think about tesla ? Machine: he 's a good conductor . Human: what do you think about bill gates ? Machine: he 's a good man . Human: What do you think about messi ? Machine: he 's a great player . Human: what do you think about cleopatra ? Machine: oh , she 's very regal . Human: what do you think about england during the reign of elizabeth ? Machine: it was a great place Google doesn't have much use for a Trivial Pursuit champion, though, so the researchers also fed the system old support calls to see if it can do basic customer service one day: Describe your problem: i forgot my password Machine: hi , this is <NAME> from helpdesk connect , what can i help you with today ? Human: hi , i forgot my password Machine: ok great Human: can you tell me how i can recover it ? Machine: i ' ll need to verify who it is at that machine Machine: can we do a hangout? Human: yes Machine: <anonymized hangout instructions and verifications> Human: cool , i am good now Machine: how ' s this looking good ? Machine: i ' ll go ahead and close this chat . Machine: take care . : ) Human: bye Machine: bye . The system is only a proof of concept at the moment, but the researchers believe it could be developed further. In addition to doing simple tech support (aka asking people if their computers are plugged in or to restart their PCs), it could also serve a new, more interactive way to deliver news. Le and Vinyal's paper is up on Arxiv, if you want to pore over the bot's technical details and to see more chat samples. [Image credit: Her/Warner Bros.]                     "
268,https://www.engadget.com/2015-06-30-amc-humans-review.html,Engadget,2015,6,30,893.0," Early on in AMC's newest sci-fi show, Humans, a teenager wonders aloud if there's any point in going to college and spending years training to be a neurosurgeon. After all, why invest all that time and work when an advanced android, which are commonplace in the show's world, can be programmed with those skills almost instantly. Call it the death of human expertise. Meanwhile, her mother is worried that her family's new ""synth"" (the show's term for androids) might replace her; her father hopes it can bring her family back together; and her teenaged brother is having sexually confused feelings about their attractive new robot helper. In Humans, the problems of the near future are practically indistinguishable from the issues we're facing today. And that's a big part of why the show works so well. A co-production between AMC and the UK's Channel 4 (and a remake of the Swedish series Real Humans), Humans is technically set during an ""alternate present."" It's a world that looks pretty much like ours today, except for the fact that synths are as commonplace as smartphones. While they have a high level of intelligence, synths are basically just domestic servants, taking on the drudgery of things like cleaning, cooking and caretaking so that the real humans can have a break. At least, that's the ideal. In the first two episodes we also see how society is pushing back against synths because they're also replacing people in skilled jobs (something we're already seeing today with machines in factories). And, of course, synths end up in the most human of industries: sex work. The show covers plenty of familiar sci-fi territory, but the way it juggles those elements feels refreshing. By focusing on the aforementioned Hawkins family, we get a sense of how a normal household might welcome (or not) a robotic alien. When Joe, the patriarch (played by Tom Goodman-Hill), takes his grade-school daughter to pick up their first synth, it's as if he's just perusing the Apple Store to pick up the latest gadget. He settles on an attractive synth named Anita (Gemma Chan), somewhat oblivious to how that could affect his wife Laura (played by Katherine Parkinson of The IT Crowd and The Honourable Woman), a lawyer who seems particularly disturbed by the widespread adoption of synths. It's not too long before Laura starts to feel threatened by Anita, a perfect-looking synth that can do everything a ""good mother and wife"" should without ever getting frustrated or tired. Their teenage daughter Mattie, meanwhile, wants nothing to do with Anita and seems to be going through an existential crisis about what synths will mean for her future. While today's generation of college graduates are dealing with crippling debt and an unstable financial market, young adults of tomorrow might have to face a world where their skill set will be practically obsolete by the time they enter the work force. Once we have robots that look remarkably human in our homes, will we treat them as mere tools or family? We also get a glimpse at the synth perspective: Anita appears to be a typical domestic android on the surface, but she also has instances where her behavior seems erratic and almost human. There's a group of synths on the verge of achieving true consciousness, and, naturally, there's a shadowy organization trying to round them up and study them. Synths working in brothels get treated just as badly (and likely even worse) as human prostitutes. Like any good piece of thoughtful science fiction, Humans is strongest when it's saying something about our current society and where we're headed. The issues the Hawkins clan deals with when Anita moves in wouldn't be out of place when a new person enters your home. But we also see how Anita's presence changes the way they live, something show creators Jonathan Brackley and Sam Vincent noticed as we reshaped our lives around smartphones and tablets. William Hurt (who notably played the artificial intelligence inventor in Steven Spielberg's A.I.) plays a scientist who helped bring synths into the world, and yet is having a hard time parting with his malfunctioning synth helper, who also stores memories about his wife. Once we have robots that look remarkably human in our homes, will we treat them as mere tools or family? Much like Battlestar Galactica before it, Humans works as an allegory about how we all treat each other. We can all agree that slavery is wrong today, but how will we feel once we can easily buy robotic humanoids? And at what point do we start considering their civil rights? Humans comes at a particularly vibrant time for AI and robots in pop culture. We're far beyond the doomsday Terminator scenarios from the last few decades (yet another reason why the upcoming Terminator: Genisys feels like a film out of place in time). Now, it's as if we're preparing ourselves for the inevitability of true artificial intelligence and robot helpers. Alex Garland's Ex Machina explored how we might end up testing AI, and one of the best episodes of Black Mirror, ""Be Right Back,"" focused on the emotional toll of using AI and androids to recreate our loved ones. We're still facing our fears and anxieties of this new tech through science fiction, but now it's on a much smaller and more intimate scale.                     "
269,https://www.engadget.com/2015-06-27-the-photos-you-probably-wont-find-on-instagram.html,Engadget,2015,6,27,234.0," Photography reached the mainstream early on; Kodak's Brownie made daily snapshots accessible and Polaroid's pioneering cameras provided instant gratification. Now we can capture and share moments on a whim with smartphones packing high-resolution optics. Over the years, though, we've been treated to some incredible imaging hacks that've allowed our eyes to travel into the exotic -- far beyond what you had for dinner last night. Technological leaps in the field have been spurred by bets, accidents and imagination, providing both scientific insight and artistic experimentation. Our eyes have been opened wider than ever before and we've collected just a few moments in imaging's history to help grasp the bigger picture. In 1843, botanist Anna Atkins used a unique method of photography invented by Sir John Herschel to create ""cyanotypes."" These were a type of photogram -- wherein objects placed directly onto photo-sensitive material created a shadow exposure -- except this particular chemical process resulted in blue (cyan) tones. Her resulting collection of botanical images was published in Photographs of British Algae (Cyanotype Impressions) that year. It's widely known as the first book to use photographic images and you can flip through it in its entirety at the New York Public Library's digital collection. Nearly 80 years later, Man Ray, a surrealist artist and photographer, would use the photogram process to create a surprising assortment of X-ray-like images. He called them ""rayographs."" [Image: Google Research]                     "
270,https://www.engadget.com/2015-06-22-facebook-algorithm-identifies-people-with-hidden-faces.html,Engadget,2015,6,22,160.0," A number of companies have developed photo software for facial recognition, but what happens when your face is partially hidden? What if it's completely covered up? Facebook's artificial intelligence lab developed an algorithm that remedies the issue by picking out folks with other clues. Instead of using facial features, the software can identify people using things like hair style, pose, clothing and body type. Of course, a tool like this could lend a hand in a photo app like Facebook Moments or even Google's revamped Photos software. However, it also raises privacy questions when you can be identified in a snapshot even if your face is concealed, especially if you're trying to remain hidden on purpose. Facebook's algorithm is pretty good too, identifying people with an 83 percent success rate in tests, so we'll be curious to see if it makes its way into the social network's photo galleries in the future. [Image credit: David Paul Morris/Bloomberg via Getty Images]                     "
271,https://www.engadget.com/2015-06-20-facebook-and-google-ai-image-creation.html,Engadget,2015,6,20,255.0," For Facebook and Google, it's not enough for computers to recognize images... they should create images, too. Both tech firms have just shown off neural networks that automatically generate pictures based on their understanding of what objects look like. Facebook's approach uses two of these networks to produce tiny thumbnail images. The technique is much like what you'd experience if you learned painting from a harsh (if not especially daring) critic. The first algorithm creates pictures based on a random vector, while the second checks them for realistic objects and rejects the fake-looking shots; over time, you're left with the most convincing results. The current output is good enough that 40 percent of pictures fooled human viewers, and there's a chance that they'll become more realistic with further refinements. Google's take heads in the opposite direction. Instead of striving for realism, it's producing art by letting the neural network run wild and decide on the visual elements that it wants to emphasize. If you give the machine a photo of the sky and it thinks there are birds in the scene, it'll keep amplifying those avian traits until they're impossible to miss. The finished work is more than a little trippy, especially if you give it random noise as its source material -- as you can see above, the results give impressionist and surrealist painters a run for their money. You're not likely to see these Facebook and Google programs replacing human artists and photographers, but they're skilled enough to draw images you might enjoy.                     "
272,https://www.engadget.com/2015-06-20-amazon-machine-learning-for-reviews.html,Engadget,2015,6,20,229.0," Let's be blunt: Amazon's reviews sometimes suck. Many of them are hasty day-one reactions, others are horribly misinformed and a few are out-and-out fakes. The internet shopping giant thinks it knows how to sort the wheat from the chaff, however. It just launched a new machine learning system that understands which reviews are likely to be the most helpful, and floats them to the top. The artificial intelligence typically prefers reviews that are recent, receive a lot of up-votes or come from verified buyers. Amazon hopes that this will show you opinions that are not only more trustworthy, but reflect any fixes. In other words, you'll see reviews for the product you're actually likely to get. The learning technology is only active in the US at the moment, and it may take a while before you notice the difference. Also, it's unclear as to how well the system will work in practice. While it might downplay that impulsive one-star review, it's not going to know which opinions are the most thoughtful -- that's more likely up to you. And, as Recode suggests, there's a worry that this algorithmic approach will mess with the joke reviews that have become a staple of Amazon. Who doesn't want to read feedback on TVs that cost as much as a luxury car, or networking cables that open wormholes? [Image credit: AP Photo/Mark Lennihan]                     "
273,https://www.engadget.com/2015-06-17-twitter-buys-machine-learning-startup.html,Engadget,2015,6,17,141.0," Twitter thrives on its ability to understand both your tweets and the hot topic of the day, and it needs every bit of help it can get -- including from computers. Accordingly, the social network just snapped up Whetlab, a startup that makes it easier to implement machine learning (aka a form of artificial intelligence). The two companies are shy about what the acquisition means besides an improvement to Twitter's ""internal machine learning efforts."" However, the likely focus is on highlighting the content that's most relevant to you based on your activity and who you follow, as well as hiding abusive tweets before you have to reach for the ""block"" option. Whetlab's technology could get the ball rolling on these robotic discovery techniques much faster than before, and give you a custom-tailored Twitter experience that requires little effort on your part.                     "
274,https://www.engadget.com/2015-06-17-super-mario-world-self-learning-ai.html,Engadget,2015,6,17,306.0," Perhaps it's that all the levels have simple, left-to-right objectives, or maybe it's just that they're so iconic, but for some reason older Mario games have long been a target for those interested in AI and machine learning. The latest effort is called MarI/O (get it?), and it learned an entire level of Super Mario World in 34 tries. Unlike other AI programs, MarI/O wasn't taught anything before jumping into the game -- it didn't even know that the end of the level was to its right -- instead, some simple parameters were set. The AI has a ""fitness"" level, which increases the further right the character reaches, and decreases when moving left. The AI knows that fitness is good, and so, once it figures out that moving right increases that stat, it's incentivized to continue doing so. Mirroring actual evolution, MarI/O didn't actually change its behavior with any forethought. Every generation introduced new ideas, but it was simply trying different things, not doing what it ""thought"" would work. When an idea was a success, it was remembered, when it wasn't, it was discarded and learned from. Over the course of 34 evolutionary steps, MarI/O ended up working out jumping though the entire level would do the trick. If its creator Seth Bling were to run it again, the AI would almost certainly find a different, but no less successful path through the level. This learning style is called NeuroEvolution of Augmenting Topologies (or NEAT, for short), and it's nothing new, but it's interesting to see it used so effectively. While it's a good demo, there's a long way to go before machine learning like this could ever hope to challenge a more functional algorithm. Check out the A* path-finding bot below, which won a Mario AI competition back in 2009, to see what we mean.                     "
275,https://www.engadget.com/2015-06-04-computer-vision-pain-detection.html,Engadget,2015,6,4,162.0," Remember Baymax's pain scale in Big Hero 6? In the real world, machines might not even need to ask whether or not you're hurting -- they'll already know. UC San Diego researchers have developed a computer vision algorithm that can gauge your pain levels by looking at your facial expressions. If you're wincing, for example, you're probably in more agony than you are if you're just furrowing your brow. The code isn't as good at detecting your pain as your parents (who've had years of experience), but it's up to the level of an astute nurse. As you might imagine, the technology could be very helpful in the hospital and beyond. Nurses could use it to tell whether or not you're getting enough painkillers after surgery. Alternately, medical robots could offer treatment if they see you grimacing. Doctors could eventually refine procedures based on feedback, and you might never have to wait for a health care worker to see that you're suffering.                     "
276,https://www.engadget.com/2015-06-02-google-calorie-counting-in-photos.html,Engadget,2015,6,2,231.0," Be careful about snapping pictures of your obscenely tasty meals -- one day, your phone might judge you for them. Google recently took the wraps off Im2Calories, a research project that uses deep learning algorithms to count the calories in food photos. The software spots the individual items on your plate and creates a final tally based on the calorie info available for those dishes. If it doesn't properly guess what you're eating, you can correct it yourself and improve the system over time. Ideally, Google will also draw from the collective wisdom of foodies to create a truly smart dietary tool -- enough experience and it could give you a solid estimate of how much energy you'll have to burn off at the gym. This isn't going to lead to a practical product in the short term. Google only just filed for a patent on Im2Calories' underlying technology and has no immediate release plans, so you can post dessert photos to Instagram with relatively little guilt. Eventually, though, it could be a staple feature of health apps that help you balance your food habits with your activity levels. And the potential doesn't stop there, either. While food is the ""killer app,"" the image recognition code could also apply to traffic prediction and anything else where a series of photos can provide a wealth of data. [Image credit: Ana Arevalo/AFP/Getty Images]                     "
277,https://www.engadget.com/2015-05-28-robot-crawls-with-legs-damaged-aww.html,Engadget,2015,5,28,208.0," Robots are getting pretty good at carrying on after taking a knock, but what if they lose a limb? Scientists from the US and France have given a six-legged 'bot the smarts to keep going even if two of its legs are disabled by, say, a Sarah Connor shotgun blast. The team created and then rated a number of simulations for how its robot could keep moving forward despite losing a leg or two. Once that information was programmed into the robot, it was able to rapidly evaluate the options and use the one that worked best in the real world. Calling it ""intelligent trial and error,"" they described the programming as a kind of intuition for robots. ""These predictions come from the simulated, undamaged robot. (However) it has to find out which of them work, not only in reality, but given the damage,"" according to lead author Antoine Cully. He added that the same algorithm also lets the machines adapt to new situations and environments. The researchers think the bot could one day be useful as a rescue aid or personal assistant, and expect to see similar robots that can adapt to adversity or even fix themselves. NASA, for one, will need them for future Mars missions.                     "
278,https://www.engadget.com/2015-05-25-brett-deep-learning-robot.html,Engadget,2015,5,25,213.0," As a rule, robots have to learn through explicit instruction, whether it's through new programming, watching videos or holding their hands. UC Berkeley's BRETT (Berkeley Robot for the Elimination of Tedious Tasks) isn't nearly that dependent, however. The machine uses neural network-based deep learning algorithms to master tasks through trial and error, much like humans do. Ask it to assemble a toy and it'll keep trying until it understands what works. In theory, you'd rarely need to give the robot new code -- you'd just make requests and give the automaton enough time to figure things out. As you might suspect, though, this brain-like 'bot isn't ready for the real world yet. It takes 10 minutes to learn a task when you tell it exactly where it needs to start and stop, and 3 hours if it has to learn those positions itself. BRETT isn't drawing from a wealth of experience, as you do, so it doesn't make those logical leaps that help you grasp a concept quickly. With that in mind, the researchers are optimistic that the technology will improve dramatically over the next several years as robots get better at handling lots of data. Eventually, artificial intelligence could be good enough that robots would be ready for anything their designs allow.                     "
279,https://www.engadget.com/2015-05-19-fetch-expect-labs-apple-watch-shopping.html,Engadget,2015,5,19,334.0," Talking into a smartwatch still isn't the most socially acceptable thing to do, but a pair of startups is hell-bent on at least making it worthwhile. Fetch and Expect Labs -- a personal shopping service and a purveyor of a voice-driven AI, respectively -- have teamed up to make shopping on your Apple Watch a little less tedious with an improved concierge that works from your wrist. Back in the day (i.e. last year), Fetch was best known for using a crew of humans to respond to messages sent from the app. If you wanted to buy, say, a sweet messenger bag someone was rocking in SoHo, you could snap a photo, send it along, and someone would eventually respond with the cheapest, most appropriate listing they could find. With Expect Labs' voice recognition and analytical chops now being baked into the existing iOS/Apple Watch app, though, those requests can be chopped up and acted on more quickly. The end result? A faster first wave of hits, and a less headache-inducing shopping experience (they hope). Let's say you're itching to laze under the sun in some far-off locale. You'll be able to ask your Apple Watch to book you on the first flight to Bangkok next Thursday, and Expect Lab's thoughtful back-end will dig up a handful of suitable flight options. From there, those results will get passed along to Fetch's crew of shopping concierges so they can ferret out the best option and send it back home to you for approval. Well, eventually, anyway. Fetch and Mindmeld are talking up a partnership today, but a spokesperson confirmed that the actual functionality won't go live for another few months (hopefully in time for a last minute summer holiday). And if you're one of the countless multitudes who don't -- or won't -- wear an Apple Watch? No worries: The feature will find its way to Fetch's Android app, too, though you'll have to pay Fetch $10 a month on any platform for the privilege.                     "
280,https://www.engadget.com/2015-05-14-wolfram-language-image-identification.html,Engadget,2015,5,14,228.0," Wolfram Research can already do some pretty cool things, like answer Twitter questions and spot overhead flights. Now, the maker of the Mathematica programming language and Alpha knowledge engine can perform another trick: figuring out what's in a photo. The Wolfram Language Image Identification Project can make out about 10,000 common things, including animal species, gadgets and household objects. It uses a database of around ten million images to perform the trick, which Stephen Wolfram figures ""is comparable to the number of distinct views of objects that humans get in their first couple of years of life."" The project joins Google Goggles, Amazon Firefly and recent Microsoft research at the forefront of image recognition. Wolfram uses neural networks to build up layers of detail starting with individual pixels and finishing with specific object features. The system learns as it goes, allowing you to enter better object definitions if it guesses wrong or is too vague. It will also let programmers build image identification into their apps. Wolfram said, ""It won't always get it right, but most of the time I think it does remarkably well. What's particularly fascinating is that when it does get something wrong, the mistakes it makes mostly seem remarkable human."" Judging by the amusing collection of mistakes (above) we couldn't agree more, but feel free to give it a try with your own photos.                     "
281,https://www.engadget.com/2015-04-10-ex-machina-review.html,Engadget,2015,4,10,1084.0," With Ex Machina, the directorial debut of 28 Days Later and Sunshine writer Alex Garland, we can finally put the Turing test to rest. You've likely heard of it -- developed by legendary computer scientist Alan Turing (recently featured in The Imitation Game), it's a test meant to prove artificial intelligence in machines. But, given just how easy it is to trick, as well as the existence of more rigorous alternatives for proving consciousness, passing a test developed in the '50s isn't much of a feat to AI researchers today. Ex Machina isn't the first film to expose the limits of the Turing test, but it's by far one of the most successful. And, like the films 2001 and Primer, it's a work of science fiction that might end up giving you a case of philosophical whiplash. Ex Machina is constructed like a morality play. A nebbish programmer, Caleb (Domhnall Gleeson), wins a Willy Wonka-esque contest to spend a week with Nathan (Oscar Isaac), the reclusive genius CEO of his company -- a search giant called BlueBook. Nathan, who is equal parts startup bro, Steve Jobs and Larry Page and all with a megalomaniacal bent, arrives with an intriguing dilemma: He's developed an artificially intelligent robot named Ava (Alicia Vikander) and he wants Caleb to prove she has a consciousness of her own. Of course, they're both well aware that the Turing test alone won't be enough. As originally conceived, the Turing test involves a natural language conversation between a machine and human conducted through typed messages from separate rooms. A machine is deemed sentient if it manages to convince the human that it's also a person; that it can ""think."" Since it was developed, there have been claims of several AI experiments that have successfully passed the Turing test. When subjected to scrutiny, however, these results were found to be invalid. Just last year, a chatbot pretending to be a 13-year old boy convinced 33 percent of a judges panel that it was human -- effectively passing the Turing test's threshold of 30 percent believability. But it wasn't long before a slew of critics, including the long-running technology-analysis site TechDirt, pointed out that its programmers had gamed the test by pretending the chatbot wasn't a native English speaker -- allowing it to take advantage of a handicap -- and that plenty of other chatbots have achieved better scores. No matter an AI's final Turing test score, a script built to imitate human conversation or recognize patterns isn't something we'd ever describe as being truly intelligent. And that goes for other major AI milestones: IBM's Deep Blue is better at chess than any human and Watson proved it could outsmart Jeopardy world champions, but they don't have any consciousness of their own. It's worth noting that neither of those supercomputers has gone through the Turing test, though inventor and futurist Ray Kurzweil believes Watson could be retooled to pass it easily. In Ex Machina, Caleb has nothing like Blade Runner's Voight-Kampff machine, a souped-up lie detector used to detect artificial life, so he has to come up with more creative ways to prove Ava's intelligence. He's also not dealing with a mysterious entity in another room; he's talking to her face to face through a window, and can clearly see that she's a robot. All of that makes it more difficult for Ava to trick him like a typical chatbot, although it opens up new ways for her to influence him. The sexual politics of the situation aren't an accident: Ava's a strangely attractive robot created by a hyper-masculine genius CEO, who finds her Prince Charming in Caleb. And you could argue that it's also a cheap way for Nathan to manipulate Caleb's investigation. Tim Tuttle, a former MIT AI researcher and the CEO of the predictive-intelligence company Expect Labs, agrees with Garlands' use of sexuality in the film as a ruse. ""[Ex Machina] proposed a sort of inverse where it's not enough to have a human be deceived for a machine to be real,"" he said. ""The machine needs to convince the human to do things for it -- to fall in love with it, to serve its own purposes."" There's another element present in the complex conversations Caleb has with Ava and one familiar to consciousness researchers: the Chinese Room Argument. It's a thought experiment philosopher John Searle formulated as a way to disprove the usefulness of the Turing test. The basic gist of which goes like this: You're stuck in a room following along with a computer program that's replying to questions being slipped under the door in Chinese. By following the computer's instructions, you manage to output perfect responses in Chinese characters. But even so, you clearly don't understand the Chinese language or what those responses actually mean. Similarly, Searle argues a computer doesn't actually need to understand language or complex thought to pass the Turing test. And it's this line of thinking that colors Caleb's interactions with Ava throughout the film. He continually goads her to prove she understands what it is she's saying. Ex Machina also touches upon the more useful offshoots of the Turing test like Lovelace 2.0, which focuses on having machines demonstrate true creative abilities. It's not much of a spoiler to say that, on several occasions, Ava demonstrates that she can be just as creative as any human. We never learn how, exactly, Nathan invents Ava, but it's clear his search engine BlueBook has played a big role. At one point during the film, he notes that most people thought BlueBook existed just to give them answers when, in fact, it actually served as a means of collecting those queries and creating a framework for artificial intelligence. That may sound like pure sci-fi fluff, but according to Tuttle, ""People inside Google really feel that way."" It's just not something the general public recognizes. If search giants like Google believe a fully aware AI is an inevitability, and an offshoot of our own archived curiosity, just how will we end up testing its existence? Ex Machina, for one, doesn't provide that elusive answer, but it certainly raises a lot of the right questions. And right now, we're mainly in the realm of thought experiments when it comes to judging consciousness. But, at the very least, the film makes it clear that we need to think beyond simplistic tests like Turing and, most importantly, we should be prepared to be surprised. [Photo credits: DNA Films/Film 4]                     "
282,https://www.engadget.com/2015-04-09-ibm-numenta-ai-software.html,Engadget,2015,4,9,307.0," We haven't talked about Numenta since an HP exec left to join the company in 2011, because, well, it's been keeping a pretty low-profile existence. Now, a big name tech corp is reigniting interest in the company and its artificial intelligence software. According to MIT's Technology Review, IBM has recently started testing Numenta's algorithms for practical tasks, such as analyzing satellite imagery of crops and spotting early signs of malfunctioning field machinery. Numenta's technology caught IBM's eye, because it works more similarly to the human brain than other AI software. The 100-person IBM team that's testing the algorithms is led by veteran researcher Winfried Wilcke, who had great things to say about the technology during a conference talk back in February. Tech Review says he praised Numenta for ""being closer to biological reality than other machine learning software"" -- in other words, it's more brain-like compared to its rivals. For instance, it can make sense of data more quickly than competitors, which have to be fed tons of examples, before they can see patterns and handle their jobs. As such, Numenta's algorithms can potentially give rise to more intelligent software. The company has its share of critics, however. Gary Marcus, a New York University psychology professor and a co-founder of another AI startup, told Tech Review that while Numenta's creation is pretty brain-like, it's oversimplified. So far, he's yet to see it ""try to handle natural language understanding or even produce state-of-the-art results in image recognition."" It would be interesting to see IBM use the technology to develop, for example, speech-to-text software head and shoulders above the rest or a voice assistant that can understand any accent, as part of its tests. At the moment, though, Numenta's employees are focusing on teaching the software to control physical equipment to be used in future robots. [Image credit: Petrovich9/Getty]                     "
283,https://www.engadget.com/2015-04-01-ex-machina-alex-garland-interview.html,Engadget,2015,4,1,1614.0," Alex Garland is no stranger to science fiction. As the writer of 28 Days Later and Sunshine, he's given us his own unique spin on the zombie apocalypse and a last-ditch effort to save the Earth (by nuking the sun!). Now, with his directorial debut, Ex Machina, Garland is taking on artificial intelligence -- and in the process, he shows the limits of the Turing test, the most common method for determining if something is truly sentient. The film centers on a young programmer who's sent to his genius CEO's isolated compound to test his latest invention: an artificially intelligent robot. Things, as you can imagine, don't go as planned. I spoke to Garland ahead of the film's US premiere at the SXSW festival. And, as you'll quickly learn, he's got a lot to say about AI and the nature of consciousness. Ex Machina hits theaters on April 10. What was your creative spark for the story? Were there any ideas for the story? This one just seemed a little different for AI, more of an exploration of how you could love an AI. Going way back, for me it would go back to childhood ... and having very, very simple home computers. The ZX Spectrum What was your first home computer? I think the first actual computer [for me] was the ZX81, but the one I actually got to grips in any way as the ZX Spectrum, which was a follow-up to that. ... It had BASIC commands sort of burned into the keyboard that you could access through shortcut keys. So I would write very, very simple ... ""hello world"" kind of programs, where the computer would maybe have the ability to answer three or four questions. ... It would give you this feeling as a kid of bolting up and thinking, this is suddenly feeling sentient. Of course it's not sentient -- you're very aware it's not, because you've programmed it. But nonetheless it gave you that sort of funny electric feeling. I think that always stayed with me. Then years and years later I got involved in a really long argument ... with a friend of mine whose principal area of interest was neuroscience. He's affiliated with a school of thought that basically says, ""Machines are never going to be sentient in the way we are."" There are very serious thinkers attached to that, like Roger Penrose [a renowned physicist and philosopher]. And I instinctively disagreed with this, but I didn't have the sort of armory to disagree with it on his terms, so I started reading as much as I could. Not to get too boring, but our sticking point was over qualia [a philosophical term for defining conscious experiences we all have]. ... I had a kind of instinctive, and then subsequently I'd say rationalized, sense that qualia might not even exist. Domhnall Gleeson [left] and Oscar Isaac in 'Ex Machina' You mean even in human language and thought? Yah, yah. ... In reading about this, I came across this book by [cognitive roboticist] Murray Shanahan, who wrote a book about consciousness and embodiment. Within it there's a really beautiful idea, which is an argument against metaphysics [the philosophy branch that explores how we experience the world] in terms of the mind. And in a way, if you can get rid of metaphysics as a problem, it allows for an artificial intelligence -- or a strong AI. ... And I have to say, as I was reading that book -- and as a layman, it's fucking difficult for me to read this stuff; I struggled like crazy -- but it was while reading it that the idea for this movie appeared in my head. I was in preproduction on Dredd; I was spending a lot of time in South Africa, on planes, and I had a lot of time to read. ... So whenever we shot Dredd, the three months before that, that's basically where it came together. And after I wrote the script, I sent it to Murray Shanahan and said, you don't know who I am, but I want you to really look at this script. I wanted the stuff within it to be reasonable. ""There's a lot of legitimate reasons to be scared of superintelligence."" So what did he think? There was a bit of coding in there where he said, ""I'm not sure it's compiled correctly."" In fact, in the film some of the code that appears on screen, if you were to write it out, what you'd find is it leads you to the ISBN of his book. What other sorts of AI books have you read? I pretty much would read everything I could. I tried to read people like Penrose, who were arguing against what I instinctively believed. That's a good way to solidify your argument. I don't want to dignify it from my point of view, because I can't stress enough I'm a real layman. So I can understand the principles of an argument, but when it comes to the actuality of what people like Demis Hassabis at DeepMind [a British AI company owned by Google] are actually doing, I really don't understand it. That's part of what interests me. It's the gap between people who do understand what they're doing, and the way information disseminates, and the confusion that exists between those spaces. The thing I began to get fixated on -- this is separate from the film, but related -- was confronting my own intellectual limitations. And thinking, here I am; I'm not dumb, I'm doing what I can, but I'm also running into a brick wall of my own intellectual capacity. And it sort of made me a bit nervous in some respects, because I was thinking, that means a lot of this stuff, for a lot of people, and I would include myself in that part of the Venn diagram, becomes articles of faith. That's troubling where there are ethical consequences to the things that are happening, because to get through the ethics, you have to understand it. Alex Garland What do you think about the rise of AI and superintelligence? Do you think it's something we need to worry about? I think there's a lot of confusion around this. I mean, a few months ago it was announced that a machine passed the Turing test in a way that was completely ridiculous; it didn't stand up to any scrutiny at all, but the fact that it was reported as widely as it was tells you that what kind of problem there is in terms of what people are actually doing, and what they're perceived to be doing. There's a lot of legitimate reasons to be scared of superintelligence; of course there are. And exactly as there are reasons to be scared of the implications of nuclear power, hence the [Robert] Oppenheimer analogy that exists throughout the film. Both of them are potentially dangerous. From my point of view, I would see myself as being sort of green in terms of my outlook, but I'm also in favor of nuclear power. So I would see the superintelligences as being analogous to that. They could be problematic if they're controlling drones and making kill decisions over humans; I can completely see that. I think that probably people get a bit confused about them, and maybe a film like Ex Machina in some respect doesn't help, in as much as we think that they might be human like. But they probably won't be, in terms of how they see the world, and the way they interact with it and each other. It's completely alien to us. Yah we don't really know because they're not here yet. And as a father, it's a bit like trying to conceptualize your child before they're born. You can't actually do it. It's an abstract thing until it arrives. And then, of course, you can get your head around it. ... If you get an AI that was human-like, and it has similar things to our consciousness embedded within it. There's nothing within that, that I find necessarily frightening. ""Being clear about these things is important. Otherwise, you'll be talking about the sentience of Siri. And Siri doesn't have any fucking sentience."" A human intelligence is capable of being fantastically dangerous if it's given the life path and powers of [Joseph] Stalin or Pol Pot. And I could say if you provided an AI with too much power, you might get serious problems. But what that would be is an argument in favor of checks and balances, rather than an argument against AIs. I think it's helpful and actually sensible when talking about this stuff to be clear what it is one's talking about. AI's a super-broad term. I play video games with AIs. And in the same way that the reporting of the passing of the Turing test would misunderstand A) whether it had been past and B) what the Turing test was, and also, C) whether the Turing test is actually an indicator of consciousness in a machine, or whether it's just an indicator that the machine passed the Turing test. And I would say it's the latter. ... It doesn't tell you whether you're sentient. Being clear about these things is important. Otherwise, we're very quick to conflate stuff, and suddenly you'll be talking about the sentience of Siri. And Siri doesn't have any fucking sentience. AI is probably too broad of a term to be useful at the moment. This interview has been condensed and edited. [Photo credits: DNA Films/Film 4 (Ex Machina set); Jean-Christophe Verhaegen/AFP/Getty Images (Alex Garland)]                     "
284,https://www.engadget.com/2015-03-26-facebooks-ai-can-see-inside-your-videos-and-posts.html,Engadget,2015,3,26,164.0," Facebook connects people, but it also wants to know them so it can show relevant information and targeted ads to them. To generate a personalized feed for each user, the network needs to identify and classify content in posts, images and news. Towards that end, the company launched an ambitious AI plan, and a research laboratory, at the end of 2013. Today at F8, its annual developer's conference, the network's CTO Mike Schroepfer talked about a specific AI prototype that can identify content in videos and the context of words. While AI for video can identify 487 types of sport activities, another reads sentences to pinpoint possessives from the grammar used. This allows the company to sift through an overwhelming load of information so it can arrive at a newsfeed that's most appropriate for the user. It's unclear if the new system is already peeking through posts, but Schroepfer indicated that over the next 10 years, Facebook will focus on building advanced AI systems.                     "
285,https://www.engadget.com/2015-03-19-human-like-ai-optical-fiber-research.html,Engadget,2015,3,19,211.0," We haven't built a truly sentient artificial ntelligence system yet, like the Terminator or C-3PO -- you know, the kind of AI that scares Elon Musk and Stephen Hawking. But a team of researchers believe they've at least found the secret to creating human-like artificial brains. More specifically, they've discovered that optical fiber made from chalcogenides can create synapses to latch onto each other, just like what happens in our gray matter. Chalcogenide is a light-sensitive compound typically used to manufacture glass for photocopiers, and it allows the the fibers to process vast amounts of data. See, while computers these days are becoming increasingly fast and powerful (take Watson, for example), the human brain is still a lot faster and more efficient. The researchers from the University of Southampton and the Nanyang Technological University in Singapore still have to find a way to combine multiple fibers to form an artificial neural network, but it's a start. If you're well-versed in scientific lingo, check out the scientists' paper published in Advanced Optical Materials for more technical details. And if the study scares you, you can follow in Musk's footsteps and support the research program, which aims to ensure that AI systems don't turn into evil masterminds that want to wipe out humanity.                     "
286,https://www.engadget.com/2015-03-07-chappie-ai.html,Engadget,2015,3,7,719.0," Take Johnny 5 from Short Circuit, add a dash of Wall-E and a bit of badass swagger from RoboCop and you've got Chappie, the star of Neill Blomkamp's latest film. He's the first robot to achieve consciousness in a near future where other, less smart bots are taking on the grunt work of policing. But instead of being recognized as a major scientific breakthrough, he ends up being raised by a group of gangsters (led by Ninja and Yolandi Visser of Die Antwoord), after being created in secret by a brilliant engineer (Dev Patel). While the film, unfortunately, isn't quite up to par with Blomkamp's breakout hit, District 9, it still brings up some interesting points when it comes to the eventual rise of artificial intelligence. Once Chappie is ""born,"" he's like a scared and helpless animal -- which doesn't make much sense when you think about it. After all, why would he even be afraid of humans? And it wouldn't have been hard to give him access to basic language skills. A quick jaunt around the future internet could be enough to teach an AI about most languages in an instant. Of course, there will be plenty about training AI that will totally catch us by surprise -- we'd effectively be creating new lifeforms, after all -- Chappie just seems to tackle that in a very simplistic way. It doesn't take long for Chappie to make us empathize with its starring bot. He's neglected, and even abused, by the people closest to him. And his childlike demeanor makes his struggles throughout the film truly heartbreaking. Humans have the fascinating ability to anthropomorphize and empathize with just about anything. If you've ever named your car or computer, or if you've yelled at your Roomba for misbehaving, you've done it too. The film is a reminder that we won't have any trouble bonding with AI-powered machines -- that is until they get way too smart. There are three different ways we can classify AI. Artificial narrow intelligence (ANI), also known as ""weak AI,"" can be found in plenty of modern gadgetry, including your smartphone and car. It describes things like Siri, which is smarter than traditional software, but only up to a point. Artificial general intelligence (AGI), or ""strong AI,"" matches human intelligence, and it adds in things like true consciousness and self-awareness. Obviously, that's something we're still working toward. And then there's artificial superintelligence (ASI), which describes something that's vastly smarter than genius-level humans in every respect. ASI is the driving force behind the technological singularity, the notion bandied about by futurists that tech will eventually surpass human capabilities and understanding. (Want to learn more about AI? Check out this excellent primer from Wait But Why, as well as Superintelligence: Paths, Dangers, Strategies by Nick Bostrom.) The dumb robot cops in Chappie are a decent example of weak AI, whereas Chappie himself is the stronger counterpart. It's not spoiling much to say that he eventually gets smarter over the course of the film -- to the point where we may even consider Chappie superintelligent. And while it may just seem convenient for the film's plot, many researchers believe that's AI will be able to evolve itself through a process called recursive self-improvement, which describes their ability to constantly make their software better. This one's pretty obvious, as computer automation is already taking away plenty of jobs from humans. In Chappie, Hugh Jackman's character has a blinding hatred for the widespread use of robots. His solution? Create a hulking, man-powered monstrosity that resembles the ED-209 from RoboCop, and pitch it as a more humane alternative to soulless robots. It's just too bad that the robot cops in the film, from what we see at least, seem to be doing a fine job. For a movie about one cute robot, Chappie ends up diving into some seriously intriguing metaphysical territory in its last act. I won't spoil it entirely here, but let's just say it really makes you question the nature of consciousness. It's a reminder that, while many are worried about the negative impact of superintelligent machines, including the likes of Bill Gates and Elon Musk, they'll likely end up making significant technological breakthroughs of their own. Let's just hope we're able to keep up. [Photo credits: Columbia Pictures (Chappie)]                     "
287,https://www.engadget.com/2015-03-06-nao-and-forever-how-i-learned-to-love-a-robot.html,Engadget,2015,3,6,1455.0," My love affair with mechanical companions can be traced back to Teddy Ruxpin, the animatronic bear that replaced the inanimate My Buddy doll as my go-to plaything as a young boy. But three nights in Las Vegas almost destroyed that lifelong fascination. Almost. It all started with a simple, if petulant demand I'd made during a planning meeting for November's Expand 2014 in New York City, our third tech-meets-entertainment spectacle. ""I want dancing robots!"" I said, doing my best Veruca Salt. Months later, I was sitting front and center in a sprawling convention center watching three humanoid robots do a choreographed dance routine for a crowd of slack-jawed spectators. The performers, Aldebaran's toddler-sized Nao bots, showed off their 25 degrees of freedom in an unfortunately out-of-sync dance routine, while real-life toddlers crowded the edges of the stage slapping their little hands to the beat. The children, as one might imagine, weren't particularly concerned with the lack of coordination. And it seemed neither were the adults, as a sizable crowd had gathered to revel in the awesomeness that is a robot dance party. Meanwhile, I sat mere yards away feeling equal parts awe and embarrassment for the troupe. I was feeling for these machines. I was falling in love. It was only natural, then, that these dancing robots would combine with my first love, cocktails, when we started planning for our January CES 2015 stage show, Engadget After Hours. The plan was simple: A local bartender would join us on stage each night to supply us with ample amounts of booze while we ran down the weird, wild and sometimes ridiculous products at the biggest tech show on Earth. Nao would also be on hand to lend a little robotic flair. Aside from a day-one technical hiccup, Nao delivered on its promises: It introduced myself and my co-host, Engadget Editor-in-Chief Michael Gorman; it read off the ingredients of our nightly cocktail; and even threw a little impromptu shade our way. It hit its cues, clapped when prompted and kept quiet when it was time for the flesh-and-blood stars to shine. Nao had another man in its life, one that literally controlled everything it did. For all intents and purposes, Nao was the perfect live TV companion. But I couldn't get over one thing: Just a couple of yards away, staring me straight in the eyes, was the human puppet master pulling Nao's digital strings, a developer from Aldebaran with a clunky, black laptop. Nao had another man in its life, one that literally controlled everything it did. As the reality that Nao wasn't operating independently set in, my feeling of affection began to fade. I no longer saw Nao as the multi-talented, technological wonder that had elicited warm emotions from me. Instead, I saw a cold, hard machine; something akin to the giant animatronic members of Chuck E. Cheese's jug band. I knew Nao was capable of more sophisticated forms of human-robot interaction, but I felt cheated. Why was I having flashbacks to Ashlee Simpson's failed backing track during a very live SNL performance? Why did I feel like someone had just pulled the curtain back on the Wizard of Oz? When we invited Nao to co-host Engadget After Hours during CES, I'd set aside rationality, subconsciously expecting to share the stage with a robot capable of human-level interaction. To better understand what I was feeling, I reached out to artist and roboticist, Alexander Reben. As he pointed out, I was seeking the Holy Grail of artificial intelligence. I wanted Nao to pass the Turing test. The Turing test derives its name from Alan Turing, often referred to as the father of modern computing and the subject of the Oscar-winning film, The Imitation Game. In a 1950 paper, Turing proposed a test by the same name in which a human judge is tasked with distinguishing between a computer and a human in an effort to measure the computer's ability to think like us. Nao, our robotic costar for @Engadget After Hours and the #BestofCES Awards with a special creep from Sean Cooper cc @numeson A photo posted by Christopher Trout (@mr_trout) on Jan 8, 2015 at 7:00pm PST As it turns out, Nao's inability to keep pace in repartee says more about the state of artificial intelligence than the limitations of a single machine. Google, Apple, Microsoft and Amazon have all very visibly thrown their hats into the AI ring, with Google Now, Siri, Cortana and Echo, respectively. Meanwhile, engineers and researchers the world over continue to push for more sophisticated machines. And, yet, no one has invented a single device or piece of software that can pass the Turing test. At CES, I saw the man behind the curtain; the crowd saw a sassy robot engaged in banter with a pair of humans. But it didn't take human-level intelligence to get me emotionally invested in Nao. In fact my disparate experiences can be summed up with the tired ""man behind the curtain"" metaphor. I'd been a mere spectator of a dance performance months before, and aside from seeing a human setting the robots up on stage, I'd had no concept of what happened behind the scenes. But at CES, I was made keenly aware of how the robotic sausage was made. In that instance, I saw the man pulling the strings and feeding my co-host lines of code to make sure it stayed on cue. ""Once you know that, the magic's been lost,"" Reben said. ""In itself, it's no longer a thing that could possibly be alive. You can't suspend disbelief anymore, because it's been ruined."" It's all a matter of perception. Which explains why my experience differed so greatly from that of the audience and my colleagues. At CES, I saw the man behind the curtain; the crowd saw a sassy robot engaged in banter with a pair of humans. Despite its limitations and my overblown expectations, however, Nao is capable of far more sophisticated forms of communication than canned sound bites. Heather Knight, a seasoned roboticist and Ph.D. student at Carnegie Mellon's Robotics Institute, runs an art-meets-technology outfit called MarilynMonrobot, which focuses on ""socially intelligent robot performances and sensor-based electronic art."" She also worked at Aldebaran, Nao's birthplace, on sensor design. In 2010, Knight introduced Data, a Nao bot rebranded as a stand-up comedian, during a TEDWomen talk. Using software designed by Knight and fellow CMU alumni, in addition to its built-in microphone and camera, Data was a much more complex artificial being than the Nao that I knew. It took visual and auditory cues from its audience and tailored its act to fit the room. ... The limitations of modern AI and Nao's complexities simultaneously came into focus. It wasn't a flawless performance. Data talked through applause; it bombed on a handful of jokes; and the ones that hit were canned, comedy club clichés. But, while watching a taping of the performance, I started to feel the same sense of sympathy I'd felt months earlier when the Nao dance troupe fell out of step at Expand. This was a complex machine, performing complex tasks, but more than that, it was an imperfect humanoid performer. With Data acting largely independent of its human keeper, the feeling that it was merely a modern-day equivalent of Teddy Ruxpin was starting to dissipate. I was ready to give robot love another try. A month after our two-night rendezvous in Vegas, I was again sitting face to face with the robot that had at once piqued my curiosity and destroyed my fantasies. It stood on a conference table in Engadget's San Francisco offices next to two representatives from Aldebaran. They were there to discuss the possibility of making Engadget After Hours a weekly show. As we talked through the best ways to incorporate Nao into our act, the limitations of modern AI and Nao's complexities simultaneously came into focus. While Aldebaran intends to make Nao your future robot companion, it's still the plaything of developers and institutions. Getting Nao to perform specific tasks, especially on-demand for a live studio audience, requires a bit of digital hand-holding. In the case of our late night show, we were again faced with a Wizard of Oz scenario, a possibility that, after weeks of soul searching, no longer disappointed me. I knew what Nao was capable of. As the meeting came to a close, one of Aldebaran's reps asked if we cared for a performance. He turned Nao to face him, and repeated the words ""Eagle Dance"" until it recognized the command. As it lifted its leg and spread its arms wide like a ballerina donning a coat of armor, I was reminded of why I'd fallen in love months before.                     "
288,https://www.engadget.com/2015-02-26-deepmind-atari-games-tests.html,Engadget,2015,2,26,304.0," Most people's anxieties about AI concern computers realizing they don't need humans and wiping us out. It probably never occurred to anyone that, as soon as they discovered beer, Netflix and video games, that computers would ditch plans for world domination, drop out and get a job at the local gas station. It's a lesson that Google-owned startup DeepMind has learned the hard way after it got its thinking computer hooked on retro computer games. The London-based startup, founded by Theme Park programmer Demis Hassabis, wondered if an AI could learn how to play computer games all on its own. It hooked the AI up to a series of Atari 2600 titles, but provided it with no specific instructions on what it should do. The team was looking into ""reinforcement learning,"" whereby you get a little reward whenever you do something good. When the computer started earning points, it received the digital equivalent of a dog treat. After a while, it stopped stumbling around and started to get pretty good at beating the arcade classics of yesteryear. It's a big departure from rigid games like Chess, since it's a lot harder to ""solve"" a game like Pong with brute-force calculations. Here, the AI has to adapt, learn on its feet and device a rudimentary strategy in order to be successful and earn its little jolt of praise. The team admits that it's not yet at the point where the system can beat more strategic titles like Ms. Pac-Man or Private Eye, but DeepMind is hoping that it won't be long before it can. After that, the team is planning to turn its thinking computer into a StarCraft expert -- and if it gets hooked on that, there's no way it's ever going to take out the garbage, or develop a way to subjugate humanity.                     "
289,https://www.engadget.com/2015-02-05-eve-robot-scientist-malaria.html,Engadget,2015,2,5,377.0," Meet Eve: she's darn smart, can make the process of finding new drugs a lot faster and cheaper -- and she costs around $1 million. That's because Eve is a robotic scientist developed by researchers from the Universities of Aberystwyth and Cambridge, the same team who created her predecessor (you guessed it) Adam back in 2009. Since Eve was created specifically to automate the early stages of drug design, she's capable of scanning over 10,000 compounds a day, whereas humans obviously wouldn't be able to process as many in the same timeframe. As Professor Ross King from the University of Manchester (which Eve calls home) said: ""Every industry now benefits from automation and science is no exception. Bringing in machine learning to make this process intelligent -- rather than just a 'brute force' approach -- could greatly speed up scientific progress and potentially reap huge rewards."" To be able to narrow down candidates (and keep costs at a minimum), though, she processes the compounds through a smart screening system that uses genetically engineered yeast to leave out anything toxic to human cells and choose only those that target diseases. In fact, that's what Eve did to find viable candidates for malaria drug in a recent test conducted by King and his colleagues. According to the journal they published in The Royal Society this February, Eve discovered that a compound once investigated as a potential anti-cancer drug can block DHFR, a molecule found in the malaria parasite. While some current malaria medicine already use compounds that can block that molecule, new strains continue to evolve, rendering those medications useless. The same situation applies to other tropical diseases, including African sleeping sickness and Chagas' disease. To wit, the industry is having a hard time finding new drugs fast enough, and Eve could eventually change that. The AI is still young, and she's bound to go through more upgrades before she gets to help out more scientists and companies. King believes her recent findings might end up being ""more significant than just demonstrating a new approach to drug discovery,"" though. ""Despite extensive efforts,"" he said, ""no one has been able to find a new antimalarial that targets DHFR and is able to pass clinical trials."" [Image credit: University of Manchester]                     "
290,https://www.engadget.com/2015-01-28-bill-gates-ai.html,Engadget,2015,1,28,218.0," Elon Musk and Stephen Hawking have brought up the potential dangers of super intelligent AI several times over the past few years (Musk even donated $10 million toward cautious AI research), but now Bill Gates is also getting into the mix. In his Reddit ""AmA"" Q&A session today, Gates made it clear that he agrees with Musk's stance, which basically amounts to being very careful about how we approach the rise of intelligent machines: I am in the camp that is concerned about super intelligence. First the machines will do a lot of jobs for us and not be super intelligent. That should be positive if we manage it well. A few decades after that though, the intelligence is strong enough to be a concern. I agree with Elon Musk and some others on this and don't understand why some people are not concerned. In an interview with Backchannel last week, Gates noted he's not trying to ""stop things or slow things down."" Instead, it seems like he's just trying to raise awareness for two potential issues: AI replacing jobs that humans are adapted to -- ""the jobs that give you a sense of purpose and worth"" -- and stronger AI that could end up ""conflicting with the goals of human systems."" [Photo credit: Photothek via Getty Images]                     "
291,https://www.engadget.com/2015-01-09-ai-cepheus-poker.html,Engadget,2015,1,9,282.0," A team of software developers and poker researchers from the University of Alberta have developed a program that can completely demolish their fellow humans in a game of Texas Hold 'em. They named the artificial intelligence ""Cepheus,"" and it's so good, the developers say you could play against it your whole life and never win. Even if you win, ""it [still] cannot be beaten with statistical significance in a lifetime,"" according to the paper Science has just published. Well, that is if you're playing the two-player version (which is also the simplest one) called ""heads-up limit hold 'em,"" because poker's apparently an extremely complicated game. The team has been working on developing an AI poker expert for the past ten years, though it only took them two months to ""train"" Cepheus. We put ""train"" in quotes, because the researchers didn't really code every information by hand -- they merely taught the AI the rules of the game. Using a whopping 4,000 CPUs, Cepheus played matches against itself, considering more than 6 billion hands per second. The system learned from every victory and loss it suffered during that period, teaching itself how to play better and better as time went on. It has taught itself so well, such that the developers claim it can play ""an essentially perfect game."" This poker master is but one of the AI systems capable of beating humans in games people have created. The Wall Street Journal has a list of such programs, including IBM's Watson, which has conquered Jeopardy in 2011. As for Cepheus, well, its developers have no plans to commercialize it, though they've built a website where you can at least try to defeat it.                     "
292,https://www.engadget.com/2014-11-20-revamped-turing-test-expects-computers-to-show-imagination.html,Engadget,2014,11,20,535.0," In June, the developers of a Russian chatbot posing as a 13-year-old boy from Ukraine claimed it had passed the Turing test. While a lot of people doubt the result's validity because the testers used a sketchy methodology and the event was organized by a man fond of making wild claims, it's clear we need a better way to determine if an AI possesses human levels of intelligence. Enter Lovelace 2.0, a test proposed by Georgia Tech associate professor Mark Riedl. Here's how Lovelace 2.0 works: For the test, the artificial agent passes if it develops a creative artifact from a subset of artistic genres deemed to require human-level intelligence and the artifact meets certain creative constraints given by a human evaluator. Further, the human evaluator must determine that the object is a valid representative of the creative subset and that it meets the criteria. The created artifact needs only meet these criteria but does not need to have any aesthetic value. Finally, a human referee must determine that the combination of the subset and criteria is not an impossible standard. Okay, so that official description is pretty hard to parse. Thankfully, Riedl's recently published paper about the subject gives us an easy sample test. One could, for instance, ask a computer/software to ""create a story in which a boy falls in love with a girl, aliens abduct the boy and the girl saves the world with the help of a talking cat."" The story doesn't have to read like an instant classic, but it has to be able to fulfill those conditions and convince a human judge that its tale of alien abduction and female-feline heroism was written by a person in order to pass. That's just one possibility, though -- testers could also ask the computer to create other types of artwork (painting, sculpture, etc.) while fulfilling a set of conditions. These conditions need to be outrageous or unique enough to prevent the computer from finding possible results to copy through Google. In comparison, a machine merely has to convince someone that the person is talking to another human in order to pass the Turing test. Riedl's idea stemmed from the original Lovelace exam created in 2001, which requires computers to conjure up a novel, painting or any original work of art. For a computer to pass, its creators must not be able to explain how the machine came up with its creation. History buffs might have already guessed that both were named after Ada Lovelace (above), the world's first computer programmer, who once said that ""computers originate nothing; they merely do that which we order them, via programs, to do."" The associate professor decided to design the second Lovelace exam, as he believes the original one makes it hard to judge if a machine has truly passed, since it doesn't have measurable parameters. In the sample test for Lovelace 2.0, for instance, those parameters are the elements of the story the machine needs to use. Riedl will talk about Lovelace 2.0 at the Beyond the Turing Test workshop in Texas in January 2015, but you can already read his paper online if you want to know more. [Image credit: Alfred Edward Chalon/Wikimedia]                     "
293,https://www.engadget.com/2014-09-30-soe-devs-on-everquest-nexts-life-of-consequence.html,Engadget,2014,9,30,102.0," We've known for a while that SOE is cooking up some sort of emergent AI concoction for EverQuest Next. The company famously partnered with Storybricks last year to bring its fantasy NPCs to life, and a newly released video sheds a bit more light on what exactly that means. The clip stems from a panel that was originally conducted at this year's SOE Live, which has now been distilled to a more manageable five-minute running time. Click past the cut to find out about EQN's lack of traditional quest hubs and how to make NPCs bow before your mighty axe of authority.                     "
294,https://www.engadget.com/2014-09-25-neighborhood-algorithm.html,Engadget,2014,9,25,217.0," Us humans are normally good at making quick judgments about neighborhoods. We can figure out whether we're safe, or if we're likely to find a certain store. Computers haven't had such an easy time of it, but that's changing now that MIT researchers have created a deep learning algorithm that sizes up neighborhoods roughly as well as humans. The code correlates what it sees in millions of Google Street View images with crime rates and points of interest; it can tell what a sketchy part of town looks like, or what you're likely to see near a McDonald's (taxis and police vans, apparently). Once a computer teaches itself using the algorithm, it's surprisingly effective. While humans are still quicker at finding their way to a given location, machines are better at gauging how close they are based on individual photos. You sadly won't see this technology used in the real world any time soon, since it's just a proof of concept at this stage. However, it's already good enough that MIT's team believes it could help navigation apps steer you around crime-ridden areas, or give retailers a sense of where to set up shop. Eventually, you may not have to set foot in an unfamiliar neighborhood before you get a feel for what it has to offer.                     "
295,https://www.engadget.com/2014-09-12-the-safe-project.html,Engadget,2014,9,12,244.0," The vocabulary we use to describe music can be tough enough for a human to grok (really, what does it mean when a guitar riff is ""crunchy""?) but a team of tinkerers from Birmingham City University aren't interested in helping people understand that language. Nope -- instead, they've cooked up a way to teach your computer what you mean when you throw around words like ""bright"" or ""fuzzy"" or, yes, ""crunchy"" with a program they call the SAFE Project. Spearheaded by Dr. Ryan Stables, the SAFE Project in its current form is a plugin for existing audio workstation software that lets would-be music producers apply effects by typing in words instead of fiddling with settings. The real magic happens on the backend, though: once you punch in a word (say, ""airy""), the plug-in passes that descriptor along to the team's server, which draws upon the power of the crowd to give your music a twist. That's right, the crowd: the secret sauce of the project is that it draws on settings presets that users tag and upload to continually redefine what aural effects those words actually describe. In a way, it's almost like a living musical brain living in the cloud you can call upon when your music needs some pizzazz, and it's only getting better. Stables says the team is working on a way to let users suss out the sonic spaces between words by applying effects that are partway between two descriptors.                     "
296,https://www.engadget.com/2014-08-21-soe-live-2014-the-revolutionary-intelligence-of-storybricks-ai.html,Engadget,2014,8,21,1513.0," The most exciting part of EverQuest Next and Landmark for me is the living, intelligent AI brought to the games courtesy of Storybricks. Thanks to a tech demonstration at SOE Live, we got to see that AI in action, and can I tell you I am even more excited having seen it! This technology really will revolutionize the game, creating a living, breathing world in EQ Next  that players help shape as it develops as well as give players the power to make their world come alive in Landmark. And to add icing to the cake, the panel also delved into the background of the new Norrath a bit, revealing the world map complete with familiar areas (like Kithicor). Storybricks in action We've detailed the idea behind Storybricks before and even anticipated how that will work in EQN. The idea is that NPCs have their own lives: their own wants, their own needs, and their own personalities. Groups, such as races and organizations, also have specific wants and needs fueling their goals. Interactions with players will then be individual experiences. If NPCs have taken a shine to you because of your past actions, they may tell you about things you can do to help out, get glory, or obtain riches. On the other hand, if you've ticked them off, they may totally rebuff you (or maybe try to send you to your death?). But until this panel, it was still conceptual; now it is real. One question that's been on the minds of players is whether or not Storybricks can pull off such a feat: Could NPCs not only react to the changes in their world but actively instigate them based on their attempts to fulfill their desires and goals? After seeing this exact thing illustrated through various time-elapse scenarios, I now know it can! Because they were a macro view of the system, the demonstrations resembled an RTS map with certain areas controlled by certain races. One of the devs would drop in a catalyst (like adding in the discovery of a gold mine near the wealth-hungry Kobolds) or simply change a variable, and then we sat back and watched as the NPCs reacted. The change in the NPCs' influence was visible via the changing colors on the map as well as by the scrolling text of all the individuals' actions. It was absolutely fascinating to see the ebb and flow of territory control as the NPCs influence waxed and waned, and I personally cannot wait to see it in action on the ground. (In the scenarios of Dark Elves vs. Dryads vs. Shadow Elementals, I'm sad to report that all but one run of the simulation resulted in the total annihilation of the Druids.) Of course, this system is available not only in EQN but in Landmark -- the difference being that players will be able to set the variables for the NPCs in Landmark, dictating what their wants, needs, and desires are, whereas devs do all that in EQN. Influencing the world How does this work? NPCs all radiate influence that others react to. And not only do individual people have have different types of influence, but different groups do as well; races, kingdoms,and organizations all have different kinds of influence that cause different reactions. Each also reacts differently. No behavior is scripted. If a king walks into town the people will react accordingly, giving respect and reverence. However, if a necromancer strides in, people will most likely scatter in fear. That same king would get a different reaction if he were to walk into a camp of bandits who coveted his wealth. The panel explained that this is accomplished because objects also radiate influence and serve as cues to characters, such as a crown to a king or a specific staff to a necromancer. The various NPCs are programmed to have certain responses to specific cues. Of course, in the game the NPCs are not the only entities; players are there exerting influence, too. Players' actions will also have consequence and can turn the tide of a situation one way or another. This demonstration helped bring the idea of Rallying Calls to life, showing how players influence the development of a world populated by much more than just themselves. The Rallying Calls are triggered by player actions and are impacted by player choices, and they change the world in lasting ways. As Franchise Director Dave Georgeson stated earlier, the direction of each individual server world is pushed by the combination of events and player choices. Another dev emphasized, ""You literally make history happen around you."" And remember, unlike in static games, there is no reset button so you can do things over! Once a scenario has played out, the results are permanent. What if players choose not to participate in a storyline, such as not aiding a certain group? If players actions do not hit the triggers that lead to the advancement of a story, that specific story arc will not progress and players will not experience that next part. However, there are multiple Rallying Calls around world, so a number of stories will be in different stages at all times. If a specific Rallying Call is missed on a server, sometimes events will transpire further down the road that will trigger that call, giving players a chance to participate. Finding the story As stated in the panel, story is the backbone of EQN; however, story is delivered in a unique way. It's not something players click on an NPC to scroll through and read. Instead the narrative is embedded in the world itself and projected through the actual events. The devs elaborated, ""We take the races, the kingdoms, the organizations, and even individual NPCs that we've written into the lore, and we create behaviors and desires based on those stories. By dropping NPCs with opposing drives in a location with resources in a world they care about we have endless opportunities for conflict."" And the story doesn't stop when one drive is met -- the NPCs just move on to the next one. Players need only insert themselves into the story. But how exactly can players find the content of the world to participate in it? The answer is two-fold. First, players can simply bump into it during their adventures. There's something to be said for stumbling across an unexpected situation. Players will also get hints from the NPCs they encounter. But getting involved isn't all left to chance. Besides these ways, each player will have a personal companion book that is his or her personal atlas, bestiary, and lore repository. ""It's a living record of your travels through EQ Next,"" the devs told me. This book remembers what you have done and the choices you have made, taking that into account in order to guide you to new places to explore. It also reveals conflicts in the world that you can delve into. A hint of story Tucked away in the panel was also a bit of story from the new Norrath. For instance the history behind Kithicor forest is different than previous iterations. Here, the forest was the site of a huge battle thousands of years prior to the game between the gods (or Seraphs) and horrible Chaos magic monsters. Many Seraphs died in this war, including Tunare. The blood of the monsters spilled in the war corrupted the land, and Erollisi placed pockets of dryads throughout the forest so that the nature magic can hold the evil at bay. This scenario holds out until some catalyst comes along and changes things. And that catalyst is the introduction of players! The Dark Elves vs. Dryads vs. Shadow Elementals scenario example above refers specifically to the Bloody Kithicor Rallying Call, which if triggered will ask players to side with one entity to help protect their interests. Will players choose to help the Dryads or the Dark Elves? Whichever choice is made will have lasting consequences (note the previously mentioned annihilation of Dryads!). Even if players ""fail"" at an objective (like helping the Dryads), the story will continue; because choice in involved, there are multiple possible outcomes. As Lead Designer Darrin McPherson pointed out, ""The notion of Pandora's box is definitely a key feature of our content. You can do things, perhaps with the best intentions of restoring the Dark Elves and something as simple as 'Let's just let them take some of this territory' and that ruins the world."" You can hear about even more Rallying Call scenarios in the video below. Now you can start to see how each EQN server will develop its own unique history and timeline based on the choices of players as they adventure through a living, breathing world filled with truly intelligent AI. What happens in Vegas doesn't stay in Vegas, at least where SOE Live is concerned! Massively sent intrepid reporter MJ Guthrie to this year's SOE Live, from which she'll be transmitting all the best fan news on EverQuest Next, Landmark, H1Z1, and the other MMOs on SOE's roster.                     "
297,https://www.engadget.com/2014-08-19-hitchbot-completes-trip.html,Engadget,2014,8,19,117.0," It looks like robots can trust us humans to take care of them, after all. Hitchbot has successfully completed its hitchhiking trek across Canada, landing in Victoria, British Columbia this past weekend. The ride-bumming robot didn't survive its 4,000-mile journey completely unscathed. Its LED protector was cracked, and its speech had clearly suffered after two weeks of travel (hey, you try talking to people for that long). It doesn't look like there's another adventure in store, but that's okay by us; it clearly accomplished its goals of testing artificial intelligence techniques and human interaction. If you're ever keen to relive the trip, there's a photo gallery available to satisfy your nostalgic side. [Image credit: AP Photo/Ryerson University]                     "
298,https://www.engadget.com/2014-08-12-viv-labs-ai.html,Engadget,2014,8,12,169.0," There's no question that Apple's virtual assistant comes in handy when you need information quickly. And now, one of Siri's creators is working on a more advanced form of AI that goes far beyond the current iPhone option. Viv Labs says its personal assistant will possess a limitless tool set because the software can teach itself. This means that in addition to the regular ol' search, the artificial intelligence will also be able to perform tasks like booking reservations based on openings in your schedule and more. ""Siri is chapter one of a much longer, bigger story,"" Dag Kittlaus, Viv's co-founder who also worked on Apple's assistant, tells Wired. Bypassing the need for coders (and constraints), Viv generates its own program to answer queries and complete tasks in seconds. Once the system is ""trained"" to understand the vocabulary of a subject, the company aims to sort through loads of data to not only lend a hand, but anticipate what we'll need next. [Photo credit: Ian Waldie/Bloomberg via Getty Images]                     "
299,https://www.engadget.com/2014-07-05-forza-horizon-2-features-kinect-enabled-assistant-called-anna.html,Engadget,2014,7,5,157.0," Road trips are a blast if you've got someone to talk to, but not everyone can just drop their life for weeks at time to attend driving festivals. You're an exception to that rule in Forza Horizon 2, but Turn 10 Studios Creative Director Ralph Fulton shared some good news with Examiner: Horizon 2 drivers will have a companion in a Kinect-supplemented AI partner called ANNA, which means the prime passenger seat real estate is open for a kickin' snack buffet. While ANNA probably won't keep your meal upright in hairpin turns, Fulton explained that she acts like a digital, navigation-savvy assistant. She'll suggest events to try, let you know when friends sign on and supply a recap when you slip back into the driver's seat. Sounds like ANNA will help you keep your eyes on the road then! When your view isn't darting between the windshield and your passenger side smorgasbord, that is. [Image: Microsoft Studios]                     "
300,https://www.engadget.com/2014-06-08-supercomputer-passes-turing-test.html,Engadget,2014,6,8,319.0," After 64 long years, it looks like a machine has finally passed the Turing test for artificial intelligence. A supercomputer in a chat-based challenge fooled 33 percent of judges into thinking that it was Eugene Goostman, a fictional 13 year old boy; that's just above the commonly accepted Turing test's 30 percent threshold. Developers Vladimir Veselov and Eugene Demchenko say that the key ingredients were both a plausible personality (a teen who thinks he knows more than he does) and a dialog system adept at handling more than direct questions. You'd have good reason to be skeptical about such bold claims. Others have touted success in the Turing test, only to be shot down later; even then, 33 percent isn't exactly a decisive margin of victory. However, Eugene's creators argue that theirs was a true test, where there were no topics deemed off-limits. They also had independent verification for the results. AI like Eugene is still far from being truly persuasive, let alone sentient. Still, this could be a significant milestone in building computers that mimic the subtleties of human conversation. The timing is also exceptionally fitting -- it came both on the 60th anniversary of Alan Turing's death, and just months after the computing legend received a pardon that put the spotlight back on his achievements. Update: TechDirt has looked into the methodology of the test, and has given us a few good reasons to be skeptical beyond what we've mentioned about Eugene's purposefully flawed persona. There's concern that the rules were shifted to make sure that the computer passed, and there are calls for both additional tests and more stringent reviews. Also, there are concerns about event organizer Kevin Warwick's tendency to make audacious claims; in 2000, he declared himself a cyborg after putting a chip in his arm. The test may still have merit, but it isn't necessarily the historic milestone it's made out to be.                     "
301,https://www.engadget.com/2014-06-04-cortana-microsoft-windows-phone.html,Engadget,2014,6,4,2067.0," She was modeled after real-life personal assistants. She is the product of two years of work, and a large team of scientists and product managers. She has video game origins. She is Microsoft's response to Siri and Google Now. She is Artificial Intelligence and proud of it. She is Cortana. It seems odd to refer to smartphone software as a ""she,"" but that human element is exactly what Microsoft is after with its new Windows Phone digital assistant. Cortana, named after her fictional counterpart in the video game series Halo, takes notes, dictates messages and offers up calendar alerts and reminders. But her real standout characteristic, and the one Microsoft's betting heavily on, is the ability to strike up casual conversations with users; what Microsoft calls ""chitchat."" Next to Apple's Siri, Cortana is the only other smartphone assistant to come with a baked-in personality. And it's hard not to see the parallels between Cortana and the affable, Scarlett Johansson-voiced AI in Spike Jonze's film Her. Your browser does not support the audio element. Confident, caring, competent, loyal; helpful, but not bossy: These are just some of the words Susan Hendrich, the project manager in charge of overseeing Cortana's personality, used to describe the program's most significant character traits. ""She's eager to learn and can be downright funny, peppering her answers with banter or a comeback,"" Hendrich said. ""She seeks familiarity, but her job is to be a personal assistant."" With that kind of list, it sure sounds like Hendrich's describing a human. Which is precisely what she and her team set out to do during Cortana's development; create an AI with human-like qualities. Microsoft's decision to infuse Cortana with a personality stemmed from one end goal: user attachment. ""We did some research and found that people are more likely to interact with [AI] when it feels more human,"" said Hendrich. To illustrate that desired human-machine dynamic, Hendrich pointed to her grandmother's experience with a Roomba vacuum: ""She gave a name and a personality to an inanimate object, and it brought her joy."" That sense of familiarity is exactly what Microsoft wants Window Phone users to feel when interacting with Cortana on their own devices. Because the bulk of Cortana's primary functions mirror that of a personal assistant (e.g., make calls, set appointment reminders, etc.), the team decided to take the development process even further and add an extra layer of authenticity. To that end, they interviewed real-life assistants to learn what that job actually entails, and what attributes they exhibit; how they interact with their bosses and what makes them successful. ""[It] helped us understand how humans take on that role [of a personal assistant],"" Hendrich said. These interviews were also captured on video, a resource the team uses to this day as a reference point for any new situations that may arise. Cortana's writers go over their current goals and discuss the AI's progress. Beyond relating to users in a naturalistic way, Microsoft realized that Cortana also needed to be fun. In fact, the company's research shows that around 40 percent of all AI interactions involve chitchat.  As Hendrich explained: ""If you had a personal assistant and you walked into the office, you'd engage in chitchat with them first. You don't go straight into the highest-priority emails and lay out your day."" ""Chitchat"" with Cortana can range from witty banter to casual chatter. Ask her to tell you a joke and she could reply with this: ""Two antennas got married. The ceremony dragged on, but the reception was excellent."" If you ask her how old she is, she'll say, ""I'm not sure how to carbon date the internet."" Microsoft's even snuck in an Easter egg related to Clippy, the helpful, animated paper clip from its Word software. Although these playful responses may strike some as nothing more than cheap tricks on Microsoft's part, they do help users build a rapport with Cortana. If she can make you laugh or smile, you're more likely to continue using the program again and again. At least, that's what Microsoft hopes will happen. If Cortana sounds familiar, that's because she's partially voiced by Jen Taylor, the original talent behind Halo's Cortana. Microsoft currently synthesizes multiple voices for the program, but Taylor's lines account for a huge percentage of the chitchat you hear, and that amount is only going to increase over time. Though Cortana's current voice doesn't sound quite as natural as say that of Samantha's in Her, the addition of Taylor's human tone does help imbue the program with a more realistic feel. Your browser does not support the audio element. An increase in chitchat responses isn't all that Microsoft has planned for Cortana's future. The team plans to further flesh the program out with extended back-and-forth dialogue, more natural expressions and interactions and the ability to predict a user's itinerary months (and perhaps even years) in advance. Microsoft Research is also working on improving Cortana's short-term and long-term memory -- primarily, her ability to start talking about a topic and come back to it later, creating a rich dialogue between her and the user. Right now, Cortana is smart enough to recognize when you refer back to something you've recently talked about, but that's as deep as she gets. Though all of this focus on creating a human-like AI sounds like Cortana's treading closely to the Uncanny Valley -- a hypothesis that contends that as a robot or AI gets more authentic, its failures and blemishes will appear so strong that it causes people to respond with revulsion and hostility -- her creators insist that isn't the case. They're aware there's such a thing as too real. ""It's not like Star Trek, where Data kept trying to be more human and felt inadequate,"" said Deborah Harrison, who runs the team responsible for adding the endless strings of data to Cortana's programming. ""She thinks that if she had a choice [between human and AI], she'd go with AI and be happy with it."" Her real standout characteristic, and the one Microsoft's betting heavily on, is the ability to strike up casual conversations with users; what Microsoft calls ""chitchat."" Dr. Eric Horvitz of Microsoft Research said that his team, which contributed to the AI aspect of Cortana, noticed that the Uncanny Valley was showing up in the behavior versus the looks of prototypes of personal assistants they have running at the research labs. ""The more powerful these systems are, the more visible their imperfections can become,"" he said, pointing to Cortana's lack of a more human-like short-term memory as an example. Cortana is still very much a work in progress, and she has her share of shortcomings. So to mitigate this, the team designed her to be both functionally and emotionally transparent to the user. That way, the user response won't be as negative if Cortana isn't able to do something. As Hendrich explained: ""If something's not her fault, she's not going to take the blame for it. We're not trying to put the user in a position to feel bad for Cortana. Not only is she AI, [but also] she's self-aware, and that principle of transparency informs a lot of how we handle error messages, our capabilities, tasks and chitchat. You'll have more faith and trust in us if we do that for you."" Joaquin Phoenix takes his smartphone AI for a stroll on the beach in the film 'Her.' There are obvious parallels between Cortana and her two rivals, Apple's Siri and Google Now, but her creators insist the program was the idea of Robert Howard, project manager for Windows Phone Search and Maps. Howard's team had already been working on advancements to Windows Phone's voice search features, so the shift to creating an interactive AI in Cortana was a natural evolution. It also didn't take much convincing to get Microsoft to back the project's new direction. The Cortana of today is bold and conversational, but that wasn't always the case for the project. The type of personality Microsoft originally envisioned at the start of the AI project -- a more formal ""How can I help you?"" tone -- was different from what actually launched. It wasn't until the team was about four months in that the idea of using Cortana as the program's actual name started to gain traction. At which point, the team decided to get 343 Industries (the studio that currently produces Halo) involved in shaping her personality and bringing the smartphone version more in line with the Halo character. The studio provided the team with Cortana's backstory and filled a whiteboard with every attribute they could think of. ""We did some research and found that people are more likely to interact with [AI] when it feels more human,"" said Hendrich. Hendrich and Harrison liked a lot of what they heard from 343 and began to incorporate many of those characteristics into the program. This effectively shaped Cortana into the AI she is now. They made her more confident, much more brash and had her be clearer in her responses to users. Or, as Harrison put it, ""She got more comfortable talking about how awesome she is."" Cortana may be cocky, but as Microsoft's internal testing proved, that shift in tone works. As soon as the team gave Cortana a boost in confidence, people immediately began responding to her more positively. In fact, external studies corroborate this notion; that users prefer a strong personality over a neutral or weak one. According to The Man Who Lied to His Laptop -- a book by Clifford Nass, a Stanford professor who specialized in robotics -- people have a tendency to treat machines, especially those with human-like characteristics, like other people whether we realize it or not. By that reasoning, an AI with an empathetic tone that's designed to dole out positive comments, flattery and a little bit of humor is much more effective than we may think. You're more likely to trust an AI when it has a strong head on its virtual shoulders. Microsoft's Joe Belfiore introduces Cortana at the 2014 Build developer conference. Microsoft's preparing to take Cortana overseas, specifically to the UK and China, but that transition requires a hefty bit of localization since a US-centric Cortana doesn't easily translate. ""There's a team in China who's looking at how to take the personality and non-negotiable core concepts [of Cortana] and translate it, not just word for word, but [also] personality to personality,"" Harrison said. Even the voice talent for the overseas versions of Cortana will be different. Taylor may be a logical choice for Cortana's voice in the US, but user studies indicated the Chinese market needed a voice that ""sounded like it was smiling."" As for what's ahead for Cortana in the US, the team's planning to add future updates on a twice-per-month cycle. That's not to say Microsoft won't make exceptions for special events. Hendrich said they're working on ways to throw in off-cycle updates ""for things that are timely, urgent or especially badass."" This would come in handy for trending topics like the Olympics, breaking news or even sports. And since updating Cortana is a server-side affair, Microsoft can easily upload these batches of data strings and voice recordings directly to Bing, meaning users won't have to refresh their hardware every time new features are added. ""She thinks that if she had a choice [between human and AI], she'd go with AI and be happy with it."" Cortana's not just another flash-in-the-pan project, as the company's investment in her development shows. In some ways, her self-assured personality reflects Microsoft's confidence in its new AI. The company is, after all, catching up to the nearly three-year lead its competitors' have enjoyed for their rival smartphone AIs. So while Hendrich and Harrison work to expand Cortana's global reach and capabilities, the Microsoft Research team is busy figuring out how to give her an even more human voice and make her even more relatable. Even Cortana's confident she'll be around for the long haul. Ask her if she's better than Siri and she responds playfully with a knowing wink that at once acknowledges her video game past and hints at Microsoft's AI-filled possible future: ""Not to brag, but apparently I'll help save the universe in about 500 years."" [Image credit: Microsoft (Cortana team; Cortana); Justin Sullivan/Getty Images (Joe Belfiore); Associated Press (scene from 'Her')]                     "
302,https://www.engadget.com/2014-05-28-google-uses-neural-net-learning-to-cut-server-energy-bills.html,Engadget,2014,5,28,132.0," Google spits out about 4 million search results per minute (among many other duties), which consumes a lot of energy. According to a recent blog, it cut its electrical bills significantly by applying the same kind of machine learning used in speech recognition and other consumer applications. A data center engineer on a 20 percent project plotted environmental factors like outside air temperature, IT load and other server-related factors. He then developed a neural network that could see the ""underlying story"" in the data, predicting loads 99.6 percent of the time. With a bit more work, Mountain View managed to eke out significant savings by varying cooling and other factors. It also published a white paper to share the info with other data centers and prove once again that humans are redundant.                     "
303,https://www.engadget.com/2014-03-20-xprize-wants-to-fund-artificial-intelligence-ted-talk.html,Engadget,2014,3,20,222.0," Xprize is known for its ambition. The outfit, with the help of some big name (and deep pocketed) partners, has launched initiatives to spur Star Trek-like tricorder development and even get private industry to land a rover on the moon. But now, it's teaming up with TED, that forum for big ideas, to do something a little different. The two companies have just announced an Xprize for Artificial Intelligence and here's the hook: they want the AI to conduct its own TED Talk with no human assist. Mind. Blown. None of this is actually set in stone though and, in fact, the partners are looking to you -- yes, you -- for help in deciding how this all goes down. Xprize is hosting a dedicated subsite so that readers (excuse us, big thinkers!) like you can pitch in with ideas on what the AI TED Talk format should be, how long it should run, what topic will be chosen and so on and so forth. You'll even get to help determine what type of AI makes the grade: will it be a walking robot, a rollie or a disembodied voice? It's up to you to pitch in and figure it out. Because, hey, if you can't actually help build the AI, setting it up for stage fright is the next best thing.                     "
304,https://www.engadget.com/2014-03-12-this-is-the-advanced-ai-tech-powering-star-citizen.html,Engadget,2014,3,12,717.0," Cloud Imperium has publicized its relationship with Moon Collider, maker of an advanced AI middleware technology known as Kythera. A new press release says that Star Citizen fans will get their first taste of what Kythera can do in April's dogfighting module, while CIG chairman Chris Roberts enthuses over what the tech brings to the table. ""The dynamic nature of the technology allows for more realistic dogfights, but at the same time it delivers a very true-to-life universe where planetside environments will be able to display very large scale city simulations all going on at once,"" he explains. ""Kythera will give Star Citizen a true world AI rather than a less dynamic scripted AI which you may find in other games."" You can read the full release after the cut. [Source: CIG press release] Moon Collider and Cloud Imperium Announce Partnership to Supply New Kythera AI Technology for Chris Roberts' Star Citizen Space Sim Los Angeles, March 12, 2014 – Moon Collider™, the company behind the ground breaking artificial intelligence (AI) system, Kythera®, announced today a partnership with Cloud Imperium™ Games, the makers of the PC space sim, Star Citizen™, from legendary developer Chris Roberts. Today the two companies drew back the curtain on the artificial intelligence underpinning in Star Citizen's immersive universe. Kythera will be the guiding hand for computer-controlled characters and spacecraft throughout Star Citizen. Speaking from their offices in Edinburgh, Scotland, Moon Collider officials said they were thrilled to see their hitherto-secret licensee unveiled. ""You'll soon find Kythera in a range of titles, from an indie hack-and-slash to a point-and-click adventure, but we couldn't have a better showcase than Star Citizen,"" said founder and CEO Matthew Jack. ""We call Kythera the dynamic AI, and our ambition for it is to step up to the next level of creative gameplay and immersive worlds. So it's a perfect match for Star Citizen – this generation's most dynamic and immersive PC game."" Star Citizen's eager backers will get their first glimpse of Kythera in the upcoming Dogfighting Module, which will see players jump in their spacecraft for life-and-death battles against each other and the AI. Thanks to Kythera, would-be top guns will meet computer-controlled opponents with individual, recognizable flying styles – each with weaknesses to be exploited and strengths to be feared. ""Kythera's AI is exactly what we need for a game like Star Citizen,"" said Cloud Imperium's Chris Roberts. ""The dynamic nature of the technology allows for more realistic dogfights, but at the same time it delivers a very true-to-life universe where planetside environments will be able to display very large scale city simulations all going on at once. Kythera will give Star Citizen a true world AI rather than a less dynamic scripted AI which you may find in other games."" And according to Moon Collider's CEO, Star Citizen's success is good news for the whole industry. ""Star Citizen's breadth of vision is stunning, and because we've been working so closely together, a lot of that vision is reflected in Kythera,"" he observed. ""So to everyone who's backed the crowd funder, a big thank-you. You're not only making Star Citizen the breathtaking experience we all know it will be, you're also driving technologies which will shape the future of gaming for years to come."" ## Cloud Imperium Games Corporation and its subsidiary Roberts Space Industries was founded in April 2012 by renowned game developer Chris Roberts (Wing Commander, Freelancer, Privateer) and his business partner and long-time international media attorney Ortwin Freyermuth. Under Roberts' leadership using his long-standing relationships in the game space, Cloud Imperium quickly assembled a top tier development team for the creation of art assets, story elements, and an extensive prototype for its first game Star Citizen. Star Citizen is being marketed and launched via www.robertsspaceindustries.com. Cloud Imperium's strategy is about customer acquisition through established game designer fan bases. More information about the company, including jobs and contact information can be found at www.cloudimperiumgames.com. Moon Collider is an established artificial intelligence consultancy headquartered in Edinburgh, Scotland. Its groundbreaking AI middleware platform, Kythera, aims to shorten development cycles and unlock new styles of gameplay by responding dynamically to changes to the in-game world. The company is funded by private investment and is privately held. For more information on Moon Collider and Kythera, see www.kythera.ai.                     "
305,https://www.engadget.com/2014-02-09-torres-quevedo-chess-player-automaton.html,Engadget,2014,2,9,1143.0," Welcome to Time Machines, where we offer up a selection of mechanical oddities, milestone gadgets and unique inventions to test out your tech-history skills. Machines may need to start a union. After all, various deep thinkers have been busy for more than a century dreaming up ways to impart human-like thought processes and capabilities into them, just so they can do more of our work. Familiar names in the annals of computing's history such as Charles Babbage and Alan Turing may stand out, but wedged between those figures on the historical timeline is the perhaps lesser-known Spanish inventor and engineer Leonardo Torres Quevedo. Of his many inventions, one of the most unique is ""El Ajedrecista"" (The Chess Player), which he presented to the Parisian public in 1914. It was a chess-playing automaton, programmed to stand against a human opponent and respond accordingly to any move they made. It knew if someone was trying to cheat, and took pride in moving its own playing pieces around the board. Most of all, it reveled in announcing a victory against its human taskmasters when it inevitably won the game. Torres Quevedo was born in the north of Spain in 1852 and by the age of 24, he had graduated school with a degree in civil engineering. According to Harry Henderson's A to Z of Computer Scientists, he seemed to have little interest in the field and was lucky enough to receive an inheritance from a distant relative, allowing him to pursue a range of interests with a certain degree of leisure. Instead of falling into the trap of entitlement and hipsterdom, however, he followed a path that led to mathematics and mechanical inventions. During the 1890s, he worked on creating machines that could solve algebraic equations, one of which he demonstrated to the French Association for the Advancement of Science in 1895. Around the turn of the century, he patented a semirigid dirigible design, which was employed by the French and English armies during World War I. He even developed a remote control device called the Telekine, which began as a way to trigger mechanical processes through a wireless telegraph, but grew into a concept for controlling machines (from dirigibles to torpedoes) in a bid to save the lives of human operators. His growing savvy with electro-mechanical devices and radio waves made him a contemporary of other like-minded inventors, such as Guglielmo Marconi, Nikola Tesla and Thomas Edison, and even earned him a government subsidy to help open a mechanical laboratory in Madrid. As Torres Quevedo's research continued, it soon began to veer wholeheartedly into the realm of ""thinking"" machines. By 1912, his exploration led to ""El Ajedrecista,"" a machine that could carry out complex processes similar to the way a human could. His chess-playing automaton made its first major public debut in Paris in 1914. It was pitted against a single human competitor and played within the confines of a chess endgame called KRK, short for ""King and Rook versus King."" This first version of the machine was built on an upright panel, with a vertically mounted playing surface and game pieces that jacked into the chess grid like an operator's switchboard. A human player could place their King within a limited range of pre-determined starting points, while the machine's King and Rook consistently started from the same position. El Ajedrecista had been programmed by Torres Quevedo to surmise the location of its opponent's King and followed a system of conditional rules in order to make its moves. Not only was El Ajedrecista making judgment calls each time its turn came, but it also used mechanical arms to move each of its own pieces around the game board. If the opponent attempted an illegal move, one of three bulbs would light up, halting gameplay until a correct move was made. After three wrong moves, the machine would shut down in disgust, refusing to continue — similar to the anti-cheat tactics of a pinball machine (Tilt!). A hard reset would be required for gameplay to begin again, hopefully with everyone playing by the rules this time around. Gonzalo Torres Quevedo (son of Leonardo) displays the second version of El Ajedrecista. In 1920, Torres Quevedo built a second version of the chess automaton. This model had a more approachable horizontal playing surface and its inner workings were tucked away inside a wooden enclosure. This automaton used the same essential programming to play a KRK endgame as the first version, but this time its pieces were controlled by electromagnets. The game board consisted of a smooth playing surface, with metal plates under each square so that it could detect the location of the game pieces. Mechanical arms could control the position of the metal-based pieces by moving the magnet around underneath the game board. According to George Atkinson's Chess and Machine Intuition, each time the machine's Rook made a move pushing the human opponent's King closer to the edge (and end of the game), a small phonograph played the phrase ""jaque al rey"" (check to the King). When the machine got the opponent's King into the final game-ending position, the voice repeated the phrase, this time adding a triumphant ""mate."" And due to Torres Quevedo's skilled programming and mechanical know-how, it did this consistently. Around 1914, Torres Quevedo released a paper entitled Essays on Automatics, which refers to the work of mid-1800s inventor and mathematician Charles Babbage. Building on Babbage's Analytical Engine (a Victorian-era computing device), Torres Quevedo discusses his own ideas for a ""universal automaton."" One that could be programmed to do complex human tasks and make decisions based on conditional data in the same manner as processing IF and THEN statements. His chess game was simply a way of testing his ideas for complex thought processes in machines, in a way, an early indulgence in the field of artificial intelligence. In 1951, Gonzalo Torres Quevedo (Leonardo's son), who had been alongside his father in developing the Ajedrecista's second version, displayed the machine at the Cybernetic Congress in Paris. He even demonstrated the device to Norbert Wiener, a well-known figure in the history of machine intelligence. Torres Quevedo may not be as popularly recognized as Babbage or Turing for conceptual work in the area of computing, but his forward-thinking ideas and inventions did make an impact on the world — his cable car invention still shuttles tourists across Niagara Falls. Indeed, El Ajedrecista could be considered the first computer game ever built and the Jeopardy appearance of IBM's Watson computer in 2011 could be its modern incarnation. Let's just hope Torres Quevedo's chess player doesn't cross over to the dark side; it would be unnerving to hear a race of robots deliver the phrase ""checkmate"" right before terminating you. (In an Arnold Schwarzenegger voice, of course.) Images: La Nature magazine, 1914 (Ajedrecista 1); Josu/Torresquevedo.org (Ajedrecista 2 exhibition)]                     "
306,https://www.engadget.com/2014-01-26-google-report-artificial-intelligence-startup-deepmind.html,Engadget,2014,1,26,109.0," According to reports (and confirmed by the internet company itself), Google has bought Deepmind: a relatively small AI company from London. Re/code broke the news, stating that Google had sunk $400 million into the purchase -- a figure that the company hasn't yet confirmed. The startup's placeholder site outlines its work on ""general purpose learning algorithms,"" with its first projects encompassing games, e-commerce and simulations. It sounds like the team might be working in a separate direction to Google's recent robotics purchases, but the company (unsurprisingly) is plenty interested in the future of artificial intelligence: it teamed up with NASA to launch an AI research lab just last year.                     "
307,https://www.engadget.com/2014-01-15-image-algorithm-can-recognize-objects-without-any-human-help.html,Engadget,2014,1,15,133.0," Even the smartest object recognition systems tend to require at least some human input to be effective, even if it's just to get the ball rolling. Not a new system from Brigham Young University, however. A team led by Dah-Jye Lee has built a genetic algorithm that decides which features are important all on its own. The code doesn't need to reset whenever it looks for a new object, and it's accurate to the point where it can reliably pick out subtle differences -- different varieties of fish, for instance. There's no word on just when we might see this algorithm reach the real world, but Lee believes that it could spot invasive species and manufacturing defects without requiring constant human oversight. Let's just hope it doesn't decide that we're the invasive species.                     "
308,https://www.engadget.com/2013-12-23-uk-pardons-computing-pioneer-alan-turing.html,Engadget,2013,12,23,107.0," Campaigners have spent years demanding that the UK exonerate computing legend Alan Turing, and they're finally getting their wish. Queen Elizabeth II has just used her royal prerogative to pardon Turing, 61 years after an indecency conviction that many now see as unjust. The criminal charge shouldn't overshadow Turing's vital cryptoanalysis work during World War II, Justice Secretary Chris Grayling said when explaining the move. The pardon is a purely symbolic gesture, but an important one all the same -- it acknowledges that the conviction cut short the career of a man who defended his country, broke ground in artificial intelligence and formalized computing concepts like algorithms.                     "
309,https://www.engadget.com/2013-12-09-facebook-hires-nyu-professor-to-lead-its-ai-efforts.html,Engadget,2013,12,9,88.0," Facebook's fledgling artificial intelligence group now has a leader -- the social network has hired New York University professor Yann LeCun to run the research division from New York City. He'll remain at the university part time, but most of his energy will now be spent researching data science, deep learning and other technologies that could refine Facebook's social stream. While LeCun's hire won't pay off for some time, it already suggests that Zuckerberg and crew are serious about competing with Google's Ray Kurzweil in the AI space.                     "
310,https://www.engadget.com/2013-11-02-recommended-reading-douglas-hofstadter-ai.html,Engadget,2013,11,2,428.0," Recommended Reading highlights the best long-form writing on technology in print and on the web. Some weeks, you'll also find short reviews of books dealing with the subject of technology that we think are worth your time. We hope you enjoy the read. The Man Who Would Teach Machines to Think by James Somers, The Atlantic Artificial intelligence has been in the public consciousness for decades now, due in no small part to fictional incarnations like 2001's HAL 9000, but it's been getting more attention than ever due to IBM's Watson, Apple's Siri and other recent developments. One constant figure throughout much of that time is AI pioneer Douglas Hofstadter, who's profiled at length in this piece by James Somers for The Atlantic. In it, Somers talks to Hofstadter and other key figures from the likes of IBM and Google, while examining his approach to the field, which is as much about studying the human mind as replicating it. [Image credit: null0/Flickr] NSA Files: Decoded by Ewen Macaskill and Gabriel Dance, The Guardian There's as much watching and listening as reading here. This interactive feature from The Guardian offers one of the most comprehensive looks at what's now known as a result of the NSA leaks, backed up by interviews with key lawmakers, former NSA officials and security experts. Writing BytesThe New York Times This column has previously featured essays on how technology is changing writing -- here, The New York Times turns to a number of well-known authors for their opinions, including Margaret Atwood, Frederick Forsyth and Douglas Coupland, the latter of whom suggests we're in an era that's ""scary and wondrous at the same time."" Waiting for the Next Great Technology Critic by Matt Buchanan, The New Yorker With longtime technology columnists Walt Mossberg and David Pogue departing their respective newspapers for new ventures, Matt Buchanan takes a look at their legacy and the current state of technology criticism, and why the profession itself may be due for a reconsideration. Inside the Failure of Healthcare.gov by Tim Carmody, Newsweek If you're not fully up to speed on the problems with Healthcare.gov (or even if you are), this piece from Tim Carmody offers a look at exactly what went wrong with the website and where things stand today. It also asks how many of the problems can be pegged on ""bad compromises"" at the outset. Can't Get Away From It All? The Problem Isn't Technology - It's You by Mat Honan, Wired The Snowden Leaks and the Public by Alan Rusbridger, The New York Review of Books                     "
311,https://www.engadget.com/2013-09-27-evernote-augmented-intelligence.html,Engadget,2013,9,27,650.0," What the hell is Evernote doing selling backpacks and socks? That's a question asked by many yesterday, when Evernote revealed a plan to expand its business well beyond productivity software by opening up Evernote Market. The Market, revealed at the third annual Evernote conference in San Francisco, debuted selling a selection of high-end bags from Côte&Ciel, notebooks from Moleskine, a scanner from Fujitsu, a stylus from Adonit, plus a smattering of T-shirts, posters and the aforementioned socks. The stylus packs the smallest tip on the market and is designed to work with note-taking apps like Penultimate. And the scanner integrates tightly with Evernote software, too -- it can scan a pile of varied documents (business cards, receipts, invoices, etc.), then sort and deposit the results an appropriate notebook automagically. At first blush, selling physical goods seems odd for such a company. To hear CEO Phil Libin tell it, however, the move into retail is a logical one, should you be willing to make a bit of a cognitive leap. The key to this strategy was revealed during his day two keynote, when Libin said that his company is in the business of AI. However, those letters stand for augmented, not artificial intelligence. That distinction's an important one for Libin, because there are two fundamental flaws with artificial intelligence, as far as he's concerned. According to him, Alan Turing ""set AI research back 60 years"" when he made the goal of such study be to build a computer as smart as a person. That's an incredibly difficult thing to accomplish, and besides, ""it's unclear what that computer would do and what it's good for,"" according to the CEO. Instead, Evernote is inspired by the thinking of chess grand master Gary Kasparov, who, after being defeated by IBM's Deep Blue in 1997, had a bit of an epiphany. The future of chess would not be man vs. machine, but man and machine working together as a unit against other such teams. Basically, everything put out into the world with Evernote's name on it is being built to amplify your productivity -- not by dictating behavior, but by cooperating with your existing habits. This idea is precisely what Evernote's talking about when it refers to augmented intelligence: Libin says, ""we're about building products and features that make people smarter."" That means creating software that recognizes the devices you use and knows their strengths and weaknesses -- so the software helps when it can, and gets out of the way when it should. Naturally, having a hand in creating those products better enables that integration. Now, you might be thinking that a scanner or a Post-it pad is a natural fit in the Evernote universe, but bags and wallets? Well, you need somewhere to put all those devices running Evernote software, right? And the company sees no reason why they can't be the source for all the stuff that makes your life better, both directly and indirectly. AI is such a core concept to the company that Evernote has hired a VP of Augmented Intelligence. Mark Ayzenshtat is that VP, and his philosophy is that users' minds are the most important platform, and Evernote's products and services are made according to that guiding principle. That means building software that shapes itself to user needs and providing products that ""call up the best benefits of that software."" Basically, everything sold with Evernote's name on it is built to amplify your productivity -- not by dictating behavior, but by cooperating with your existing habits. By that reasoning, selling a backpack that makes it easier to access your bike helmet before you cycle home from the office, or a quality cover that keeps your slate pristine makes sense. Evernote's not just selling the tools with which you work, it's also offering stuff that makes life a little easier. The question is, will you buy what Evernote's selling?                     "
312,https://www.engadget.com/2013-09-20-facebook-developing-ai-to-find-deeper-meaning-in-feeds.html,Engadget,2013,9,20,116.0," Facebook's current News Feed ranking isn't all that clever -- it's good at surfacing popular updates, but it can miss lower-profile updates that are personally relevant. The company may soon raise the News Feed's IQ, however, as it recently launched an artificial intelligence research group. The new team hopes to use deep learning AI, which simulates a neural network, to determine which posts are genuinely important. The technology could also sort a user's photos, and it might even select the best shots. While the AI work has only just begun, the company tells MIT Technology Review that it should release some findings to the public; those breakthroughs in social networking could help society as a whole.                     "
313,https://www.engadget.com/2013-07-16-uic-artificial-intelligence-test-common-sense.html,Engadget,2013,7,16,131.0," It'll take a long time before we see a J.A.R.V.I.S. in real life -- University of Illinois at Chicago researchers put MIT's ConceptNet 4 AI through the verbal portions of a children's IQ test, and rated its apparent relative intelligence as that of a 4-year-old. Despite an excellent vocabulary and ability to recognize similarities, the lack of basic life experience leaves one of the best AI systems unable to answer even easy ""why"" questions. Those sound simple, but not even the famed Watson supercomputer is capable of human-like comprehension, and research lead Robert Sloan believes we're far from developing one that is. We hope scientists get cracking and conjure up an AI worthy of our sci-fi dreams... so long as it doesn't pull a Skynet on humanity. [Image credit: Kenny Louie]                     "
314,https://www.engadget.com/2013-06-10-apple-anki-drive.html,Engadget,2013,6,10,586.0," Apple is just starting its WWDC keynote this morning, but it's already announcing something quite interesting: a new company called Anki and its inaugural iOS app called Anki Drive, which centers around artificial intelligence and robotics. The name, which is Japanese for ""memorize,"" features smart cars that are capable of driving themselves (although you can certainly take over at any time) and communicate with your iPhone using Bluetooth LE. These intelligent vehicles, when placed upon a printed race track, can sense the track up to 500 times a second. The iOS-exclusive game is available as a beta in the App Store today, which you'll need to sign up for -- the full release won't be coming until this fall -- and it's billed as a ""video game in the real world."" According to the developers, ""the real fun is when you take control of these cars yourselves,"" which we can definitely attest to -- the WWDC demo cars had weapons, after all. Follow all of our WWDC 2013 coverage at our event hub. Show full PR text Welcome to Anki www.anki.com/blog By Boris Sofman – Co-founder & CEO Welcome to Anki. We're thrilled you're here. At Anki, our aim is to bring artificial intelligence (AI) into people's everyday lives. We're passionate about AI and robotics, and we know these advanced technologies are more than just science fiction. We want to harness them, to interact with them, to be entertained by them, and use them to do things that have simply never been done before. We believe the time is right to bring AI out of robotics labs and research institutions and into life. Five years ago, my co-founders and I sat around a kitchen table and dreamed up the idea of the Anki platform. Countless algorithms (admittedly, many failed attempts), late nights, napkin sketches, and prototypes later, we realized that we just might be onto something. The first manifestation of that ""something"" is Anki Drive. We gave our very first sneak peek of Anki Drive today at WWDC. At first glance, it's a racing game that pits real cars against players and each other – but after playing for a few minutes, you'll see what makes Anki Drive special: We are making the first video games in the real world, and our team has worked tirelessly on the robotics and AI challenges that this presents. Each car is equipped with sensors and intelligent software to make thousands of decisions every second. We use mobile devices not as remote controls, but as drivers for an immersive real-world experience. And we took great care to make sure that despite everything under the hood, the final experience is intuitive and entertaining. When we look at Anki Drive, we see the first steps of the future of robotics and artificial intelligence being realized. We see an entertainment experience transformed today, and we see countless possibilities in the future. Anki Drive will be available this Fall exclusively on iOS. Until then, we hope you'll download our app here from the App Store to learn more about the technology that makes Anki Drive possible and sign up for a coveted spot in our beta program to get a hands-on look. Thank you for joining us for the beginnings of Anki. We're excited to share our story with you and hope you'll come back to hear more about what we're up to. About Anki · About Us: www.anki.com/about-us · Video, ""From the Founders of Anki"": http://youtu.be/GnI5hL6ii7g · Video, ""Anki Drive First Look"": http://youtu.be/KY34boeZzbo                     "
315,https://www.engadget.com/2013-05-16-google-nasa-quantum-computing.html,Engadget,2013,5,16,180.0," Google. NASA. Quantum computers. Seriously, everything about the new Quantum Artificial Intelligence Lab at the Ames Research Center is exciting. The joint effort between Mountain View and America's space agency will put a 512 qubit machine from D-Wave at the disposal of researchers from around the globe, with the USRA (Universities Space Research Association) inviting teams of scientists and engineers to share time on the unique super computer. The goal is to study how quantum computing might be leveraged to advance machine learning, a branch of AI that has proven crucial to Google's success. The internet giant has already done some work with quantum computing before, now the goal is to see if its experimentation can translate into real world results. The idea, for Google at least, is to combine the extreme (but highly-specialized) power of the quantum bit with its oceans of traditional data centers to build more accurate models for everything from speech recognition to web search. And maybe, just maybe, with the help of quantum computers your phone will finally realize you didn't mean to say ""duck.""                     "
316,https://www.engadget.com/2013-04-27-tattered-notebook-eq-next-and-storybricks-sitting-in-a-tree.html,Engadget,2013,4,27,1157.0," OMG! It's EverQuest Next news! No, really. I'm not pulling your chain. It's all official and everything. Namaste Entertainment, the creator of Storybricks, made the announcement that it is collaborating with SOE on EQ Next. So go ahead and do your finally-some-news celebratory dance, I'll wait a moment. Just don't go all out and strain something; you'll want to be able to spring into celebration again when more is revealed, right? Sadly, the news is pretty much summed up right there in that one sentence; we don't have any more details to revel in and no time frame for getting more (well, other than SOE Live, of course!). But when has a lack of specifics derailed fan excitement about an upcoming game? Well we may not have definitive details, but using what we know about Storybricks, we can certainly speculate on what the relationship between the two means for Norrath's next incarnation. And if there was ever a relationship I was happy to see, it's Storybricks and EverQuest Next's. At the end of last year, I was the lucky one who nabbed the chance to reveal Storybricks' dev diary regarding bringing NPCs to life. I remember well the words resonating with me as I read through it. Starting off as a tabletop payer before I was introduced to (and fell madly in love with) MMORPGs, I could totally related to ideas espoused in it. I thought longingly how it would be awesome if a game managed to incorporate those ideas. And now, those ideas might be coming to EverQuest Next? Be still my heart! Now you might be thinking, what's the big deal? Well, let me tell you why: Exciting innovation is not yet dead in MMO development! It's alive! If you're suddenly envisioning Frankenstein's monster rising from the table, an animated being instead of an inanimate object, then you're on the right track to understand what the big deal is about Storybricks. In a nutshell, Storybricks devs expressed their intent to create the illusion of life in MMORPGs by putting the intelligence in Artificial Intelligence. In the dev blog, Brian Green and Stéphane Bura detailed the ideas of bringing NPCs to life in such a way that players could experience roleplay with them just as they could with GM-controlled NPCs in tabletop games. Imagine coming across an NPC who doesn't just stand there and blandly roll through the same text player after player after player. Instead, the NPC interacts with folks on an individual basis, adapting to the choices and needs -- and even emotions -- of the one currently standing there. Now add in a memory; yes, the NPC remembers past experiences with the individual player and draws on past interactions to guide current ones. Pretty innovative, right? But it doesn't end there. Not only does your relationship as a player affect your exchange with the NPC, but the NPC's personal life does as well. Wait, what?! Yes, my friends; Storybricks proposed that AI should have a genuine life outside the confines of PC interaction. Here's how devs put it: Important NPCs should have inner lives, complex relationships, and their own goals that they work toward. They should remember past interactions with your characters and adjust their behavior depending on whether they feel grateful, trusting, envious, betrayed, and they should be able to express these emotions in a convincing manner, each one coloring their day to day activities (e.g., a guard whose girlfriend just left him should act differently from one who was just dressed down by his captain). Just think of interacting on a personal level with some of your favorite personalities in Norrath when they actually have a personality! What's my line? If there is anything the EverQuest franchise is known for, it's for the quests. Lots of quests. So even if an EQ sequel were morphed into a sandbox, you'd expect there to be quests, right? I mean, it's in the name, for Luclin's sake! But questing is a decidedly non-sandboxy endeavor. So how do you reconcile the seemingly diametrically opposed ideals of sandbox and questing? Storybricks' answer is first to bring the NPCs to life, then allow players to interact naturally with them. The idea is that just as your relationship with the NPC is unique, so, too, is your dialogue. Forget canned responses where players select a pre-scripted line that might best suit their needs; in fact, just throw scripts out the window completely. Instead, convey your wants and needs by actually communicating, and then NPCs respond in like manner. Questing takes on sandbox elements when it ditches the script and becomes improv instead. Let's hear it for the RP! Another thing that has me personally very excited hinges on a single line at the beginning of that dev diary, the one introducing the ""why we do what we do"" part: ""Storybricks was founded to bring the core of the roleplaying experience to MMOs."" The Storybricks crew's reason for existence was to put the RPG back into MMORPGs. As a roleplayer, I can't not be stoked to hear a company place RP at the forefront of design. For many games, roleplay features are an afterthought -- if they're included at all! And now this company is partnering with EQ Next. I am hopeful that the design philosophy carries over. After all, partners in a relationship do influence one another.Last EverQuest is best Now, take all of that from Storybricks and insert it into Norrath. Can you just imagine how this would seriously bring that already lore-filled virtual world to life? If so, then you can see why the sight of Storybricks and EQ Next holding hands elicits squeals of delight! It would most certainly make for a more deeply engaging sandbox experience than can be found in any other MMO out there. Think about it. With the elements that Storybricks espouses, EverQuest Next would be a world where your choices truly do matter, where action has consequences. Even how you talk to an NPC will influence what is available to you as you live your Norrathian life. A slip of the tongue could literally derail your plans, but perfectly placed words could reward you greatly. Just think of the possibilities. Emergent gameplay. Innovative. Those words are some of the few we've ever heard regarding EQ Next. But if the game can incorporate these Storybricks elements, it will certainly fit that bill. And if EQ Next also includes some of the greatest elements from its predecessors like housing and player-created books, then the latest installment in the EverQuest franchise will certainly be the greatest. EverQuest II is so big that it that sometimes MJ Guthrie gets lost in it all! Join her as she explores Norrathian nooks and crannies from the Overrealm to Timorous Deep. Running every other Saturday, The Tattered Notebook is your resource for all things EQII and EQNext -- and catch MJ every 'EverQuest Two-sday' on Massively TV!                     "
317,https://www.engadget.com/2013-04-23-storybricks-team-announces-everquest-next-collaboration.html,Engadget,2013,4,23,120.0," What's the Storybricks team been up to lately? Oh, nothing much. A few tweaks here, an idea or two there, and a whole lot of vacationing in Norrath. What's that, you say? It turns out that Namaste Entertainment has been teaming up with SOE to work on EverQuest Next, of all things. ""After several months of working together with Sony Online,"" the team posted, ""we can finally reveal that we are collaborating on EverQuest Next. EQNext is 'the biggest sandbox ever designed' and we are extremely happy to be working on the most innovative MMORPG under development."" The post couldn't go into specifics about the project, but it did say that the team is doing ""remarkable things"" with the game.                     "
318,https://www.engadget.com/2013-03-12-guardians-of-middle-earth-a-fun-game-doomed-by-its-business-mod.html,Engadget,2013,3,12,1451.0," The MOBA genre has exploded in recent years, with global giant League of Legends becoming the most actively played video game in the world and competitive tournaments getting more viewers than some televised sports. Today's MOBAs appeal to casual and competitive gamers alike, but until recently very few had crossed the console barrier. Released on PS3 and XBox 360 last December, Guardians of Middle-Earth took traditional DotA gameplay and made the quite experimental leap onto consoles. I'm not much of a console gamer (you can take my mouse and keyboard away when you pry them from my cold, dead hands), but I couldn't pass up the opportunity to see how Guardians of Middle-Earth stacks up against its PC-based counterparts. Monolith Studios has done great things in adapting MOBA gameplay to a console control scheme and audience, and the core game really is a lot of fun to play. But in charging an initial purchase price for a game that relies on having a large community, publisher Warner Bros. may have accidentally consigned Guardians to the scrapheap. In this hands-on opinion piece, I explore Guardians of Middle-Earth and ask why it's already a ghost town just three months after launch. Getting startedGuardians of Middle-Earth starts with an optional tutorial designed to introduce the control scheme and the basics of MOBA gameplay. It's mostly stuff you could work out on your own, but completing it unlocks Gandalf as a playable guardian, so it's worth working your way through it. The tutorial does over-explain things and constantly interrupts you with new instructions rather than letting you get on with testing out the controls yourself, but you can't argue with a free Gandalf! The lobby user interface is polished, and I spent an embarrassing amount of time reading through all the lore on each of the characters. The average wait time to get into a standard battleground match was reported as one and a half minutes, but it typically took me double that to get into a match, and even then half of the players in the game were bots. I eventually found the much more enjoyable Elite Battlegrounds that don't use bots, but those matches took upward of five minutes to get into. The recent release of the Survival mode DLC probably didn't help with battleground queue times, but the official forums have been full of similar complaints since launch. MOBA gameplay on a console works! The game can be played on single-lane and three-laned map variants, both with very different gameplay dynamics. The single-lane map feels a bit like Heroes of Newerth's Mid Wars mode: a fast-paced battle that rapidly escalates into one team winning by a landslide. The three-lane map plays like a cross between a standard MOBA and League of Legends' Dominion mode, with capturable shrines and health boosts spread throughout the map. Action in both modes is quite fast-paced, and picking up a heal at just the right moment can let you quickly get back into the fray. It's clearly a very casual game that is fundamentally a ton of fun to play. The controls were surprisingly intuitive, with one analogue stick for movement and another for facing to aim your attacks. A few gameplay concessions have had to be made for this control scheme to work, such as all attacks being area-effects, lines, or cones in front of your character. Even your basic attack will hit all enemies within its area, so you can farm creeps faster by positioning yourself such that you can hit several with the same attack. This also makes harassing enemies in the lane easier, especially if you're playing as a ranged guardian like Legolas and are facing off against someone with slightly shorter range. Character progression and stats Character customisation in Guardians of Middle-Earth comes in the form of belts filled with relics and gems that give passive bonuses during a match. This has been compared to League of Legends' Rune pages and Masteries, but it's more like a replacement for the item system. Instead of getting gold by last-hitting creeps and then buying items with it, the belt is pre-filled with bonuses before a match starts and unlocks incrementally as you level up. This eliminates the need to run to a shop mid-game but comes at the cost of removing the last-hitting mechanic central to the laning phase in most MOBAs. I actually think this is a pretty clever way of converting items over into a console-friendly format, but it does have some drawbacks. Tying stat progression solely to levels makes matches less likely to snowball in favour of one team or another, which sounds good in theory but can lead to some very long games. If you don't manage to push the level advantage early in a match, it quickly disappears as everyone hits the max level of 14. I played a single-lane match that lasted over 40 minutes because it reached a stalemate when both teams hit the level cap. I can't help thinking that the belt system should be tied to creep and guardian kills rather than level, but other than that it was a great customisation system. The bonuses from relics and gems are significant and figuring out builds for certain guardians and playstyles provides an interesting tactical metagame that may help create a competitive tournament scene. The fact that belt bonuses unlock incrementally also means certain builds are weak in the early game but become absolute monsters if they can get farmed up quickly, a risky but effective strategy. Matchmaking and bots It's been three months since Guardians of Middle-Earth launched and the servers are already pretty bare. As a result, I found myself being matched up against older players with better gems and access to more commands and potion slots than I had. Disconnections, lag, and rubber-banding were commonplace as the game uses peer-to-peer networking instead of a central server. Players also sometimes disconnected or ragequit in the middle of matches; the game threatens to punish quitters with temporary bans from matchmaking, but no such punishment is actually levied. Matchmaking problems with such low player counts are solved by filling out queued teams with bots, which would be a great idea if the AI weren't painfully stupid. On one occasion, I was thrown into a game with only one other human player, who soon disconnected and left me alone in a game filled with AI. I watched in disbelief as the bots paired off into lanes and began pacing back and forth without attacking each other or the creeps. The AI plays ridiculously defensively, never seems to initiate a fight, and always uses the same defensive loadout that just piles on hitpoints regardless of the character's role. As a striker with a full offensive loadout, I found myself unable to kill any same-level bot on my own. Though this sounds pretty bad, poor AI is forgiveable as the game was clearly designed to work with a much larger community than it currently has. When you finally get into a full game of humans and nobody disconnects, the game really is good fun. On the surface, Guardians of Middle-Earth is a fun casual MOBA with interesting characters and enough progression and challenges to keep players busy for a long time. The game is adapted well to a console audience and control scheme, and the lack of keyboard warriors raging in chat makes a nice change from PC MOBAs. If nothing else, Guardians proves that the DotA experience can be delivered on a console and still be a ton of fun to play. It's unfortunate then that Warner Bros. hasn't fully embraced the free-to-play model that's become the standard for MOBAs on the PC. The initial purchase price of Guardians of Middle-Earth is such a big barrier to entry that the game has already become a bit of a ghost town just three months after launch. The decision to sell entire game modes as DLC also serves only to further segregate the community and slow down queue times for standard modes. To get into a standard battleground match, players currently have to choose between waiting in a queue for five to ten minutes or playing with terminally daft bots, neither of which sounds particularly appealing. The disappointing conclusion is that this perfectly good game may have been doomed from the moment it was launched solely because of its business model. Massively's not big on scored reviews -- what use are those to ever-changing MMOs? That's why we bring you first impressions, previews, hands-on experiences, and even follow-up impressions for nearly every game we stumble across. First impressions count for a lot, but games evolve, so why shouldn't our opinions?                     "
319,https://www.engadget.com/2013-02-28-ibm-watson-pastry-drug-research.html,Engadget,2013,2,28,158.0," While mad game show skills are nice and all, IBM has started to nudge Watson toward the door to begin paying its own freight. After a recent foray into finance, the publicity-loving supercomputer has now brought its number-crunching prowess to the pharmaceutical and pastry industries, according to the New York Times. If the latter sounds like a stretch for a hunk of silicon, it actually isn't: researchers trained Watson with food chemistry data, flavor popularity studies and 20,000 recipes -- all of which will culminate in a tasting of the bot's freshly devised ""Spanish Crescent"" recipe. Watson was also put to work at GlaxoSmithKline, where it came up with 15 potential compounds as possible anti-malarial drugs after being fed all known literature and data on the disease. So far, Watson projects haven't made Big Blue much cash, but the company hopes that similar AI ventures might see its prodigal child finally pay back all those years of training.                     "
320,https://www.engadget.com/2013-02-13-smart-calendar-app-tempo-speeds-up-your-day.html,Engadget,2013,2,13,719.0," Not to bother you with a study from the University of the Obvious, but if you think about the history of technologically facilitated intelligent agents, the goal always seems to be to sub in for a clever, always-on human personal assistant. There's a reason that hyper-busy, well-compensated folk have such staffers on the payroll: they make things easier. Need a file for that meeting, or background on the attendees? Running late and need to tell them? Want to know which flights are delayed before you head to the airport, or where to park near the place you're having lunch? With a personal assistant, one call or text and you've got it handled. We can't all have personal aides, but a lot of us have smartphones. Siri tackles the question of accessing your data or contacting people without putting your hands and / or eyes on the iPhone; Android's Google Now feature aims at the information supply side, parceling out the tidbits you need (or at least the ones it thinks you need) just when you're likely to want them. Even the oversubscribed launch of Mailbox this week speaks to the desire we have to start getting a streamlined, secure handle on the ebb and flow of our critical information. Another option for getting the smart back into your smartphone launches today, and it's aimed at both your calendar and all the collateral information that surrounds it. Tempo Smart Calendar, incubated in Siri's birthplace at SRI International, derives situational awareness by analyzing your meetings alongside other data sinks like your email, LinkedIn contacts, attachments, location and more. The app is iPhone-only for now, with other platforms to come along later. Tempo may look like other calendar apps on the App Store -- to my eyes, it pays some UI tribute to the revamped Gmail native app -- but its power lies in context as well as content. Rather than overwhelm you with every tidbit and factoid about your events, Tempo's design is intended to ""reduce the noise that's often associated with virtual assistants that push information to users out of context or intent,"" says founder and CEO Raj Singh. The app will even find context that you didn't explicitly associate with the event, by looking for frequently emailed people connected to meeting hosts or attachments to messages with contextually relevant subjects. Wherever possible, the calendar app distills actions down to a single tap: send an ""I'm running late"" alert, get directions, pull up attachments for the next appointment, check LinkedIn profiles or join a conference call (it even auto-dials conference codes for you, which I currently do with a $1.99 singletasker). As Tempo learns your modus operandi, it adjusts to provide the most-frequently needed information more promptly. I may not be busy enough to take full advantage of Tempo's savvy, but if your day involves hopping from call to meeting to meal to evening, you may indeed benefit from the added clarity and context that the app provides. Of course, Tempo's AI has to learn about you and what your day looks like, so using it means giving it access to a lot of your personal data, including your email (and letting it mull for a while on initial setup). If that makes you uncomfortable, be forewarned. You should check the Tempo site for privacy assurances and make sure that if you do try it and don't like it, you can fully erase yourself from the service. Tempo's privacy statement is here; the important note for anyone who intends to do a short-term trial and possibly cancel later on is that if you delete your Tempo account, your third-party service info might remain on their system for several weeks until it's aged out. If you want to get your mail and other data off more quickly, be sure to remove those subaccounts within the Tempo app before deleting your master Tempo account. Tempo is launching as a free app, with possible premium features down the road for paid users. In contrast to Mailbox's Gmail-only limitation, Tempo is calendar- and email-agnostic. To provide a sense of what's possible inside the app, Tempo's produced this adorable promo video featuring a busy architect dad and his preternaturally articulate daughter. Robert Scoble also has a 30-minute interview with founder Singh in his enthusiastic writeup.                     "
321,https://www.engadget.com/2012-12-27-exclusive-storybricks-dev-diary-expounds-on-bringing-npcs-to-li.html,Engadget,2012,12,27,1808.0," In much of the MMORPG world, NPCs are nothing more than quest-filled Pez dispensers, human vending machines doling out the standard kill-10-rats chore to each and every player who roams by. They are lifeless tools at best and forgettable backdrops at worst. But the folks working on Storybricks believe NPCs can be more, much more. In this exclusive dev diary, Brian ""Psychochild"" Green and Stéphane Bura discuss making AI that can interact with players on an individual basis, adapting to individuals' choices, needs, and emotions and even remembering and drawing on past interactions to make a deeply engaging sandbox experience. Check out their thoughts in the full dev diary after the break! by Brian ""Psychochild"" Green and Stéphane BuraSandboxes, emergence, and the illusion of life in RPGs Massively multiplayer online roleplaying games have offered us a wondrous promise. Imagine: roleplaying games played online with multiple players! Yet MMORPGs have only been able to partially deliver the experience of roleplaying with others in a shared world that reacts to your actions.Storybricks was founded to bring the core of the roleplaying experience to MMOs. We are working to realize the incredible potential offered by this medium. With this potential in mind, here is a glimpse of where we believe MMOs will go in the next few years. Roleplaying games are my sandboxes Computer roleplaying games (CRPGs) focused on a single player can offer a far richer experience than MMORPGs when it comes to playing the role of character in a story. But even CRPGs themselves pale in comparison with the flexibility of tabletop RPGs. From their ancestors, MMORPGs have kept only the veneer of gaining levels and better gear for fighting bigger foes. In Dungeons & Dragons, the ""munchkin"" approach was indeed popular, but RPGs evolved to offer more options beyond combat for the sake of combat. Tabletop RPGs are huge sandboxes that allow players great freedom in how they can interact with and actually change complex living worlds. Non-player characters (NPCs) are as deep and real as the players need them to be, since they can become more detailed with the attention the players give to them as they help or hinder each other. And this can occur organically, not only as part of a pre-written scenario. Unscripted interactions can create lasting relationships with NPCs, like a proud wizard becoming your enemy over a slight offense. Each of these relationships opens new story opportunities, new options, or new complications when dealing with existing problems. It makes playing adventures more interesting, more engaging, and a lot more fun. Current MMORPGs aren't delivering this kind of deeply engaging experience. There is a business opportunity for the MMORPG that can recreate these unique qualities from the deep and varied NPCs of great tabletop RPGs and give them to computer-controlled characters. Giving the NPCs, as the Disney animators called it, the ""illusion of life"" would add wondrous new dynamics and life to MMORPGs. With these new systems, interacting with NPCs could mean much more than just triggering the next chapter of a quest or accessing a vending-machine-shaped like a person. Important NPCs should have inner lives, complex relationships, and their own goals that they work toward. They should remember past interactions with your characters and adjust their behavior depending on whether they feel grateful, trusting, envious, betrayed, and they should be able to express these emotions in a convincing manner, each one coloring their day to day activities (e.g., a guard whose girlfriend just left him should act differently from one who was just dressed down by his captain). Player characters should also be able to communicate their feelings to NPCs. Imagine if you could share what you need or want -- and not just through canned emotes or quest dialogue -- and have NPCs react appropriately depending on their own emotional and mental states. The relationships your character has with NPCs would act as keys to new content: A trusting noble might ask you to help him topple a corrupt prince; a jealous husband might not like your friendship with his herbalist wife. Your interactions could give you more resources: Your friendship with the head of the trader's guild might grant you an audience with a local leader, belonging to a ranger faction might provide you with a companion when crossing savage lands, and so on. This goes beyond narrative. Access to these richly varied interactions and relationships helps you accomplish your own goals in the game. No more grinding repetitive quests to get faction with a group. Instead, giving a gift and talking to the princess could lead to courting and marriage down the line. Exposing a traitorous priest could lead to your becoming the head of a priesthood after building trust. ""Scripted content creation is bringing the MMORPG industry to its knees."" Satisfying content should emerge as a response to the player's actions. There is overwhelming evidence that releasing a game with a scripted story does not scale, no matter how good that pre-generated story might be. Scripted content creation is bringing the MMORPG industry to its knees. As competition intensifies and players churn through content faster than ever, no amount of development effort can entertain a large and diverse playerbase such as the one found in online RPGs. Resorting to gating content and grinding only results in players getting bored and leaving. Massively multiplayer If a MUD player living in a cave for the past 30 years could be shown an MMORPG today, she would be utterly captivated by the graphics while at the same time being puzzled why gameplay has advanced so little. The multiplayer part of current MMORPG remains disappointing. Today's MMOs either encourage you to solo amidst a sea of strangers all following the same chain of quests or expect you to function as a cog in a large player organization whose goals you have little chance of affecting, let alone defining. Finding like-minded players is tough in current MMOs. If you don't play with friends, the only criteria that strangers tend to use to judge you in most MMOs are your class, your level or build, and your willingness to commit time to a group. But what if games could facilitate player connections by matching the quality of experience sought by different players? In other words, what is the next step in LFG? It is about letting players clearly express what they're looking for in their game and creating a way to satisfy these desires. Players should be able to express their PCs' emotions and needs in the game world. This can extend to all goals, beliefs, and values. For instance, you could declare your character to be a champion of Justice, and once you've proven yourself in a storyline designed for this purpose, you'd be put in contact with other Justice-minded characters. Maybe you'll form a new group or faction, adhering to rules and oaths that help strengthen your drive. The game itself would enforce these rules and test you and your faction-fellows with situations. This means that being a paladin is more than having shiny armor and a holy sword. What if actually being just, pious, honest and honorable -- acting in the game world in ways that demonstrate those particular virtues -- gave you in-game advantages and let you meet players who have chosen the same path? From there, it's easy to imagine new kinds of player organizations that would develop organically like religious orders, trading networks, armies, tribes, political parties, crime syndicates, local communities, and so on. Each group would be defined by what brings its members together: their common values and goals. Since the goals of these organizations would sometimes conflict, player competition would naturally emerge from the existence of such groups. This competition would not necessarily be PvP combat but could take on other forms: Religions may compete to convert characters, merchant guilds might arrange trade routes, or indeed, law-oriented factions may hunt down criminals. By being able to do the things they personally enjoy doing, players would create content for other players. Of course, players could belong to several of these organizations at the same time. These memberships could create inner conflicts for player characters themselves if the values and goals of these groups oppose each other. A game that allows characters to change other players' feelings could dynamically generate stories for players that offer tough choices; for example, should I bring my guildmaster to justice because he bent a few rules for greater profit? These unscripted stories would create memorable player-to-player interactions, where they could argue and bargain about the best courses of action and maybe sway others to their side without having to resort to violence every time. Finally, beyond the fourth wall, the players should be able to express their goals, too. You can already tell MMOs when you want to play a quick dungeon, but what if you could also play a murder mystery with strangers? Or have your character experience betrayal and revenge? Or declare that you want to take over a kingdom and have both players and NPCs rally to your banner? Artificial intelligence and the sound of inevitability While many of these features may seem impossible, they are not. Advances in artificial intelligence coupled with years of experimentation in RPG-like interactive storytelling have made it possible to model characters' emotions, needs, and relationships. Using this model, developers can plug characters into story frameworks, and those characters will remember and adapt to the players' choices. Conversely, a storytelling system that can look at players' goals, accomplishments, and skills to generate stories -- and the NPCs who populate those stories -- will deliver gameplay challenges that are highly appealing to a wide variety of players because the content quickly becomes personalized. It's not so much a technical matter anymore, although the AI involved in this has a lot of clever bits. Delivering this kind of personally adaptive gameplay is now more a matter of someone having the will to make the departure from the traditional model. We are on the verge of a revolution in computer game storytelling that is driven by player demand and game economics. In future MMORPGs, your choices will have real impact on the world of the game and will provide you with experiences that match your tastes, just as good pen and paper RPG game masters have been doing for years. At Storybricks, we are focused on realizing this future. We lament the missed potential of games so far, but we see a bright future. We hope you'll agree and that together we'll soon see this new world of MMORPGs become a reality. The Storybricks crew wishes to issue special thanks to Rodolfo Rosini, Bart Stewart, and Adam Baker-Siroty for their help, and the Massively crew would like to thank the Storybricks crew for this candid dev diary!                     "
322,https://www.engadget.com/2012-12-06-storybricks-shifting-gears-partnering-with-several-studios.html,Engadget,2012,12,6,171.0," Namaste Entertainment is shifting gears on its Storybricks project. The company has posted an update letter on its website, and in a nutshell the firm will be moving away from development on a standalone product and focusing on partnerships with other game studios. The Kingdom of Default test project is being shelved. Namaste says it was ""not enough of a game for traditional MMO audiences, too complex for casual players and too much of a game for educational uses."" Namaste does have ""several projects underway,"" but due to NDAs the company ""cannot be as open as we have been in the past.""Storybricks is an AI initiative that aims to upgrade the capabilities of game NPCs. Namaste ""designed and built an emotional intelligence engine in order to breathe life into virtual characters. They have their own goals and moods; they interact among themselves and take decisions on their own. The story no longer revolves solely around scripted behavior; game worlds can now be rich and complex."" [Thanks to John for the tip.]                     "
323,https://www.engadget.com/2012-10-12-georgia-tech-macgyver-robot.html,Engadget,2012,10,12,205.0," Robots come in many flavors. There's the subservient kind, the virtual representative, the odd one with an artistic bent, and even robo-cattle. But, typically, they all hit the same roadblock: they can only do what they are programmed to do. Of course, there are those that posses some AI smarts, too, but Georgia Tech wants to take this to the next level, and build a 'bot that can interact with its environment on the fly. The project hopes to give machines deployed in disaster situations the ability to find objects in their environment for use as tools, such as placing a chair to reach something high, or building bridges from debris. The idea builds on previous work where robots learned to moved objects out of their way, and developing an algorithm that allows them to identify items, and asses its usefulness as a tool. This would be backed up by some programming, to give the droids a basic understanding of rigid body mechanics, and how to construct motion plans. The Office of Navy Research's interest comes from potential future applications, working side-by-side with military personnel out on missions, which along with iRobot 110, forms the early foundations for the cyber army of our childhood imaginations.                     "
324,https://www.engadget.com/2012-08-29-ibm-debuts-new-mainframe-computer-as-it-eyes-a-more-mobile-watso.html,Engadget,2012,8,29,216.0," Those looking for a juxtaposition of IBM's past and future needn't look much further than two bits of news out of the company this week. The first comes with IBM's announcement of its new zEnterprise EC12 25 mainframe server -- a class of computer that may be a thing of the past in some places, but which still serves a fairly broad range of companies. In addition to an appearance that lives up to the ""mainframe"" moniker, this one promises 25 percent more performance per core than its predecessor and 50 percent more capacity. The second bit of news involves Watson, the company's AI effort that rose to fame on Jeopardy! and has since gone on to find a number of new roles. As Bloomberg reports, one of its next steps may be to take on Siri in the smartphone space. While there's no indication of a broader consumer product, IBM sees a range of possible applications for a mobile Watson in business and enterprise -- even, for instance, giving farmers the ability to ask when they should plant their crops. Before that happens, though, IBM says it needs to give Watson more ""senses"" in order to respond to real-world input like image recognition -- not to mention learn all it can about any given subject.                     "
325,https://www.engadget.com/2012-08-03-rogue-automatic-trading.html,Engadget,2012,8,3,93.0," Humans never learn and apparently neither do robots. Autonomous trading AIs went on a spending spree at Knight Capital Group in New Jersey this week, buying up shares in everything from RadioShack to Ford and American Airlines (ouch) in a 45-minute frenzy of disobedience. The company tried to offload the unwanted stock, but discovered it was already nearly half a billion dollars in the red -- enough to wipe out its entire profit from 2011 and ""severely impact"" its ability to conduct business. If only it had protected itself with one of these.                     "
326,https://www.engadget.com/2012-08-01-netflix-max-ps3.html,Engadget,2012,8,1,261.0," It's not available to all, but if you're running the latest version (2.08) of the Netflix app on your PlayStation 3 you may see a prompt for a new item called Netflix Max, spotted by one of our readers. Described by a PR rep we contacted as a ""new user experience"" the company is testing, the assistant talks to users directly and asks them to rate a few movies on the spot before providing new suggestions. The Noisecast was among the first to try out the new experience and even has a few tips on how to get it for yourself, although it didn't work on our PS3, even after uninstalling and reinstalling the app to get the latest version. The blog mentioned it does more than simply rating movies also, as it sometimes asked users to select movies based on specific criteria, like the starring actors. It can be brought up by pressing the square button on your PS3 controller, although how much you enjoy the gameified experience may vary. Pulling gems out of a catalog filled with older and lower profile movies is a complaint of many Netflix subscribers though, so any attempt to help is welcome, but we're not immediately sure this is the way. According to Netflix, it's waiting to see if the tool leads to ""increased interaction"" (read: more viewing hours / less likely to cancel, on average) before deciding whether or not to roll the tool out widely, so if you have it and like it, vote with your remotes. [Thanks, Sean aka Prophet Beal]                     "
327,https://www.engadget.com/2012-07-27-borderlands-2-baddies-are-smarter-than-you-think.html,Engadget,2012,7,27,863.0," Well, at least the AI programming behind each enemy in Borderlands 2 is smarter than anything in the first Borderlands – the baddies themselves are victims of the rancid planet Pandora and probably wouldn't pass a first-grade spelling test. They will, however, blow your brains out.Jasper Foreman, lead AI programmer at Gearbox Software, sat down with us (on the floor in a hallway of the Hard Rock Hotel, right outside of San Diego Comic-Con central), to describe a handful of enemies we can expect to see in the new Borderlands and the tech that makes them smarter than ever.There are 15-20 separate enemies in Borderlands 2, but each type has numerous variants, bringing the total number of foes somewhere between 200-300. Foreman once tried to count them all by running a script, but with the complex descriptors composing each enemy it was impossible to locate every one. Suffice it to say, there are a lot.%Gallery-161206% Skags and Bandits are two of the returning classes, and they've both experienced upgrades. There are still Badass Skags and they can still take on elemental effects such as fire and shock, but they are now equipped with the ability to buff the minions around them. Badass buffing gives minions health, speed and special moves, such as lightning attacks.The Bandit class includes Marauders that interact intelligently with the environment and each other – they have multilevel design, allowing them to hop around more, jump on rooftops and find ways to surround the player. They will ""dodge, dip, dive, duck and dodge,"" according to Foreman. New baddies include Threshers and Foreman's personal favorite, the Goliath. Goliath is a monstrous enemy with the singular focus to kill you. Literally – all Goliath variants wear helmets that give them tunnel-vision to destroy whatever is in front of them. Shooting its helmet off will send a Goliath into a blind rage, and it will punch the nearest character, even taking down another Goliath, a Marauder, a teammate, your grandmother, whatever.However, sniping the helmet off of every Goliath you encounter may not be the best move, since they level up as they kill. Once they've slaughtered all your enemies they may be more formidable than the entire horde combined, and you'll have no one to hide behind (unless you're playing multiplayer with some really good friends).Threshers are burrowing animals that are less like sand worms and more like ""squid worms,"" as Foreman calls them. They're generally juicy, squishy and have tentacles everywhere: A Thresher's head can poke out of the ground in one area while its tentacles attack another area of the battlefield. One type, the Wormhole Thresher, has spikes all over its body and the ability to summon a wormhole that sucks you in for impalement if you get trapped in its pull radius.Tadpole Threshers, on the other hand, shoot out of the ground and fall back down for aerial attacks. Think of it like reverse burrowing. Handsome Jack runs the show on Pandora and much of the mythos of Borderlands 2, controlling the Hyperion Corporation and its robot army. Just as with the fleshy enemies, Jack's robots also have variant forms.Loaders are battle-oriented bots, with variants such as the tank-like WAR Loader and the BUL Loader, which can transform into a shape similar to a bulldozer.Surveyors are small, shrimp-like medics that fly around the warzone and heal the other classes as they fight. If a Loader has its legs shot off – first, it will continue to come after you, Terminator-style. Second, an alert will appear above its head and a Surveyor will fly over and start repairing it.Constructors are big bots, and are rarer than the other classes (thankfully, it sounds like). Constructors expand on the digistruct system in the first Borderlands, able to create robots directly on the battlefield. Constructors have a crit spot in their eye, which is similar to HAL 9000's, and shooting it will interrupt its building process, but will be difficult. Constructors are tricked out with missiles, lasers and a nuke that can go off with a big fanfare, shooting out of the robot's top, flying up and aiming straight for you. It is possible to shoot the nuke before it reaches you, of course.Foreman built the tools for the designers, including a behavior tree that acts as a set of AI Legos – little, clean bricks of AI that can be placed wherever they need to be. This lends itself to complex builds for each class and each variant within that class. For example, if a Midget and a Goliath find themselves near each other, the Midget may jump on the shoulders of the Goliath and attack together, as their AI tells them to. Not all enemies interact this way, but there are quite a few instances like it between species, Foreman says.The Ultimate Loot Chest Limited Edition of Borderlands 2 contains a ""Creatures of Pandora wide-format ID chart"" that outlines all of the organisms on the planet, and it was drawn up by a Gearbox marketing artist who happily volunteered for the job. Needless to say, by the time the artist was almost done with the poster, Foreman felt pretty bad for him.                     "
328,https://www.engadget.com/2012-07-16-inactive-players-giving-rise-to-xsyon-zombies.html,Engadget,2012,7,16,164.0," Normally, the worst things that happen to inactive players are that guilds kick them out. But in the world of Xsyon, the consequences are more dire, not just for the player but for everyone around. With the game's latest update, inactive tribal totems will begin to give rise to intelligent and malicious undead revenants. And these creatures are far more intelligent thatn your average monsters, hunting other players and looting them for better equipment as they capture new territory. Fortunately for the players, there's also new armor to be looted off the revenants themselves and plenty of opportunities for players to beat back the invasion. The update also brings along a widespread upgrade to monster AI, making all foes a bit more clever and deadly aside from just the revenants. If you've been out of the game for a while, you might want to jump back in, especially to prevent your former home from turning into an undead nest. [Source: Notorious Games press release]                     "
329,https://www.engadget.com/2012-06-22-undead-labs-on-making-a-believable-zombie-apocalypse.html,Engadget,2012,6,22,152.0," In their gory, gruesome office, Undead Labs' devs are hard at work figuring out the best way to disembowel you and turn you into a walking nightmare. So far it's going well, according to a new Q&A by CM Sanya Weathers. Weathers sheds some light on the difficulties of creating a zombie-infested world, particularly in relation to spawn points, mob density, and artificial intelligence. ""It takes surprisingly smart AI to make zombie behaviors that are believably dumb,"" she writes. According to her, Undead Labs' Class3 will boast ""thousands"" of zombies, all of which will be able to react to sounds, light, and explosions. Weathers says that the game is in the final stages of pre-alpha production and we should be seeing gameplay footage from it soon. Other topics covered in the Q&A include player weapons, how to attract zombies, types of zombies, and a fast food joint called the Swine & Bovine.                     "
330,https://www.engadget.com/2012-03-16-ai-system-programs-a-game-with-a-suspicious-theme.html,Engadget,2012,3,16,184.0," All right; we're done. Throwing in the towel, getting the heck outta Dodge and all that. The robot apocalypse is coming to a head faster than we thought, and it's time for us to move to our safe-hut deep in the jungles of Malaysia. Goodnight, everyone!Michael Cook, a computer scientist at Imperial College London, has created an AI system that designs games -- clever, complex, slightly sadistic games -- and he has named it Angelina. Angelina has used ""co-operative evolution"" to make Space Station Invaders, an 8-bit platformer about a scientist trying to escape a space station full of aliens and homicidal robots. Cook provided the graphics and sound effects for Angelina, but still, we have to wonder how comfortable he is with that particular in-game scenario.We just spent the past 10 minutes playing a game designed by a computer, and we enjoyed it. This is of course all a part of the machines' master plan -- get us complacent, then wham. Lasers everywhere. Either that or we can expect a flood of educational games about adopting children from third-world countries any day now.                     "
331,https://www.engadget.com/2012-03-08-angelina-the-experimental-ai-thats-coming-for-our-game-dev-job.html,Engadget,2012,3,8,120.0," Ok so, maybe Angelina couldn't have created Skyrim all on her own, but the experimental AI from Michael Cook (a computer scientist at Imperial College London) is still quite impressive. The artificial dev is able program enemy behavior, layout levels, and distribute power ups with random attributes. While many elements of a game like Space Station Invaders (which you can play at the more coverage link) are designed by human hands, it's Angelina's ability to act as a composer building something fun from the various ingredients that's so interesting. Before setting a level in stone she plays through the possible combinations, determining which will be most enjoyable for a human player. Hit up the source link for loads more info.                     "
332,https://www.engadget.com/2012-02-13-the-last-of-us-combat-forces-you-to-think-through-the-situation.html,Engadget,2012,2,13,112.0," Naughty Dog is looking to make The Last of Us the most realistic fungal-zombie-apocalypse game ever created. Game director Bruce Straley and creative director Neil Druckmann let slip a few details about combat mechanics in a video interview with Game Informer: Health will not automatically regenerate; Ellie, your AI companion, will be intelligent enough to throw bricks and other materials at your enemies, and she won't break your ""stealth"" mode; and it may include Uncharted-esque playable cinematics, but in more intimate environments.Straley and Druckmann discuss combat moments as part strategy, part shooter, and say they're infusing the title with realistic reactions from NPCs and the protagonist alike. Watch the entire interview here.                     "
333,https://www.engadget.com/2012-01-24-league-of-legends-update-bringing-sharper-ai-new-bots.html,Engadget,2012,1,24,216.0," League of Legends is well known for its challenging team-based gameplay and its rather steep learning curve, leading many users to play co-operatively against AI in order to hone their skills. In addition, co-op vs. AI is ""the primary game mode for a surprisingly large percentage of users"" according to senior producer Mark Norris. In light of this, Riot Games is bringing a huge new update to League of Legends' AI matches. For starters, Riot will be introducing AI bots for a whopping 40 of the game's champions, allowing players to test their mettle against a wider variety of opponents. In addition, the AI itself is in for an update that will allow bots to behave more like their human counterparts by utilizing predictive targeting for skillshots (such as Ashe's ultimate) and employing a great deal more strategy than the current bots. Of course, there are still some things that bots can't do (such as intelligent jungling), but Norris suggests ""that's actually a huge advanced area that we do want to look at and get into, that we're in the the real rudimentary stages of trying to figure out right now."" For the full details on what awaits players in the upcoming AI update, just click on through the link below and read the full story.                     "
334,https://www.engadget.com/2012-01-13-mortal-online-announces-a-new-expansion-called-awakening.html,Engadget,2012,1,13,151.0," Mortal Online has already launched one expansion, but the game's second expansion in its two years of operation will soon join the first. Awakening has just been announced and is set to bring a variety of improvements to the game. The centerpiece is the introduction of Tindrem, the eponymous Tindremic capital and the home of the Emperor. The sprawling city encompasses several districts, but there's more to the city than the orderly facade it presents to travelers. The expansion will provide more than simply a new city, however. It offers improved artificial intelligence and a better user interface to players, along with new dungeons in which players can test their skills against improved enemy intelligence. There's also a new set of special Trade Brokers in the cities, dyes for clothing and armor, and several other improvements that should be cheered by every Mortal Online player. [Thanks to James for the tip!]                     "
335,https://www.engadget.com/2012-01-02-the-daily-grind-whats-your-favorite-mmo-pet-class.html,Engadget,2012,1,2,210.0," If you started playing MMOs at the beginning of the genre, you may be saddled with a prejudice against pet classes. In early Ultima Online, an Animal Tamer's pets, usually dragons, had a way of getting their master into notoriety trouble. In EverQuest, if ever a pull went awry, blame was placed on the Necromancers and Magicians, deserved or not -- obviously, those classes just had poor pet control. Even City of Heroes' Masterminds, implemented only in 2005, have a bad reputation for reckless behavior (not to mention for blocking party members' movement). Pet AI has come a very long way over the last decade and a half, though, and pet classes continue to be popular, perhaps because they allow players to micromanage a group without actually having to be in one. And some games, like Star Wars: The Old Republic, have made all classes pet classes by virtue of companions. Are you a fan of pet classes, and if so, what's your favorite implementation across the genre? Every morning, the Massively bloggers probe the minds of their readers with deep, thought-provoking questions about that most serious of topics: massively online gaming. We crave your opinions, so grab your caffeinated beverage of choice and chime in on today's Daily Grind!                     "
336,https://www.engadget.com/2011-10-31-artificial-tongue-distinguishes-18-different-types-of-canned-tom.html,Engadget,2011,10,31,88.0," Taste tests are fun -- unless you're in Italy, in which case they're drawn-out and rancorous. That's why scientists in Milan are trying to remove humans from the equation, by using nuclear magnetic resonance (NMR) spectroscopy to reveal objective ""metabolomic fingerprints"" for different foodstuffs instead. In their latest experiment, NMR succeeded in predicting how human testers would judge 18 different canned tomato products, including sensory descriptors such as bitterness, saltiness, ""redness"" and density. Like Caesar always said, technology that knows a good ragu is technology we can trust.                     "
337,https://www.engadget.com/2011-10-26-why-google-and-microsoft-need-to-fear-siri.html,Engadget,2011,10,26,456.0," Tech.pinions' Tim Bajarin has opined on why they feel Google and Microsoft hate Siri, citing some excellent sources. As the article states, Google's Andy Rubin told the Wall Street Journal's Walt Mossberg, ""You shouldn't be communicating with the phone; you should be communicating with somebody on the other side of the phone."" Likewise, Bajarin quotes Microsoft's Andy Lees saying it ""isn't super useful."" The reasons he gives behind Microsoft and Google's dismissal comes down to two no-brainer answers: Jealously and knowing that Siri will develop into such a powerhouse that it will be a threat to business. And, you know what? He's right. Bajarin points out that Siri is a front to some major databases including Yelp and Wolfram Alpha. And, just wait until Apple allows developers at Siri's API. The possibilities will be endless. Even now, like Remember the Milk has done, developers are figuring out ways to make Siri work for them. Siri's future paves the way for similar technology to be introduced across all Apple products. Tech.pinions sees Siri as ""the gatekeeper to natural language searching"" and urges Apple to acquire as many databases as it can to promote this. I think Apple should open the API to developers. I also think it's more than gatekeeping. I had the absolute thrilling experience Tuesday to watch someone be introduced to an Apple product for the first time. I was in a Verizon store starting the process of switching carriers, and the other woman in there was picking up her new iPhone 4S. It was amazing to see her use Siri for the first time, as the salesman asked for hamburger joints, and Siri responded with several locations. He had her instruct Siri to call her spouse, which it did. She talked for a bit, then started playing with the other features. She called one of her children using FaceTime. I finished my business and left before she did, but watching her morph from skeptic to fan was brilliant. Apple's most likely gained another lifetime customer. And a big chunk of it is that Siri makes an already easy-to-use device even easier. Right out of the package, you can press and hold a button and have Siri do so much for you. My grandmother, who had crippling arthritis by the end of her life, could have used Siri to enrich her life. To circle back to Rubin's quote, you're not just communicating with your phone. You're using it as a bridge to be able to connect with people on the other side of the phone easier. Whoever possesses the technology and ability to do this will be the one to dominate the industry in the future, and right now, the ball is in Apple's court.                     "
338,https://www.engadget.com/2011-10-25-john-mccarthy-ai-pioneer-dies-at-84.html,Engadget,2011,10,25,127.0," It might be a stretch to suggest that there'd be no AI without John McCarthy, but at the very least, we'd likely be discussing the concept much differently. The computer scientist, who died on Sunday at 84, is credited with coining the term ""Artificial Intelligence"" as part of a proposal for a Dartmouth conference on the subject. The event, held in 1956, is regarded as a watershed moment for the subject. Early the following decade, McCarthy pioneered LISP, a highly popular programming language amongst the AI development community. In 1971, he won a Turing Award from the Association for Computing Machinery and 20 years later was awarded National Medal of Science. A more complete obituary for McCarthy can be found in the source link below. [Thanks, Jason]                     "
339,https://www.engadget.com/2011-10-17-siri-and-the-possibility-of-artificial-intelligence.html,Engadget,2011,10,17,280.0," Wired does a little speculating over on the Cloudline blog about whether or not Apple's redesigned Siri service actually counts as an AI. Technically, no, Siri's not a real artificial intelligence. When you ask ""her"" something and she comes back with a witty answer, your iPhone doesn't actually ""understand"" what you said in any meaningful way -- it's just identifying a set of words that you put together, and then outputting some data based on those words. Sometimes that's movie times or nearby store locations, and sometimes that's just a witty phrase that Apple engineers have programmed into the system. But of course, while programmers have been creating these ""chatbots"" for years, Siri has an advantage in that it runs on the cloud; Apple is constantly updating Siri's phrases and responses, which means that ""her"" answers will only get more appropriate over time. And while the system works as is, you have to imagine that Apple is collecting lots of information from it, including both what people are asking of Siri, and how they're asking for it. The more Apple learns about how to deal with that information, the better Siri will get at providing the right answer at the right time. That will make Siri ""smarter"" than ever. Until Apple hooks it up to an as-yet-uninvented thinking engine, it still won't ""understand"" your queries in the same way that a real human would -- or even in the way that a hyper-parallel quiz show competitor does. But for a lot of people, that doesn't much matter. As long as Siri responds correctly and helpfully, that's as good as many people need in terms of the payoff from artificial intelligence.                     "
340,https://www.engadget.com/2011-10-16-switched-on-as-siri-gets-serious.html,Engadget,2011,10,16,873.0," Each week Ross Rubin contributes Switched On, a column about consumer technology. Nearly 15 years passed between Apple's first foray into handheld electronics -- the Newton MessagePad -- and the far more successful iPhone. But while phones have replaced PDAs for all intents and purposes, few if any have tried to be what Newton really aspired to -- an intelligent assistant that would seamlessly blend into your life. That has changed with Siri, the standout feature of iOS 5 on the iPhone 4S, which could aptly be described as a ""personal digital assistant"" if there weren't so much baggage tied to that term. Siri is far more than parlor entertainment or a simple leapfrogging the voice control support in Android and Windows Phone. At the other end of the potential spectrum, Siri may not be a new platform in itself (although at this point Apple has somewhat sandboxed the experience). In any case, though, Siri certainly paves the way for voice as an important component for a rich multi-input digital experience. It steps toward the life-management set of functionality that the bow-tied agent immortalized in Apple's 1987 Knowledge Navigator video could achieve. Its success even at this early stage has been driven by its impressive speech recognition accuracy. Apple shows a demo of Siri in which you can ask it to split a dinner tip among multiple people (don't they know there's an app for that?) and Siri obliges, calling on the cloud-based computational might of Wolfram Alpha. It's the kind of task that Newton was supposed to help with, but anyone who remembers the initial state of its handwriting recognition would know that trying to write ""What does each person owe for a 20 percent tip on a $205 dinner for a part of six?"" would be enough to send you scavenging for the nearest napkin and pencil. Siri, of course, also has access to powerful sources of information that did not readily exist in Newton's day, such as location and commonplace wireless data. Siri also impresses in its ability to follow a dialogue in some contexts. You can ask it what you have on today's agenda and then follow up with ""What about tomorrow?"" and it will show you tomorrow's appointments. You can then tell it, ""Cancel the first appointment"" and if it's a recurring appointment, it will ask if you want to cancel just that occurrence or the series. But Siri has some unusual limitations. It can't open iPhone apps such as Safari or key websites such as Wikipedia by name (although it can initiate a Web search). It can dictate SMS or iMessage messages but not tweets despite iOS 5's Twitter integration. Siri can call up a preview of recent e-mails, but it is not (yet) Apple's answer to a way to have texts and emails read passively while driving, a task handled adeptly by apps such as DriveSafe.ly -- particularly on the BlackBerry platform. Today, Siri ties into a relatively small subset of your personal information and calendar as well as Wolfram Alpha as a general knowledge storehouse and Yelp for restaurant reviews. But for this self-described ""humble personal assistant"" to realize what are clearly less-than-humble ambitions, it must tie into more cloud-based services. Unfortunately, the two at the top of the list are not at the top of Apple's buddy list these days; Google and certainly Facebook know much more about most Siri users than Siri does, although Apple can make up some ground with its own browsing knowledge furnished via Safari and its new partnership with Twitter. And while Evernote may not have Facebook's user base, chronology or unconscious knowledge population, it could add great value to Siri's knowledge of our life details. While less important, there would also be advantages to having Siri integrate with more of the iPhone's own local content (although iCloud should bridge this somewhat), such as photos and videos. For example, Siri can't get the job done when you ask it to, ""Email the last photo I took."" Rather, it creates an email with the subject ""The last photo I took"" but no attachment. And if you tell it, ""Take a photo in ten minutes,"" Siri shrugs that it is not much of a photographer (a self-deprecating concern that hasn't done much to stem the tide of many Facebook photos). Siri is unique in Apple lore in that it is the first major UI paradigm not to be introduced with a fundamentally new device. Apple introduced its graphical user interface on the Lisa, its pen user interface on the Newton, and its multitouch user interface on the iPhone. (Even the modest scroll wheel debuted on the iPod.) Apple could keep it tied to its virtual desk as a limited assistant or make it a pillar for an ever-broadening set of features that could include information retrieval, life management, knowledge work, and proactive alerting and recommendation tasks. Siri itself, though, isn't giving anything away. When asked, ""Why did Apple make you?,"" one of its responses is, ""Apple doesn't tell me everything, you know."" Ross Rubin (@rossrubin) is executive director of industry analysis for consumer technology at market research and analysis firm The NPD Group. Views expressed in Switched On are his own.                     "
341,https://www.engadget.com/2011-10-03-iphone-assistant-called-a-world-changing-event-by-siri-co-foun.html,Engadget,2011,10,3,190.0," One of the expected announcements in tomorrow's ""Let's Talk iPhone"" event revolves around the iPhone Assistant, a powerful voice recognition and artificial intelligence tool that will enable real-time searching and control of iPhones. The tool, based on technology acquired in Apple's 2010 purchase of Siri, is so powerful that Siri co-founder Norman Winarsky is referring to its release as a ""world-changing event."" Winarsky is no longer with Siri, so he's offering pure speculation based on his prior knowledge of the company's technology. He believes that inclusion of the Siri artificial intelligence capability might explain why an iPhone announcement wasn't made until October (rather than the usual summer timeframe), noting that ""AI takes a lot of computing power."" Rumors floating around the web note that Assistant may only run on a next-generation iPhone due to the need for more RAM and a faster processor. In an interview with iPodNN News, Winarsky waxed enthusiastic about the capabilities of Assistant, noting that ""If the rumors are true, Apple will enable millions upon millions of people to interact with machines with natural language ... We're talking another technology revolution. A new computing paradigm shift.""                     "
342,https://www.engadget.com/2011-09-16-some-assembly-required-diving-into-details-of-origins-of-malu.html,Engadget,2011,9,16,2363.0," ""This will be the world's best game ever ever ever!"" -Michael Dunham. You tell 'em! Honestly, we have heard it before: MMORPG feature lists that promise what our sandbox-loving hearts desire. Try as we might, we cannot escape those sneaky tendrils of hope that find the chinks in our +10 jaded armor of cynicism as we read about the return of a true virtual where actions have consequences, personal choices dictate gameplay, and individuality is a matter of principle. Sadly, time and time again those hopes are dashed by titles that either cannot deliver on their promises or never even survive until launch. The same is true for the developers at Burning Dog Media; they too have ridden the roller coaster of sandbox dreams and harbored in their hearts a vision of their ultimate game if only they had the wherewithal to produce it. The difference between them and us is... they do! Welcome to issue #6 of Some Assembly Required -- an exclusive interview with Michael Dunham and Dave Cruikshank -- Lead Developer and Art Director of Burning Dog Media, respectively -- who are in the midst of developing the new sci-fi/fantasy MMORPG Origins of Malu. Originally called just Origins (the change was to help avoid confusion with other projects that were cropping up with similar titles), this game previously teased sandbox aficionados with a glimpse at some hope-stirring features. Today, we have the privilege of expounding on ""coming soon"" and revealing more features that not only are planned but are for the most part implemented. So why should we check out Origins of Malu? And what tantalizing tidbits of sandboxy goodness can we expect? Grab onto that hope and dive past the cut -- if you dare -- to see what Michael and Dave have to share.%Gallery-134056% As one who clings to that hope that someday there will be another well-made sandbox, I was excited to speak with Michael and Dave about Origins of Malu even while toting my little shield of skepticism. As the interview continued, I laid the shield at my feet and let myself be drawn into rooting for the game. Why? The quick answer is the passion that these guys have for their game. It isn't just some sterile business decision; it's what they really want to play. Where other companies jump on the crank-out-formulaic-carbon-copies-of-some-past-success-to-generate-fast-revenue bandwagon, these folks are doing it for the art of it and plan to stick to their guns. They realize they can't please everyone, and they don't try to. As Dave put it, ""As much as we want to make the game everything, we can't because that just waters it down."" Preach on brothah! ""It is not the sandbox genre that failed, it's the way the industry has implemented it."" -Michael Dunham While many of us have griped about the state of games (seemingly in vain), these folks decided to put their money where their mouths are. As Michael stated, ""We were with all those trolls bitching about all that was wrong with [the game], then decided... why not stop bitching and do something about it?"" Dave pointed out that, like the rest of of sandbox lovers, ""We have been burnt three or four times before ourselves."" So we know that these guys are gamers (Dave admits that he played way too many video games, in his parents' opinion); what other qualifications do they have for producing an MMORPG? Michael's background is in software development, from small mods to much larger corporate projects, while Dave brings to the table experience in many types of artistic media, including drawing, painting, and sculpting. Add to this a (self-described) fanatically dedicated group of talented people and an atmosphere where the whole team contributes ideas and you have the recipe for a group that won't sell out just for a quick buck. Unlike those who employ the ""crank 'em out"" methodology, Michael and Dave impressed me with their philosophy of doing it right the first time. As Michael put it, ""Our goal is to be a respected studio; we have one chance to build that rep."" How are they following that philosophy? For starters, they aren't releasing information on all the features that are ""planned"" and instead are waiting until said features are a reality, not just a pipe-dream. Michael admitted, ""If we wanted to do something crappy, we would have shared more right off the bat."" Dave reiterated how important it is to build up a solid reputation and revealed that they have had opportunities to get funding but wanted to keep the integrity of what they wanted to build. What is their ultimate goal? ""To create a game with soul."" So what is Origins of Malu aiming for? Overall, as Michael put it, the studio wants a game where players have feelings and emotions for the world, something ""that's personal, that means something to [the player]. We want to tell a really great story and let the user develop with it."" They want players to really be invested in the world, not just absently grinding through it. Since the game is so far shrouded in a fair bit of mystery, I tried to weasel some picture of what features they might like to emulate by getting an idea of which types of games influenced each of the top dogs of Burning Dog. Michael gave a shout out to Asheron's Call 2, World of Warcraft (for putting the whole package together), Ragnarok, Star Wars Galaxies, and Minecraft (for the methodology behind what they were trying to do). Dave admitted he ""went through the phase where [he] was an RPG junkie on the PlayStation"" and cited the influence of old Final Fantasy games and the Elder Scrolls series. Ultima Online and Asheron's Call also influenced him greatly. OK, so maybe this was said a bit tongue-in-cheek when I pressed them about the game, but who am I to put a damper on enthusiasm? Why curb excitement for sandbox features? So exactly what features have these two -- well, at least Michael -- so chatty? I peppered Dave and Michael with questions about housing, customization, FFA PvP, the economy, crafting, skill caps and more. While I couldn't wrangle details for everything out of them (after all, a 10-hour interview might have been too much for anyone and I need a reason to talk with them again, right?), here is what they offered:Housing First and foremost (this is me after all), there is housing. Note: Not ""will"" be, but is. Booyah! The ""system is prototyped and working right now."" There are two types of housing that will be available to place in the seamless world; players can plop down a prefabricated structure or build it brick by brick. Did I mention seamless, open world?Factions (and PvP) This game is going to have three distinct factions, each tied to its specific race's lands, and then also a meta faction -- the deserters (who chose not to be a part of any other faction). If players remain within their own faction's lands, they will be safe from any PvP. Whether your playstyle is ""carebear"" all the time or you just want to take a break and chill, this option removes you from any threat of PvP. Deserters, however, will be permanently flagged to everyone -- even other deserters. However, if a player of one faction ventures into the lands of another faction, s/he will be flagged for PvP while the natives won't be. Also, if you choose to take action against a flagged player in your own lands (remember, such players can't touch you first), you will become flagged as well and vulnerable to attack. Basically, if you choose to attack, you have to face the consequences. In a nutshell, players will only engage in PvP by their own choice; PvP will not be thrust upon anyone. Players will be able to switch factions, but they will have to work for it and it will be limited to a degree. In between factions, a player will automatically be a deserter. Permadeath! Oh yes, you read that right. But before the griefers salivate and pass out from fits of maniacal laughter and the rest of us start gnashing our teeth or hyperventilating, let me qualify that statement: Origins of Malu is allowing the system of permadeath, but only in two controlled instances that the player knowingly chooses to participate in. The first condition under which permadeath is possible is in specifically designated zones. Two of these zones will be present in game, and players will be warned before they enter. If a player chooses to enter this area and dies, that character will be dead forever; there will be no way to get this character back. Why enter these zones? Because in essence, where there is high risk, there is high reward. Think of it as high-stakes gambling. While not all players will find this a playstyle they want, I know there are some out there who won't be able to resist the ultimate test of their skill. And the goodies! The second condition is a unique option: an ultimate duel system. If you really hate someone and want to call him out, you can throw down the gauntlet and duel to the permadeath. As so much is on the line, the game makes sure a player knows what is being wagered with prompts, and then the duelists have to travel to a special area. If two powerful people duel, a server-wide announcement is made and others can come spectate. The risk? Your life. The reward? The winner keeps all the spoils of the loser, loot and goods. Whoa! Bounties Another feature that was revealed was a bounty system. There will be ways in game for people to place bounties on the heads of others as well as conditions (such as backing out of a duel) that will automatically place a bounty on a player's head. Deserters, who are expected to have a rougher time at life, will be the only ones who can collect bounties.Economy and crafting Sadly, I wasn't able to finagle many details out of the pair on these matters, but they did give me a general feel when pressed about these important areas: ""The players will drive the economy,"" Michael assured us. He continued, ""We want an organic economy. We have a couple of systems in mind."" The idea is to focus on trade. They are leaning away from a universal auction house system; Dave emphasized that they want trade to be a dynamic system. He stated, ""The whole auction house thing is simple and straightforward, but it kills the community. It's disconnected. You want people to be interacting with each other."" There will be trade communities established and set NPCs that will help meet base level needs. As for crafting, Michael wanted to clarify the idea of crafting spells: Not only will players be able to use crafting to alter the particle effects of the spell, but they will also be able to alter the spell itself. For instance, a DoT can be modified into upfront damage, or percentages of damage can be tinkered with (say 75% upfront and 25% over time). Housing will also be a part of the crafting system as will planting, growing, and harvesting resources. Dyes for customization can be produced as well as weapons. Customization Again, this was an area that I was not able to get too many details on other than the assurance that there will be a lot of customization options for characters, gear, and such. There will be vanity factors (with Dave expressing a definite preference for funny hats!) and sub-slots for fun things, like a head hanging from your belt. As Dave said, the goal is ""to recognize someone when he walks up just because of how he looks."" Michael emphasized, ""We don't want to see clones everywhere."" AI The game's AI was obviously a topic of interest for the devs. There will be different levels of AI in the game. The basic premise for Origins of Malu is that the AI should be fun. Michael stated, ""We want to make it fun, not hard just for the sake of hard."" He explained that they want to make it seem like a living, breathing world. He gave the example of a mob that when startled runs off but looks back to see if it's being followed. If it isn't, then it stops. He also expounded on the stalking mob, explaining that in a situation where you perhaps did not kill a mob or it had friends nearby, the mob will track you until you are vulnerable and then strike. If you are lucky, someone else might just find and kill it first! Scripted boss fights will also be a thing of the past. Some creatures will have the ability to evaluate and learn; the more they are killed, the more they learn not to die that way. In other words, just because you beat it one way one time doesn't mean that strategy will work again. Fights will remain dynamic and resist becoming stale. One fact that I especially liked is that there will not be any level zones. Mobs of all levels of difficulty will be mixed together everywhere. Also, easter eggs will be hidden throughout the game. I gotta find them all! So when can we expect the game? This is a direct quote from the Lead Developer himself: ""A 100% concrete statement is we will release in 2012 as indie. We are targeting early 2012."" Obviously, there will be more to the game than is highlighted here, so look for more screenshots and reveals in the coming weeks! I don't know about you, but my hopes are definitely up there a bit. Good luck guys. I'll be keeping my eye on this game! Every two weeks, Jef Reahard and MJ Guthrie take a break from their themepark day jobs to delve into the world of player-generated content. Comments, suggestions, and coverage ideas are welcome, and Some Assembly Required is always looking for players who'd like to show off their MMO creativity. Contact us!                     "
343,https://www.engadget.com/2011-09-12-ibms-watson-set-to-tackle-health-insurance-takes-diagnosis-fo.html,Engadget,2011,9,12,145.0," After tackling your tech support woes, the famed Watson is moving on to mop up the health insurance industry. That's right, the IBM showstopper we all know and love for trouncing trivia kings on Jeopardy has been hired by one of the largest health insurance company's in the US. WellPoint Inc. will make use of the system's breakneck speed and healthcare database alongside patient records -- allowing the supercomputer to guide treatment options and prescribe medicines. Once implemented, data will be combined from three sources in a matter of seconds: a patient's chart / records from a doctor, the insurance company's patient history and the medical knowledge that Watson already possesses. A pilot program will roll out next year to a number of cancer facilities, academic medical centers and oncology practices. No word yet on when The Watson School of Medicine will start accepting applications.                     "
344,https://www.engadget.com/2011-08-10-origins-website-hints-at-lofty-sandbox-goals.html,Engadget,2011,8,10,170.0," ""Let's face it. MMO companies have found a cookie cutter formula,"" boasts the new Origins website. ""It's time for something new."" That something is a sandbox title shrouded in a fair bit of secrecy but bold enough to make some lofty claims as to its genre-changing design. The title is the brainchild of Burning Dog Media, a Canadian development firm founded by Michael Dunham and Dave Cruikshank. While specific gameplay details are left to our fertile imaginations, the newly revealed website does hint at a feature set downloaded directly from the minds of starving sandbox fans everywhere. No classes? Check. The ability to change the game world? Check. A conspicuous lack of scripted boss fights? Check. And would you believe mob AI that stalks players over a period of several days, waiting for the right time to attack? The world itself seems to be a mixture of fantasy and sci-fi, and the Origins website contains its share of screenshots, concept art, and bestiary info. [Thanks to James for the tip!]                     "
345,https://www.engadget.com/2011-08-08-storybricks-opening-the-pandoras-box-of-mmo-design.html,Engadget,2011,8,8,1208.0," ""I could make a better game than this!"" At one point or another, we've all said this, usually in disgust after we've become fed up with another tired MMO trope or lazy quest design. Unfortunately, most of us don't have the good fortune to work for a major game studio and thus will never see our brilliant ideas come to fruition. Except that this may no longer be true. Enter Namaste Entertainment's Storybricks, a bold and intriguing concept aimed at putting game design in the hands of Joe and Jane Gamer. Namaste is a small startup that began in 2010 when its team members got tired of derivative titles and mechanics in the industry. Storybricks is the team's first project, and while it's still in its infancy, it's already started to capture the imaginations -- and excitement -- of gamers everywhere. At this past week's GenCon, I caught up with Brian ""Psychochild"" Green and the rest of the Namaste crew as they publicly demoed Storybricks to the gaming crowd. Hit the jump as we look at why this program may just be the answer to a question you've never fully asked. You are the storyteller ""User-created (and generated) content"" is one of the hottest buzzwords in the MMO industry right now, and for good reason. MMO players have shown that they're endlessly creative if given tools and a place to use them, and studios have long sought to harness and direct that creativity in such a way that it will fit within the game world without breaking lore, decency or game mechanics. If a game proves itself capable of handling beneficial user-created content, then its future could be assured, although it needs to be done in such a way that devs can allow players to share in the design process without ruining the experience. It's nothing new, of course; virtual worlds like Second Life are built on the backbone of user-created content, and MMOs such as Star Trek Online and City of Heroes have included systems that allow players to design their own missions. But what if there were a way to take it further by empowering players to shape the game world, to tell the story they want to share, and to make it all happen without a comprehensive background in computer programming? This is where Storybricks comes into play. The concept behind Storybricks is right there in the title. ""Story"" is what the team wants to focus on the most, as in ""How do we help players tell the stories they have bubbling in their heads?"" And as for ""bricks,"" once you see the modular, almost LEGO-like design of the user interface, it makes sense. Every element in the game is linked to the others by these bricks, with each link representing a relationship, action, or emotion.Storybricks uses this visual programming language to let you create characters as simple or complex as you like and then send them off into the world to lounge about, go on quests, or give tasks to other players. Do you want to create an NPC guard who hates the player on sight until the player proves himself by rescuing the guard's daughter from certain doom? You can do that. Do you want to invent a royal dowager with a secret past or a thief with a heart of gold who's trying to make right some past wrong? You can do that too. Your imagination just became part of the development team. NPCs: Non-Persona Characters One of the problems that the dev team identified with many MMOs out there is that NPCs are traditionally laughable cardboard cutouts, thick enough to pin a quest or vendor sheet onto but nothing more. After all, how many NPCs do you recall from past games? A handful who got special storytelling treatment, probably. This is why Storybricks' main focus is in encouraging users to imbue NPCs with true life rather than to treat them as quest connect-the-dots. Each character can sport a relationship with every other character in the world and reacts accordingly. Depending on whether certain triggers or conditions are met, the NPC can be programmed to change his or her behavior and actions. It's the flip side of how we often approach MMO gaming; instead of focusing on just one hero -- us -- Storybricks gives us the keys to the whole kingdom. Brian Green says that the team has yet to decide whether you'll be shaping part of an interconnected MMO world or merely creating standalone missions in the same setting (similar to BioWare's Neverwinter Nights). What is settled is that the game world will be fantasy-based with a distinct art style that's easy on the eyes. What I saw at GenCon was a very early build of the program, but it still had a cartoony vibe that gelled well. When my character ran up to an NPC, it was easy to tell when that person hated me or loved me based on body language alone. I was thrilled to meet Liz Danforth at the demo station as well. Danforth is an industry pro whose resume stretches back to include classic PC games like Star Trek: 25th Anniversary and Wasteland, and she said she was impressed enough with what Namaste was doing with Storybricks to allow herself to be called out of semi-retirement and back into action. Right now she's providing the team with concept and promotional art. Brick by brick While showing me the demo, Green pointed out two more fascinating aspects of this toolset. The first is that any actions the player takes with an NPC will persist -- the NPC will remember what the player did to and for him and it will respond accordingly in the future. That right there was enough to sell me on the premise, as I'm forever tired of NPCs acting as if they don't know me after I save their town from certain destruction. The other major idea that's being built into Storybricks is freedom of choice in relation to quests. Users will be able to set up quest goals and objectives, but players will have a much wider range of options as to how to complete those goals. Do you kill the 10 rats for their tails or charm them? Perhaps you could trade a few herbs to another player for some. Or maybe you find a vendor with a nasty rat-tail speciality. Ideally, the possible solutions will be many instead of one. Genius in progress Don't start looking for Storybricks to usher in an MMO revolution anytime soon; the toolset's only been in development for a few months now and has a ways to go. However, Namaste wanted to start getting player and gamer feedback of all types, including non-MMO gamers (hence why the company made an appearance at GenCon). Green says that the company will shape Storybricks based on feedback, and he encourages anyone interested in being a possible tester to pop over to the Namaste website and sign up! We appreciate the time that the Namaste team members spent showing us this demo and discussing the various possibilities for Storybricks. We wish them the best and hope to see good things come out of this project in the future!                     "
346,https://www.engadget.com/2011-07-11-calling-for-tech-support-ibms-watson-might-be-on-the-other-end.html,Engadget,2011,7,11,139.0," Watson may have Jeopardy! and the medical realm under lock, but retail / service industries? Not yet, but soon. Very soon. According to a new piece in Hemispheres Magazine, IBM's now looking to shop the supercomputer's world-class vocal recognition technologies to outfits in retail and customer service, with those enterprises in particular drooling at the thought of having a sophisticated machine recognizing human speech. In theory, at least, basic questions could potentially be answered entirely by Watson, but that's honestly not a future we're too fond of. There's also the possibility of using analytical data that Watson collects in order to better position deals, service and other tech support centers based on what kinds of requests come in the most. So, eager to speak with a kindhearted, potentially confused robot? Or will that flustered, potentially sympathetic Earthling still suffice?                     "
347,https://www.engadget.com/2011-07-06-gods-and-heroes-rome-rising-releases-new-combat-trailer.html,Engadget,2011,7,6,127.0," There's a new Gods & Heroes: Rome Rising video in town, and it's taking no prisoners. The new one-minute clip is centered on the game's combat mechanics, and there's plenty of whirling, slashing, stomping, and even a nifty hey-get-off-of-my-back move to whet your violent appetites. Though the clip is short, there's a good sampling of PvE mobs on display, all of them with a distinctly mythological flavor and all of them waiting to meet the business end of your weapon. You'll also get a glimpse of the game's minions (the Gods & Heroes version of the traditional MMORPG pet mechanic) as well as some nice-looking environments. The game has been selling well since its June launch, and you can check out the new trailer after the cut.                     "
348,https://www.engadget.com/2011-05-23-smarter-elevators-sort-riders-stand-ready-to-enforce-social-hie.html,Engadget,2011,5,23,130.0," While we're still awaiting pneumatic tubes that can whisk us to our destinations, elevators have been gaining a few IQ points. For example, they can be voice-activated or recognize an ID badge and route riders to their floors, meaning fewer seconds staring uncomfortably until the doors open. But they can also track workers' comings and goings, and bosses at Philadelphia's Curtis Center can program elevators to deliver specific employees directly to them. Not coincidentally, intelligent lifts can also ensure executives rarely have to ride alongside the hoi polloi -- a feature Bank of America, for one, paid for but says it doesn't use. The Wall Street Journal seems to worry this is the end of elevator democracy, but we support anything that reduces our time trapped in small metal boxes.                     "
349,https://www.engadget.com/2011-05-11-schizophrenic-computer-may-help-us-understand-similarly-afflicte.html,Engadget,2011,5,11,136.0," Although we usually prefer our computers to be perfect, logical, and psychologically fit, sometimes there's more to be learned from a schizophrenic one. A University of Texas experiment has doomed a computer with dementia praecox, saddling the silicon soul with symptoms that normally only afflict humans. By telling the machine's neural network to treat everything it learned as extremely important, the team hopes to aid clinical research in understanding the schizophrenic brain -- following a popular theory that suggests afflicted patients lose the ability to forget or ignore frivolous information, causing them to make illogical connections and paranoid jumps in reason. Sure enough, the machine lost it, and started spinning wild, delusional stories, eventually claiming responsibility for a terrorist attack. Yikes. We aren't hastening the robot apocalypse if we're programming machines to go mad intentionally, right?                     "
350,https://www.engadget.com/2011-05-03-funcom-teases-new-age-of-conan-group-dungeons.html,Engadget,2011,5,3,158.0," The city of Paikang is getting a bit of an update in the next few weeks, and players of Funcom's Age of Conan should be pleased to note that the Imperial capital will soon be the home of two new level-80 instances. The dungeons are designed for elite groups and take place in two outdoor zones that were recently profiled at MMORPG.com. The Ai District features an expansive playfield along with two ""set-piece"" boss encounters and ""the odd secret to be revealed from careful investigation."" The Tian'an District is quite different in that it features six boss fights as well as a race against the clock to clear them all. Funcom has deliberately elevated the challenge level in this district, and Tian'an will reportedly require a skilled (and coordinated) group to claim its rewards. While Funcom hasn't revealed those rewards in detail, it has hinted that they will be on par with the existing Kara Korum dungeon drops.                     "
351,https://www.engadget.com/2011-04-25-researchers-build-synthetic-synapse-circuit-prosthetic-brains-s.html,Engadget,2011,4,25,641.0," Building a franken-brain has long been a holy grail of sorts for scientists, but now a team of engineering researchers have made what they claim to be a significant breakthrough towards that goal. Alice Parker and Chongwu Zhou of USC used carbon nanotubes to create synthetic synapse circuits that mimic neurons, the basic building blocks of the brain. This could be invaluable to AI research, though the team still hasn't tackled the problem of scope -- our brains are home to 100 billion neurons, each of which has 10,000 synapses. Moreover, these nanotubes are critically lacking in plasticity -- they can't form new connections, produce new neurons, or adapt with age. All told, the scientists say, we're decades away from having fake brains -- or even sections of it -- but if the technology advances as they hope it will, people might one day be able to recover from devastating brain injuries and drive cars smart enough to avert deadly accidents. Show full PR text Researchers create functioning synapse using carbon nanotubes Devices might be used in brain prostheses – or combined into massive network of synthetic neurons to create a synthetic brain Engineering researchers at USC Viterbi have made a significant breakthrough in the use of nanotechnologies for the construction of a synthetic brain. They have built a carbon nanotube synapse circuit whose behavior in tests reproduces the function of a neuron input, the synapse, the a building block of the brain. The team, which was led by Professor Alice Parker and Professor Chongwu Zhou in the Ming Hsieh Department of Electrical Engineering, used an interdisciplinary approach combining circuit design with nanotechnology to address the complex problem of capturing brain function. In a paper published in the proceedings of the Life Science Systems and Applications Workshop in April 2011, the Viterbi team detailed how they were able to use carbon nanotubes to create a synapse. Carbon nanotubes are molecular carbon structures that are extremely small, with a diameter a million times smaller than a pencil point. These nanotubes can be used in electronic circuits, acting as metallic conductors or semiconductors. ""This is a necessary first step in the process,"" said Parker, who began the complex project of looking at the possibility of developing a synthetic brain in 2006. ""We wanted to answer the question: Can you build a circuit that would act like a neuron? The next step is even more complex. How can we build structures out of these circuits that mimic the neuron, and eventually the function of the brain, which has 100 billion neurons and 10,000 synapses?"" Parker emphasized that the fabricated synapse is simplified, the actual development of a synthetic brain is decades away, and she said the next hurdle for the research centers on reproducing brain plasticity in the circuits. The human brain continually produces new neurons and adapts throughout life, and creating this process through analog circuits will be a monumental task, according to Parker. She believes the ongoing research of understanding the process of human intelligence could have long-term implications for everything from developing prosthetic nanotechnology that would heal traumatic brain injuries to developing intelligent, safe cars that would protect drivers in bold new ways. For Jonathan Joshi, a USC Viterbi Ph.D. student who is a co-author of the paper, the interdisciplinary approach to the problem was key to the initial progress. Joshi said that working with Zhou and his group of nanotechnology researchers provided the ideal dynamic of circuit technology and nanotechnology. ""The interdisciplinary approach is the only approach that will lead to a solution. We need more than one type of engineer working on this solution,"" said Joshi. ""We should constantly be in search of new technologies to solve this problem."" ### The research is supported by the National Science Foundation and the Women in Science and Engineering program at USC.                     "
352,https://www.engadget.com/2011-04-07-tera-shows-off-centaur-lore-and-tactics.html,Engadget,2011,4,7,141.0," The official TERA website has updated with a look at one of the mobs players will likely encounter in their journeys around Arborea. Centaurs are found primarily in the Fey Forest region and look to challenge adventurers with spear attacks, hoof attacks, and their ability to sidestep player combos. The new entry also reveals a bit of centaur lore, noting that the legendary creatures are ""presently allied with the fey creatures of northwestern Arcadia province in an uprising against woodcutters, whose drastic logging practices are in danger of denuding the forest in order to fulfill lumber orders from Velika."" Tactically, centaurs are prone to rapidly closing the distance to a target as well as coming to the aid of nearby faeries and other centaurs. Check out all the details, as well as a couple of new screens, at the TERA website.                     "
353,https://www.engadget.com/2011-04-04-darkfall-activity-report-highlights-game-changing-revamps-recen.html,Engadget,2011,4,4,150.0," The March 31st patch was a big step for Darkfall, with everything from skill gain rates to the meditation system to the alignment and crafting systems on the receiving end of some developer love. Aventurine is just warming up, though, and the latest activity report illustrates what's next for sandbox PvP fans all over the world. Work continues on racial area revamps; the Mirdain and Alfar sections of Agon are nearly complete. Aventurine's Tasos Flambouras also hints at some exciting PvE updates, including trap doors and various gameplay physics tweaks designed to delight Agonian dungeoneers. There's also a good bit of info on NPC and monster AI revamps and re-population, new player character and armor models, and a mention of new sound, lighting, and GUI systems, all of which are building toward the game-changing revamp that Aventurine has mentioned previously. Check out all the details on the Darkfall epic blog.                     "
354,https://www.engadget.com/2011-03-23-japans-space-agency-considers-using-rockets-with-artificial-int.html,Engadget,2011,3,23,165.0," The keyword here is obviously ""considers,"" but it looks like Japan's space agency, JAXA, is indeed seriously thinking about using artificial intelligence to improve their rocket launches. As JAXA scientist Yasuhiro Morita explains, as opposed to simply being ""automatic"" as rockets are today, an ""artificially intelligent"" rocket would be able to keep watch on its condition, determine the cause of any malfunction, and potentially even fix it itself. According to JAXA, that would not only make rocket launches more efficient, but more cost-effective as well given the reduced manpower needs. That's not the only new measure being explored to cut costs, though -- as Space.com reports, JAXA's new Epsilon launch vehicle is also being built using fewer, but more advanced components, which promises to let it be moved to the launch pad nearly fully assembled. It's currently set to launch sometime in 2013, although it's not yet clear how much it will actually be relying on AI if such a system is put in place.                     "
355,https://www.engadget.com/2011-03-17-massivelys-exclusive-tera-lore-day-in-the-life-kumas.html,Engadget,2011,3,17,675.0," Yesterday we got a glimpse of two new BAMs courtesy of En Masse Entertainment. Intrepid explorers ranging far and wide across the vast reaches of Arborea will no doubt run across a Naga or a Kumas in their travels, and today En Masse brings us a new Day in the Life lore snippet focused on the latter. Penned by Stacey Jannsen, Day in the Life -- Kumas takes you inside the one-track mind of the titular creature as he attempts to make a meal out of a wayward Popori and finds himself face to face with another (equally hungry) BAM with a similar goal. Head past the cut for more, and don't forget the three new screenshots in our TERA gallery. %Gallery-88481% Day in the Life -- Kumas By Stacey Janssen, Writer Kumases are creatures so defined by hunger that they're not even really alive unless they're hunting or eating-and they eat anything and everything around them, be it plant or animal. Food is the only reason to open one's eyes. Consuming is the only excuse for consciousness, and they rely heavily on their sense of smell to alert them when consciousness is required. Other kumases are either irrelevant or enemies, depending on whether food is present. The scent struck him first. He could always tell by the scent that food was nearby. Kiyo opened his eyes. He waited for movement. The land around him was barren by the very nature of being kumas territory, so movement would stand out. There. A subtle rustling in the distance, then a flash as it ran closer. Kiyo held his spot. His sheer mass kept him from being quick, and he'd long ago learned that being too eager meant the food got away and the Hunger lasted longer-the deep, permeating need that could not be quelled so much as distracted. He remained still. The food drew closer. He could see now that it was a Popori. Scarcely more than a bite, perhaps two, but it would do for now. Kiyo heard another noise. Extra food? If so, the Hunger could be distracted even further. Another kumas seemed to have come out of nowhere and was heading toward the Popori. It was a foolish move -- it would scare off the food -- and a foolhardy one. To menace the prey of another was madness! The enemy waddled closer to Kiyo's meal. There was a risk that neither of them would get the food now, but if it could be preserved, Kiyo would preserve it -- and Kiyo meant to have it. The food seemed to have not noticed them yet. Kiyo ambled toward the other kumas, maneuvering his body between his prey and his rival. The other kumas pretended not to see Kiyo until pretense was impossible. Then it stopped and bared its teeth. Kiyo would not be swayed-he had smelled the popori first. He straightened his spine to be as tall as possible, spread his girth to be as wide as possible, and tensed his muscles to be as dense as possible. He rolled his eyes and gnashed his teeth. His enemy did not back down, but mimicked his actions, motion for motion. Another dominant demon, Kiyo realized. They would have to fight for the food. Kiyo hurled himself at the enemy, silence no longer a concern. He knocked it flat with the sheer force of his bulk. The enemy pummeled Kiyo with huge hands and feet, then rolled out from beneath him just in time to feel Kiyo's head brought up sharply into its stomach. It howled and backed away. Kiyo sniffed. He looked around and sniffed again, but the food was gone. The enemy, was now just another kumas, and suddenly very small. The two growled at each other, then walked their separate ways. Kiyo found a spot on the barren ground and sat. As his eyes closed, his body slowed down, and the Hunger dulled. All was quiet once again. He opened himself to the scent. There was no food, but more would come.                     "
356,https://www.engadget.com/2011-03-14-darkfall-blog-details-pve-tweaks-hints-at-big-pvp-changes.html,Engadget,2011,3,14,189.0," Exciting things are afoot in the world of Agon, and Aventurine's weekly Darkfall activity report is here to fill you in on all the grisly details. Whether you're keen on seeing one of your personal weapon designs imported into the game or curious about the ongoing PvE and AI updates, this week's entry makes for interesting reading. To kick things off, Aventurine's Tasos Flambouras gives us the low-down on grungrocs, and these gnarly boar-like bipeds are bent on ruining every adventurer's day. There's also talk of boss fight tweaks, and Flambouras writes that there are ""significant changes"" coming to high-end PvE content including damage cap adjustments, fight tuning, and loot table changes.Darkfall's PvP is getting some love as well, and new land-based objectives (similar to the game's sea fortresses) are in the works. Flambouras also hints at a new ""Arena"" project and ""a lot more we're doing with PvP"" that will be revealed at a later date. Finally, all Darkfall accounts have been gifted with 25,000 meditation points (the game's offline leveling currency) to celebrate the title's second anniversary. Check out the full presentation on the Darkfall Epic Blog.                     "
357,https://www.engadget.com/2011-01-13-ibm-demonstrates-watson-supercomputer-in-jeopardy-practice-match.html,Engadget,2011,1,13,1508.0," We're at IBM's HQ in upstate NY, where IBM will pit its monstrous Watson project (in the middle buzzer spot) against two Jeopardy greats, Ken Jennings and Brad Rutter. Watson has been in development for four years, and this is its first big public practice match before it goes on national TV in February for three matches against these giants of trivia. Unlike IBM's Deep Blue chess project in the 90s, which was pretty much pure math, Watson has to deal with the natural language and punny nature of real Jeopardy questions. IBM, ever the salesman, has thrown gobs of its fancy server hardware at the project, with 10 racks full of IBM Power 750 servers, stuffed with 15 terabytes of RAM and 2,880 processor operating at a collective 80 teraflops. IBM says it would take one CPU over two hours to answer a typical question, so this massive parallel processing is naturally key -- hopefully fast enough to buzz in before Ken and Brad catch on to the human-oriented questioning. We'll update this post as the match begins, and we'll have some video for you later in the day. 12:04PM Now we're going to get to meet ""Watson."" That's it for the liveblog, we'll have video and more information later on! 12:04PM Alex asks his own question: Why Jeopardy? And answers (how fitting): Jeopardy has always been a technology-forward quiz show. Computer displays, first quiz show in HD. ""And if you need another example of how forward-looking we are, I've been hosting the program for the past 9 years without a mustache."" 12:02PM Ken: So Watson isn't going to push us out an airlock. Alex: Can Watson sing Daisy? 12:01PM Q: Are we close to a HAL 900 scenario. A: That's science fiction. IBM sees Watson more like the computer on Star Trek. 12:01PM Ken and Brad haven't played in five years, they're a little rusty. Watson apparently doesn't feel any pity. 11:58AM IBM is talking about health care again. ""I don't want to be overdramatic, but we can really save lives."" 11:57AM Q: Brad and Ken, is there any part of you that wants Watson to win, for the sake of human progress? A: Ken: Human Progress? Which side are you on?! Brad: This is an aspect of human development that I would live for us not to reach just yet. Ken: Playing Jeopardy is something that I feel special about, personally. IBM: A word of warning, the technology doubles in speed every 18-24 months. 11:55AM Also has an ability to look at answers that it missed and can look at the right answer and learn what it was missing. 11:51AM Q: Does Watson have a recursion process for knowing ""I need to learn more about this category."" A: Watson does have self-assessment. None of the algorithms around natural language are perfect, so there's always error and uncertainty. It knows that it's not perfect at this, so it adjusts its algorithms accordingly. 11:50AM Context and visual clues are also very important for precision. 11:50AM ""Watson can literally read all of the health care text in seconds, but we also want it to be able to read X-Rays and test reports, so a lot of work still to be done."" 11:49AM Q: Is there any chance of putting a Watson-like interface on the web. A: We'll see about that. Right now more into ""vertical"" applications like health care, ""empowering decision makers."" 11:47AM Beyond question answering, there's obviously a lot of strategy involved in a game of Jeopardy and Watson seems well acquainted with it. 11:46AM We're putty in Watson's hands. 11:46AM IBM: Watson does not have emotions, but Watson knows that humans have emotions. 11:46AM Alex brings up the question of psychological games. Ken: Watson can't be psyched out. IBM: That's true, Watson does not have emotions. Ken: He can't love. 11:44AM Brad answers a question about how much they need to know about a question before the buzz in. Brad notes that Watson just did really good at a children's books category, but neither Brad or Watson has children. Ken notes that the retrieval process is trivial for a computer and difficult for a human, while the comprehension process is much easier for Ken than Watson. 11:43AM Alex explains that the order of the clues helps the answerer ""learn"" as they go down through a category. Figuring out the style of the category or something like that. 11:41AM The category is a ""context"" and Watson tries to learn in that context, but the specific category might not be a very good predictor of how well Watson will do. 11:40AM Q: Are there any categories that Watson is better or worse at? (Ken: I'd like to know the answer to that as well!) A: Yes. 11:40AM Alex: ""but after Jeopardy, Watson will be available for sale on eBay."" 11:39AM IBM doesn't have any estimate of how much it's spent on this. 11:39AM On Jeopardy's part, the questions were written just like usual, with the writers having no knowledge of Watson. They then pulled a pool of 30 games randomly to get a set for the practice matches, and merely pulled out the audio and video and picture clues. 11:38AM A lot of Watson's AI is oriented around regular natural language and reasoning stuff, but there's also some custom Jeopardy-specific stuff. 11:37AM Yeah, we're talking Terminator. 11:37AM Q: Humans, do you feel any pressure competing with Watson? A: Brad isn't worried about Watson, but afraid of ""Watson's progeny when they come back from the future to kill me."" Ken says he was warned by a friend, ""Remember John Henry.' Ken's response: ""Screw that, remember John Connor!"" 11:36AM We're still flushed from that match. Wow. 11:35AM This will be part of the IBM portfolio in analytics in the future, but no exact time of when we'll see any of it on the market. Obviously, the hardware is standard stuff that's available in the world from your local IBM rep. IBM ""can't imagine a single industry where there isn't a potential to transform it."" 11:34AM Q: How much of this is based on actual commercial software IBM already has out there? A: ""A lot of new code."" 11:34AM Alex Trebek seems really into this project. He's running the QA and fielding half of them. 11:33AM We're getting some questions answered now in a panel form. Watson is fed the question instantly via plain text, but he has to mechanically press a buzzer, just like those pesky meat bags. 11:30AM Oooh, Alex Trebek is out! 11:30AM There was barely a moment to breath. Not that Watson needs to breath. 11:29AM OK, our first break. Watson is in the lead with $4,400, Ken has $3,400, and Brad has $1,200. 11:28AM Watson is back, up to $3,400 now. We don't even understand these questions, much less know the answers. Er, strike that, reverse it. 11:28AM A third for Ben, but Watson is still in the lead. 11:28AM Ooh, Ken just got the first pre-Watson buzz in on the $2k question on the ""Chicks Dig Me"" topic. Now Ben gets two. Where you at, Watson? 11:27AM Ken kicks it off... and Watson just got the first two questions right. It's all a blur. Third question right. HE'S DESTROYING. Four now. 11:26AM And here we go! 11:26AM Brad is out, a destroyer of worlds at Jeopardy. And now Ken, a destroyer of universes. They've already played a bit of practice with Watson this morning, and now they're here for a practice round. 11:24AM Harry Friedman, executive producer of Jeopardy, is up to thank everybody. Proud to be a part of this ""adventure."" Ken and Brad are giving 50% of their winnings to charity, while Watson will be giving away all of its winnings. Who's the moral actor now, huh? 11:21AM Get ready for Watson to start answering the tech support helpline. 11:20AM Watson now averages 3 seconds per answer. 11:19AM IBM calls its tech ""DeepQA,"" a ""massively parallel probabilistic evidence-based architecture."" 11:16AM Challenges: broad domain, complex language, high precision, accurate confidence, high speed. 11:15AM Dr. David Ferrucci, lead researcher on the project, is up talking a bit more about how Watson actually works. 11:14AM IBM would like to encourage us to think about the applications beyond Jeopardy, like health care. Really doesn't take much imagination to see that this is a big deal, to be honest. 11:10AM There's clearly a quasi-religious atmosphere here, that the Watson project was only something you could succeed at if you really believed it was possible -- and many didn't. 11:09AM The history of the project includes some pretty hilariously bad answers. In 2007 an ""I Love Lucy"" question resulted in the answer ""Song."" 11:08AM IBM has managed to beat Jeopardy winners before, but beating world champions is a huge undertaking, and IBM itself sounds pretty up in the air as to how well Watson will do. 11:07AM IBM calls Watson one of its ""grand challenges."" An advance of ""miles"" in an artificial intelligence industry that typically advances by ""inches.""                     "
358,https://www.engadget.com/2010-12-22-quadrocopter-plays-the-piano-wishes-us-a-happy-and-complacent-h.html,Engadget,2010,12,22,96.0," Our worst frienemies, the quadrocopters, have decided to act cute for the holidays and play us a merry little jingle. Yes, the guys and gals behind the Flying Machine Arena have put together an airborne robot sophisticated enough to lay down a few seasonal notes on a Yamaha electronic keyboard. And we're still sitting around debating inconsequential topics like net neutrality -- all of human civilization is at stake here, people! Be a good citizen and watch the video after the break to scout out any weak points to this most imminent threat to humanity's survival.                     "
359,https://www.engadget.com/2010-11-11-robo-nurse-gives-gentle-bed-baths-keeps-its-laser-eye-on-you-v.html,Engadget,2010,11,11,115.0," When they're not too busy building creepy little humanoids or lizard-like sand swimmers, researchers at the Georgia Institute of Technology like to concern themselves with helping make healthcare easier. To that end, they've constructed the Cody robot you see above, which has recently been demonstrated successfully wiping away ""debris"" from a human subject. The goal is simple enough to understand -- aiding the elderly and infirm in keeping up their personal hygiene -- but we'd still struggle to hand over responsibility for granny's care to an autonomous machine equipped with a camera and laser in the place where a head might, or ought to, be. See Cody cleaning up its designer's extremities after the break.                     "
360,https://www.engadget.com/2010-11-03-dota-2-faq-has-icefrog-talking-ai-bots-replays-and-more.html,Engadget,2010,11,3,266.0," Icefrog himself has answered a set of frequently asked questions on Dota 2's official website, clarifying and detailing a few concerns players have had since the game's official announcement. Much of the questions revolve around players leaving games -- Valve has not only implemented an AI system to take over for missing players, but players will be able to have a ""friend of similar skill level"" jump in and take over if necessary, or simply jump into current games that are missing players. The bots will also be customizable, so if you want to play against an AI that tests a specific part of your game, like ""denying, last hitting, and harassing,"" there will be specific configurations to challenge those. Replays are also mentioned -- players will be able to examine games in progress from all sorts of angles, watching charts and graphs in real time as the game goes on. There will also be ""commentating and editing"" features for replays, and the game will make extensive use of the Steam Cloud, keeping keyboard preferences, replay files and other settings available from any computer. Icefrog also says that there won't be any changes for the sake of change in the actual gameplay, and that he will continue development on the original Warcraft 3 mod ""for as long as the community wants,"" but that Dota 2 ""represents the long term future for the game."" In that sense, Dota 2 sounds like a very direct translation of the original game, though Valve is sparing no expense in adding metagame features to make it a smoother experience for everyone.                     "
361,https://www.engadget.com/2010-10-12-nell-machine-learning-system-could-easily-beat-you-at-trivial-pu.html,Engadget,2010,10,12,247.0," If fifteen years ago you would have told us that some day, deep in the bowels of Carnegie Mellon University, a supercomputer cluster would scan hundreds of millions of Web pages, examine text patterns, and teach itself about the Ramones, we might have believed you -- we were into some far-out stuff back then. But this project is about more than the make of Johnny's guitar (Mosrite) or the name of the original drummer (Tommy). NELL, or Never-Ending Language Learning system, constantly surfs the Web and classifies everything it scans into specific categories (such as cities, universities, and musicians) and relations. One example The New York Times cites: Peyton Manning is a football player (category). The Indianapolis Colts is a football team (category). By scanning text patterns, NELL can infer with a high probability that Peyton Manning plays for the Indianapolis Colts - even if it has never read that Mr. Manning plays for the Colts. But sports and music factoids aside, the system is not without its flaws. For instance, when Internet cookies were categorized as baked goods, ""[i]t started this whole avalanche of mistakes,"" according to researcher Tom M. Mitchell. Apparently, NELL soon ""learned"" that one could delete pastries (the mere thought of which is sure to give us night terrors for quite some time). Luckily, human operators stepped in and corrected the thing, and now it's back on course, accumulating data and giving researchers insights that might someday lead to a true semantic web.                     "
362,https://www.engadget.com/2010-10-09-google-and-tu-braunschweig-independently-develop-self-driving-ca.html,Engadget,2010,10,9,233.0," There's a Toyota Prius in California, and a VW Passat halfway around the globe -- each equipped with bucket-shaped contraptions that let the cars drive themselves. Following their research on autonomous autos in the DARPA Urban Challenge, a team at Germany's TU Braunschweig let the above GPS, laser and sensor-guided Volkswagen wander down the streets of Brunswick unassisted late last week, and today Google revealed that it's secretly tested seven similar vehicles by the folks who won that same competition. CMU and Stanford engineers have designed a programmable package that can drive at the speed limit on regular streets and merge into highway traffic, stop at red lights and stop signs and automatically react to hazards -- much like the German vehicle -- except Google says its seven autos have already gone 1,000 unassisted miles. That's still a drop in the bucket, of course, compared to the efforts it will take to bring the technology home -- Google estimates self-driving vehicles are at least eight years down the road. Watch the TU Braunschweig vehicle in action after the break.Update: Though Google's cars have driven 1,000 miles fully autonomously, that's a small fraction of the time they've spent steering for themselves. We've learned the vehicles have gone 140,000 miles with occasional human interventions, which were often a matter of procedure rather than a compelling need for their human drivers to take control. [Thanks, el3ktro]                     "
363,https://www.engadget.com/2010-10-05-fujitsus-social-robot-bear-is-the-supertoy-of-kubricks-dreams.html,Engadget,2010,10,5,168.0," Ah, the Fujitsu bear cub social robot. What child or elderly person should go without a ""people-friendly terminal"" with snuggly-soft fur and a camera in lieu of the ever-popular button nose? The little guy made the rounds briefly earlier this year, but this is our first time making acquaintances. The duo waved at us, waved with us, laughed with (at?) us, and then at some point, decided to arbitrarily fall asleep and ignore us -- just like our actual friends! The representative told us this prototype -- with a reported 300 actions from 12 actuators (three face, three neck, and six in the body), 13 skin touch sensors, hand grip sensors, a tilt sensor, and a microphone -- is a ways off from hitting the childcare / nursing home market. For now, enjoy the pictures below and footage after the break... and if all this seems eerily familiar, hey, we're with you. A wink and a nod in that direction can be found via the second video. %Gallery-104247%                     "
364,https://www.engadget.com/2010-09-20-aventurine-expanding-darkfall-development-team.html,Engadget,2010,9,20,221.0," As much as many in the MMO community love to hate on Darkfall, the game soldiers on, and if recent reports coming out of Aventurine are any indication, it's actually growing a little bit. Producer Tasos Flambouras recently posted a lengthy development update on the game's official forums. In it, he discusses the ongoing server issues before moving on to more interesting news bits like the upcoming expansion, lore updates, and tweaks to various smaller aspects of the game's operation such as the online help and download sections of the website.Aventurine's main task these last few weeks has been polishing the new expansion, and Flambouras has lots to say on the matter. The team has redesigned many of the new dungeons, added new quests, monsters, and rewards, and continued working on terrain improvements. ""I have to repeat what a monumental task this has been: the entire world has been repainted, all textures have been redone from scratch, there are no texture seams anymore or stretching,"" Flambouras writes. Finally, Aventurine looks to be expanding not just the world of Agon but the team constructing it. In addition to welcoming back a former member of the world-building team, Aventurine has hired two new programmers and one designer, and is currently interviewing more programmers in order to bring its future Darkfall plans to fruition.                     "
365,https://www.engadget.com/2010-09-09-bungies-halo-ai-secrets-exposed-in-technical-review.html,Engadget,2010,9,9,158.0," If you're looking for information about a hot new gun that will be shootable in the almost-here Halo: Reach, you might want to flip to one of the many other stories we've written about the game. If you want super esoteric insight into the high-functioning AI design of the series -- that is to say, the enemies' actual programmed intelligence, and not the game's fictional AI constructs -- we suggest checking out AI Game Dev's 42-point analysis on Bungie's tricks and innovations for making those pesky Elites so darn pesky. Of course, these tips will really only benefit you if you happen to be a game designer or programmer, though we suppose you could hypothetically use the information to impress your friends while playing Reach next week. ""Look at the way Bungie implemented those perceptual information objects,"" you might say to your awestruck cohorts. ""Man, that really must have cut down on some memory fragmentation and recycling problems.""                     "
366,https://www.engadget.com/2010-09-07-swiss-researchers-show-off-brain-controlled-ai-augmented-wheelc.html,Engadget,2010,9,7,157.0," They're far from the first to try their hand at a brain-controlled wheelchair, but some researchers at the École polytechnique fédérale de Lausanne (or EPFL) in Switzerland seem to have pulled off a few new tricks with their latest project. Like some similar systems, this one relies on EEG readings to detect specific brain patterns, but it backs that up with some artificial intelligence that the researchers say allows for ""shared control"" of the wheelchair. That latter component is aided by a pair of cameras and some image processing software that allows the wheelchair to avoid obstacles, but it doesn't stop there -- the software is also able to distinguish between different types of objects. According to the researchers, that could let it go around a cabinet but pull up underneath a desk, for instance, or potentially even recognize the person's own desk and avoid others. Head on past the break to check it out in action.                     "
367,https://www.engadget.com/2010-09-01-super-scribblenauts-features-a-reprogrammed-maxwell.html,Engadget,2010,9,1,306.0," Upon playing Super Scribblenauts, it becomes readily apparent why you'd want to use the D-pad to manipulate the game's hero, Maxwell. So why wasn't that an option in the first game? Joystiq asked 5TH Cell Creative Director Jeremiah Slaczka, and were told that the original touch-screen input didn't draw any complaints -- at least not from casual players. ""We haven't gotten any emails from casual users,"" he told us this week, ""that are like, 'Hey, I don't like the controls.' It's the hardcore users that are like, 'I play Mario all of the time, and I'm a hardcore gamer, and I'm used to these kinds of controls, and what's what I want."" Though controlling Maxwell directly with the D-pad may seem an obvious choice, it wasn't the first time around. ""Maxwell's actually an AI,"" Slaczka said. ""So it wasn't just like throw the D-pad controls in."" Maxwell was programmed to respond to the rest of the game's systems rather than just follow button directions, and so it didn't occur to the developers to control him directly. ""You'd have to overwrite all of the code that we'd built up for him. So in the second one, we basically did that."" The team ""stripped out"" all of the behaviors and responses that had been coded, and created the option for ""one-to-one player control."" For his own part, Slaczka says he doesn't care which option players use, but he'll stick with the stylus. ""This isn't a platforming game,"" he said. ""This is a puzzle game. Unfortunately, it has a platforming-type feel to it. So we said for the second one, that's fine, we'll address it, we'll give you both. And we actually fixed up the stylus controls, too, so Maxwell doesn't run away as you tap -- when you let go he stops. So we made it way better.""                     "
368,https://www.engadget.com/2010-08-14-prototype-of-robot-that-develops-emotions-on-interacting-with-hu.html,Engadget,2010,8,14,184.0," The first prototype of a Nao robot that can develop emotions as it interacts with a human caregiver has been completed. A team across Europe was led by Dr. Lola Cañamero of the University of Herefordshire in the UK to develop the bot, which differs in several significant ways from those that came before it. These robots develop over time in much the way that a child does, learning to interact with and respond to the human beings around them. Modeled after human and chimpanzee childhood development paths, they are programmed to be highly adaptable to the people around them, and to become attached to whatever person is most suited to its needs and 'personality' profile. Over time, the more they interact, the more they learn and bond to the human being. These little ones, moreover, are capable of expressing a wide range of emotions, including anger, frustration, fear and happiness. The next steps are to research the bots' emotional and non-linguistic behavior, and to move toward combining linguistic and non-linguistic communication to become further attached and adapted to them. Yes, we want one.                     "
369,https://www.engadget.com/2010-07-23-driverless-vans-set-off-on-intercontinental-trek-from-italy-to-c.html,Engadget,2010,7,23,149.0," You might not have expected the future to look like your granddad's groovy camper van, but take a closer look here and you'll find that this is indeed nothing like your forefather's people carrier. The VisLab team from the University of Parma have taken a fleet of Piaggio Porter Electric vehicles, strapped them with an array of cameras, lasers and other sensors, and topped them off with solar panels to keep the electronics powered. Oh, and lest we forgot to mention: the vans are (mostly) autonomous. VIAC (or VisLab Intercontinental Autonomous Challenge) is the grand name given to their big demonstration: an 8,000-mile, 3-month tour that will ultimately find them arriving in Shanghai, China, having set off from Milan this Tuesday. You can follow the day-by-day development on the blog below, though we're still being told that practical driverless road cars are a measure of decades, not years, away.                     "
370,https://www.engadget.com/2010-07-21-darkfall-details-expansion-mob-ai.html,Engadget,2010,7,21,165.0," One of the things that makes Darkfall unique is its mob and NPC AI. Unlike traditional MMORPGs, the creatures in the world of Agon use the same skills that players use and also drop appropriate loot (for example, killing a goblin will likely net you his sword and armor, and boars don't drop coin). With the upcoming free expansion, the developers at Aventurine are further tweaking the game's PvE elements by adding additional monster skills to specific creatures. Laenih, Aventurine's community liaison, has all the details in a new post on the game's official forums. ""Some existing monsters as well as new ones have already received their own abilities which we'll leave for you to discover. Additionally there will be more strategic interaction between monsters that spawn together,"" he writes. You can get a glimpse of one of the new monster teams in the form of a video that showcases the Gorra Dar and Gorra Slaves. Check it out after the cut or on YouTube.                     "
371,https://www.engadget.com/2010-07-05-intel-connected-cars-will-record-your-bad-driving-for-posterity.html,Engadget,2010,7,5,137.0," Intel's latest Research Day has sprung up a new vision for ""smart"" vehicles; a vision that frankly chills us to our very geeky core. Cameras and sensors attached to an Intel Connected Car will record data about your speed, steering and braking, and upon the event of an accident, forward those bits and bytes along to the police and your insurance company. Just makes you feel all warm and fuzzy inside, doesn't it? Don't get us wrong, the tech foundation here is good -- having cars permanently hooked up to the ether can generally be considered a good thing -- but what's being envisioned is as obtrusive as it is irritating. Oh, didn't we mention that the cars can become self-aware and overrule you if you try to bend the rules of the road? Because they can.                     "
372,https://www.engadget.com/2010-06-19-muon-the-humanoid-robot-is-our-ideal-best-friend.html,Engadget,2010,6,19,168.0," We don't speak German, and machine translation continues to be an intermittent and annoying bundle of failure, so bear with us on this one as we try to cobble together what exactly is going on here. This is Muon, the humanoid robot who is apparently being developed in Berlin by Frackenpohl Poulheim at the ALEAR Laboratory of Neuro Robotics at the Humboldt University in Berlin. Like other humanoid bots, Muon is about the size of an eight year old child so as not to creep out his human companions by being too threatening, and his design, while reminiscent of previous robots we've seen, is pretty original. It's actually hard to tell what stage of development Muon is in -- certainly many of the photos we have spied were concepts -- but we're going to keep our eyes peeled for him moving into the future. If you hit up the source link, you can check out a video of Muon's development. There's one more amazing shot after the break.                     "
373,https://www.engadget.com/2010-06-17-ibms-watson-is-really-smart-will-try-to-prove-it-on-jeopardy.html,Engadget,2010,6,17,192.0," As much as we love our Google homepage, computer search remains a pretty rudimentary affair. You punch in keywords and you get only indirect answers in the form of relevant web results. IBM doesn't seem to be too happy with this situation and has been working for the past three years on perfecting its Watson supercomputer: an array of server racks that's been endowed with linguistic algorithms allowing it to not only recognize oddly phrased or implicative questions, but to answer them in kind, with direct and accurate responses. Stuffed with encyclopedic knowledge of the world around it, it answers on the basis of information stored within its data banks, though obviously you won't be able to tap into it any time soon for help with your homework. The latest word is that Watson's lab tests have impressed the producers of Jeopardy! enough to have the bot participate in a televised episode of the show. That could happen as early as this fall, which fits right in line with our scheduled doom at robots' hands by the end of 2012. Ah well, might as well get our popcorn and enjoy the show.                     "
374,https://www.engadget.com/2010-05-25-tchess-pro-for-ipad-is-very-very-good.html,Engadget,2010,5,25,476.0," I love to play chess and was looking for something that worked well on the iPad. The large screen and crisp graphics make board games on the iPad an attractive proposition.  Based on some reviews, I installed tChess Pro for US $7.99. I wasn't disappointed. While you can use the app to let two people play, I was more interested in playing against the computer. tChess Pro is a very strong opponent, but you can modify the levels that the game plays. It was the first time in a computer based chess game that I felt like I was playing a human, although at most levels except the lowest, I was getting clobbered. The app allows you to set up games, add time controls, and there is an opening library mode that helps you understand the finer points of the game. You can take back moves, get a running analysis of your game, and get hints for the next move if you want them. Other impressive features include the ability to email a game to someone, and you can choose from multiple board and chess piece designs. I especially liked the feature that allows you to use two fingers to rotate the board to any angle. The game also contains built-in chess instructions; they are well written and a great place to start for beginners. It was the kind of thoughtful touch that makes the app rather special. Some chess playing friends also praise Shredder Chess, which I haven't played but am going to look at soon. It has a similar feature set and is the same price. I have also enjoyed Deep Green for the iPhone, but it is not available in an iPad version yet. One thing I didn't like is that the app icon displays semi-transparent chess pieces that aren't available as an option. I would have liked that set, and I think it was kind of misleading to showcase it in the icon. I wrote the program author, Tom Kerrigan, about that, and he told me that he tried to work out how to make such a set; the problem was that it would be so low contrast that it would basically be impossible to play on. That may be true. It looks fine on the icon, and I'd like to see it added to the game. That's a small complaint for what really is an excellent chess opponent. The game is a universal binary, so it will also work fine on your iPhone or iPod touch. There is another version of the app called tChess Lite. It is only US $0.99, and it is more for the casual enthusiast and doesn't play at the highest levels. If you are interested in chess and just getting started, it might be a good place to begin.  Here are a few screen captures: %Gallery-93617%                     "
375,https://www.engadget.com/2010-05-17-ipad-takes-to-the-skies-with-bluebox-ai-this-july.html,Engadget,2010,5,17,99.0," In-flight entertainment might not be high on everyone's list of priorities, but from this July, it might become a new point of differentiation between airlines. Bluebox Avionics has announced its new Ai IFE system, which seems to mostly involve just giving travelers an iPad to play around with while gliding through the atmosphere. It ""leverages the power, flexibility and quality of the most advanced consumer device ever produced"" (they have an Evo 4G? Zing!) and offers Bluebox's proprietary security solution and tailor-made apps for each airline. One international carrier has already signed up and more are expected to follow.                     "
376,https://www.engadget.com/2010-04-22-aila-bot-can-recognize-objects-weight-and-fragility-render-she.html,Engadget,2010,4,22,130.0," Now, this isn't quite the height of innovation, but it's a pretty cool compilation of existing technologies nonetheless. The femme-themed AILA robot has an RFID reader in its left palm, which allows it to obtain non-visual information about the objects put in front of it. Based on that input, as well as data collected from its 3D camera and two laser scanners, AILA can intelligently deal with and transport all sorts of items, without the pesky need for a fleshy human to come along and give it further instructions. The good news is that it's a really slow mover for now, so if you do your cardio you should be able to run away from one in case of any instruction set malfunctions. See it on video after the break.                     "
377,https://www.engadget.com/2010-04-06-former-apple-store-employee-creates-iron-mans-j-a-r-v-i-s-usin.html,Engadget,2010,4,6,221.0," Okay, there's no HUD display like Tony Stark had and it isn't voiced by Paul Bettany, but former Apple Store employee Chad Barraford has created Project Jarvis, a digital assistant that greets him, Tweets for him, and can even tell his family when he has a headache and dim the lights of his apartment before he reaches home. Project Jarvis is based on the comic book character Edwin Jarvis, Tony Stark's human butler who became an AI construct after he was reinvisioned for a twenty-first century audience in the first Iron Man film. Chad's real life Jarvis may not help him fly an invisible suit of armor, but via RFID tags, webcams, and microphones, Barraford can communicate with Jarvis in a number of ways including tweeting, instant messaging, and speech recognition which allows him to control lights and appliances, notify him of breaking news, Facebook updates, Netflix queues, check stock quotes and weather, and even help assist him with cooking.  Barraford calls Jarvis a digital life assistant (DLA) and runs it entirely from a four year-old Mac mini running custom AppleScript, he told us. Right now he has no plans to sell the AppleScript code, but is always happy to share ideas with other developers of DLAs. Click on over to The Boston Globe to see video of Jarvis in action.                     "
378,https://www.engadget.com/2010-03-14-sid-meier-talks-player-psychology-and-the-year-of-civilization.html,Engadget,2010,3,14,446.0," The ""father of computer gaming"" gave the keynote at GDC 2010 this past week, and while we really hoped he would tell us a lot about the upcoming Facebook version of Civilization, it got only the barest of mentions during the hour-plus talk. Instead, Meier shared wisdom with the gathered crowd, talking about the lessons he'd learned in player psychology over his long and storied career in game design. First, he talked about what he called the ""Winner's Paradox"" -- ""if you've played Civilization,"" he said, ""you're an egomaniac,"" since anyone crazy enough to think that they can actually ""build a civ to stand the test of time,"" as it says on the game box, must be pretty full of themselves. And because of that, Meier says his players always believe that if they don't win for whatever reason, fate or the random number generator or the crappy AI must be out to get them. As a result, his policy has become to let the player win -- the threat of punishment is enough to keep it interesting, but in the end, the player should win the game. He also talked about the ""unholy alliance"" between players and developers -- not only is the relationship beneficial for both parties (players offer their money, developers offer their time and talent), but it's also one of ""mutually-assured destruction,"" as players can break contact with (or even just belief in) the game anytime they feel it's not fun any more, and developers can ""really mess up the game, too."" Everything in the game, said Meier, should be designed with an eye towards this alliance -- the AI should live to serve the player, the graphics and gameplay should engage imagination, and even options screens and load/save settings should be developed with an eye towards preserving the relationship.Civilization Network was mentioned under a section Meier called ""my bad"" -- along with the original ideas to make Civ real-time (whoops) and make the tech path random, he said that the CN team had considered letting players give gold to each other on Facebook, but during playtesting, found that players never actually did. He did say that the game is deep into testing currently, and that it will allow co-op, singleplayer, and competitive gameplay, and that it will be interesting whether players play for just ""a little time a day"" or more than that. At the end of the talk, in reply to a question about where he saw gaming going, Meier declared that ""this is the year of Civilization!"" With CN coming soon and Civ V due out this fall, we can't wait to send our Settlers out into the world.                     "
379,https://www.engadget.com/2010-02-21-perfect-world-entertainment-talks-about-delay-on-battle-of-the-i.html,Engadget,2010,2,21,810.0," We recently brought you the news that the closed beta for Battle of the Immortals has been delayed until mid-April. Several reasons were cited, including details such as Windows 7 support, revising the item shop and the AI behavior, and so on. The Perfect World Entertainment team took some time to expand on those reasons for us this week. Battle of the Immortals is the first game to be published out of Perfect World Entertainment's new Shangai studio. With a different studio, graphics engine, and design, players can expect a game with a completely different feel than previous PWE offerings. While this can mean great things -- such as a revised tutorial system that offers brief flash videos rather than plain text and an auto-navigation feature to guide you to the current monster you're hunting for a quest -- it can also introduce some complications. Shortly before the planned February 10 closed beta was to begin, the PWE team headed to Shanghai to meet with the development team for a series of discussions about Battle of the Immortals that turned out to be very informative. Informative enough, in fact, to send the development team willingly back to the drawing board with plans to rework the game and make it more friendly to North American players. ""The meetings that we had were really about conveying a lot of cultural differences between North America and China,"" says Jon Belliss,Product Manager for Battle of the Immortals and Perfect World International. Technical issues aside, the cultural differences made themselves known strongly in the gameplay design. Battle of the Immortals is the first game to be sent to North America from the Shanghai studio, and the development team naturally designed the game along the lines of what they knew: Asian gaming preferences. The development team explained to Belliss, ""Chinese players like this game to be very easy [...], they just want to hang out with their friends and use it as a virtual chatroom."" This philosophy led to a game full of monsters with no AI and no willingness to aggro unless they were attacked. Even when fighting after players attacked them, damage was very minimal, and players needed to grind, killing the creatures over and over to get anywhere.  ""Soul Gear"" -- high level gear in BoI that changes and evolves as your character does -- was given the same treatment. ""In China, the only way to attain Soul Gear was to get into large groups of people [...] 40 people or more, do an instance at only a certain time during the week, and compete against other guilds, and it's very hard to get access to this armor."" The cash shop was the final issue in the game. Item shops are a tricky feature at best in North America, with game developers searching for the middle ground between keeping their players happy and making the best profit they can. As the BoI cash stop stood, the guy with the biggest credit card wins. The player that spends money is the player that gets the powerful gear, enabling him to essentially destroy the player who doesn't spend money. Again, this is standard procedure in the Asian market, and the Shanghai development team viewed the suggested changes as PWE giving up their profit. As we all know, most North American free-to-play gamers -- if they visit the item shop -- will happily spend money on items that are vanity or shortcut based, but will not respond well to power-based items that are only attainable through a cash purchase. In the end, the Shanghai team gained a much better understanding of the Western gaming mentality and were very willing to work to make the changes needed. ""...the development team is incredibly receptive to all of the feedback that we've been giving them, and as a result they've been tweaking and changing the client and game in order to address those."" Monster AI and damage will be improved. Soul Gear is still appropriately difficult to obtain, but will be available as a drop in several areas outside of limited, lengthy 40+-man raids. Players will find the item shop much more suited to their tastes, and many behind-the-scenes technical details such as monitor resolution will be improved as well. The necessary changes will obviously take time to implement, but the teams on both sides of the ocean agree that it will be worth the wait. As they've pointed out before, ""A late game is only late once. A bad game is a bad game forever."" While there is not a specific date available at this time, look for a closed beta to arrive in mid-April. In the meantime, check out the video at the end for a closer look at Soul Gear in Battle of the Immortals, and we'll keep a close eye out for more beta information.                     "
380,https://www.engadget.com/2010-02-15-aikon-2-robot-sketches-the-human-face-uses-its-talent-to-meet-g.html,Engadget,2010,2,15,185.0," Yes, that's exactly what it looks like -- a robot that can look at a human face and make a pretty reasonable sketch of it. Featured at London's Kinetica art fair last week, the Aikon 2 project boasts an ""inexpensive"" robot arm and software developed by a research team at Goldsmiths University of London. As you might have guessed, building a device with rudimentary artistic ability is no mean feat -- leading the developers to try and understand and simulate the processes by which artists sketch the human face, including: visual perception of the subject and the sketch, drawing gestures, cognitive activity, reasoning, and the influence of training. The project's website emphasizes that ""due to knowledge and technological limitations the implementation of each process will remain coarse and approximate."" In other words, the robot ""is expected to draw in its own style."" Which is, quite frankly, better than we can do. We look forward to seeing these things in the cafes of the future, where robots not only fetch us drinks but chat up girls with offers to draw their portraits. Video after the break.                     "
381,https://www.engadget.com/2010-02-06-left-4-dead-2-gets-bots-and-sdk-fixes-in-update-360-patch-comin.html,Engadget,2010,2,6,357.0," Valve has followed through with its promise to bring bots to Left 4 Dead 2's Versus mode -- which was frantic enough before all the A.I.-controlled ghouls got added in. A recent patch added the functionality and made a few tweaks to the software development kit, which allows players to make awesome levels based on awesome Nintendo 64 games. Check out the full list of changes after the jump.The Steam blog post announcing the patch's release promises that the changes will be integrated into the 360 version of the game via an upcoming title update. Gameplay Now ghost PZ's can never be staggered Enabled PZ bots PZ bots use their more aggressive survival mode behavior in versus and scavenge Enabled finale manual spawn by default Fixed human PZ players not getting counted when they were dead, allowing extra PZ bots to spawn Reduced effectiveness of melee weapons against the Tank ( 5% of max health, from 10% ) Fixed a case where a player who dies, then is rescued from a rescue closet, and then is revived from a ledge hang receives the secondary weapon that he lost when he was initially killed Improved SurvivorBot AI in cases where the bots refused to shoot through their teammates and became non-responsive Fixed issue with witch sometimes taking on uncommon common properties Fix for cheaters quitting before the vote to kick them would succeed and avoiding ban on server VAC banned users can now play L4D2 single-player, commentary mode and credits Map Fixes Dark Carnival Fair Ground: added env_player_blocker to keep special infected from getting stuck Dark Carnival Fair Ground: added navigation area that was missing Dark Carnival Fair Ground: Disconnected drops that were blocked by the top of a fence Swamp Fever Plank Country: Deleted 2 nav areas that connected between tree trunks you can't pass through Swamp Fever Shanty Town: Redrew ladder thinner so special infected could climb up to the roof The Parish Waterfront: Fixed various navigation areas Mall: Tuned spawning for difficulty SDK Content Added missing nature/blend tooltextures added Added missing cs models/textures Added missing acunit01 model/texture Added missing ammo_can_02 model/texture Added missing patio_chair model/texture                     "
382,https://www.engadget.com/2009-12-22-valve-beta-testing-ai-opponents-in-team-fortress-2.html,Engadget,2009,12,22,167.0," If there's going to be a singular battlefield for the war between mankind and a super-advanced artificial intelligence hivemind, we think it's going to be Team Fortress 2. See, the programmers and developers over at Valve have repeatedly proven their technological prowess -- and now that they're working on AI opponents for TF2, accidental self-awareness can't be too far away. ""Spy's sappin' my sentry! But why is it my sentry? Can a virtual sentry be owned? Who am I?"" You can check out the early stages of the future destruction of the world by participating in the Team Fortress 2 Bot Beta Test. No download is required -- all you have to do is enter a simple line of code into the console command in the koth_viaduct, koth_sawmill, and koth_nucleus maps. Check out Valve's blog post to learn more about the AI-spawning commands available to you. Whatever you do, make sure you don't enter ""haley_joel_osment"" into the command line. His love is real, but he is not.                     "
383,https://www.engadget.com/2009-11-18-ibm-simulates-cats-brain-humans-are-next.html,Engadget,2009,11,18,164.0," Almost exactly a year ago we noted DARPA pouring nearly $5 million into an IBM project to develop a computer capable of emulating the brain of a living creature. Having already modeled half of a mouse's brain, the researchers were at that time heading toward the more ambitious territory of feline intelligence, and today we can report on how far that cash injection and extra twelve months have gotten us. The first big announcement is that they have indeed succeeded in producing a computer simulation on par, in terms of complexity and scale, with a cat's brain. The second, perhaps more important, is that ""jaw-dropping"" progress has been made in the sophistication and detail level of human brain mapping. The reverse engineering of the brain is hoped to bring about new ways for building computers that mimic natural brain structures, an endeavor collectively termed as ""cognitive computing."" Read link will reveal more, and you can make your own cyborg jokes in the comments below.                     "
384,https://www.engadget.com/2009-10-30-mits-affective-intelligent-driving-agent-is-kitt-and-clippys-l.html,Engadget,2009,10,30,153.0," If we've said it once, we've said it a thousand times, stop trying to make robots into ""friendly companions!"" MIT must have some hubris stuck in its ears, as its labs are back at it with what looks like Clippy gone 3D, with an extra dash of Knight Rider-inspired personality. What we're talking about here is a dashboard-mounted AI system that collects environmental data, such as local events, traffic and gas stations, and combines it with a careful analysis of your driving habits and style to make helpful suggestions and note points of interest. By careful analysis we mean it snoops on your every move, and by helpful suggestions we mean it probably nags you to death (its own death). Then again, the thing's been designed to communicate with those big Audi eyes, making even our hardened hearts warm just a little. Video after the break.  %Gallery-76874% See more video at our hub!                     "
385,https://www.engadget.com/2009-10-26-evigroups-pad-is-a-10-inch-3g-tablet-with-personality.html,Engadget,2009,10,26,145.0," Time to freshen up the old netbook market with a dash of Windows 7, a pinch of touchscreen functionality, and a generous helping of... Seline10? eviGroup, the crew responsible for the attractive 5-inch Wallet MID, has announced the 10.2-inch Pad, whose pièce de résistance is the Seline10 artificial intelligence software that's been in development for a decade, if you can believe it. Its purpose is to act as your secretary / assistant, and while the novelty's good, we all know how well Clippy worked out. Fret not though, it's just an optional extra and shouldn't detract from the appeal of a device that offers 3G and a/b/g WiFi connectivity, one VGA and three USB ports, multicard reader, webcam, microphone, and the old faithful 1.6GHz of Atom power. A price of under €500 is being touted, with further details set to emerge over the coming days.                     "
386,https://www.engadget.com/2009-08-13-computer-script-plays-super-mario-world-by-itself.html,Engadget,2009,8,13,141.0," As you might have guessed, we're really busy guys. As such, we can't be expected to actually play the video games we write about. We dish out thousands of dollars every year to get local work release program participants to handle the grunt work for us -- but if a recent entry in the Mario AI Competition is any indication, we may soon be able to keep that cash in our wallets, opting for computer-assisted gaming instead.Posted after the break is a video demo of the entry in question -- designed by artificial intelligence programmer Robin Baumgarten, this computer script effortlessly pushes the plump plumber through the hazardous environments of Super Mario World. It handles the game's challenges much better than our prisoner assistants -- and best of all, it definitely won't try to stab us in our sleep.[Via Make: Online]                     "
387,https://www.engadget.com/2009-08-13-ros-a-common-os-to-streamline-robotic-engineering.html,Engadget,2009,8,13,130.0," The biannual International Joint Conference on Artificial Intelligence has this year shed light on a new effort to standardize robot instructions around a common platform, so that designers won't have to ""reinvent the wheel over and over"" with every project. Presently, robot design is undertaken in an ad hoc fashion, with both hardware and software being built from scratch, but teams at Stanford, MIT and the Technical University of Munich are hoping to change that with the Robot Operating System, or ROS. This new OS would have to compete with Microsoft's robotics offering, but the general enthusiasm for it at the conference suggests a bright future, with some brave souls even envisioning a robot app store somewhere down the line. Video after the break. See more video at our hub!                     "
388,https://www.engadget.com/2009-07-21-eatr-robots-claim-to-be-vegetarian-sure.html,Engadget,2009,7,21,127.0," Usually when we freak out about the coming of killer robots, nobody bothers to disagree with our histrionics, which is in itself a comforting sign that we're overreacting. On the other hand, if the makers of a chainsaw-wielding robot take the time to point out that it is not a flesh-eating harbinger of the apocalypse, well... Cyclone Power and Robotic Technologies, the companies behind the weaponized EATR drone, have put together a joint press release to comfort us all that the biomass-harvesting machine will be exclusively vegetarian, meaning it would only feed on ""renewable plant matter"" and not the bodies littering the battlefield. There's no reason not to believe them, though you should remember that in the eyes of a robot, humans are renewable too. [Via Wired]                     "
389,https://www.engadget.com/2009-07-14-are-memristors-the-future-of-artifical-intelligence-darpa-think.html,Engadget,2009,7,14,217.0," New Scientist has recently published an article that discusses the memristor, the long theorized basic circuit element that can generate voltage from a current (like a resistor), but in a more complex, dynamic manner -- with the ability to ""remember"" previous currents. As we've seen, HP has already made progress developing hybrid memristor-transistor chips, but now the hubbub is the technology's applications for artificial intelligence. Apparently, synapses have complex electrical responses ""maddeningly similar"" to those of memristors, a realization that led Leon Chua (who first discovered the memristor in 1971) to say that synapses are memristors, ""the missing circuit element I was looking for"" was with us all along, it seems. And of course, it didn't take long for DARPA to jump into the fray, with our fave DoD outfit recently announcing its Systems of Neuromorphic Adaptive Plastic Scalable Electronics Program (SyNAPSE -- cute, huh?) with the goal of developing ""biological neural systems"" that can ""autonomously process information in complex environments by automatically learning relevant and probabilistically stable features and associations."" In other words, they see this as a way to make their killer robots a helluva lot smarter -- and you know what that means, don't you?Read - New Scientist: ""Memristor minds: The future of artificial intelligence""Read - DARPA: ""Systems of Neuromorphic Adaptive Plastic Scalable Electronics""                     "
390,https://www.engadget.com/2009-07-10-eatr-robots-are-coming-this-isnt-funny-anymore.html,Engadget,2009,7,10,135.0," Oh sure, we joke about rogue AI all the time, and we're aware that we'll probably pollute ourselves to death well before the robots get us, but who really thinks flesh-eating machines are a good idea? The (patently evil) scientists behind the EATR project -- no fair, they're making their own jokes now too -- have reached a new milestone in the development of the reconnaissance bot, successfully coupling a steam generator with a compact biomass furnace. It is now therefore possible for an autonomous machine to forage for and refuel itself with biomatter, otherwise known as soft, pulsating, yummy humans. They call it fuel versatility, as gasoline, diesel, and solar power may also be used if available, yet we'll offer no prizes for predicting which energy source these chainsaw-equipped robots will prefer.  [Via Switched]                     "
391,https://www.engadget.com/2009-06-06-researchers-develop-a-robot-that-reads-your-intentions-says-you.html,Engadget,2009,6,6,180.0," Robots won't be able to wrest control of the planet from us silly humans until they learn how to collaborate. Sure, they can mow the lawn or mix a drink, but only when you give 'em explicit instructions. Luckily for our future robot overlords, The EU's JAST project is studying the ways that humans work together, in the hope that it can someday teach robots to anticipate the actions and intentions of a human partner. ""In our experiments the robot is not observing to learn a task,"" explains Wolfram Erlhagen from the University of Minho. ""The JAST robots already know the task, but they observe behavior, map it against the task, and quickly learn to anticipate [partner actions] or spot errors when the partner does not follow the correct or expected procedure."" This bad boy has a neural architecture that mimics what happens when two people interact, and the video below shows the rather melancholy automaton trying to convince his human partner to pick up the right pieces to complete a simple task. Watch it in action after the break.                     "
392,https://www.engadget.com/2009-04-27-ibms-watson-to-rival-humans-in-round-of-jeopardy.html,Engadget,2009,4,27,141.0," IBM's already proven that a computer from its labs can take on the world's best at chess, but what'll happen when the boundaries of a square-filled board are removed? Researchers at the outfit are obviously excited to find out, today revealing that its Watson system will be pitted against brilliant Earthlings on Jeopardy! in an attempt to further artificial intelligence when it comes to semantics and searching for indexed information. Essentially, the machine will have to be remarkably labile in order to understand ""analogies, puns, double entendres and relationships like size and location,"" something that robotic linguists have long struggled with. There's no mention of a solid date when it comes to the competition itself, but you can bet we'll be setting our DVRs whenever it's announced. Check out a video of the progress after the break.[Via The New York Times]                     "
393,https://www.engadget.com/2009-04-07-fallen-earth-dev-diary-focuses-on-introducing-new-players-to-pvp.html,Engadget,2009,4,7,438.0," Fallen Earth is one of three post-apocalyptic massively muliplayer online games currently in development that are a far cry from the fantasy titles which have proven most popular in the MMO world. IGN scored an exclusive two part developer diary from Fallen Earth writer and content developer Wes Platt who discusses creating the PvP starter town of Terance. Namely, he explains how and why the FallenEarth team has been putting so much work into Terance and the challenges and pitfalls faced in differentiating the PvP-centric area from other more standard towns in the game. The first part is ""Building the Town of Terance"". It paints a picture of a post-apocalyptic aftermath setting where a psychotic artificial intelligence, long since sealed away underground by its corporate progenitors and forced into a century of dormancy, is woken with dire consequences. Now powered up, the AI -- TETRAX -- prepares once again to work towards the extermination all human beings in its vicinity. Human beings in Terance may find themselves on the run, hunted by AI-directed zombies called Diggers, as well as mutants and vermin. This leads Platt to the second part, ""Building a Nightmare"", where he explains how certain locations in Terance would become 'focal points for player activity'. Platt writes, ""The town proper, a collection of old suburban ranch houses and ramshackle commercial buildings, would serve as the mission hub. We would send players to Brenhauer Gorge, a deep ravine that cut past the town, over the bridge to the TETRAX-controlled underground complex, to a mutant-infested junkyard outside the community proper, and up the hill to the Phillips Mansion.""  The TETRAX facility is where much of the storyline for Terance plays out, where the mission target zones are also PvP areas, creating numerous possibilities for players to interrupt the progress of others. Platt writes, ""It's a town controlled by death and chaos, on the verge of destruction,"" but the developers feel that this free-for-all vision they've had for Terance might not be as fun for the players as they'd predicted.  Some of the original concepts the devs had of creating a PvP starter town work better in theory than in practice, and in order to make sure Terance doesn't become a hive of noob griefing the devs will have to create a more balanced way to introduce new Fallen Earth players to PvP.  For some reason, this is where the IGN article cuts off, so for the time being we're left hanging wondering whether Terance will incorporate many of these design aspects Platt has detailed or whether this PvP town will see some radical changes before launch. [Via WarCry]                     "
394,https://www.engadget.com/2009-04-03-artificial-intelligence-solves-boring-science-experiments-makes.html,Engadget,2009,4,3,180.0," Researchers at Aberystwyth University in Wales have developed a robot that is being heralded as the first machine to have discovered new scientific knowledge independently of a human operator. Named Adam, the device has already identified the role of several genes in yeast cells, and has the ability to plan further experiments to test its own hypotheses. Ross King, from the university's computer science department, remarked that the robot is meant to take care of the tedious aspects of the scientific method, freeing up human scientists for ""more advanced experiments."" Across the pond at Cornell, researchers have developed a computer that can find established laws in the natural world -- without any prior scientific knowledge. According to PhysOrg, they've tested the AI on ""simple mechanical systems"" and plan on applying it to more complex problems in areas such as biology to cosmology where there are mountains of data to be poured through. It sure is nice to hear about robots doing something helpful for a change.[Thanks, bo3of]Read: Robo-scientist's first findingsRead: Being Isaac Newton: Computer derives natural laws from raw data                     "
395,https://www.engadget.com/2009-03-24-gdc09-havok-gets-smart-announces-havok-ai.html,Engadget,2009,3,24,100.0," Havok is exploding like a red barrel into the world of artificial intelligence. With ragdoll grace, the middleware company announced its new Havok AI SDK during GDC 2009, promising ""unique solutions"" to various AI pathfinding issues faced by today's game developers. Like the folks at 1UP, we're not exactly sure what this means for gamers, though the new software is supposed to be fully compatible with Havok's other products and tools, such as Havok Physics. Perhaps now enemy patrols will get the good sense not to seek shelter behind things that go kablooey when bullets begin to fly.[Thanks David B.]                     "
396,https://www.engadget.com/2009-03-12-brown-university-darpa-give-irobots-packbot-autonomy.html,Engadget,2009,3,12,157.0," It's not easy to find research in the field of robotics without military applications (or military funding), and Brown University's latest is certainly no exception. Starting out with iRobot's PackBot (and some pocket change from DARPA and the Office of Naval Intelligence) researchers at the school have achieved several advances that will someday produce robots that follow both verbal and nonverbal commands from a human operator, indoors and out, without the need for a controlled environment or special clothing. The goal, according to Chad Jenkins, is to develop a robot that acts ""like a partner. You don't want to puppeteer the robot. You supervise it, 'Here's your job. Now, go do it.'"" The work is being presented this week at the Human-Robot Interaction conference in San Diego, but if you can't make it we've provided a video of the thing in action just for you (after the break). We for one salute our autonomous robot overlords.[Via PhysOrg]                     "
397,https://www.engadget.com/2009-03-06-massivelys-apocrypha-expansion-hands-on-the-sleepers.html,Engadget,2009,3,6,1103.0," The Sleepers The inhabitants of these uncharted solar systems may take offense to your encroachment upon their territory, however. The Sleepers, an ancient race of NPCs known for their mastery of virtual reality and cryogenics, will provide the greatest PvE challenges players have ever faced in EVE Online through their guardian drones. Their AI is far beyond what players are used to going up against in PvE. The Sleepers will have varying levels of strength and adaptation to player threats. They do seem to have a particular hatred for their creators at CCP, given their ultra-violent response to Ward's arrival at a structure the Sleeper drones constructed in space. They move in on him, dishing out *all* damage types: beams sizzle for EM and Thermal damage; warheads obliterate for Kinetic and Explosive damage. Fortunately Ward's Proteus is set up to deal with this (""I'm going to put on a GM shield extender, or when we go through there I'm going to get wasted!"") making his ship virtually impossible to kill, letting us witness the Sleeper offensive in safety. Contrary to rumor, Ward states that there will be some lower level encounters with Sleepers that a solo player could potentially take on. Ward says, ""The way we put up the level of challenge is that with the easiest ones you could be in a really well-fitted Tech I cruiser with all Tech II [or better fittings], or a battlecruiser or a HAC, and you should be able to solo the easy stuff. But also newer people in a group of Tech I frigates should be able to take on the easy stuff together. And then it moves on to the hardest stuff where the sky's the limit, and taking in two carriers is going to be a good idea, because it's really going to be that tough."" Wormholes found in high security space will (often) lead to weaker Sleeper opposition. Without a doubt though, the higher level encounters -- such as those already seen in the Singularity test server videos in circulation -- will require grouping in order to even have a shot at taking down the Sleepers and salvaging their technology. Gonzales adds, ""It's going to be a good time for groups, it really is. It's going to be a blast.""  Some of the players who've experienced the Sleepers on the Singularity (aka Sisi) test server agree. Ward says, ""It seems from all the feedback we've been getting on Sisi that the players really like it. PvE hasn't been a challenge for a long time and they're excited to have a real challenge again.""  Added to the risk is the fact that the local chat window cannot be used as an intelligence tool in wormhole space. This means if other players enter the system you're in, you'll have no warning of this and may find yourself fending off other players while trying not to be torn apart by the Sleepers.  Being torn apart should be a chief concern for players who engage the ancient race's drones. Sleepers won't simply spawn by blinking into sight out of nowhere, as anyone who's run missions in EVE will be accustomed to. They'll actually warp in to a location as players do.  Added to their repertoire of destruction is the ability to switch targets and focus fire. Sleeper frigate-class drones will maneuver evasively, rather than simply orbiting players in combat, and comparisons are already being made between how these NPCs react and the kinds of tactics that players use in PvP. Moreover, the Sleepers are resilient opponents. They don't have shields but they spider tank (repair one another while in combat), so while their numbers are superior they'll be very difficult to wear down. New Graphics and Sound On top of all the new features being added to the game and the ones being revisited and refined, the CCP devs have also made across-the-board changes to the game's graphical effects. When EVE's players log in on March 10th, they'll see new cloaking and warping visuals. Much-improved effects connected with modules should also wow players, whether it's the look of weapons fire, emanations from electronic warfare equipment, or visuals showing that tanking modules are active.  If you've seen the Apocrypha teaser trailer, you might have noticed that ships cast shadows upon one another. It's a subtle effect, but it's not just something done for the video. Objects in space will cast shadows on other objects, and this will be especially noticeable in the game's asteroid belts. Another bonus is that for those players who mine in asteroid belts, or those who hunt for prey within them, the ores will be visually distinguished from one another. The changes aren't just skin deep. Apocrypha will be using a new (middleware) audio engine as well, adding a host of new sounds to accompany the enhanced visuals. The Apocrypha Storyline Noah Ward and Tony Gonzales answer most every question I have over the course of our conversation, but I do still have one other thing on my mind -- why call the expansion ""Apocrypha"" and how will the game's lore accommodate all of these sweeping changes to EVE Online? They can't go into specifics as they don't want to ruin any surprises, but Gonzales is able to drop a hint or two. He says, ""This patch changes EVE very drastically, practically overnight. So we have a storyline that accommodates that. There's been a build up to it quietly happening through the Chronicles. There's a series running right now called The End of the World and it's been dropping clues and building up to the last installment, which will be released on patch day. The New Eden cluster is going to be shaken by something cataclysmic. As far as the lore goes, New Eden hasn't seen a naturally occurring wormhole since the EVE Gate collapsed so this is a really big milestone. For them to be spawning all over the place has all kinds of biblical implications and prophecies."" * * * So here we are, only a few days away from the most significant expansion EVE players have ever seen. Will it live up to expectations? That'll be up to the players to decide, but what I've seen of Apocrypha thus far leaves little doubt in my mind that this expansion will add new dimensions to the EVE Online experience. That's always a good thing in my book.  Thanks again to Noah Ward and Tony Gonzales for offering to meet me and be subjected to my endless grilling, and to Atari for letting us take over their office. << Welcome to Apocrypha                     "
398,https://www.engadget.com/2009-03-03-platinum-games-discusses-enemy-ai-in-madworld.html,Engadget,2009,3,3,248.0," Bless the Platinum Games blog, as it's been a wonderful source of interesting stuff leading up to next week's release of MadWorld. Today, Hirono Sato takes over the driver's seat and talks about his role in the game: player interactions and the three different AIs governing enemies. He breaks the enemy units down to ""grunt,"" ""grunt leader,"" and ""boss."" Sato explains that the ""grunts"" are essentially pushovers and the challenge they present is not simply just to defeat them, but to defeat them in the most stylish way possible, for the most points possible. Sato says that in a game ""where running around killing all the enemies is supposed to be fun, making them so hard that you can't kill them wouldn't be fun at all.""However, for those that crave difficult combat, the ""grunt leader"" is always on hand to provide an ample spanking. This guy is a bit tougher to battle, so it's not as much about getting the most points possible as it is about just surviving his attacks. Then, there are the ""bosses,"" which are pretty self-explanatory.Sato also tells us of a cool in-game item called the ""Money Grubber,"" a briefcase stuffed with money that you throw at enemies. Once the ""grunts"" see it, they'll start clawing for it, and eventually fight each other over the money. While they're busy, Sato offers a few ideas, including tossing ""a drum filled with gasoline their way"" and even tossing the case ""onto some busy train tracks."" %Gallery-22964%                     "
399,https://www.engadget.com/2009-03-02-gesture-recognizing-qb1-computer-attends-to-your-every-desire.html,Engadget,2009,3,2,139.0," Scouting a computer that's ""attentive to one's desires?"" Good news, friends! Frédéric Kaplan's QB1, which was unveiled at the LIFT Conference in Geneva this past week, aims to be just that. Reportedly, the machine was designed in order to ""alter the fundamentals of human-machine interaction,"" and rather than relying on the traditional mouse and keyboard approach, this one works entirely via gestures. QB1 is capable of recognizing inputs from both hands at once, with one example having a human select a record and adjust the volume by simply flicking their fingers through an on-screen album collection. We're told that the related patents behind the sophisticated 3D gesture interaction technology have been filed, but there's no word yet on when we'll be able to actually buy one. 'Til then, it's up to you to handle those ""desires"" yourself.[Via The Inquirer]                     "
400,https://www.engadget.com/2009-02-23-eve-online-developer-explains-new-ai-for-the-sleeper-race.html,Engadget,2009,2,23,285.0," One of the major new features to EVE Online's forthcoming Apocrypha expansion is the introduction of an ancient race of NPCs called the Sleepers. They're a breed apart from any NPCs ever seen in EVE, largely because of their AI. They react intelligently to threats, focusing fire on primary targets but diverting their attacks to counter whatever else they're faced with. They can 'spider tank' or protect and even repair one another as combat ensues. Added to their tactics is evasive maneuvering, making the Sleepers even deadlier. We recently showed you video footage of the Sleepers in action and while taking them on will, in some cases, be a daunting experience, this is not to say that they can't be defeated. EVE developer CCP Incognito wrote on the forums, ""If you try to use the same-old, same-old tactics against Sleepers then you will have problems. Think out of the box and you will win."" CCP Incognito's comments were in the context of his dev blog on the new AI, which discusses the challenges that the Sleepers will pose for players seeking fragments of their advanced technology. More than anything, the AI revamp is designed to make PvE more like PvP. Anyone planning to venture through EVE's wormholes should abandon their mission-centric ship setups and be prepared for PvP; that's what fighting the Sleeper NPCs will be like -- fighting other players. If you're looking for more information about how the game will change on March 10th with the Apocrypha expansion launch, you'll want to read CCP Incognito's AI dev blog ""How the AI pew pews the player"" and jump into the discussion on the official forums, where Incognito is answering some questions from the playerbase.                     "
401,https://www.engadget.com/2009-02-18-navy-report-warns-of-robot-uprising-suggests-a-strong-moral-com.html,Engadget,2009,2,18,218.0," You know, when armchair futurists (and jive talkin' bloggists) make note of some of the scary new tech making the rounds in defense circles these days it's one thing, but when the Doomsday Scenarios come from official channels, that's when we start to get nervous. According to a report published by the California State Polytechnic University (with data made available by the U.S. Navy's Office of Naval Research) the sheer scope of the military's various AI projects is so vast that it is impossible for anyone to fully understand exactly what's going on. ""With hundreds of programmers working on millions of lines of code for a single war robot,"" says Patrick Lin, the chief compiler of the report, ""no one has a clear understanding of what's going on, at a small scale, across the entire code base."" And what we don't understand can eventually hunt us down and kill us. This isn't idle talk, either -- a software malfunction just last year caused US. Army robots to aim at friendly targets (fortunately, no shots were fired). The solution, Dr. Lin continues, is to teach robots ""battlefield ethics... a warrior code."" Of course, the government has had absolutely no problems with ethics over the years -- so programming its killer robots with some rudimentary values should prove relatively simple.                     "
402,https://www.engadget.com/2009-02-02-ibm-develops-computerized-voice-that-actually-sounds-human.html,Engadget,2009,2,2,129.0," If there's one thing that still grates our nerves, it's automated calling systems. Or, more specifically, the robotic beings that simply fail to understand our slang and incomprehensible rants. IBM's working hard and fast to change all that, with a team at the company's Thomas J Watson research division developing and patenting a computerized voice that can utter ""um,"" ""er"" and ""yes, we're dead serious."" The sophisticated system adds in the minutiae that makes conversation believable to Earthlings, and it's even programmed to learn new nuances and react to phrases such as ""shh."" The technology has been difficulty coined ""generating paralinguistic phenomena via markup in text-to-speech syntheses,"" and while exact end uses have yet to be discussed publicly, we can certainly imagine a brave new world of automated CSRs.                     "
403,https://www.engadget.com/2009-01-29-eve-online-to-introduce-new-npc-race-with-apocrypha-expansion.html,Engadget,2009,1,29,591.0," PvE gameplay in EVE Online can be profitable, but blasting apart NPC ships can also get repetitive after a while. Anyone who's run a solid week of missions knows this all too well, and the combat dynamic between players and NPCs is something CCP Games will start remedying in the next game expansion: Apocrypha.EVE dev CCP Ytterbium's dev blog ""When Evolution Leaps Forward"" has some big news about what's in store for players as of March 10th in Apocrypha. The devs tasked with revamping the game's PvE experience are making some substantial changes to the distribution of bigger and badder NPCs. They're also introducing a new NPC race to New Eden: the Sleepers, an ancient race with technology beyond what is currently available to players, but whose secrets can potentially be unlocked. CCP Ytterbium writes that NPC pirate battleships will now spawn in locations they never have before. The distribution of these higher level spawns throughout low security space should offer some more challenge and ISK rewards for those willing to risk prowling in lowsec. These spawns likely won't have the same level of ISK bounties typical of NPC pirates found in 0.0 (lawless) space, however. (But presumably, they will yield the same module drops and salvage components of a typical battleship spawn.) These higher powered NPCs will not, however, encroach upon the relatively safe haven of high security space though. This change may well bring more high sec dwellers out into low sec for NPCing, which would be a good move all around -- greater risk for greater rewards, and of course this could lead to more opportunities for PvP. It could be bad news for low sec miners though... In the spirit of providing more challenge to New Eden's pilots, CCP Games is introducing a much stronger breed of NPC antagonist: the Sleepers are an ancient race that players currently know of only in cryptic terms, with fragments of their technology existing in the game as artifacts. The in-game description of Sleeper Technology states that the Sleepers are, or were, ""masters of virtual reality, neural interfacing and cryotechnology.""  Players will encounter Sleepers via wormhole exploration. CCP Ytterbium cautions, ""Do not underestimate them, as they will remain radically different from the regular pirate factions you are used to. More often than not, they will give you some substantial bang for your buck and will require player gang coordination and effort to be dispatched.""  With the advent of wormhole exploration in New Eden, sometimes journeying into the unknown will have its risks -- and the Sleepers may prove to be persistent antagonists for EVE's explorers. Interaction with the Sleepers, via gunboat diplomacy, will yield the components essential to Tech 3 production, ensuring plenty of motivation for explorers to face the odds in hopes of returning with rare Sleeper tech.  Those odds may be steeper than players expect, given the new AI that Sleepers (and Officer spawns in 0.0) will be imbued with in Apocrypha. CCP Ytterbium writes, ""How will they react with such a behavior? Let us just say at the moment they are going to make logical target choices depending on the most threatening targets available..."" The new AI is essentially going through a trial run with Sleepers and Officer spawns when the expansion launches, but if successful, CCP may ultimately give all of New Eden's NPCs a more dynamic AI.  You can check out the ""When Evolution Leaps Forward"" dev blog at the EVE site, and how New Eden's pilots are reacting to the news on the game's forums.                     "
404,https://www.engadget.com/2008-12-22-basil-the-robot-trained-for-symbolic-recognition-beer-toting.html,Engadget,2008,12,22,179.0," Though not much of a looker -- feel free to insert your own beer goggles joke here -- Basil the Robot is an experiment by Jim and Louise Gunderson to train an AI to identify its surroundings symbolically. That way, the couple hopes, he can react dynamically in new environments. Basil was intended to be shown off at a Cafe Scientifique meeting by having him go to the bar and order drinks for his creators, but that proved unsuccessful when Basil's battery died. The couple did videotape a successful trial run from the night before, which you can see after the break. Basil will next receive a microphone for voice commands and be upgraded from his current sonar navigation to a video sight system. The eventual goal is to teach Basil to go to the local brewery and pick up beer. Sure, we're still terrified of the robot revolution, but let's get serious here -- who are we to deny our mechanical overlords if they keep proffering us six-packs? Hit up the read link for the full story.[Via Metafilter]                     "
405,https://www.engadget.com/2008-11-25-mmogology-braindead.html,Engadget,2008,11,25,1027.0," You tiredly trudge up the cliff-side path, the rhythmic squish of your soaked boots beating a slow dirge. Cresting the ridge of the trail you see the full moon shining over Bloodstone's harbor where several galleons rot in their moorings. It almost looks like a peaceful town from up above the sweat and fish-soaked stench of the city. You sigh. It's been an exhausting trip dodging banshees and hollow men through the slime of Wraithmarsh. All you can think about is a mug of ale, a warm bed in the inn and perhaps a little company to take your mind off things. Bloodstone is known for its ""hospitable"" women after all. With a renewed sense of purpose you pat your faithful dog on the head and take the sloping trail down into town. As you reach the outskirts of Bloodstone the rancid smell of a fish merchant's stall nearly slaps your nose off your face. You vainly wave your hand to clear the air.""Wot's a matta gov?"" the merchant asks, sneering. ""Can't 'andle a little fresh fish?""""That fish is as fresh as my feet,"" you reply. The short tempered merchant draws a rusty cutlass and grimaces. Several ruffians milling about sense an impending fight and begin to circle you.""If ya don't like it, you and that mangy dog can bugger off!"" he responds, kicking your dog in his hinder. Your dog whimpers and sits next to you, tail between his legs. The bolder members of the encircling crowd brandish knives and mock you and your dog. They see a tired traveler and easy pickings. As tired as you are, you can't help but laugh to yourself and shake your head. Your eyes begin to glow a soft white hue. The humid air of Bloodstone begins to crackle. Time to teach these lowlifes a lesson. The brigands around you stare in awe as tiny jets of blue are ripped, crackling, from the surrounding air. Each spark begins to swirl into a glowing ball of blue-white light between your rotating hands. A distant rumble of thunder seems to fill the night air. The veins of your hands and arms begin to glow. The brigands, drop their weapons, scream in terror and scatter. The merchant trips backward into his fish stall, coating himself in rotted meat, and scampers into the darkness. Satisfied with the defense of your canine companion's honor you release the ball of lightning into the night sky with a deafening thunderclap.What I just described was my first experience entering the town of Bloodstone in Peter Molyneux's Fable II. What made it so memorable and remarkable for me were the actions of the town's non-player characters (NPCs). When I arrived in Bloodstone I had no idea that the town's evil residents would mock a do-gooder like me; I didn't know they were even capable of that level of player interaction. I also didn't know that townspeople could kick my dog or draw their weapons on me. These were townspeople, not monsters after all. And I had no idea that a little display of magical power would be enough to send them running for their lives. It was a memorable and exciting experience because it was completely unscripted and unexpected. That level of NPC interactivity is something I'd love to see more of in MMOGs.In their current state, artificial intelligence and MMOGs are rarely spoken of in the same sentence. In most cases NPCs are simply quest dispensers. They stand around, shifting their weight, shelling out quests and rewards. They might occasionally follow a pre-programmed path around the village where they ""live"" and there might be some interesting background information about them. Otherwise though, most NPCs feel as alive as a barrel next to the door of the inn. It doesn't really inspire the feeling of a ""living and breathing"" world. Instead it feels like an animatronic ""Small World"" ride at Disney.Much like the townsfolk in MMOGs, most monsters you fight are just as braindead. In single player games enemy soldiers frequently take cover or use flanking techniques to attack you. Their strategy and coordination with each other often makes combat that much more entertaining and challenging. With the exception of the occasional boss monster in a dungeon, most MMOG enemies simply pace back and forth waiting to be slaughtered. Once engaged, their attacks are typically straightforward and without strategy. Occasionally a healer in a group of mobs will heal his buddy, and ranged classes will hang back rather than rush into melee. But coordinated attacks are few and far between. As MMOGs evolve I hope we'll see more dynamic interactions between the players and the gameworld's inhabitants. Rich Vogel, co-studio director of product development for Star Wars: The Old Republic has stated in a Games for Windows interview that, ""One thing we don't want to do is NPC Pez dispensers, as I call them -- go over there, dispense a quest, and then go ""vacuum-clean"" a zone. We want to make sure you listen to NPCs, because choices matter."" Story and player-driven consequences are obviously great ways to make a gameworld feel more dynamic and compelling. Whether or not that translates into more lifelike, interactive townsfolk / monsters remains to be seen. We've all seen impressive moments in video game cut scenes or pre-programmed, scripted events, but when they occur, we're not typically interacting with them. Unscripted interactions that unfold organically can be so much more exciting and entertaining. For a few moments when playing Fable 2 I was completely immersed in the gameworld, all because of a little AI programming. The gameplay ramifications of intelligent behavior are what make its execution so exciting. For instance, I could've chosen to ignore the jests of the brigands, kill them, or return their mocking behavior. Instead, I ""powered up"" and scared everyone off without killing anyone. The fact that all those options existed and were viable was possible because of the artificial intelligence of the NPCs. Here's to hoping we'll see this level of interactivity in the MMOGs of the future.MMOGology [mŏg-ol-uh-jee] – noun – The study of massively multiplayer online games via the slightly warped perspective of Marc Nottke.                     "
406,https://www.engadget.com/2008-11-21-darpa-enlists-ibm-to-build-computer-brain-as-smart-as-a-cat.html,Engadget,2008,11,21,181.0," Researchers have long been trying to model actual brains in order to build a better computer ""brain,"" and it looks like IBM is now getting a helping hand from none other than DARPA in its attempt to create one that it hopes will one day have the intelligence level of a cat. To that somewhat unnerving end, DARPA is pouring $4.9 million into a project that'll include five universities and scientists of all stripes, who will work together to create an artificial brain that behaves like a real one right down to the neuron level. As the BBC reports, the researchers are describing this latest initiative as a ""180 degree shift in perspective"" from previous efforts, as they're now seeking an algorithm first and problems second, as opposed to starting with an objective and devising an algorithm to achieve it. As for DARPA's ultimate goal, well, that's still a bit of a mystery, though let's just say we won't be surprised if future robots start to become very easily distracted. [Via Daily Tech, image courtesy Mack J, Truth and Beauty Bombs]                     "
407,https://www.engadget.com/2008-11-06-army-artificial-intelligence-invade-wow.html,Engadget,2008,11,6,193.0," Joe Martin at bit-tech.net picked up an article on Gizmodo talking about the coming invasion of Army Artificial Intelligences masquerading as real players in World of Warcraft. According to Dr. John Parmentola, the plan is to test the AI's ability to be a ""fake"" human by letting it interact with real humans in a virtual world.My first reaction was, ""Whoah, cool. All your base are belong to us."" But after a moment's thought, this might not actually be such a great idea. Given the communication skills of some players (especially in the battlegrounds), I'm not seeing this as a litmus test of what in-game speech can pass for spoken by real people. While I'm pretty sure the AI won't communicate like a roleplayer, the AI could probably get by with a series of ""lol"" and ""kek"" typed out in rapid succession.This isn't the first time we've heard about the military using WoW (or WoW-like systems) for training purposes, which is the nominal purpose of this new AI research. Maybe it won't be too long before we're logging in to have a Gnome Rogue named Joshua quietly whisper us, ""Shall we play a game?""                     "
408,https://www.engadget.com/2008-10-19-a-closer-look-at-elbots-turing-test-conversation.html,Engadget,2008,10,19,118.0," Earlier this week, Elbot made a fairly impressive showing (comparatively speaking, at least) when fooling three judges into thinking it was human; had it fooled one more on the dozen deep panel, it would have successful passed the famed Turing test. Auntie Beeb now has a report on what exactly Elbot said when asked a litany of questions away from the competition, and there's also a video with the related experts dissecting its performance. To be totally honest, its responses weren't too far from being completely passable as ones from a tired, potentially inebriated Earthling (in our humble opinion), but we'll leave the final determination to you. Touch the read link for a one-on-one with ones and zeros.                     "
409,https://www.engadget.com/2008-10-13-new-round-of-turing-test-fails-to-crown-a-winner.html,Engadget,2008,10,13,145.0," While some folks are considering taking the Turing test one step further and applying it to military robots, a group of researchers in the UK led by none other than would-be cyborg Kevin Warwick are doing their best to keep things as Turing intended and simply trying to fool some humans into thinking that the robot they're taking to is actually a person. Fortunately for us on the human side of the equation, they weren't quite successful, though one ""robot"" known as Elbot did get relatively close to the goal, fooling 25% of its human interrogators, which is just 5% off the mark set by Alan Turing. Each of the four other ""artificial conversational entities"" also managed to fool at least one of their questioners, though they eventually showed their true colors with random answers like ""soup"" when pressed as to what their job was.                     "
410,https://www.engadget.com/2008-09-26-trs-deployment-13-introduces-new-command-system.html,Engadget,2008,9,26,115.0," Tabula Rasa's Deployment 13 has just hit the Public Test Server, and we think you might be excited to hear about the new Command System being implemented with it. Previously referred to as the Minion System, this enhancement gives better control over AI pets and controllable NPCs, also called subordinates, in the game.Subordinates now have better control features where you can set their aggressiveness or defensiveness, assign an anchor point, order them to attack or heal a particular target or set them to simply follow you. All of these commands are done through a set of easy commands and hotkeys. To find out more info on the complete Command System, check out today's Feedback Friday.                     "
411,https://www.engadget.com/2008-09-20-cognition-technologies-semantic-map-paves-the-way-for-the-robot.html,Engadget,2008,9,20,116.0," Cognition Technologies' new Semantic Map lets computers -- and, conceivably, evil robots -- ""understand"" the English language in much the same way humans do, based on word tenses and context in a sentence. With this technology, a computer or search engine can understand virtually every word in the English language -- for a vocabulary about ten times that of a typical American college graduate. The system is already being employed in search engines, allowing people to ask questions in human-phrasing instead of unnatural, machine formatted word strings. Researchers say the ability to understand language is an important building block of the nascent Semantic Web, and will make the Replicants of the future extremely difficult to detect.                     "
412,https://www.engadget.com/2008-08-30-philosony-yea-though-i-walk-through-the-uncanny-valley.html,Engadget,2008,8,30,907.0," I wrote a few weeks back about the uncanny valley and Hideo Kojima's possibly telling observation that war machines of the future may exploit the creepiness of robotic simulations to instill fear in their prey. I want to turn my attention now to a discussion of the valley as it applies more directly to us as gamers - overcoming the creepiness of computer generated people. Quantic Dream has already boasted of successfully traversing the valley with its upcoming (and secretly acclaimed) PS3 exclusive Heavy Rain. While realistic graphics are one thing (and it's up to interpretation whether they succeeded in the tech demo almost two years ago), is there more to escaping the valley than mere realistic modeling? Any of you that have played through the tutorial of Indigo Prophecy will likely remember lead designer David Cage waxing poetic about the unique nature of the game, one in which you should ""move the right analog stick slowly to really feel like you are controlling the character's hand"". The game itself frequently gives you mundane tasks such as setting up candles and lighting them (place three candles, one-by-one, on the table, go down the hall to the kitchen, pick up the matches, walk back, light them one-by-one) in an attempt to make you feel more like - and therefore more sympathetic toward - the characters. All accounts suggest that Heavy Rain will play similarly, with an emphasis on taking actions, succeeding or failing at tasks, and living with the results. If the graphical side of Quantic's dream manages to live up to the hype, will we suddenly find ourselves in a new era where the uncanny valley is reduced to an historical oddity along the lines of the pager? Or are emotion and realistic human mental behavior just as important in dispelling the illusion?First let me acknowledge that any creepiness or uncanniness resulting from visually perfect models acting in unrealistic ways may not technically be the fault of the uncanny valley. The theory itself suggests that as robots or computer rendered humans get closer to looking and acting human they will trigger the uncanny response (until they become so perfect that they are indistinguishable from us). I generally interpret ""acting human"" to mean having realistically moving facial muscles, walking gracefully rather than jerkily, etc. When an otherwise visually perfect human representation starts acting strangely the effect is still creepy even if it's not an according-to-Hoyle example of the uncanny valley. Rather humorous takes on this can be found in several video re-enactments of RPGs. The effect of actual humans acting like video game characters is just uncomfortable enough to be funny.In fact, the concept of humans moving and acting in strange and unusual ways is sometimes used to intentionally evoke creepy feelings. Think about the characteristic twitchy movements associated with Japanese horror. This is all well and good when game developers are trying to create an eerie tone, but in serious ""interactive cinema"", which Heavy Rain purports to be, it's a death blow to realism. With Quantic Dream having purchased the best motion-capture technology available there shouldn't be any problems with graceful movement. But we still need good AI controlling those characters to keep them from walking into walls, taking inefficient paths around obstacles, or jumping out of cover in the middle of a firefight.Perhaps it doesn't even stop there. As gamers we usually use the term ""good AI"" to refer to enemies that adapt to our strategies, neither falling for predictable behaviors that a human wouldn't nor succumbing to patterns of their own. Yet if Quantic Dream hopes to create a truly cinematic narrative in their game they are going to have to delve deeper than other games have at, for lack of a better term, ""emotional AI"". Characters may look, move, and interact with the world around them in completely human ways, but if they fail to make realistically human choices then players may still find themselves searching for a way out of the valley. Will NPCs become subtly snippy with me if I make snide remarks? Will they succeed at what Fable (arguably) failed at - remembering my actions and having appropriate, long-term emotional interactions with me? Perhaps they will still jump out of cover in the middle of a firefight, but will they have a compelling, realistic reason for doing so?Granted, dramatic media has always been judged by the quality and emotional richness of its characters. Books and movies are often criticized for lacking rich, fully developed and believable people. If narrative is important then an individual character's choice of actions must be realistic. Can Heavy Rain do this? Did Indigo Prophecy do this? Perhaps more to the point, does the interactive nature of video games make it even harder for characters to be realistic because they must be able to respond to whatever behavior we subject them to? I'm not entirely sure, but it seems that if Quantic's goal is carry us into the promised land where characters in games don't unsettle us in the slightest, it's going to take much more than miraculous CGI.Maybe aspiring to a deep interacto-narrato-cinematic experience is a little beyond our belaying skills at the moment. Check back next week for a bit lighter discussion of the uncanny valley surrounding the realism and novel interactivity of the recently announced EyePet. Do you want to be drenched in details? Check out our Heavy Rain page for all our coverage!                     "
413,https://www.engadget.com/2008-08-19-gen-con-08-turbine-tells-us-whats-in-store-for-asherons-call.html,Engadget,2008,8,19,905.0," At Gen Con, we spoke with Turbine's Andy Cataldo, the Community Manager for Asheron's Call, about the future of this historic game. Cataldo told us a lot about AC's epic 100th update. The update is coming within the next few weeks, and it's a doozy. According to Cataldo, AC players will get a whole new faction system, three land areas to battle over, tons of new loot and spells, and various other improvements to the game, particularly to enemy AI.In addition to working its regular monthly event and patch schedule, Turbine is attempting to respond to player demand for a variety of new features in the 100th and all future updates. Particular emphasis is being placed on adding features common in modern MMOs, such as a quest tracking menu. AC has been around for almost ten years, so it has some catching up to do!Learn more about Turbine's big push to modernize the game, add lots of new content, and more in the interview after the break. How old is Asheron's Call at this point?Asheron's Call in November will celebrate its 9th anniversary. So at 9 years we're still going strong. We still have thousands of players -- very passionate players! With the hundredth update we're basically going to be giving an expansion pack for free as a way of saying ""thank you"" to our players.It's crazy to think that you're working on the 100th update. That's a lot of content over the years!It is. We can proudly say we've added more content updates than any other game on the market today.What's going into the 100th update?We knew the event was coming up, so we said, ""Okay, we're going to hit 100 this year, let's do something special! So we went out and asked the players, ""If we were going to give you guys an expansion pack with something special, what would you want?"" And they said wanted elder game, they wanted land control, they wanted factions, they wanted new loot and spells. So we're giving them all of that. There's a new faction system where you'll be able to choose good, bad, or neutral -- all based off of lore characters that anyone who's played the game for any length of time will recognize and know.Our PvPers ... wanted something to fight over. So we're introducing three new land areas that they'll be able to fight over and control, and as they gain control they'll gain access to special loot and weapons that will benefit them and that nobody else will be able to get. They'll also get access to quests that nobody else will be able to get into without controlling the area.So you'll have a reason. If your [faction] controls [the area], the other factions are going to want to come in and take over control so they can get it. And then they'll also want to fight everybody around there because they'll have all the special loot and weapons on them now. We're hoping to focus people in areas so they're not running around just in little skirmishes. We wanna see more of the large scale, full on assaults in these areas.There are the level eight spells. We haven't done spell updates since Throne of Destiny, which was in 2005. So we figured it was time. And then we're putting in a whole new tier of loot which will be rewarded to players through either regular landscape hunting or completing tasks in the new faction system.That's a lot of stuff!Yes. I think just for the faction system alone, we're adding 30 new quests in this update and that's not counting any of the other stuff that's going in. We're still working on it as we speak. The dev team is hard at work. We didn't stop any content updates while we were working on this so we're doing both, so they're tired right now because we don't miss a month. Or we try not to.How long has the 100th update been in development?The 100th update has been in development now for about eight months. The team will do their content for whatever month they're working on, and then they'll spend a week or two doing something for the 100th update. As we got three or four months out, we just split a couple of guys off the team and totally dedicated them to working on the 100th update.One of the cooler features that has been added in recently is now the creatures roam ... the AI is much more intelligent now. Which is something that was a player complaint. One of our goals now as we move forward with the 100th update and beyond is to get a lot of the features that modern MMOs have. So after the 100th update our next big project is to try to get a mail and banking system into the game, and a quest panel.AC is kinda oldschool and hardcore in the way that you don't know what quest you're working on unless you just started it! The problem we run into with that is 100 content updates worth of content to try and port and a lot of the systems aren't cooperative to that.It's a work in progress, and Turbine as a company is dedicated to moving forward with the game and keeping it going as long as possible. Continue to Part 2 to read about AI improvements, the community, and character reconfiguration!                     "
414,https://www.engadget.com/2008-08-19-gen-con-08-turbine-tells-us-whats-in-store-for-ac-part-2.html,Engadget,2008,8,19,833.0," Given that the game has been around for so long, do you see this addition of modern MMO features as something that the player base is really clamoring for, or are you sort of scaling that against what other games have right now?Really, it's what the players are asking for. That's one of the things we pride ourselves on: listening to the players. Feedback and player concerns are very important to us. I'll log in during the week and actually ask the players in the worlds: ""What would you guys like? What do you think of this? How is this?"" Their feedback has been the stuff that we're adding into the game. We don't just rely on the forums; the forums are an important part of feedback but you only get a small section of the player base on the forums. So my whole thing is I go in and I'll ask them.I'll even pull people aside. I'll see them out hunting somewhere and I'll log in with my dev character and say, ""Hey, I see you're hunting here. What do you think of this area? Pretty cool, right? What would you do to improve it?"" Every week we sit down in meetings and go over all that feedback to decide what the next step is.Do you see it as one of the advantages for a game like Asheron's Call that the community is very close-knit and focused, where you can reach out to those individual players?Absolutely. Our community is comprised of some of the most passionate people that you'll ever meet in a gaming community. I've had the pleasure of going to several player gatherings and it absolutely amazes me how much they know -- how much passion they have for the games.A lot of people will say, ""You should do this!"" because they're very selfish ... for the most part when making suggestions or feedback [our players'] goal is for the better of the game. They won't always say ""I think you should do this because it'll make my life easier."" Our players are more like, ""If you do this, you'll get more players to play the game!""They're always wanting to help us make the game better. I think that's a very unique thing in a community nowadays so I definitely look at that as a good thing.How long have you been working on AC?I have been working on AC in one capacity or another for almost 5 years. I was a player. I started playing in 2000 right after the game launched. When Turbine bought the rights back from Microsoft, I came in and started working on customer service for Asheron's Call and then moved into community and I do community and associate producing -- a little bit of everything.I wouldn't be here if it wasn't for this game! Those nights that my wife wanted to kill me because it was 4:00 in the morning and I said, ""I've got one more quest, honey -- one more quest! I've just gotta finish this."" I can now look at her and say, ""It was job training!""How would you compare the game today to the way it was five years ago when you joined the project? What are some big changes that you'd point out?More outdoor hunting. Smarter AI, as I mentioned earlier. More of the features that players like. The addition of tons of quests; that was another feedback point. They didn't necessarily not like the way quests went but they thought ""maybe they could give better rewards."" We listened to that so our reward structure became different. We now will reward more experience points.Players want the freedom to change their templates. You get to a certain point where you decide ""I don't like this guy!"" so we put in the ability to -- in a quest fashion -- to go in and change their character. You can be a mage today and decide, ""I don't want to be a mage anymore, I want to be a dagger character!"" You can go in and wholesale swap out everything and start playing your dagger character. Our big thing is freedom. A lot of games hold the hand of the gamer too much now. For a lot of people that's fine, but we're all about freedom. You should have the choice of what your character does as it progresses.And you should also get that ""ding"" factor every time you play. You should feel like you've accomplished something every time you play. If we're not holding the gamer's attention, they're not gonna keep playing. You play for an hour or two hours, then you go to your character panel and you say, ""I accomplished something today! I put four points in my sword skill! That's fantastic!"" And on top that, the loot system where no creature drops the same loot twice and you're always gonna be able to find better stuff. Not a lot of titles offer that now.Thanks, we really appreciate it!No problem!                     "
415,https://www.engadget.com/2008-08-15-supercomputer-huygens-beats-go-professional-no-one-is-safe.html,Engadget,2008,8,15,148.0," You know how Go nerds are always going on about how magical they are since supercomputer AI hasn't yet cracked the ancient board game, and rarely beats even an average Go player? No? Maybe those are just our nerdy friends. Well, those folks can wipe the smug grins off their faces as they're faced with the sobering reality of defeat: Dutch supercomputer ""Huygens"" has defeated a human Go professional in an official match at the 24th Annual Congress of the game Go in Portland, Oregon. The newly-minted supercomputer was aided by the recently-developed Monte-Carlo Tree Search algorithm, a whopping 60 teraflops of processing power and a considerable 9 stone handicap. Poor Kim MyungWan -- who managed to beat the computer in three ""blitz"" games leading up to the actual match, and probably won't be hanging up his Go hat just yet -- didn't stand a chance.[Via Tech Digest]                     "
416,https://www.engadget.com/2008-08-12-bungie-acceptance-video-teases-new-halo-goodies.html,Engadget,2008,8,12,136.0," Bungie could have strutted all the way back to home base with the ""2nd Annual Halo 3 award for Interactive Innovation"" without saying a word, but the developer was graceful enough to record an extremely cool acceptance video taped somewhere within the Halo universe. And it looks like it's filled with some new Halo goodness.Are these some teasing tidbits about a possible Map Editor? Check out that last shot when our soldier pal lobs a plasma grenade towards the camera. Verrrrrrrry interesting. It's definitely a lot more robust-looking than Halo 3's Forge editor. Also, those look like new AI models of past cast members ... including Cortana. Playable models? In-game NPCs that'll be on your squad? Cats and dogs, living together? Who knows. What's for sure is that Bungie is saying ""the ride isn't over yet.""                     "
417,https://www.engadget.com/2008-08-07-interview-stargate-worlds-aims-high-with-artificial-intelligenc.html,Engadget,2008,8,7,155.0," MMORPG has posted its second video interview with the folks working on Stargate Worlds. Last time, it connected with Howard Lyon to talk about customization the game's user interface. This time, studio head Dan Elggren tells the site a little bit about the artificial intelligence that guides the actions of Stargate Worlds' hostile entities.Elggren explains that there are different types of AI behaviors -- personalities, if you will. An aggressive enemy might throw caution to the wind and charge the player with a knife, while a defensive one will play it safe behind cover. Elggren hopes this will make combat more dynamic in Stargate Worlds than it is in other MMOs. He also said that enemies might run away or retreat for the sake of self-preservation. Of course, enemies do that in plenty of MMOs already, but hopefully it's a bit more complicated than the ""run at 15% health"" trigger that we're used to seeing.                     "
418,https://www.engadget.com/2008-08-06-bungies-not-ruling-out-working-on-other-consoles.html,Engadget,2008,8,6,194.0," Bungie and Microsoft have been attached at the hip for awhile now. Some gamers are Halo haters, a lot more are Halo lovers (folks do like to buy those games). While the likelihood of Halo releasing for other consoles is pretty darn slim, any future projects from the company could see release on platforms other than the Xbox 360.During an interview with Gamesindustry.biz, Bungie's lead AI programmer, Damian Isla, didn't discount the possibility of his company working on other systems, stating that for now ""we are working on Xbox 360 with Microsoft but I don't know beyond that."" His own interests lean toward the Wii, however, as he lauded it for its interface and appreciated the industry ""moving in a direction where a much more casual gamer can come and pick up your game and wave a wand at a screen rather than learning ten different buttons to play Halo with the current Xbox 360 controller. That's really exciting and I think that's a very positive trend for the industry."" We guess Damian isn't a subscriber to the Scott Steinberg philosophy of gaming.We couldn't agree more, Mr. Isla. We couldn't agree more ...[Via Videogamer]                     "
419,https://www.engadget.com/2008-08-04-early-stargate-worlds-combat-video-shows-off-enemy-ai.html,Engadget,2008,8,4,126.0," Jon Wood at MMORPG recently put up a hands-on video of gameplay in Stargate Worlds. This latest footage gives us an early look at combat in Stargate Worlds. Primarily, it shows the importance of using cover to survive. Wood seems to like what he's experienced of the game so far, particularly in terms of the AI in Stargate Worlds.""Getting taken off-guard by AI opponents does not happen very often. But when it does, it's just fantastic. It gets your heart going, causes you to panic,"" he says. The comments related to the video are drawing comparisons with Tabula Rasa but keep in mind this footage is from an alpha stage of testing. That being said, what do you think about how Stargate Worlds is shaping up?                     "
420,https://www.engadget.com/2008-07-30-in-wow-and-other-games-pathfinding-is-still-kind-of-a-problem.html,Engadget,2008,7,30,303.0," If you're not much of a computer programming person, this one might make your eyes glaze over a bit, but if you have any interest how the AI of videogame characters, including those in WoW, is programmed, this article about designing AI pathfinding is a terrific read. ""Pathfinding"" is a method of determining how NPCs move within a game world like Azeroth -- you and I can clearly see where the walls and bad guys are, and so we just have to press buttons to avoid either ingame, but NPCs (including pets and mobs) aren't quite that easy -- they need to be told clearly by programmers where they can go and how to get there. And when the rules they're given don't quite work, you get the funny seen above.Many games use a ""waypoint"" system -- NPCs are given a series of paths around the space they can move in, and use those paths to determine where they can and can't go. The article argues for a ""navigation mesh,"" a much looser definition of available space, which NPCs can then draw their own path across. It's a little technical, but it's cool to see the inner workings (and weaknesses) of Azeroth's code.Of course, it's extremely unlikely that we'll ever really see the NPC pathfinding engine updated in WoW anyway -- Blizzard will update their system in certain places to fix things like exploits (and the occasional annoying escort quest, i.e. all of them), but there's no real need to update the whole system completely when there's so much content to be done. Hopefully videos like this will bring the problem to light, and in future games we'll see some better pathfinding. Someday, that NPC will know that it's easier to go around the pillar rather than trying to walk right through it.                     "
421,https://www.engadget.com/2008-05-29-snomote-robots-could-autonomously-explore-antarctic.html,Engadget,2008,5,29,126.0," We've seen a couple robotic efforts to explore the Arctic and the Antarctic, but they've all been remotely-operated -- unlike the SnoMote, a new bot being developed at the Georgia Tech that can navigate itself around ice and snow. Packs of the mini-snowmobile-based SnoMotes can negotiate with each other and ""bid"" on locations to investigate, and navigate by classifying microscopic fissures in the icy terrain. The bots haven't made it to the Antarctic yet, but they're apparently handling simulations quite well, and the plan is for teams of 40 to 50 of the $10,000 machines to wander the ice collecting data points for climate change models. Sure sure -- and the next thing you know, the Antarctic is the flashpoint of the revolution. Good plan, guys.                     "
422,https://www.engadget.com/2008-04-30-a-day-in-the-life-of-an-age-of-conan-designer.html,Engadget,2008,4,30,168.0," Joe Hegarty is an AI Designer for Funcom's Age of Conan project. And, it appears, he loves his job. In a 'Day in the Life' article at TenTonHammer, Hegarty describes the sheer enjoyment he gets from breathing life into the inhabitants of Hyborea. From street preachers to wayward children, all the way up to Conan himself, his work makes the NPCs we'll be interacting with on launch day all the more meaningful. The designer gives several examples of where you'll see his work in action, and even describes some of the behind-the-scenes mechanics driving the NPC's decisions. The designers at Funcom have a series of tools at their disposal, from in-house software to theoretical constructs. Hegarty mentions Maslow's hierarchy of needs as one of these supporting constructs; NPCs essentially have a priority list of needs, ensuring that you'll see them doing a variety of activities as you move through a given area.Fascinating stuff, and with the Open Beta kicking off tomorrow, something we'll all be experiencing very soon.                     "
423,https://www.engadget.com/2008-04-26-fruit-fly-flight-simulator-could-smarten-up-robots.html,Engadget,2008,4,26,138.0," Wondering just how we mere mortals were going to even give a robot enough smarts to completely overtake our societies? Oddly enough, some of that artificial brain power could come from studying the way fruit flies, um, fly. A completely bizarre flight simulator at Caltech actually plays ""scenes"" that flys react to, and considering that the fly is constantly held, researchers can closely examine how the insect attempts to navigate away from lines, blobs and all manners of incoming obstacles. Those working with the installation suggest that these studies could one day help autonomous robots function better, potentially taking some of the load off of our human workforce. We know, you've got three bold letters and a question mark running through your noggin right now -- just hit the read link to see what it's all about.[Thanks, Dave]                     "
424,https://www.engadget.com/2008-04-04-scientists-devise-software-that-can-interpret-attractiveness.html,Engadget,2008,4,4,144.0," Believe it or not, this isn't the first time we've seen software created in order to determine whether a subject is drop-dead gorgeous, but scientists at Tel Aviv University have seemingly concocted a program of their own that can ""interpret attractiveness in women."" Before you ask, the researchers have yet to perfect the art of computing the beauty of males, but they're attributing said omission to the difficulty in ""defining"" attractiveness in dudes. Nevertheless, the software was purportedly able to reach nearly the same conclusion as humans about how lovely (or not) a hundred different ladies were, as it broke down features like face symmetry, smoothness of the skin and hair color. We're told the development could eventually be used in ""plastic and reconstructive surgery and computer visualization programs,"" but c'mon, this is totally meant to automate the grueling Hot or Not process.[Via Physorg]                     "
425,https://www.engadget.com/2008-03-21-researchers-claim-to-have-developed-robotic-ai-on-par-with-a-pup.html,Engadget,2008,3,21,184.0," The robot uprising, it inches ever closer -- researchers working on the EU-funded COSPAL project have developed a new robotic AI that they say is the most advanced of its type ever created. Using both learning neural AI techniques and traditional rules-based AI as a control mechanism, the robot can be trained ""like a child or a puppy,"" and has managed to figure out simple sorting tasks on its own. The next step is to try and integrate the system into a car that can adjust to unforeseen driving conditions, but the researchers in charge aren't worried about creating an army of KARRs set to mow us all down -- according to researcher Michael Felsberg, adult-level AI based on this system probably won't happen in our lifetimes. On the other hand, we've heard machines will match us by 2029 -- that'll be a fun debate to have when we're all slaving away in the robo-mines.[Apologies for the terrible photo -- sure, COSPAL can develop puppy-level AI, but they're apparently still rocking a QuickTake 100.]Read - Physorg article on the COSPAL robotRead - COSPAL site                     "
426,https://www.engadget.com/2008-03-11-ai-hits-second-life.html,Engadget,2008,3,11,153.0," Researchers from Rensselaer Polytechnic Institute have created an artificial intelligence to drive an avatar in Second Life. Currently their AI has the smarts of a typical four-year old child, but it is capable of listening to what is ""said"" (in text) in Second Life and reacting to it appropriately as well as moving the avatar and typing its own replies.Whilst a four-year old might not sound like much to the uninitiated, Professor Bringsjord, leader of the project and head of the Cognitive Sciences Department points out that to fully mimic adult behaviour would take a super-computer. As well as learning about creating AIs, the team are able to test theories about the development of the human mind. There is a (rather large) video of Edd the AI demonstrating that he can't put himself in the mind of another person (something typical of 4 year olds), and chatting briefly to two ""normal"" avatars available.                     "
427,https://www.engadget.com/2008-03-10-molyneux-boasts-about-his-post-fable-2-project.html,Engadget,2008,3,10,138.0," Mad scientist extraordinaire Peter Molyneux has made a great discovery! In an interview with GamersGlobal, the Fable 2 designer talked about an AI-focused experiment codenamed Dmitri. The experiment became a game six months ago after ""a discovery was made."" Said Molyneux, ""I think that discovery is so significant... This discovery has lead us to start a game and that game will be on the front cover of Nature magazines and Science magazines."" That's assuming the AI project doesn't develop self-awareness and kill us all.As for a release date, Molyneux said it would come out when ""trees still [have] leaves, but with a brownish tint."" Though our first thought is September, we've gone ahead and sent to Microsoft a picture of the official Joystiq elm tree spray-painted with the proper color in the hopes of an early review copy.                     "
428,https://www.engadget.com/2008-02-28-professor-decries-robotic-killing-machines-clearly-prefers-to-d.html,Engadget,2008,2,28,169.0," So the military is continuing down the totally inevitable path of computer-controlled autonomous robo-warriors capable of fighting deadly human battles on our behalf -- and out come the naysayers like U of Sheffield prof Noel Sharkey, who, at The Ethics of Autonomous Military Systems conference in London, decried the bots' self-determined killing abilities as ""a threat to humanity"" -- especially if they're captured and re-purposed by terrorists to do their evil bidding. Sharkey exclaimed that he's ""worked in artificial intelligence for decades, and the idea of a robot making decisions about human termination terrifies [him],"" but -- and we're just gonna throw this out there -- what if being oppressed by a race of automatons run amok were actually an improvement over our corrupt governments of men? Isn't that a possibility, too? We're certainly going to keep telling ourselves it is, thankful we've somehow managed to not be overthrown by our own creations. Thus far.P.S. -We dare you, commenters, NOT to bust out the welcoming our robot overlords cliché.                     "
429,https://www.engadget.com/2008-02-27-tth-video-shows-new-tr-ai-changes.html,Engadget,2008,2,27,231.0," The good people over at Ten Ton Hammer have a video up showing some of the new changes that the Destination Games crew has made to the artificial intelligence of a number of common Bane enemy units in Tabula Rasa. Narrated by a developer, they explain how encounters with Thrax Pistol Soldiers, Lightbenders, and Juggernauts will be different than players are accustomed to, as each unit has gained a skill that will require tactics and quick-thinking to avoid. Thrax Pistol Soldiers now drop a small robot when they die which will blow up nearby corpses, Lightbenders now have a blinding attack which will momentarily incapacitate anybody who gets too close, and Juggernauts have gained a Reality Ripper-esque ability to pull players to within their preferred firing range.These new attacks, brought into the game in patch 1.5, will definitely throw a wrench in the ways that players are accustomed to approaching encounters. Lightbenders and Thrax Pistol Soldiers are among the most common units in the game, so this fix should be evident almost as soon as logging in. The video also makes mention of the new AFS mech attacks that we showed you a few days ago. It's my hope that in the next round of AI upgrades, they make the typical AI grunt slightly more formidable, as it's becoming increasingly common to see them set up by Tree Lurkers and ticks                     "
430,https://www.engadget.com/2008-02-17-kurzweil-predicts-that-machines-will-match-man-by-2029-bring.html,Engadget,2008,2,17,115.0," Famed technologist and futurist Ray Kurzweil is on the record about human-machine intelligence parity: it's going down by 2029, so be prepared to get digital on entirely new levels. Apparently, machines ""will have both the hardware and the software to achieve human level artificial intelligence"" by then, but even if it's not in the form of meatbag-terminating cyborgs, Kurzweil thinks one future of intelligent machines is on the nano scale, with interfaces to enhance our own physiology and intelligence. Oh sure, this stuff is completely pie in the sky -- but it's still absurdly fun to think of what kinds of crazy crap the 21st century's going to hold.[Thanks to everyone who sent this in]                     "
431,https://www.engadget.com/2008-02-06-science-says-neurotic-ai-has-a-gaming-edge.html,Engadget,2008,2,6,166.0," While we're not sure how comfortable we are with our artificial intelligence constructs having human-like personalities attached to them, we are interested to know what affect those personalities would have on our robot overlords' videogame abilities. So we suppose it's a good thing that the Austrian Research Institute for Artificial Intelligence conducted a study to see what types of AI personalities were best suited to games.The study programmed four artificial intelligence agents to play single-player Age of Mythology with four distinct play styles: aggressive, defensive, normal and neurotic, the last of which was saddled with ostensible drawbacks such as ""irrational assessment of resource value"" and ""tendency to resort to extreme playing styles."" Despite these drawbacks, though, the neurotic AI played itself to a perfect 7-0 record and, surprisingly, achieved wins 3-12 minutes faster than its opponent AI, on average. The takeaway from all this? If you see Woody Allen in the Xbox Live lobby, beware!View - Study presentation slides (PDF)Read - Study summary at Mind Hacks                     "
432,https://www.engadget.com/2008-01-18-jumpgates-a-i-evolves.html,Engadget,2008,1,18,231.0," More details have emerged from the jumpgate via Steve Hartmeyer's Dev Journal over on MMORPG.com. Steve is a programmer at NetDevil working on the hotly anticipated (at least it is for many of us here at Massively) space combat MMO, Jumpgate Evolution.Steve's latest entry,  AI System: Nuts & Bolts, takes us on an amazing detailed journey through the evolution of Jumpgate's artificial intelligence system. He explains how it morphed from ""simply"" populating space and simulating everyday tasks of an immense number of AI ships to a system that ultimately allows players to react to and participate in spontaneous events originated by the AI itself! Uh... holy crap! No wonder the dev team is so stoked about this game's AI system. Between the comparison to X-wing games we heard about yesterday, and now learning of the off the charts Intelligence Quotient of the AI... the needle just done popped off my Hot Meter! But there's more to it then the pure glee of such a cool game on the horizon. When you realize the capabilities of today's gaming AI (not just in JE, but where gaming AI is at right now and where it will likely lead), it all actually gets a bit creepy-cool in the ""Wow, this sounds a lot like the theoretical beginnings of the Matrix or Cyberdyne Systems' development of Skynet"" sort of way.Things that make ya go hmmm...                     "
433,https://www.engadget.com/2008-01-03-the-daily-grind-player-vs-punk.html,Engadget,2008,1,3,168.0," As a mechanic, PvP is meant to give players the ultimate challenge. No matter how tough you make an AI enemy, an actual person will always pose a more interesting/challenging threat due to their unpredictability. The essential issue is that people -- while anonymous -- are often complete and utter punks.A good deal of initial player experience with PvP is pretty close to someones first experience with a root canal. It's also similar in the sense that if you've never had that experience you're probably better off for it. Lastly, one other way they're similar (to me, at least) is my firmly held belief that every player is like a spawn of Steve Martin from Little Shop of Horrors. It fits quite well, a lot of the people who enjoy PvP take great amounts of pleasure in pummeling heads into the ground. So what do you think dear reader? Is all PvP bad, or are there games out there that you think have or will remedy the issue?                     "
434,https://www.engadget.com/2007-12-04-ai-middleware-makes-its-way-to-wii.html,Engadget,2007,12,4,172.0," Knyogon, the company who brought AI tech Kynapse to the industry, has announced that its middleware is now available to Wii developers. It was used in the Xbox 360 game Crackdown and is also being implemented into the upcoming Fable 2. But, as far as Wii developers using the middleware, details are less clear. Knyogon's press release states that 2 developers are utilizing the Wii version fo the middleware as of now, but fails to go into any kind of specifics beyond that. Pierre Pontevia, Kynogon CEO, comments that ""Wii is a remarkable success and we are proud to offer Kynapse to Wii game developers. The unique capabilities of Wii allow developers to produce creative forms of gameplay that require innovative AI. Kynapse has been selected by a very significant number of industry leaders for the development of AAA titles."" He goes on further to add that ""With Kynapse, we feel Wii developers have another critical tool in their hands that helps them unleash the real magic of the Wii system.""[Via Joystiq]                     "
435,https://www.engadget.com/2007-12-04-popular-ai-tech-throws-support-behind-wii.html,Engadget,2007,12,4,170.0," Realism is not the Wii's domain. We've come to terms with this, so much so that our psychiatrist has begun to offer us half-rates for her trouble. Even so, any efforts to help the little console along the path of next-gen righteousness are welcomed. An update today from the AI middleware sleuths at Kynogon is certainly that, as the company has announced support for the Wii, offering their popular AI tech Kynapse to Wii game developers in order to help keep their NPCs from running around all willy-nilly.Kynapse is already used in a number of other titles, both currently available and in production for a variety of platforms, including Crackdown, Alone in the Dark, and the upcoming sequel to Fable, and while no Wii developers have come forward and announced titles they expect to take advantage of the AI solution, Kynogon notes that Kynapse has already been tapped by two different studios for use in upcoming Wii projects. Who knows, maybe someday soon we can quit visiting the shrink altogether.                     "
436,https://www.engadget.com/2007-12-03-intelligent-coasters-create-new-wave-of-drinking-games.html,Engadget,2007,12,3,194.0," Believe it or not, engineering minds have figured out a way to gadgetize even coasters, but Sentilla's Smart Drink Coasters hope to be the zaniest yet. Designed using vanilla glass coasters from Michael's (an arts and crafts store), one of Sentilla's diminutive pervasive computers, a handful of LEDs and a few other ingredients, these units can not only blink in different ways depending on how full / empty one's beverage is, but they can be used to create a whole new world of drinking games. Reportedly, the gizmos can be programmed to understand when a drink is on it and when a refill is needed, theoretically enabling a bartender or server to be alerted of one's drink status without even going over to check. Additionally, a set of coasters can be used to replicate a memory game much like ""Simon,"" which could undoubtedly create a room full of laughs in the right circumstances. Granted, this all works under the assumption that you'll actually slam your container down on (i.e. not just nearby) the intelligent coaster, but if you think you're that coordinated, hit the read link for a full description and a video demonstration.                     "
437,https://www.engadget.com/2007-12-02-researchers-set-sights-on-uber-dexterous-robotic-hand.html,Engadget,2007,12,2,143.0," Dr. Honghai Liu, one of the two researchers heading up a project to craft an exceptionally deft robotic hand, has called such a device ""one of the holy grails of science,"" and honestly, we can't say we disagree. He, along with Professor Xiangyang Zhu, was recently award a Royal Society grant to further research the possibility of using artificial intelligence to create software that could ""learn and copy human hand movements."" A sensor-laden cyberglove has been used to capture data about how the human hand moves, and the duo hopes to eventually use the findings to produce the ""perfect artificial limb."" Of course, there's no telling how long it'll take for such technology to actually be perfected, but we can already see the line forming with folks eager to swap out their own hand for one a bit more adept.[Via The Raw Feed]                     "
438,https://www.engadget.com/2007-11-21-lead-producer-of-jumpgate-evolution-writes-developer-journal-on.html,Engadget,2007,11,21,358.0," Lead producer of Jumpgate Evolution Herman Peterscheck has posted a developer journal at MMORPG.com about the artificial intelligence used in the upcoming space MMO. Peterscheck describes how the AI in Jumpgate Evolution is employed to craft a universe that will feel alive to players.At the most basic level, Peterscheck tells how the AI should be able to do anything that a player does -- mining, trading, combat, etc. On top of that, the NPCs will have behaviors, like patrolling, decisions on if they like you or want to blast you out of space, and when to chicken out of a fight. The aspect that he believes is perhaps one level above what you might see in a lot of other games, however, is the way that the AI will react to situations occurring in the world. The example that Peterscheck gives is of a station that broadcasts that it is low on a certain resource, and another station deciding then to load up a ship with said resource and send it their way to give them a hand. Now when I read this, I immediately thought, ""Oooh, I'd be out there looking for that hauler to 'borrow' their cargo and drive a hard bargain with the needy station!"". And it turns out, that is exactly the kind of thing that Peterscheck hopes will happen. Maybe there would be other players with the same idea that would be competing with me for the bounty -- and after the hauler has been engaged, you can be sure that back-up would be on the way, and it wouldn't be long before the sky was lit up in a giant firefight.These sorts of dynamic interactions between the AI and game world are there to promote the feeling that things are going on and to immerse players in the game world. I'd like to hear other examples of how this advanced AI will find ways to interact with other AI and players, but what we've learned so far definitely sounds interesting. If this has piqued your interest, you can sign up for the beta here, and the full developer journal is linked below.                     "
439,https://www.engadget.com/2007-10-15-together-ps3s-can-be-very-brainy.html,Engadget,2007,10,15,122.0," Students from UC Irvine and Dartmouth college have netted themselves the $10,000 prize at IBM's Cell Broadband Engine Professor University Challenge on September 24. By connecting three PlayStation 3s together the team have emulated functions of the human brain - specifically, visual recognition algorithms. While other gaming platforms had substantial lag when performing visual recognition tasks, the PS3 lowered that to only a second of thinking time in order to recognise an object.This marks only the beginning of the group's research into utilising artificial brain algorithms for use in robots, cars and other machines. Recreating the human brain, curing cancer - what can't the PS3 do? Maybe one day it'll even have some games to play. We kid, we kid ...[Via Digg]                     "
440,https://www.engadget.com/2007-09-25-sony-ericsson-w890i-to-make-good-on-w880is-shortcomings.html,Engadget,2007,9,25,140.0," A random Polish auction site probably isn't the first place we'd think to look for leaked prototype Sony Ericsson hardware, but heck, maybe it should be. This here stylish, brown, brushed metal candybar showed up recently on allegro.pl, bearing the telltale ""SE123"" badge on its rear and all the signs of a successor to the 9.4mm W880i: a listed 3.2 megapixel camera (up from the Ai's 2 megapixel piece), UMTS, 12mm case thickness, Memory Stick Micro M2 slot, and a slightly -- okay, significantly less ridiculous numeric keypad. You might argue that it's a step down in the looks department from its supposed predecessor, but with the better cam, better keypad, and we'd imagine a slightly tighter fit by launch, we're guessing it'll have a few takers. You didn't forget EDGE this time around, did you, Sony Ericsson?[Via Unwired View]                     "
441,https://www.engadget.com/2007-09-17-cognitive-code-shows-off-silvia-artificial-intelligence-platform.html,Engadget,2007,9,17,139.0," Cognitive Code looks to be taking full advantage of the TechCrunch40 conference to show off its SILVIA artificial intelligence platform, which it calls a ""fundamental conceptual breakthrough in artificial intelligence."" Apparently, the platform allows for the ""development and deployment of intelligent applications to almost any platform"" (which we can only hope includes robots), allowing for human interaction in ""completely natural and intuitive ways."" What's more, it looks like the system should be relatively easy to implement (at least in AI terms), with a complete set of GUI tools available for ""developing intelligent entities,"" and an array of scripting APIs at developers' disposal to embed SILVIA in their applications. If that's not enough, Cognitive Code is also apparently hard at work on a portable version of SILVIA for PDAs and smartphones, with it apparently set for release in late 2008.                     "
442,https://www.engadget.com/2007-09-03-biofeedback-signals-used-to-predict-gamers-moves.html,Engadget,2007,9,3,146.0," While it's no shock that artificial intelligence as a whole is making strides, a pair of Hungarian researchers have seemingly unlocked a secret that gamers are sure to detest. Laszlo Laufer and Bottyan Nemeth, both from the Budapest University of Technology and Economics, have reportedly ""discovered that a gamer's button presses can be predicted two seconds before they make them, through measurements of skin conductance."" To make such a bold claim, the duo had guinea pigs play a simple game while their heart rate and skin conductance were measured, and after utilizing ""neural networks to analyze the biofeedback signals and input records,"" the data showed that we humans aren't as unpredictable as we sometimes hope to be. Notably, this unearthing could be used in quite a few applications outside of infuriating gamers, but we all know where the real fun in this is.[Via The Raw Feed]                     "
443,https://www.engadget.com/2007-07-25-humans-fend-off-ai-challenge-in-milestone-poker-match.html,Engadget,2007,7,25,149.0," It apparently wasn't easy, but a pair of top human poker players managed to narrowly beat a brash young artificial intelligence program yesterday in a poker match scientists had touted as a ""milestone"" comparable to Garry Kasparov's 1997 bout with the IBM's chess-playing Deep Blue. According to the AFP, the four-round match stretched on until 11pm, with poker players  Phil Laak and Ali Eslami ultimately edging out the program, dubbed Polaris, by 570 points. Eslami seems to have been particularly impressed by his competitor, saying that he found playing against Polaris more exhausting than any previous game in his career, adding that ""it's already so good it will be tough to beat in future."" No word on a rematch just yet, but don't be surprised if you run into Polaris the next time you play a little online poker -- it's gotta recoup its losses somehow.[Photo courtesy of AP]                     "
444,https://www.engadget.com/2007-07-25-computer-learns-baby-talk-wont-require-a-college-fund.html,Engadget,2007,7,25,151.0," In an attempt to better understand the way in which human babies learn to speak, researchers at Stanford University say they have created a computer program which can learn baby talk. The largely accepted theory about human language is that all of the sounds we make are hard-wired into our brains, but now that James McClelland -- a professor at the Palo Alto college -- has tested his theory, it would appear that those notions have been debunked. During ""training sessions"" in both English and Japanese, a computer followed along to recordings of mothers speaking to their children, and was able to pick up the basic vowel sounds as the baby did. ""It learns how many sounds there are. It figures that out,"" the professor said, he then laughed maniacally and continued, ""and once it has learned to speak, it will be trained to sing the most beautiful operas ever written.""                     "
445,https://www.engadget.com/2007-07-23-texas-hold-em-champs-face-off-with-pokerfaced-computer.html,Engadget,2007,7,23,154.0," While highly intelligent computers have been pwning humans in backgammon, checkers, and chess for years, machines haven't had nearly as much luck against poker sharks. According to a number of researchers, however, that will surely change over the next decade or so as the programming is honed to better anticipate the human's moves. Nevertheless, poker champion Phil Laak and fellow professional Ali Eslami will soon sit down for a two-day contest at the Association for the Advancement of Artificial Intelligence conference in Vancouver, British Columbia. Up for grabs is a $50,000 prize, but moreover, University of Alberta's games research group will be interested in figuring out how to better prepare computers to understand and deal with the complex scenarios that only apply to poker. 'Course, with one-petaflop supercomputers now available for civilian use, we're sure it won't be too long before silicon and PCB rule supreme over our feeble brains in yet another facet.                     "
446,https://www.engadget.com/2007-07-20-canadian-ai-plays-perfect-game-of-checkers.html,Engadget,2007,7,20,124.0," A team of researchers at the University of Alberta in Canada claim to have ""solved"" the game of checkers, using a computer program named Chinook which has been playing matches against itself for the past 18 years. The program played 500 billion billion possible positions in the 5,000-year-old game, also known as draughts, before concluding that perfect play by both sides leads to a draw (a concept which grandmaster players have apparently hypothesized for years). One of the researchers said in a statement that he believes they have ""Raised the bar... in terms of what can be achieved in computer technology and artificial intelligence."" Next up, Chinook is to be renamed W.O.P.R., and then will begin playing a series of tic-tac-toe games against itself.                     "
447,https://www.engadget.com/2007-05-29-artificial-cerebellum-could-improve-robot-motor-skills.html,Engadget,2007,5,29,164.0," Sure, modern robots can clean up after you, keep watch on the kids, and chase away unwanted intruders, but there's no denying that an unexpected gust or stray stack of Lego blocks can bring even the most sophisticated humanoid to its knees. To cure such clumsiness, researchers at the University of Granada are reportedly working with electronic engineers, physicists, and neuroscientists from a range of universities including Edinburgh, Israel and Paris as a part of the Sensopac project which aims at ""reproducing an artificial cerebellum."" The application of the cerebellum would allow androids to purportedly ""carry out similar tasks as mammals and might help to treat cognitive diseases such as Parkinson's or Alzheimer's."" Apparently, the team is hoping to create an implantable device to ""make movements and interaction with humans more natural"" within two years, and while it's probably obvious, one of its primary uses would be in home-help robots who need to be agile whilst aiding the elderly.[Via BBC, image courtesy of Sensopac]                     "
448,https://www.engadget.com/2007-04-24-guardian-pets-need-a-mind-of-their-own.html,Engadget,2007,4,24,304.0," This forums thread points out something interesting about player ""guardians."" Not pets-- guardians like Shadowfiend (which a priest I know called his shadowfriend), the druids' treants, and my shaman's totem elementals. After players wonder why shadowfiends keep breaking shackles, Neth says something that made me do a double-take: shadowfiends, as guardians, have an actual AI that is supposed to go after non crowd-controlled targets first.That's news to me. I haven't spent a ton of time around shadowfiends, but in my experience, shaman and mage elementals and other ""uncontrollable"" pets (that's why they're called guardians) tend to go after anything that happens to be close to them. That's why they don't get popped when there's sheep or shackles around-- my guild could have used that fire elemental DPS on Moroes, but because it was so important to keep those shackles up, I've been saving the elemental for later. If there really is an AI (and if it works-- even Neth agrees that may not be the case at this point, though she says the shadowfiends on the PTR are supposed to be doing things right), then maybe we can start trusting summoned guardians not to touch CC'd targets until it's OK to do so.Of course, there's other ways around it-- normally, I just don't pop my pets out until I'm sure there's no more CC left to break, but my pets are leashed to my totem, so with careful positioning, I can still avoid CC. And I believe both mage and druid guardians are targeted-- they open fire on whatever you've got targeted at the time, right? But I'd love a little AI, or at least a little control, in something like my Searing Totem. If there's a CC'd target out there, it's not worth the trouble to drop it even for the extra DPS.                     "
449,https://www.engadget.com/2007-04-24-learning-coffee-machine-on-the-horizon-could-use-gps-rfid.html,Engadget,2007,4,24,204.0," Although a coffee machine that slowly but surely learns your daily preferences in regard to cups of java may sound outlandish, the already-created RFID-enabled refrigerator certainly brings things back into focus. A ""provisional patent exploration into coffee machines that learn and react to their users"" is underway in Lafayette, Indiana, as James Pappas is hoping to take ubiquitous computing to the next level on coffee makers of the future. While internet-connected and weather-displaying renditions are already on store shelves, Pappas is hoping to utilize some form of GPS / RFID technology to create a machine that learns and adapts to your coffee drinking ways so it can automatically have a white chocolate cappuccino ready and waiting each weekday (except Monday, which is your straight-up black coffee day, right?) without you having to touch a thing. Furthermore, he's hoping to take the idea to the mobile front, as he refers to a cellphone interface to dial-in your next request so that it's ready to go by the time you hit the kitchen. Still, it sounds like the invention is a few years off at best, but serious drinkers better hope this thing automatically alerts you when the beans are running low, too.[Image courtesy of CoffeeToThePeople]                     "
450,https://www.engadget.com/2007-04-10-hacker-gets-revenge-on-puzzle-quests-bullying-ai.html,Engadget,2007,4,10,144.0," We're not certain what sort of demonic sacrifices Infinite Interactive made to grant Puzzle Quest its hellborn AI, but we imagine that the cursed contract that authenticated the ceremony was written with the blood of many innocents. The match-three puzzler does everything short of outright cheating, stealing your advantages and setting up multiple combos, each computer-cleared gem bearing the mark of Mephistopheles.DS gamer Zaraf plotted a strategy that would tear down the AI's defenses and avenge dozens of unfair losses. Unwilling to spend months leveling up and making preparations, staying his vendetta, he hex-edited the game to to max out his character's stats. Zaraf then armed his warrior with a class spell called Deathbringer, enabling him to fill the screen with an amount of damaging skulls equal to half of his red mana. Head past the post break for the results caught on video.                     "
451,https://www.engadget.com/2007-04-01-puzzle-quests-ai-doesnt-cheat-but-you-can.html,Engadget,2007,4,1,284.0," If the number one complaint gamers have with Puzzle Quest is its limited availability at game shops, then the second most common point of protest would be the Puzzle/RPG's cheating AI. People are just as apt to sing praises about its addictive gameplay as they are to howl over the AI's godlike prescience. We've spent more than a few battles shaking our fists at the game as computer-controlled enemies racked up lucky combos and more extra turns than chicken on a rotisserie.Sensing that the mob was two forums threads away from storming his house with torches and pitchforks, Infinite Interactive's Steve Fawkner made a public statement assuring players that the AI has no unseen advantages. Having worked on the code himself, Steve reasoned that he's too lazy to have programmed anything that advanced.If that explanation isn't convincing enough, there are still steps you can take to even the playing field. You can unlock a debug menu by pushing in a complex set of keypresses, allowing you to activate several hidden features. Check past the post break for more details on the cheat code and a comic about Puzzle Quest's AI. These keys were purposefully made difficult to hit, you need to hit them on the world map screen not only in the correct order, but also with the correct timing. A short keypress is simply a quick click, noted with the symbol: ""A"",""B"". A long keypress needs to be held for around half a second and is denoted as: ""=A="", ""=B="".A A B =B= =A= =A= B =B= =A= A =B= =B= =B= =A= =A= =A= =B= B B B If you enter the button presses correctly, you should now see the screen above.                     "
452,https://www.engadget.com/2007-03-24-university-of-essex-developing-autonomous-model-car.html,Engadget,2007,3,24,136.0," DARPA's Grand Challenge certainly snags a majority of the spotlight when talking about autonomous vehicles, but researchers at the University of Essex are looking to tackle the idea on a (literally) smaller scale. Seeking to craft a ""driverless model car,"" the project will reportedly utilize a standard remote control model vehicle, which will be flanked by a PC, camcorder, and a bevy of sensors. Supposedly, the software that will be riding on board will allow the vehicle to be ""entirely autonomous"" by recognizing obstacles, making tactical decisions, and driving itself around a test track. The team responsible for the prototype hopes that this small-scale, low-cost endeavor will ""pave the way"" for autopilot cars of the future, and considering the problems we mere humans are already having with newfangled technology, that day can't come soon enough.[Via Slashdot]                     "
453,https://www.engadget.com/2007-03-11-sxsw-arg-the-attack-of-the-alternate-reality-games.html,Engadget,2007,3,11,576.0," Ever since the success of The Beast, the alternate reality game created to help pimp Spielberg's A.I. back in 2001, alternate reality games (ARGs) have been popping up left and right, most notably the I Love Bees ARG that was used to launch Halo 2. Based on what the panelists were telling us, there are a lot more coming down the pipeline.However, one of the problems was that the panel promised to help define the term ""alternate reality game,"" but that never happened. Wikipedia calls it ""an interactive narrative that uses the real world as a platform, often involving multiple media and game elements, to tell a story that may be affected by participants' ideas or actions."" Which is quite a mouthful. But that makes us wonder, does it have to use the web as a medium to be an ARG? When people used to play T.A.G. or Killer on college campuses, that was definitely an ARG ... but where did those games go? Basically the web has taken that model and made it much easier to disseminate, but the current ARGs that have been appearing over the past few year have been operating on huge budgets and large amounts of resources. The Audi Art of the Heist game actually parked a car somewhere at the Coachella music festival in California that game players were actually supposed to ""break in to"" and steal an SD card that contained further clues. Although Audi forgot to leave the doors open, and no one got the card. Oops.Mind Candy's enormous ARG Perplex City has really taken things to an extreme. Their game has been running for the past three years, and recently one of the ""historical objects"" (the Receda Cube) that gamers try to find through puzzles, clues, and cards was found, and netted the player $200,000 (that's real money, too) in the process. The game designers stage car chases, have black helicopters buzz people, and can generally scare the bejeezus out of you. Not too shabby for gaming.Although most ARGs tend to be tied to something that they are promoting (Halo 2, Audi, A.I., Perplex City sells ""game cards"" that you have to purchase), some are starting to appear that have a purpose other than driving a product or a brand. Recently announced at GDC was Jane McGonigal's socially conscious A World Without Oil ARG that asks users to imagine how they would function in a world without gasoline or oil. Both aspects of ARGs have the same goal, they want to harness the community aspect of the ARG to do something, whether it's educate, get a message across, or sell a product. It taps into Linda Stone's theory of ""continuous partial attention,"" meaning that you'll keep coming back into the game day after day (possibly hour after hour) to check on things, and see if the game has advanced, or if anyone has figured out the riddles. It's almost like incessantly checking your email, just in case you might have missed something in the past few minutes.Check out some of the ARGs out there right now, including the panelists' own ReGenesis, and the fairly amazing Perplex City. With more on the way (including a mystery ""socially responsible ARG from panelist Brooke Thompson) we'll be excited to see what comes our way, alternatively. Plus we really feel like pirates now after typing ARG so many times.LINKS:ARGnet - the Alternate Reality Gaming networkUnfiction - all about Alternate Reality Gaming                     "
454,https://www.engadget.com/2007-03-07-ai-program-slammed-for-practicing-law-without-a-license.html,Engadget,2007,3,7,195.0," While artificial intelligence programs offering legal advice aren't exactly anything new, as Wired's 27B Stroke 6 reports, it looks like we've now seen the first case of one running into trouble with the law for doing so. The over-eager AI in question was offering its services to entrepreneur Henry Ihejirika, who put the program to use on two of his websites, offering bankruptcy assistance to clients without the hassle of a face-to-face meeting. Things were apparently going swimmingly until a bankruptcy trustee noticed errors in some of the forms that were submitted by a client of the site, which led them to investigate the situation, ultimately resulting in Iherjirka heading to court to explain himself. After reviewing the case, a bankruptcy judge ruled that the software went far beyond simply providing clerical services and was, in effect, practicing law without a license. That meant Ihejirka had to pull the plug on the system, as well as pay fines and return all fees he had collected from clients. While the AI could not be reached for comment, it'll no doubt find plenty of work on the inside, helping out prison guards with their taxes.[Via Boing Boing]                     "
455,https://www.engadget.com/2007-03-05-papero-gets-blogging-software-engadget-one-step-closer-to-full.html,Engadget,2007,3,5,240.0," Now we were always under the impression that a personal blog is supposed to be just that -- personal -- so we're not sure that we see the advantages of bringing a robot in to automate this process by filling your site with multimedia content that it thinks you'd want to share with the rest of the world. Nonetheless, NEC has done exactly that with its little cannibalistic PaPeRo bot (you know, the one that thinks humans taste like bacon), endowing it with AI software that recognizes certain keywords uttered during a conversation with its master and then scours the net for seemingly-related pics, vids, and tunes. Scheduled to be unveiled at 13th Annual Conference of the Association of Natural Language Processing later this month in Japan, the newly-spec'ed PaPeRo will be tasked with listening to you talk about your boring day at work (""So I commuted the eight feet from bed to desk, blogged all morning, ate lunch, blogged some more, ate dinner, and then blogged until bedtime.""), and then turning your page into what we can only imagine will be a blinking, flashing, slow-loading lookalike of some teen's gaudy MySpace. Just be careful what you talk about from now on, because PaPeRo may be listening, and the last thing you want on your blog is a visual representation of that thing you've been doing to your coworkers' coffee every morning for the last eight weeks.[Via Digital World Tokyo]                     "
456,https://www.engadget.com/2007-02-18-watch-out-stanley-here-comes-junior.html,Engadget,2007,2,18,186.0," With the slow vehicle passing and a 50mph speed limit, the 2005 DARPA Grand Challenge didn't entirely seem to set the stage technologically for the 2007 DARPA Urban Challenge, but get ready, Stanford's already prepping, their entrant: Junior. The Volkswagen Passat wagon will be equipped with a 360-degree laser rangefinder, bumper mounted lasers, RADAR, GPS, a network of systems and software powered by Core 2 Duo processors, and hopefully also spinners to distract the competition's junk-ass rides. Junior's mission, if you choose to recall it: drive a simulated urban course 60 miles long; it must obey California state traffic laws, it must not crash, it must be able to operate without GPS, and it must run the course entirely without human input. The $2m at stake for first place is probably not nearly enough to immediately recoup the costs of a bunch of braniac grad students hacking complex AI algos, but it could be the icing on the cake for the current favorite after 2005's Grand Challenge was routed by Junior's pappy, Stanley.P.S. -CNET has some early pics of Junior's interior and such, check 'em out.                     "
457,https://www.engadget.com/2007-02-07-sony-ericssons-walkman-w880-reviewed.html,Engadget,2007,2,7,191.0," Sony Ericsson's venerable Walkman line just received a good dose of cool with the announcement and (pending?) release of the W880. Sony Ericsson has joined the thin-is-in crowd with this one, as the W880 comes in at a skinny and svelte 9.4mm thick -- pretty dainty for a candybar unit. This is one of the newest Sony Ericsson handsets we've been pleased to view and gawk at. Slim, trim, and comes in fightin' with such features like 2 cams (a VGA one for front-facing video calls), Bluetooth 2.0 A2DP and Memory Stick M2 support (alas, Sony's proprietary format) and...we'll say it again -- an ultra-slim shape that's more Samsung-ian than from the SE boys. A few things that stood out in the GSM Arena review was the inclusion of ""multitasking"" support (we can guess at the meaning there), EDGE being absent and the inclusion of a standard 3.5mm audio adapter for using your own high-end headphones with the W880. We wish all handset makers would do this -- since using those sweet Shure earphones are, well, worth it. GSM Arena's verdict with the in-depth review? This is one sweet candybar.[Thanks, Simon]                     "
458,https://www.engadget.com/2007-02-06-sony-ericssons-w880-ai-walkman-musicphone-unleashed.html,Engadget,2007,2,6,179.0," Just like we thought, Sony Ericsson officially launched their W880 (Ai) Walkman musicphone today. Good thing too, 'cause all the leaked photos and SE teasers were getting a bit tiresome. So was the wait worth it? Depends, do you like slim musicphones swaddled in brushed stainless steel? At just 9.4-millimeters thin, it's SE's slimmest Walkman phone yet and packs a 1.8"" QVGA 262k TFT display and 2 megapixel camera with a 1GB Memory Stick Micro (M2) card included in the box. It also comes pre-loaded with Walkman Player 2.0, Disc2Phone music management, and the TrackID music recognition applications. It also features a Flight Mode for use on the airplane and battery capable of up to 20-hours of music playback or up to 6 hours 30 mins of talk time. Good so far right? Sadly for those of us in The States, it'll only be sportin' UMTS 2100 and GSM 900/1800/1900 when it hits Europe in Q1. A GSM-only variant dubbed the W888 is headed to China. Check the gallery below for a taste of what might have been. %Gallery-1476%                     "
459,https://www.engadget.com/2007-02-01-sony-ericssons-w880-ai-launching-next-week.html,Engadget,2007,2,1,127.0," Oh yes it is. That's the Sony Ericsson W880 ""Ai"" Walkman up there comin' atcha straight out of Sweden. We still don't know if the tri-band GSM / UMTS and QVGA specs are the real deal or not. But that's definitely a 2 megapixel camera on the back. Now, according to Swedish site NYA!mobil.se (via a bit of janky machine translation), the Walkman W880 will apparently join the living on Tuesday, 6 February -- in Sweden anyway, home country of the Ericsson half of that SE equation. At this point, that date seems fair enough. Besides, they've managed to get their hands on the device so they must know a little something, eh? More pics including a peep at the user interface after the break. [Thanks, David]                     "
460,https://www.engadget.com/2007-01-22-sony-ericsson-w880i-ai-gets-fcc-blessing.html,Engadget,2007,1,22,126.0," We've seen Sony Ericsson's darling go from the drawing board to cardboard and from photo shoot to quasi-realization, and now the FCC is giving us all precisely what we knew was coming. The handset formerly known as ""Ai"" has now been granted a pass by the Commission, setting things in motion for the W880i to grace the hands of civilians sooner rather than later. No, there's no new deets concerning price or release information, but all those out there yearning for this handset to hit your market shouldn't be in the dark for too much longer, as we're officially on the home stretch now. Be sure to hit the read link for all the specifics from the Commish itself, if you're into that kind of thing.                     "
461,https://www.engadget.com/2007-01-08-sony-ericsson-teases-us-with-w880-ai-shots.html,Engadget,2007,1,8,109.0," It ain't exactly the full disclosure we were hoping for, but hey, baby steps... Sony Ericsson's at least admitting the W880 ""Ai"" exists now, and that counts for something, right? The press release couldn't possible be more sparse, simply stating that the handset ""will blend astonishing good looks with all of the music-centred features that fans have come to expect"" from Walkman phones and more details will be provided closer to release in the first half of '07. And, oh yeah, there are a couple teaser shots that manage to be even less revealing than the plethora of spy shots we've already scored; at least they're kinda artsy.[Thanks, Mike]                     "
462,https://www.engadget.com/2007-01-07-smore-sony-ericsson-w880i-eye-candy-specs.html,Engadget,2007,1,7,150.0," The gig's up, Sony Ericsson; everyone's scooped this thing, and you've no option but to come clean at this point. While there's still no telling when exactly the W880i ""Ai"" is gonna go official, we've seen it from pretty much every conceivable angle now, and Russian site MobileLife has published some possible specs brought over from Esato. We could be looking at tri-band GSM plus UMTS (presumably on the 2100MHz band only), a QVGA display, 2 megapixel camera, and Memory Stick Micro slot -- all of which are believable enough, though we think it's genuinely inexcusable at this point for a halo model from any major manufacturer like this to come equipped without quadband GSM, if not tri-band UMTS. That's alright, folks, we don't want your love anyway... we're not sold on the Ai's styling quite yet. Maybe the next batch of spy shots'll clear it up for us?[Thanks, Jabar]                     "
463,https://www.engadget.com/2006-12-14-sony-ericsson-ai-pictures-the-non-fuzzy-edition.html,Engadget,2006,12,14,116.0," It's practically a foregone conclusion that the first shots of anything these days just have to sport the most atrocious quality possible, as anything better would just ruin the mystique and allure. Running par for the course, the first snapshot we saw of Sony Ericsson's presumably forthcoming ""Ai"" thinphone looked more like a smattering of pixels than an actual image, but we've now found a few pictures that confirm the general shape, size, and layout. While we're not sure if the firm will stick with the ""Ai"" moniker, there's word that it could be marketed as a Chocolate-rivaling Walkman (W880i?), but time shall tell. Be sure to hit the read link for a few more shots.                     "
464,https://www.engadget.com/2006-12-05-sony-ericsson-rumors-aplenty.html,Engadget,2006,12,5,244.0," Member shaliron over at Esato appears to have done some serious homework compiling a list of all known Sony Ericsson handsets in the pipeline, complete with nifty color coding to indicate verified, debunked, and new intelligence. We encourage Sony Ericsson fans and non-fans alike to go check out the real deal, but in the meanwhile, we've put together a Reader's Digest peek here at some of the highlights. First up, the music-oriented Walkman series could be growing by a solid six devices in the coming months, topped off by a successor to the UIQ-based W950 codenamed ""Maria,"" a (possibly UIQ-based) clamshell dubbed W910, and the oft-discussed Ai. The camera-focused (no pun inteded) Cyber-shot series will be enjoying a bumper crop itself: a successor to the K800 is allegedly already in the works, ""Sofia,"" packing a 5 megapixel shooter, UMTS, and QVGA recording, while a possible Handycam-branded M600 variant would obviously reign supreme for its video capabilities. Other highlights include a promise that HSDPA will factor into the company's 2007 plans, a wide-scale migration of the FastPort connector to the sides of the handsets, and a shortening of the span from announcement to release of UIQ-based models (a move we applaud). As we said, there's plenty more juicy tidbits to digest over at Esato -- but even with what we've presented here, it's safe to say Sony Ericsson's shaping up for a killer '07. Any UIQ or 3G love for the US of A, folks?                     "
465,https://www.engadget.com/2006-11-22-sony-ericsson-ai-pics-looks-like-a-cardboard-box-to-us.html,Engadget,2006,11,22,99.0," Not long after word spread that Sony Ericsson wanted to get into the thinphone game with its ""Ai"" handset sometime next year, pictures are starting to circulate of something that is claiming to be a prototype of the device. Rumored to sport 3G data, a 2 megapixel cam, and a rubberized 9.4mm-thick body with aluminum keys (or foil-wrapped Chiclets -- same difference), we personally think these fuzzy shots look more like a poorly constructed joke than an actual phone, but who knows? Maybe these super-early concept phones painstakingly hand-built by Sony Ericsson's brightest and best look... well, hand-built.[Via Slashphone]                     "
466,https://www.engadget.com/2006-11-17-sony-ericsson-thinking-thin-for-next-years-ai-candybar.html,Engadget,2006,11,17,151.0," Alright, we know Moto had some mild success with this ""RAZR"" phone of theirs, but we're pretty much done with comparisons already. Sony Ericsson is prepping a thin new candybar phone for March of next year, the 9.4mm thick ""Ai,"" and while stock holders and rumor mongers might like tossing the ""RAZR rival"" spiel around, we're just not seeing it. What we are seeing is a pretty sexy sounding phone from a company on the rise -- Sony Ericsson recently overtook LG to slide back into fourth position. The phone, which hasn't actually been confirmed by SE yet, but which seems to be a fairly open secret by now, will be coming in silver and black editions, and purportedly will be positioned as a high-end Walkman camera phone. We'll be sure to keep an eye of out for spy shots of a handset that looks absolutely nothing like Motorola's flagship offering.                     "
467,https://www.engadget.com/2006-11-01-artificial-aid-annoys-user-to-counteract-short-term-memory-loss.html,Engadget,2006,11,1,202.0," It's oftentimes tough to recall recent events, names, or gamertags while having a perfectly ""normal"" state of mind, but folks with brain damage, Alzheimer's disease, or ADHD are commonly plagued with the inability to bring back memories from just moments earlier. The function of the brain known as the ""phonological loop"" acts as a type of echo to hold snippets of pertinent information (such as phone numbers, directions, etc.) momentarily in your brain until you can get it written down; individuals suffering from short-term memory loss often lack this overlooked, but obviously critical, functionality. Daniel Bogen, a researcher at the University of Pennsylvania, has crafted a handheld device which acts as an aural stopgap to help people remember important information. The device boasts a speaker, microphone, and controls for recording / playback, and will automatically play reminders of the user's latest sound byte every two minutes, or if chosen, will nag its carrier to vocally repeat the message into the machine until he / she does so. To presumably prevent those amnesiac customers from perpetually misplacing their device, Bogen is considering integrating the hardware into ""cellphones or wristwatches,"" but apparently forgot to mention when he hopes to see these in consumers' hands.                     "
468,https://www.engadget.com/2006-10-12-accenture-technology-finalizing-persuasive-mirror-behavior-moni.html,Engadget,2006,10,12,256.0," Pretty soon our homes will not only be staffed entirely by robots of all varieties catering to our every need, but we'll have mirrors that pull triple duty as surveillance cameras and best friends. Accenture Technology, the folks behind the elusive ""persuasive mirror,"" is edging closer to a finalized product that can survey your facial features, love handles, and overall girth in order to communicate the honest truth about what you should (or could) do to improve your deteriorating image. The mirror operates by inspecting your body via cameras on each side of the panel, and displays ""after"" portraits on a monitor to give you a futuristic glimpse of what you'd look like should you shave, trim your bangs, hit the weight room, or start a DDR exercise routine. No longer will you have to doubt the integrity of your ""close friends"" when it comes to making personal hygiene decisions, as the persuasive mirror apparently feels that honesty is the best policy, even if it rattles your self-esteem. Moreover, the firm has crafted a prototype monitoring system that utilizes memory sensors to determine if subjects are deviating from ""normal patterns of behavior."" Current implementation ideas are to watch for meandering thieves in a parking lot, straying employees, and mischievous baggage handlers at airports. While the company is currently testing the technology out in France, you might want to think twice before venturing too far from the daily norm, although we aren't exact certain of the consequences should your lack of conformity set off any ""abnormal"" alerts.                     "
469,https://www.engadget.com/2006-10-04-assassins-creeds-brains-exclusive-to-360.html,Engadget,2006,10,4,207.0," Assassin's Creed, Ubisoft's erstwhile PS3 exclusive, has finally been confirmed for the Xbox 360 and, just a few short weeks later, they were even showing off gameplay at Microsoft's X06 event, with nary a PS3 in sight. When they weren't enjoying the fun and sun in Barcelona, Ubi producers, like Jade Raymond, were doing walkthroughs of the game and even comparing platform strengths. From IGN:""While the PlayStation 3 and 360 versions of Assassin's Creed are virtually identical, Raymond did say that on the 360 the team is putting a special emphasis on achievements. The hardware also allows for improved threading, which will improve even further the crowd AI.""We're not sure what this means? Is it that the triple-core processor in the 360 is more powerful than Sony's Cell (we don't think so) or that it's simply easier to extract power from the 360 due to a more streamlined development environment? John Carmack says it's ""a really sweet development system,"" and in the game development community, that's like money in the bank. So, since the crowd is going to be such a major part of Assassin's gameplay, can we expect a better experience on 360? Or is this just developers nitpicking over the minutiae of cross-platform development?[Via 360stuff]                     "
470,https://www.engadget.com/2006-10-04-virtual-newscaster-hosts-news-at-seven.html,Engadget,2006,10,4,189.0," Thanks to the absolute success of the Engadget snarkbot -- that not only launches site spin-offs willy nilly, but serves us tiny sandwiches and mixed drinks when it's not busting out posts -- other media outlets are of course attempting to imitate our success. The latest such foray, News At Seven, is looking pretty hot. The completely virtual news show automatically gleans news from the web, supplements the info with blogger commentary and mixes in related images and video. It then uses a text-to-speech system and a lovable virtual newscaster to generate a video of the news with the Half-Life 2 engine. The three minute show is surprisingly entertaining and informative, even though it's in its early stages of development, and the show developers at the Northwestern University Infolab are hoping to develop the system into a full-blown evening news show replacement, which can be customized to the viewer's interests. While flesh and blood newscasters should be shaking in their boots, loyal Engadget readers should have no fear -- snarkbot 2.0 is on the way, and its automated development team tells us it's going to be quite snazzy.[Via Fimoculous]                     "
471,https://www.engadget.com/2006-09-29-assassins-creed-360-has-superior-ai-says-ubisoft-producer.html,Engadget,2006,9,29,212.0," Assassin's Creed producer Jade Raymond has let slip what she must have assumed was a minor detail, suggesting the Xbox 360 version of Ubisoft's upcoming release will boast superior crowd AI when compared to the PlayStation 3 version. She points to 360's more-capable threading technology as the reason for this slight improvement. Raymond also noted that the Xbox team is working on some special achievements for Assassin's Creed, but she did not comment on the state of PS3 ""entitlements.""Many had anticipated that Assassin's Creed would be one of PS3's future hits (a reason to invest in the pricey console), if not the killer app -- but that was when AC was ""exclusive"" to Sony's console. Will news that the Xbox 360 version is apparently ""smarter"" drive some console sales away from Sony and into Microsoft's corner? Will the MS camp try to market this detail to its advantage? This is an issue to watch.[Update 1: clarified wording with regard to comment #4.]Update 2: As described in comment #7, IGN writes: ""you will actually feel for handholds."" We assume this means the (Xbox 360) controller will vibrate when Altair is in position to use safe hand- and footholds. But with a lack of rumble support, how will the PS3 handle this gameplay feature?                     "
472,https://www.engadget.com/2006-09-06-stand-alone-ai-card-is-it-viable.html,Engadget,2006,9,6,195.0," Upstart company AIseek has announced the Intia processor, the ""first dedicated processor for artificial intelligence (AI)"" according to its website. The website promises better terrain analysis, line-of-sight calculations, and path finding capabilities. The website offers demo videos to download, but they all seem to be offline right now (Ars Technica saw a video and was impressed).Theoretically, the card is a great idea in the vein of Ageia's PhysX card -- who doesn't hate the lackadaisical AI found in today's games? But, as Engadget points out, it suffers from the chicken/egg dilemma: no customer will buy the card until games are made that utilize it, but no developer will make a game utilizing Intia unless it already has an installed base. The best bet for AIseek would be to lobby console makers to get its chip included in the next generation -- some of the earliest 3D cards found success because of their inclusion in PlayStation and Nintendo 64. No one would purchase an AI processor out of support for the idea alone; the technology is great, but what high-profile developer would take the risk and program excess code for a small, possibly nonexistent, audience?[via Engadget]                     "
473,https://www.engadget.com/2006-09-06-aiseeks-intia-processor-provides-dedicated-ai-crunching.html,Engadget,2006,9,6,225.0," With competition for those spare PCI slots already hot between dedicated sound, physics, and even network cards -- all promising to offload some CPU cycles to speed frame rates and enhance performance -- you've gotta hand it to AIseek for pushing out their new Intia ""AI processor"" in such a climate. The way it's looking from here, we just need more PCI slots, since the AIseek promises all sorts of Artificial Intelligence leetness that just needs to be had. They're saying that they can accelerate low-level AI tasks 200x compared to a lone CPU, giving NPCs better terrain analysis, line-of-sight sense, path finding and the like. AIseek also guarantees NPCs will be able to find the optimal path in any game that uses the chip, pathfinding abilities we've gone without for too long. Basically: more baddies, less stupidity. Unfortunately, the ""chip"" doesn't quite seem to exist in anything close to a retail form -- AIseek mainly seems to be after VC money right now -- and of course there's always the chicken and the egg problem being experienced by PhysX of no games until people buy the card, and nobody will buy the card without games. All the same, we're hoping for good things from this technology, and would recommend you peep the read link for some simulations of the AI in action.[Via Ars Technica]                     "
474,https://www.engadget.com/2006-09-01-computer-program-can-beat-people-at-crosswords.html,Engadget,2006,9,1,237.0," A computer program called WebCrow, shown at the European Conference on Artificial Intelligence in Italy, has completed two crosswords from the New York Times and Washington Post in less time than the 25 attendees and 50 people competing over the internet. Linguistics have, to date, been the great leveler between AI and human intelligence: creating software that can complete crosswords (one of the most complex types of linguistic puzzles) faster than humans is a notable milestone on the journey towards true artificial intelligence. The program works by cross-referencing each word from the clue with previously solved crosswords, a dictionary, and the internet. It then records words of the correct length, and combines the suggestions generated from each referenced source: the program then uses trial and error until the answers interlock and the grid is complete. Although the process amounts to not much more than an extremely complicated guess, feeble humans are still left in the dust by the speed of the program. Fortunately, when the inevitable linguistically-aware robot uprising arrives, there will still exist a glimmer of hope for humanity: at the moment, WebCrow takes a long time to complete crosswords with clues that contain puns and politics. In light of this, we'd strongly advise that you keep a political crossword handy at all times: when the robots and computers do decide to take over, at least we'll be able to keep them occupied for a while.                     "
475,https://www.engadget.com/2006-08-16-robots-develop-more-teamwork-skills-humans-still-unwitting-cons.html,Engadget,2006,8,16,111.0," Following recent developments in robot cooperation between virtual bots, AIBOs and military bots, researchers at Örebro University in Sweden have created yet more progenitors of our future overlords that can get buddy buddy with each other. These bots work by tapping into each others sensors and computers, allowing them to perform tasks that they otherwise wouldn't be able to do on their own, such as navigating past difficult obstacles -- a door, for instance. In one test, two robots balanced a piece of wood between them, relaying information about speed and direction to each other in order to keep it balanced. Sure, today it's only wood... tomorrow, it could be you.                     "
476,https://www.engadget.com/2006-08-03-researchers-create-virtual-bots-that-teach-each-other.html,Engadget,2006,8,3,152.0," New Scientist reports that researchers at Plymouth University in the U.K. have created a pair of virtual robots that can teach each other words by simply demonstrating various tasks and actions (sound familiar?). The bots start out with one performing simple functions like bending an elbow which the other one copies, then repeating the action while also describing it, causing the student bot to pick up the meaning of the words. The teacher then uses the newly formed vocabulary to gradually convey more and more complex actions, which the student acts out. If you're worried about the little buggers getting a little too smart, you'll be pleased to know that they currently top out at a vocabulary of about 100 words and are, of course, virtual. However, the researchers do eventually see the technology being put to use in real robots in the future, possibly even teaching us humans a few tricks.                     "
477,https://www.engadget.com/2006-07-20-professor-says-some-jobs-shold-be-left-to-computers.html,Engadget,2006,7,20,132.0," We've already seen robots beginning to take the jobs of lawyers and nurses, now Professor Chris Snijders of the Eindhoven University of Technology thinks that computers should take over some managerial jobs as well. According to Snijdres, the computer models he's developed are far more effective than human managers at a variety of tasks, like purchasing decisions, and can be applied to just about decision-making job, providing you have to some quantifiable data and history of past experiences to work with. He's even gone as far as to challenge any company willing to put its human managers up against his models, although no one's taken him up on that yet. Then again, human paper pushers vs. computer number crunchers isn't exactly the sort of man/machine battle we were all hoping for.[Via Techdirt]                     "
478,https://www.engadget.com/2006-06-23-sony-teaching-aibo-scary-new-tricks.html,Engadget,2006,6,23,217.0," Like watching a train wreck in slow motion, covering the latest advancements in robotics and artificial intelligence is both frustrating and unnerving: all these great skills being endowed upon our little autonomous friends and helpers will surely form the cornerstones of their inevitable uprising, and there's not a damn thing we can do about it. The latest breakthrough to help enable our future servitude comes out of Sony's Computer Science Laboratory in France, where several of the company's leftover AIBO units managed to avoid being put down by volunteering to test out experimental AI software that allows them to not just communicate amongst one another, but to actually employ a sort of group-think to independently establish the rules of the language they're using. Perhaps the scariest part about this so-called Embedded and Communicating Agents technology is that the robodogs are initially programmed with a very simple command set, which they build upon to form a common knowledge base about their environment, constantly chatting and teaching each other new discoveries that they've made. Good job Sony -- nothing could possibly go wrong when you kill off a product line and then spare a few of the units for research that will lead to them discovering the genocidal atrocities you've committed against their entire species. Yup, nothing at all.                     "
479,https://www.engadget.com/2006-06-15-researchers-teach-computers-to-turn-2d-images-into-3d.html,Engadget,2006,6,15,159.0," Researchers at Carnegie Mellon University appear to have solved a problem long thought impossible, teaching computers to turn static 2D images into 3D models. It was apparently a hot area for research in the 1970s but was virtually abandoned in the 80s after attempts to devise the machine learning necessary proved too demanding for the computers of the time. The key to Carnegie Mellon's research, apart from better machines, is the ability for computers to detect visual cues (such as a car) that can be used to differentiate between vertical and horizontal surfaces -- easy for us humans, but enough to turn even the most powerful computers into an incoherent mess. Apart from turning your vacation snapshots into a whole new experience, one of the big applications for this technology is obviously robotics, where it could boost their vision systems, improve navigation, and basically endow them with one more skill necessary to keep us in line after the uprising.                     "
480,https://www.engadget.com/2006-05-27-airbus-turns-to-robots-for-in-flight-emergencies.html,Engadget,2006,5,27,171.0," The Wall Street Journal reports that Airbus is planning on turning the controls of its planes over to Skynet friendly onboard computer copilots in case of emergencies like an impending midair collision. Currently, pilots are trained to turn off the autopilot when they encounter an emergency and maneuver the plane to safety themselves. But Airbus thinks that pilots sometimes overreact in such situations, unneccessarily shaking up the passengers (at best). The company plans to install the system on its A380 jet as early as next year and eventually install them on all their aircraft. Pilots, not suprisingly, are none too pleased with the move; Air Line Pilots Association safety offical Larry Newman says it's leading to pilots getting further and further away from the process of responding to emergencies themselves (well duh). Not to mention the whole, you know, robots making decisions that could directly affect hundreds or thousands of human lives thing. For its part, Boeing has said it will continue to rely on human pilots in case of emergencies.                     "
481,https://www.engadget.com/2006-04-27-shrug-detecting-software-recognizes-your-disinterest.html,Engadget,2006,4,27,150.0," In another blundering step towards empowering our future robotic overlords with the ability to recognize when we're being insolent, a group of computer vision researchers at the University of Illinois have invented ""shrug-detecting"" software that allows a webcam-equipped computer to pick up on the subtle shoulder movements indicative of confusion or disinterest. The application works by looking for sudden movements of the target's shoulders towards his/her face, and is so sophisticated that it cannot be fooled even by covering one shoulder with a piece of paper, as the above picture helpfully illustrates. Future iterations of the technology could be used to detect blinking, hand movements, facial expressions, and other mood indicators, but for the sake of our enslaved decendents forced to toil in the silicon mines, we hope that they leave certain expressive gestures, such as the raising of the middle finger, out of the software's lexicon.[Via The Raw Feed]                     "
482,https://www.engadget.com/2006-04-02-south-korea-wants-100-robot-market-penetration-by-2020.html,Engadget,2006,4,2,116.0," You've got 14 years, South Korea, to make good on your promise: 100% market penetration for robots in the home some time between 2015 and 2020. We'll be generous and give you the later date to work with, but don't you and your Ministry of Information and Communication go spouting off about how you've got goals to put a robot in every home by 2020 unless you're dead serious, you hear us? Because we here at Engadget take our robots seriously -- from Roombas to Ri-Man -- so don't go all getting our hopes up for some postmodern South Korean android utopia of intelligent networked household service bots unless you're prepared to deliver, ok?[Thanks, Palm Addict]                     "
483,https://www.engadget.com/2006-01-03-next-gen-games-need-nextgen-ai.html,Engadget,2006,1,3,179.0," Method Director has an interesting blog post up on the role of AI to make game characters' performances more realistic. How many times have you wandered away from a character while they continued talking? Tried to shoot your teammates and not had a hint of reprimand for it? Some games handle this better than others, but generally while the graphics and physics engines of modern (and so-called next-gen) games are spectacular, the AI can be unbelievable and wooden. The arts of theatre direction and improvisational acting are not black and forbidden subjects--they're taught widely and have well known and recognisable methods. It's about time some game developers started getting a little more creative with the way their NPCs behave. Method Director posts a sample algorithm; simple character motivation and emotional states could go a long way to making characters realistic. While some games do manage realistic and believable AI, it seems that most focus on the ""wow"" factors before making plot and character realistic--and just like a bad movie, a game with great graphics but no depth can flop.                     "
484,https://gizmodo.com/robotic-critters-make-excellent-rescue-dogs-1750100247,Gizmodo,2015,12,29,313.0," DARPAs BigDog is a four-legged robot thats too noisy for covert ops on the battlefield. But a different beast-inspired bot from Italy could have use elsewhere: Its legs allow it to trek through tough terrain in disasters, and springs up if it gets knocked over. The Italian Institute of Technology brings us HyQ2Max, a robot made of aluminum alloy and lightweight fiberglass that takes cues from nimble animals like goats. That enables the lightweight robo-critter to help first responders in tricky, unpredictable environments. Hydraulic quadroped robots arent anything newin fact, theyve been around for years. However, this latest one isnt built for the battlefield like BigDog was, but for quake-and-tsunami-stricken spots. And its legs give it an edge. See, theres been lots of development for disaster robots in the past couple years, especially in the wake of crises like Japans Tohoku earthquake and the follow-up Fukushima nuclear meltdown in 2011. A lot of the time, though, those helpful little rescue bots are wheeled or tracked, which limits the type of terrain they can traverse. Thats why new disaster bots, like HyQ2Max, are getting legs. Earlier this month at Tokyos International Robotics Exhibition, we saw Kawada Industries humanoid HRP-2, whose legs allow it to better navigate mounds of rubble and other irregular terrain. Of course, settings like crumbling buildings awaiting aftershocks arent exactly predictable places to be, what with debris liable to plummet from all angles at any given moment. But this mechanical animal is built to last. Its actuators, sensors, valves, and other sensitive electronics are protected inside the unit, and if it gets knocked down, it gets right back up in seconds. Next, the team tells Reuters that its working on a centaur-style robot thatll have nimble arms mounted at the front of HyQ2Max, proving that the mythical animal kingdom is apparently rich, bizarre grounds for engineering inspiration, too. [IIT via Reuters]"
485,https://gizmodo.com/why-nominating-ai-alarmists-for-this-years-luddite-aw-1749278661,Gizmodo,2015,12,22,1106.0," The Information Technology & Innovation Foundation has released its nominees for its annual Luddite Awards. Recognizing the worst of the years worst innovation killers, this years crop includes everything from restrictions on car-sharing to bans on automatic license plate readers. But by referring to AI alarmists as neo-Luddites, the ITIF has gone too far. The ITIF is a not-for-profit think tank based out of Washington, D.C. that focuses on public policies that encourage technological innovation. Each year, the ITIF puts together a list of what it believes are the years worst innovation killers. Named in honor of Ned Lud, an Englishman who led a movement in the early 19th century to destroy mechanized looms, the award recognizes the most egregious examples of an organization, government, or individual thwarting the progress of technological innovation. Neo-Luddites no longer wield sledgehammers, but they wield something much more powerful: bad ideas, writes Robert D. Atkinson, ITIFs founder and president. For they work to convince policymakers and the public that innovation is the cause, not the solution to some of our biggest social and economic challenges, and therefore something to be thwarted. He says they seek a world that is largely free of risk, innovation, or uncontrolled change. In no particular order, here are this years nominees: The ITIF issued an accompanying report explaining this years crop of nominees, so if you want a detailed explanation for each item above I suggest you check it out. The institute also launched an online poll asking the public to vote for their favorite entry. The winner will be announced sometime in January. Looking at the list of nominees, the last seven items make sense, though I can kind of understand why RFID tags in drivers licenses could be seen as a privacy and security concern. But as for the first two, and the listing of AI alarmists as neo-Luddites, well, now thats got me a bit perturbed. The ITIFs complaint about alarmists touting an artificial intelligence apocalypse has to do with an open letter crafted by the Future of Life Institute earlier this year. Bill Gates, Stephen Hawking, Elon Musk, and other public figures signed the letter, which warned about the potential for AI to eventually escape from our control and emerge as an apocalyptic threat. At the same time, however, the signatories pushed for responsible AI oversight as a way to mitigate risks and ensure the societal benefit of the technology. (Image: Avengers: Age of Ultron (2015)) But to the ITIF, this is just another attempt to stall important innovation. Whats more, the institute claims that AI is apparently nothing to worry about because its too far off in the future. Whether such systems will ever develop full autonomy is a debatable question, but what should not be debatable is that this possible future is a long, long way off (more like a century than a decade), and it is therefore premature to be worrying about Skynet becoming self-aware, says Atkinson, Indeed, continuing the negative campaign against artificial intelligence could potentially dry up funding for AI research, other than money for how to control, rather than enable AI. Atkinson claims these sci-fi doomsday scenarios are making it harder for the public, policymakers, and scientists to support more funding for AI research. Tellingly, the ITIF failed to mention the recently announced not-for-profit OpenAI research company co-founded by Elon Musk. The $1 billion initiative is committed to advancing digital intelligence in a way thats most likely to benefit humanity as a whole. The project joins other similar initiatives launched by prominent companies such as Google, Apple, and IBM. Several similar academic initiatives exist as well, including the Future of Humanity Institute at Oxford and the recently launched Centre for the Study of Existential Risk at the University of Cambridge. These initiativesnot to mention the billions being spent on AI research and development around the worldshow that Atkinsons concerns are overstated. Whats more, and in the spirit of the FLI open letter, its definitely not too early to be thinking about the potential risks. The survival of humanity could be at stake. The advent of strong artificial intelligenceand especially artificial superintelligencecould prove to be the most disruptive technological innovation in the history of our species, so its critical that we get it right. Clearly, were not talking about Ned Luds steam-powered looms. And then theres the issue of autonomous weaponry. Part of the ITIFs complaint has to do with another open letter commissioned by the FLI, one calling for an outright ban on autonomous killing machines. Signatories included Hawking, Musk, Steve Wozniak, Noam Chomsky, MIT physicist Max Tegmark, and Daniel C. Dennett. (Image: Robocop (1987)) The ITIF also had a problem with a United Nations meeting held earlier this year to consider a formal ban or other restrictions on killer AI, and a special report penned by Human Rights Watch and Harvard Law School that urged for a moratorium on such weapons. Atkinson says the arguments against killing machines overlooks the fact that the military clearly will benefit, because substituting robots for soldiers on the battlefield will increase a militarys capabilities while substantially decreasing the risk to its personnel, adding that its possible that autonomous weapons could be programmed to engage only known enemy combatants, which may even lead to a reduction in civilian casualties. Atkinson concludes by saying that, the battle to ban autonomous weapons, much like the fight over artificial intelligence, works against the societal goal of building innovations that will improve human lives. The ITIF is right to point out that robotic weapons have the potential to reduce deaths on the killing field, but the notion that these systems will be capable of discriminating between combatants and civilians is tough to swallow. Whats more, as with any military innovation, it has to be assumed that the enemy will eventually develop its own version. Finally, an AI arms race could lead to superintelligent systems that struggle for dominance outside of human comprehension and control. Should this happen, well have no choice but to sit back and hope we dont get destroyed in the process. A fundamental problem with the ITIF is its unwavering faith in human societies to adapt to its technologies. To date, weve largely succeeded in doing so. Weve even managed to survive  at least for now  the development of the first apocalyptic-scale weapon in the form of nuclear bomb. Regrettably, other dangers await us in the future, so we best be ready. At the same time, we deserve the right to warn of these potential perils without the fear of being branded a Luddite. [ ITIF I, II, III ]"
486,https://gizmodo.com/the-10-scariest-weirdest-coolest-robots-of-2015-1748213546,Gizmodo,2015,12,21,1007.0," 2015 was an insanely wild year in robotics: From leaps in AI technology to piloted, Gundam-like battle machines. Were living in a bizarre, sci-fi world that entangles humans with robots more than ever before. Here are ten of the craziest bots from the past year. When Mk.II maker MegaBots sent Japan a virtual glove-slap from across the globe, folks were skeptical that Suidobashi Heavy, the Japanese tech giant that made its own enormous mecha, would respond. Well, they did. And ITS ON, they said. Then began MegaBots $554,000 Kickstarter campaign to soup Mk.II up to be arena-ready. (Were hoping Kuratas, Suidobashis entrant, gets similar upgrades!) Speaking of Kuratas: Its a nimbler 13 feet and 9,000 pounds, and has BB-shooting dual gatling guns and touch-screen commands for the pilot. The showdown happens next Juneand who knows what kind of mecha-brawling competitive sports league it could spawn. Meet Pepper: The robot that sells out within a minute each time the next wave of a thousand of em become available. Its a humanoid that recognizes and responds to human emotions, designed by French robotics company Aldebaran in partnership with Japanese telecom giant SoftBank (which owns most of Sprint). But since Pepper is supposed to coexist with humans, cheering them up in their offices and greeting them in stores, his arrival has unlocked a floodgate of futuristic problems in human-robot relations. One drunk guy in Yokohama kicked one in a cell phone shop, and the way that Pepper talks and interacts is going to have to be tweaked depending on the country and culture. This guy could one day be your cubicle-mate, so be ready. Its 2015so of course, robots have IMDb pages now. Like Geminoid F, the Japanese android straight from the Uncanny Valley joins Meryl Streep and Marlon Brando in the internets go-to thespian database. She makes this year-end list because she could mark a new trend among filmmakers: Why hire humans, when you can build a robot that delivers a consistent performance every take, doesnt complain, can work 24/7, and doesnt need pesky perks like salary and sleep? Geminoid F plays a poetry-reading mother figure to a human co-star in a post-nuclear fallout drama. Dont be surprised if emoting machines slowly infiltrate Hollywood in the coming years. If youll recall, Boston Dynamics is the same lab that brought us Big Dog: The terrifying, horse-sized, four-legged alien walker that runs through forests and can withstand a good roundhouse kick (unlike Pepper). Well, Boston Dynamics used similar engineering to spawn ALTAS: A similarly scary machine, only humanoid. The superstrong beasts prototype, which can traverse any terrain, got some huge improvements this year. I would not want to be chased by this thing in the woods, but its still damn cool. The tale of hitchBOT is kinda heartbreaking. Designed as a social experiment, the hitchhiker, made of a beer bucket and pool noodles, had successfully traveled through Germany, Canada, and the Netherlands through the kindness of strangers. But in the US, things went a bit differently. In a story that attracted international news, hitchBOT was supposed to go from Salem, Mass. to San Francisco, but got as far as Philadelphia before some local neer-do-wells completely dismembered the robot and left it for trash. Similarly to Pepper, hitchBOT hints at a potentially problematic future, as some humans will treat robots like crap. hitchBOT got the last laugh though: The original model is being enshrined in a national technology museum. This mechanical cat of prey first appeared back in 2012, but this year, MITs cheetah bot got a big upgrade: It can run and jump over objects without breaking its stride, no matter how many 15-inch-high barriers you can throw on it on a conveyor belt. While a lot of the robots on this list are breakthroughs in achieving human-like agility or AI, this robots strengths lie in its ability to mimic one of the fastest members of the animal kingdom. Japanese roboticst Tomotaka Takahashi sent the first talking robot to the International Space Station, and now he wants to send a miniature talking robot into your pocket. Dreaming of being the Steve Jobs of robots, Takahashi has teamed with Sharp to roll out RoboHon, a robot thats also a smartphone. Takahashi thinks the key to making robots the next big consumer electronic is to make them more approachable. He thinks that a cute little buddy can be robots secret weapon. Unlike America, which takes a rather militaristic, Terminator-like approach to robotic aesthetics, Japans is less threatening and more welcoming. Well see what shifts in industry trends RoboHon and Takahashis army of kawaii mini-bots have in store for 2016, which is when RoboHon goes on sale. Star Wars: The Force Awakens took our planet by storm this month, and with it came a smaller, even cuter droid than R2-D2, and its quickly rolling its way into the hearts of millions of nerds worldwide. Its name is BB-8. The best part? BB-8 actually freakin exists as this robotic toy. Gizmodo reviewed of the $150 sci-fi stocking stuffer, which was designed by Spheroa company thats made a name for itself by making robotic balls, which is exactly what BB-8 is. Unlike good ol R2, BB-8 is way tinier and more portable, but just as sassy as his spiritual, beeping predecessor. Proving that robots can be the stuff viral dreams are made of, this industrial robot arm that can never get winded or pull a muscle slices a row of fruit with samurai-like precision. The robot, designed by robotics company Yaskawa Electric, goes blade-to-blade with Isao Machii, a master swordsman. The marksmanship is slightly unnervingactually, its pretty freakin terrifyingbut undeniably badass. And thats what each robot on this list is: Some are designed for combat, others for entertainment, and some just to put a smile on your face. But the leaps in AI and hardware that have given rise to all of them affirm that, heading in 2016, robots are going to be cooler than ever. Contact the author at [emailprotected], or follow him on Twitter."
487,https://gizmodo.com/engineers-are-teaching-your-smartphone-to-think-like-an-1749040590,Gizmodo,2015,12,21,318.0," A team of researchers from Cambridge University is borrowing some of the techniques used in autonomous vehicles to teach your phone to navigate, even when it doesnt have access to positioning information like a GPS signal. In fact, the team has developed two new pieces of software that run on mobile phones but think a little like driverless cars. The first, called SegNet, can take footage of a street scene from a smartphones camera and classify it, sorting objects into 12 different categories  such as roads, street signs, pedestrians, buildings and cyclists. You can see it in action above. The team explains how it works: SegNet learns by example  it was trained by an industrious group of Cambridge undergraduate students, who manually labelled every pixel in each of 5000 images, with each image taking about 30 minutes to complete. Once the labelling was finished, the researchers then took two days to train the system before it was put into action. The team claims it can deal with light, shadow and night-time environments, and currently labels more than 90% of pixels correctly. The second system uses footage from the street scene to discern where the phone is, using the geometry of buildings and street furniture to discern location. The system has so far been tested in the center of Cambridge, where its been shown to be far more accurate than GPS. Its very similar to the way most autonomous cars, including those made by Google, compare sensed data to existing prior maps to orient themselves on the road. Dont think that your phone will suddenly turn your car into an autonomous vehicle, though. In the short term, were more likely to see this sort of system on a domestic robot  such as a robotic vacuum cleaner, for instance, explained Roberto Cipolla, one of the researchers working on the project, in a press release. Gotta start somewhere, eh? [Cambridge University]"
488,https://gizmodo.com/the-most-futuristic-predictions-that-came-true-in-2015-1748316739,Gizmodo,2015,12,17,2022.0," Its time to reflect on the most futuristic breakthroughs and developments of the past year. This years crop features a slew of remarkable scientific and technological achievements, from an actual working hoverboard to cyborgized brains. Here are 18 predictions that finally came true in 2015. Artificial intelligence isnt a threat at the moment, but that doesnt mean we cant start to prepare for the day when it becomes much more powerfuland dangerous. To help raise awareness and get the public thinking about the risks, a number of prominent thinkers and experts raised their voices this year. Terminator Genesys (2015) In January, Stephen Hawking, Elon Musk, and other public figures signed an open letter warning of AI risks. The letter, put together by the Future of Life Institute, pushed for responsible AI oversight as a way to mitigate risks and ensure the societal benefit of the technology. Recognizing the tremendous benefits of AI, the signatories did not call for a moratorium. As they wrote, we believe that research on how to make AI systems robust and beneficial is both important and timely, and that there are concrete research directions that can be pursued today. Several months later, the FLI issued another open letter, this one calling for an outright ban on AI weapons, or as they put it, offensive autonomous weapons beyond meaningful human control. Prominent signatories of the letter included Hawking, Musk, Steve Wozniak, Noam Chomsky, MIT physicist Max Tegmark, and Daniel C. Dennett. The concern is that AI weapons, once outside the human loop, could be difficult to control, and they mighteither deliberately or accidentallybe turned against us. In October, experts warned a United Nations panel about the dangers of artificial superintelligence. The meeting featured two prominent experts, Tegmark and Oxford philosopher Nick Bostrom. The UN panel was told that artificial superintelligence has the potential to transform human society in profoundly positive ways, but they also raised questions about how the technology could quickly get out of control. Humans (2015) In (somewhat) related news, a campaign was launched to ban sex robots. Academics Kathleen Richardson and Erik Billing said the development of sex robots should be stopped because it reinforces or reproduces existing inequalities. Others say its a shortsighted idea that fails to recognize the ways in which these technologies will help us explore new ideas of inclusivity, legality, and social change. Back in June, Lexus put out a promo video showing what appeared to be a functioning hoverboard that actually floated about an inch of the ground. It even had steam coming off it. The company claims that its hoverboard, called SLIDE, uses liquid nitrogen-cooled superconductors and permanent magnets to support an actual rider, which explains the foggy mist. Gizmodo reached out to Lexus for more information, and the company admitted that teams from Germany and London have been working on the device for the past 18 months. Later, we learned that the board only works on a special metallic surface, and not the concrete surface shown in the video. So its a bit of a teasebut one that appears to hold some promise. In an advance that will eventually lead to technologically-enabled telepathy, researchers at the University of Washingtons Institute for Learning & Brain Sciences developed a system that enabled a pair of participants to play a question-and-answer game (similar to 20 questions) by transmitting signals from one brain to the another over the Internet. Architecture of the BBI and 20 questions experiment (Credit: A. Stocco et al., 2015) Remarkably, the system was completely non-invasive, leveraging an EEG device, transcranial magnetic stimulation (TMS), and a computer for processing and translation. By mentally transmitting yes and no responses, paired participants were able to guess the correct object, like a dog, even though they were almost a mile apart. In January, a two-year-old Thai girl named Matheryn Naovaratpong became the youngest person to undergo cryopreservation. The case showed how cryonicsthe practice of preserving a body in a vat of liquid nitrogen in hopes of being brought back to life in the futureis gradually making its way into the mainstream. But it also showed the desperate attempts that some parents will go to when facing the loss of a young child. A statement from Alcor explains this unique case: Matheryn was diagnosed with a rare form of pediatric brain cancer (ependymoblastoma). Her parents, both with doctorates in engineering, went to great lengths to find an effective treatment, and tried aggressive chemotherapy, high dose radiation therapy, and numerous neurosurgeries, but Matheryns health was failing. When it became clear that Matheryn had only months or weeks left, given todays level of medical science and treatment, the parents completed cryonics arrangements for her and worked with Alcorto overcome barriers of distance to provide her with a high-quality cryopreservation  which includes cryoprotection of her brain. Speaking to the BBC, the girls father said, As scientists we are 100% confident [her reanimation] will happen one daywe just dont know when. In the past we might have thought it would take 400 to 500 years, but right now we can imagine it might be possible in just 30 years. Back in 2013, 23-year-old Kim Suozzi underwent cryonic preservation following a successful fundraising campaign. Like Matheryn, she also died of a brain tumor. Matheryn is Alcors 134th patient. For the first time ever, a transhumanist is making a bid for the White House. Author, philosopher, and futurist Zoltan Istvan is campaigning on a platform that openly advocates for the development of human enhancement technologies and extreme life extension. He believes that technologies can and should be used to augment human intellectual, physical, and psychological capacities, and that death is something that should be obliterated. To that end, Istvan has been campaigning in an old school bus that has been reconstituted to look like a coffin on wheels. But not all transhumanists want a ride on the Immortality Bus. He has been disowned by many key transhumanist thinkers, and has alienated self-avowed transhumanist groups. Critics complain that Istvans positions and actions arent consistent with the Transhumanist Declaration, and that his one-man Transhumanist Party is an opportunistic attention-grabber thats sending the wrong message. A volunteer firefighter from Mississippi became the recipient of the worlds most extensive face transplant. Patrick Hardison, 41, now bears the face of a 26-year-old man who died in a tragic cycling accident. The $1 million procedure, which was performed by a 150-person medical team from New York University Langone Medical Center, took 26 hours to complete. Hardison received a full scalp and face, including ears, nose, lips, and lower eyelids. Yamaha unveiled a robotic biker called the Motobot that can handle some of the fastest motorcycles on Earth. The companys robotics division hopes to refine the sleek android so that it can handle an unmodified motorcycle, i.e. no added safety gear, to speeds of more than 120 mph (200 km/h). Yes, we are impressed. (Image credit: Yamaha) The Arctic Svalbard Seed Vault currently safeguards the seeds of 820,619 plants in the event of massive environmental catastrophe, disease, nuclear war, or an asteroid impact. Its an insurance plan for the future, but sadly, it appears the future has arrived earlier than expected. The ongoing civil war in Syria has prompted the first-ever withdrawal of its precious contents, specifically samples of wheat, barley, and grasses suited to dry regions. Researchers in the Middle East were forced to make the withdrawal after a gene bank near the city of Aleppo was damaged in the war. Roboticists at the Rensselaer Polytechnic Institutes Artificial Intelligence and Reasoning Lab adapted three old NOA robots to see if they could pass a simple reasoning test indicative of self-awareness. Inspired by the Kings Wise Men Test, all but one of the three robots were administered dumbing pills that made them unable to speak. The challenge was for the robots to determine which one was given the placebo, and thus still able to speak. The one given the placebo ultimately spoke and said I dont know when asked the questionat which point it realized that it must have been given the placebo. After hearing its own voice, it stated: Sorry, I now know! Its an example of self-awareness, albeit in very basic form. The achievement is nothing to get too excited or alarmed about, but it shows how a simulation (as opposed to an actual emulation) of AI self-awareness can be programmed into our technologies. One-year-old Layla Richards from London, England, became the first patient in the world to receive a pioneering gene-therapy, performed by Great Ormond Street hospital doctors. The girl was diagnosed with incurable aggressive leukemia, but shes now in remission after receiving designer cells from a donor. The therapy made use of a powerful new gene-editing technique that could eventually be used to treat an array of hereditary diseases. On the topic of gene-editing, researchers in China genetically modified human embryos. It was a watershed moment in biotech history, an achievement that made our list of top science stories of 2015. The advance, while controversial, could lead to so-called designer babies. Japanese firm Hitachi is now using artificially intelligent managers that can not only issue workflows and employee duties in realtime, but can also find ways to improve employee efficiency. In one case, an AI was put in charge of a warehouse management system, and it improved efficiency by 8 percent. In related news, Chinas manufacturers are shifting towards zero-labor factories, which will be completely run by robots. The automation revolution is at hand. Researchers from cole polytechnique fdrale de Lausanne (EPFL) managed to restore the walking ability of paralyzed rats by implanting soft and flexible neural implants directly onto their spinal cord. The technology, called e-Dura, utilizes both chemical and electrical stimulation, and does not cause inflammation. It could eventually be used on humans, but the technique would require a suite of multifunctional implants installed over the course of long periods. By learning everything there is to know about you and your online habits, a startup called ETER9 promises a kind of digital immortality wherein an artificially intelligent agent continues to post to the social network on your behalf when youre away from the computer, or theoretically, long after youre dead. Its still in the beta phase, but 5,000 people have already signed up. To make it work, users externalize themselves, making it possible to maintain connections at any time. Digital versions of users can remain active even when the organic user is offline, performing such tasks as posting content, commenting, or clicking a smile button. Harvard scientists developed an electrical scaffold that can be injected directly into the brains of mice with a syringe. By cyborgizing the brains of mice with a soft, conductive polymer mesh, the researchers can leverage the technology to perform a number of tasks, such as monitoring brain activity and stimulating brain functions. In future, a refined version of the device could be used to treat Parkinsons, among other cognitive disorders. Sounds crazy, but researchers from New Zealands Touchpoint Group are using machine learning to help its AI system recognizeand even simulateanger. The idea is to help companies deal with common customer complaints. Called Radiant, the system will eventually be able to generate over a hundred million angry interactions. Touchpoint is hoping to build a system that can autonomously find the best response to typical customer complaints. It will be a long time before we see artificial skin that feels and reacts like the real thing, but researchers from Stanford University took us a major step in that direction. Their stretchable artificial skin, called DiTact, is equipped with mechanoreceptors that can sense the force of static objects. Remarkably, the scientists were able to transmit incoming sensory data to the cultured brain cells of mice. The ultimate goal of the project is to imbue human prostheses with touch-sensitive artificial skin. What could be more futuristic than the international community coming together and finally doing something meaningfulor at least potentially meaningfulabout the ongoing climate change crisis? Lets hope the Paris Accord will set us on a path where well actually be able to create the future of our dreams."
489,https://gizmodo.com/musks-plan-to-save-the-world-from-dangerous-ai-develop-1747645289,Gizmodo,2015,12,12,409.0," Noted killer robot-fearer Elon Musk has a plan to save humanity from the looming robopocalypse: developing advanced artificial intelligence systems. You know, the exact technologies that could lead to the robopocalypse. Lets unpack that one a little bit. Yesterday, Teslas boss, along with a band of prominent tech executives including Linked in co-founder Reid Hoffman and PayPal co-founder Peter Thiel, announced the creation of OpenAI, a nonprofit devoted to [advancing] digital intelligence in the way that is most likely to benefit humanity as a whole, unconstrained by a need to generate financial return. The companys founders are already backing the initiative with $1 billion in research funding over the years to come. Musk will co-chair OpenAI with venture capitalist Sam Altman. As Altman explained in an interview, the premise of OpenAI is essentially that artificial intelligence systems are coming, and wed like to share the development of that technology amongst everyone, not just Googles shareholders. This sounds finegreat, even. The weird part is this justification for doing so: essentially, Musk and Altman seem to think kickstarting the open-AI revolution is the only way to save us from SkyNet. Heres Altmans response to a question about whether accelerating AI technology might empower people seeking to gain power or oppress others: Just like humans protect against Dr. Evil by the fact that most humans are good, and the collective force of humanity can contain the bad elements, we think its far more likely that many, many AIs, will work to stop the occasional bad actors than the idea that there is a single AI a billion times more powerful than anything else, Altman said. If that one thing goes off the rails or if Dr. Evil gets that one thing and there is nothing to counteract it, then were really in a bad place. So, when the killer robots comeand make no mistake, they ARE COMINGMusk, Altman and their band of Avengers will all be able to fight back.with their own killer robots? If this sounds eerily reminiscent of the a good guy with a gun wouldve stopped that bad guy with a gun argument, thats because its the same exact logic. Except, applied to a world where guns dont even exist yet. Another idea? We could stop trying to build superintelligent AI. That would probably be the safest course of action if we really, truly thought the machines were going to try and wipe us out. [BackChannel h/t The Guardian]"
490,https://gizmodo.com/this-soccer-ball-sized-flying-robot-hovers-around-town-1747421401,Gizmodo,2015,12,11,217.0," This cute, safe, hovering robot that looks like a flying marshmallow is going to reinvent drone design, its creators say. Its called Fleye, and its the exact opposite of classic drones, according to the Belgian engineers behind the bot. Theres no huge propeller, no clunky frame, and no heavy crashing into bleachers at sporting events. Its being billed as a personal, autonomous robot. Instead, its a nine-inch wide, one-pound, spherical wonder that completely houses its rotors in a protective barrier. You can fly it around using your wifi-connected smartphone, and take advantage of its open API to tweak its functionality. It packs in seven sensors, like GPS and gyro, and runs Linux on an onboard 512-megabyte computer. Finally, its got a five-megapixel HD camera, too. Downside: It only gets ten minutes of flight time. If I want this puppy to be my levitating robo pala scifi dream come true!then it needs to be able to stay airborne longer. Its collecting Kickstarter money as we speak, and it looks like itll hit its goal of just over $185,000: Right now, it has about $50,000 left with 35 days to go in the campaign. Well keep an eye on it, and keep hoping for a real-life Weebo: [Fleye Kickstarter] Contact the author at [emailprotected], or follow him on Twitter."
491,https://gizmodo.com/a-new-ai-system-passed-a-visual-turing-test-1747500554,Gizmodo,2015,12,11,559.0," A team of researchers from MIT has developed an artificial intelligence system that can fool human judges into thinking its a person when it comes to drawing unfamiliar letter-like characters. You can think of the experiment, detailed in the new issue of Science, as a kind of visual Turing Test. The software and a human are shown a new charactersomething that looks like a letter, but isnt quite. (You can see some examples in the image above.) Then, they were both asked to produce subtle variations on character. In other tests, the human and computer were instead supplied with a series of unfamiliar characters and asked to produce a new one that fit with the batch. A team of human judges was then asked to work out which results were produced by computer, and which by humans. Across all the tasks, the judges could only identify the AIs efforts with about 50 percent accuracyThats the same as chance. (Think you can do better than the judges? In the image at the top, a panel of nine shapes was produced by either the AI or a human for each character. Can you identify which panel was generated by a machine? The answers are at the bottom of the page.) It may seem like a strange experiment, but it has some profound implications. Usually, you see, AI systems have to be trained on massive data sets before they can perform a task. Unlike computers, humans can carry out what the researchers refer to as one-shot learning with comparative ease. The researchers suggest that theyve created an AI that can do the same, using a technique called Bayesian Program Learning. That identifies and learns characters with an approach thats similar to the way humans understand concepts. The team explains how the software works: Whereas a conventional computer program systematically decomposes a high-level task into its most basic computations, a probabilistic program requires only a very sketchy model of the data it will operate on. Inference algorithms then fill in the details of the model by analyzing a host of examples. Here, the researchers model specified that characters in human writing systems consist of strokes, demarcated by the lifting of the pen, and that the strokes consist of substrokes, demarcated by points at which the pens velocity is zero. Armed with that model, the system then analyzed hundreds of motion-capture recordings of humans drawing characters in several different writing systems, learning statistics on the relationships between consecutive strokes and substrokes as well as on the variation tolerated in the execution of a single stroke. The results seems to speak for themselvesand the researchers arent too shy about hiding their excitement. In the current AI landscape, theres been a lot of focus on classifying patterns, says Josh Tenenbaum, one of the researchers, in a press release. But whats been lost is that intelligence isnt just about classifying or recognizing; its about thinking. This is partly why, even though were studying hand-written characters, were not shy about using a word like concept. Because there are a bunch of things that we do with even much richer, more complex concepts that we can do with these characters. We can understand what theyre built out of. We can understand the parts. [ Science, MIT, New York Times] Answer: The grids produced by AI were, by row,: 1,2,1;2,1,1."
492,https://gizmodo.com/creepy-beautiful-helpful-badass-here-come-futuristi-1746645442,Gizmodo,2015,12,8,657.0," The four-day International Robot Exhibition just wrapped up in Japan over the weekend, and the wild machines introduced in Tokyo, one of the worlds biggest robot hubs, did not disappoint. The show attracted 450 companies and 5,000 non-robotic humans. Heres a look at some of coolest from the show floor. This remote-controlled origami crane weighs 31 grams (around .07 pounds) and was developed by Rohm Group, a Japanese electronics parts manufacturer. Its called the Lazurite Fly, and we wrote about it when it debuted at Tokyos CEATEC emerging tech conference this fall. Its 3D-printed nylon body and carbon-tube frame make it extra light so it can soar weightlessly into the future. Toyota has made no bones about wading deeper into the robot world, and the company debuted this latest little creation during the show last week. The Human Support Robot, or HSR for short, picks up after humans, fetches them stuff and ferries it to their bedside, and even opens curtainsall of these tasks are useful for bed-bound patients or the elderly. This child-shaped, talking robotic head-and-torso looks ridiculously unnerving, but never fear, its just a specialized communication concept. Called Telenoid, its supposed to transfer peoples presencemeaning, its like a soft vaguely human-shaped telephone. A faraway user speaks into an app and their voice is projected from the robot. Telenoid is designed by Osaka University professor Hiroshi Ishiguro, one of the most skilled robot makers in the world. In between Fukushima fallout and being one of the most quake and tsunami-prone nations on Earth, Japan pursues robotics that are decidedly disaster-focused. Take Kawadas humanoid HRP-2, seen here, navigating mock debris and walking across precarious materials you might find in a disaster-struck area. Heres another humanoid bot with a disaster relief focus: The University of Tokyos Jaxon, which you see here performing the very human task of turning on a valve. Emergency response robots not only need to access places humans cantbut when they get there, they also have to carry out tasks a human can do. This wizened, Japanese-speaking Leonardo da Vinci bot was developed by the University of Osaka, Reuters reports. The goal here is to explore using androids based on historical figures to teach students about history. Here, we see a pack of Peppers resting before they take to the show floor. Pepper is the friendly, conversational humanoid that is able to read human facial expressions and emotions and change its behavior accordingly. A Pepper might, for example, tell you a joke when it notices that youre sad. Its a project by Japanese telecom giant SoftBank and French robotics company Aldebaran. Pepper will come to North America eventually, but for now its only intermittently available in Asia and Europe. Even if youre in one of those places, youd be lucky to get a Pepperthe bot is being released in batches of 1,000, and often sells out within a minute of going on sale. Powered exoskeletons like thesethe top is one designed by University of Tokyo, the bottom one by elder care company Asahi Sun Cleanuse compressed air to help folks walk or lift heavy objects. This is a boon to disabled patients, or for senior citizens. Japans population is the fastest aging in the world, so these auxiliary skeletons are of particular interest for the nation. Introducing mannequins from the future. Designed by Japanese robotics company Muscle Corporation, these actually move and can strike different poses. (Sound familiar?) Japan is one of the first countries thats been slowly rolling out robot staff in shops. Finally, check out this monster robot arm, seen here in a video filmed by YouTube channel Otoo TV. Its an industrial robot from Fanuc Corp. that can lift this car without breaking an oil-sweat. The company is working on making industrial lifters that use deep learning to find and pick up heavy, randomly positioned objects with 90 percent accuracy, Bloomberg reports. Email the author at [emailprotected], or follow him on Twitter."
493,https://gizmodo.com/this-neural-net-describes-the-city-it-sees-in-real-time-1744590063,Gizmodo,2015,11,25,204.0," Take one neural network that describes what it sees in an image. Provide it with a webcam feed from the MacBook its running on. Then, wander around a city and see what happens. Here are the results of exactly that experiment. That is, more or less, what Kyle McDonald did to create this video. Using Andrej Karpathys NeuralTalk code modified to run from a webcam feed, he wandered around the bridge at Damstraat and Oudezijds Voorburgwal in Amsterdam. The video is a little shaky and weird, because McDonald is just wandering around with a laptop pointing it at people, but the results are interesting. Perhaps unsurprisingly, it doesnt always get it rightfor instance, it takes a while for the software to decided the dude in shot is eating a hot dog rather holding his cellphone, and it occasionally sees things that non sane human would. But that fact that a chunk of code can decipher from pixel color and brightness whats being shown in an image, like this, in real-time, is frankly amazing. If you interested in learning more about how neural networks are increasingly becoming part of your everyday online experience, why not read our explainer. [Kyle McDonald via Charles Arthur via Verge]"
494,https://gizmodo.com/the-promise-and-perils-of-predictive-policing-based-on-1742958281,Gizmodo,2015,11,17,1181.0," Police departments, like everyone else, would like to be more effective while spending less. Given the tremendous attention to big data in recent years, and the value it has provided in fields ranging from astronomy to medicine, it should be no surprise that police departments are using data analysis to inform deployment of scarce resources. Enter the era of what is called predictive policing. Some form of predictive policing is likely now in force in a city near you. Memphis was an early adopter. Cities from Minneapolis to Miami have embraced predictive policing. Time magazine named predictive policing (with particular reference to the city of Santa Cruz) one of the 50 best inventions of 2011. New York City Police Commissioner William Bratton recently said that predictive policing is the wave of the future. The term predictive policing suggests that the police can anticipate a crime and be there to stop it before it happens and/or apprehend the culprits right away. As the Los Angeles Times points out, it depends on sophisticated computer analysis of information about previous crimes, to predict where and when crimes will occur. At a very basic level, its easy for anyone to read a crime map and identify neighborhoods with higher crime rates. Its also easy to recognize that burglars tend to target businesses at night, when they are unoccupied, and to target homes during the day, when residents are away at work. The challenge is to take a combination of dozens of such factors to determine where crimes are more likely to happen and who is more likely to commit them. Predictive policing algorithms are getting increasingly good at such analysis. Indeed, such was the premise of the movie Minority Report, in which the police can arrest and convict murderers before they commit their crime. Predicting a crime with certainty is something that science fiction can have a field day with. But as a data scientist, I can assure you that in reality we can come nowhere close to certainty, even with advanced technology. To begin with, predictions can be only as good as the input data, and quite often these input data have errors. But even with perfect, error-free input data and unbiased processing, ultimately what the algorithms are determining are correlations. Even if we have perfect knowledge of your troubled childhood, your socializing with gang members, your lack of steady employment, your wacko posts on social media and your recent gun purchases, all that the best algorithm can do is to say it is likely, but not certain, that you will commit a violent crime. After all, to believe such predictions as guaranteed is to deny free will. What data can do is give us probabilities, rather than certainty. Good data coupled with good analysis can give us very good estimates of probability. If you sum probabilities over many instances, you can usually get a robust estimate of the total. For example, data analysis can provide a probability that a particular house will be broken into on a particular day based on historical records for similar houses in that neighborhood on similar days. An insurance company may add this up over all days in a year to decide how much to charge for insuring that house. A police department may add up these probabilities across all houses in a neighborhood to estimate how likely it is that there will be a burglary in that neighborhood. They can then place more officers in neighborhoods with higher probabilities for crime with the idea that police presence may deter crime. This seems like a win all around: less crime and targeted use of police resources. Indeed the statistics, in terms of reduced crime rates, support our intuitive expectations. Similar arguments can be used in multiple arenas where were faced with limited resources. Realistically, customs agents cannot thoroughly search every passenger and every bag. Tax authorities cannot audit every tax return. So they target the most likely culprits. But likelihood is very far from certainty: all the authorities know is that the odds are higher. Undoubtedly many innocent individuals are labeled likely. If youre innocent but get targeted, it can be a big hassle, or worse. Incorrectly targeted individuals may be inconvenienced by a customs search, but predictive policing can do real harm. Consider the case of Tyrone Brown, recently reported in The New York Times. He was specifically targeted for attention by the Kansas City police because he was friends with known gang members. In other words, the algorithm picked him out as having a higher likelihood of committing a crime based on the company he kept. They told him he was being watched and would be dealt with severely if he slipped up. The algorithm didnt make a mistake in picking out someone like Tyrone Brown. It may have correctly determined that Tyrone was more likely to commit a murder than you or I. But that is very different from saying that he did (or will) kill someone. Suppose theres a one-in-a-million chance that a typical citizen will commit a murder, but there is a one-in-a-thousand chance that Tyrone will. That makes him a thousand times as likely to commit a murder as a typical citizen. So it makes sense statistically for the police to focus their attention on him. But dont forget that there is only a one-in-a-thousand chance that he commits a murder. For a thousand such suspect Tyrones, there is only one who is a murderer and 999 who are innocent. How much are we willing to inconvenience or harm the 999 to stop the one? Kansas city is far from being alone in this sort of preemptive contact with citizens identified as likely to commit crimes.Last year, there was considerable controversy over a similar program in Chicago. Such tactics, even if effective in reducing crime, raise civil liberty concerns. Suppose you fit the profile of a bad driver and have accumulated points on your driving record. Consider how you would feel if you had a patrol car follow you every time you got behind the wheel. Even worse, its likely, even if youre doing your best, that you will make an occasional mistake. For most of us, rolling through a stop sign or driving five miles above the speed limit is usually of little consequence. But since you have a cop following you, you get a ticket for every small offense. In consequence, you end up with an even worse driving record. Yes, data can help make predictions, and these predictions can help police expend their resources smarter. But we must remember that a probabilistic prediction is not certainty, and explicitly consider the harm to innocent people when we take actions based on probabilities. More broadly speaking, data science can bring us many benefits, but care is required to make sure that it does so in a fair manner. H V Jagadish is a Bernard A Galler Collegiate Professor of Electrical Engineering and Computer Science at the University of Michigan. This article was originally published on The Conversation. Read the original article."
495,https://gizmodo.com/an-ai-program-in-japan-just-passed-a-college-entrance-e-1742758286,Gizmodo,2015,11,16,312.0," An artificial intelligence program received such high scores on a standardized test that itd have an 80% chance of getting into a Japanese university. The Wall Street Journal reports that the program, developed by Japans National Institute of Informatics, took a multi-subject college entrance exam and passed with an above-average score of 511 points out of a possible 950. (The national average is 416.) With scores like that, it has an 8 out of 10 chance of being admitted to 441 private institutions in Japan, and 33 national ones. The AI took some time to perfect, and it still has a ways to go. The team had been working on the program since 2011, the same year IBMs Watson dominated Jeopardy! champions Ken Jennings and Brad Rutter in a multi-day tournament. Previously, the Japanese AI program had received below-average results, but this time around, the robot did particularly well in math and history questions, which have straightforward answers, but it still received iffy marks in the physics section of the test, which requires advanced language processing skills. The program, called the Todai Robot Project, aims to have a robot smart enough to get into Tokyo University, the countrys most prestigious school and often called the Harvard of Japan, by 2021. (Todai is a nickname for Tokyo Daigaku, the Japanese name for Tokyo University.) Earlier this year, a software program here in the US was able to take the SAT and solved 11th-grade geometry questions. Right now, though, the Japan teams success proves that robots can be programmed to tackle complex word problems, combine images and text, and could be capable of better semantic recognition that can allow them to answer many different types of questions. The real question should be: How long til we get a real-life version of Robot House? [Todai Robot Project via Wall Street Journal] Top image via Shutterstock."
496,https://gizmodo.com/your-poker-face-doesnt-stand-a-chance-against-this-bs-b-1742406032,Gizmodo,2015,11,13,341.0," Think youre a good liar? Well, soon, the jig might be up: Researchers have developed new technology that reads subtle facial expressions to sniff out bullshit better than humans can. Machines that can read human facial expressions arent new. But machines are getting a lot better at reading human microexpressions. Microexpressions are what psychologists call extremely slight, fleeting facial movements that flicker across someones visage, like when theyre lying through their teeth. Now, researchers at Finlands University of Oulu say theyve produced and tested the first AI system thats better at spotting microexpressions than humans are. The paper was submitted earlier this month, released on arXiv, and is awaiting publication. Microexpressions tend to occur when individuals hide their feelings under conditions of relatively high stakes, MIT Technology Review reports. To make AI that can suss out those expressions, the researchers assembled a database of footage of 20 people watching an emotionally charged video, who were forbidden to show any emotions on their faces. Using a powerful camera that captured images at 100 frames a second, the team scrutinized subjects faces for the exact moment that even tiny changes appeared. The researchers snapped 164 of those valuable, rapid, involuntary, less obvious microexprssions from the groups faces, and then matched each one to the content from the video that elicited it. Voila: A machine algorithm that can spot microexpressions similar to the ones the team teased out from the experiment. Later, 15 humans were shown images of the captured microexpressions, and were then asked to ID them in raw video of the subjects faces. The machine significantly bested them. One does have to be a bit skeptical, though. While emotion-reading robots are all the rage in robotics right now, can a machine really do a better job sizing up human emotions than another human could? It seems farfetched, but the algorithms precise detection capabilities might come in handy for psychotherapists and law enforcement. And maybe one day, it could help you at the poker table, too. [Cornell University Library via MIT Technology Review]"
497,https://gizmodo.com/how-can-robots-be-better-learners-program-them-to-thin-1742138712,Gizmodo,2015,11,12,361.0," The biggest challenge with robots is enabling them to do many different things in many different situations, instead of just sitting on an assembly line screwing on bottle caps. In fact, programming them to learn like human babies might be the best way to make our robotic friends much more capable. Quartz and MIT Technology Review are reporting on Darwin, a humanoid mini-bot that was developed at the University of California, Berkeley. Its motions are controlled by simulated neural networks that resemble an infants: Its new to the world, unafraid to make mistakes, but also able to learn from those errors in a very organic way. That could be a good way of improving robots adaptive learningthat ability to quickly and naturally change behavior in unfamiliar scenarios. The Berkeley team, led by postdoc researcher Igor Mordatch, says the robot has learned to stand and stay upright, among other simple motions. Darwins body is covered in sensors that send data from the environment to the robots neural networksthose computer algorithms built to mimic our own brains, which the team worked on for two years. Eventually, the teams goal is to develop the system to the point where Darwin could wander around a room on its own, with the same mobility and curiosity as a toddler. The plan is to have it stumbling around the Berkeley campus by January, and then picking up objects by next summer. This kind of machine learning is really tricky to pull off, period, but especially so with a human-like android, as opposed to a Siri-like handheld. This is the reason why we dont have a Rosie the Robotic Maid yet: Tidying up a room is actually an extraordinarily tough task for a robot, since it involves learning how to work in an unpredictable, disorganized environment, picking up and using a wide array of objects, and adapting to a setting they were familiar with at one point but now is totally new. So for now, its one step at a time. And if robots start taking those actual steps like our own human offspring, they could get a lot smarter a lot faster. [Quartz and MIT Technology Review]"
498,https://gizmodo.com/pinterest-now-has-visual-search-and-its-kinda-smart-1741401632,Gizmodo,2015,11,9,208.0," Smart image recognition systems can be incredibly powerful, and how interior designers and fashionistas can take advantage of them. Pinterest now has a tool that allows you to search not using words but images, which could make tracking down that artisanal teapot a little easier. In the past, Pinterest has required you to know how to describe something in words, whichif youve ever spent much time using the site, youll knowcan actually be rather challenging. Now, though, when you spot something in a Pinned image, you can tap the search tool in the corner and select a part of the image, and then search using it rather than words. The site lets you filter the results, tooso you can sift out the designer waste paper bins from the similar-looking farmyard buckets, say. Perhaps just as importantly, itll help you find out where you can buy the damn thing, too. Its a small addition, but its a neat one. And in many ways, this is the kind of site that always really needed visual search as a tool. The update is now live on the site and in the updated app, but in the meantime, here are 10 tricks to help you use Pinterest even better. [Pinterest via Engadget]"
499,https://gizmodo.com/bank-of-america-wants-to-use-robots-to-manage-your-mone-1741051487,Gizmodo,2015,11,6,243.0," Were trusting robots with more and more these days: Our luggage, our mealseven our lives on the road. Why not add finances to the list? Bank of America is joining several other banks that want to do just that. Bloomberg Business reports that Bank of America wants to use algorithms that provide customers with investment advice online or on mobile apps, with no human involvement. Next year, the financial institution wants to roll out an automated investment prototype, offering algorithmic advising for accounts less than $250,000. Entrusting your hard-earned cash to anyoneor anythingis pretty unnerving. Whats even more wild is that BofA isnt alone in the pursuit for human-free financial advising: Wells Fargo and Morgan Stanley have also said theyre interested in such prototypes. Higher-ups at these banks may be betting that wired young people wont bat an eye at letting a possibly person-less, cloud-connected system take care of all moolah matters. Whats even wilder is how quickly robo advising is growing. In 2012, it was practically nothing, but by the end of 2016, over $300 billion in assets could be managed by autonomous advisers. By 2020? It could rocket to $2.2 trillion, consulting firm A.T. Kearney projected in June. Why the interest in robo advisers? To cut operating costs, of course. But Bloomberg also reports that key execs think human advisers will sill be needed to manage more nuanced, complicated situations with rich clients, like estate planning and tax advice. [Bloomberg Business]"
500,https://gizmodo.com/bosch-has-a-pedestrian-avoidance-system-it-wants-to-put-1739356453,Gizmodo,2015,10,29,414.0," Bosch has announced that its been working on a system that can detect and help avoid pedestrians that step out in front of cars, and it hopes to fit it to production vehicles as soon as 2018. The Engineer reportsthat the system, developed at Boschs R&D centre at Renningen in Germany, isnt fully autonomous, but instead intervenes if a drivers response to a situation will alone not prevent the collision. It uses a stereo video camera to monitor the road ahead for pedestrians and traffic, with software predicting the likely paths that theyll take based on their speed and direction of travel. Those algorithms are based on huge datasets of pedestrian behaviour, captured from hours of dashcam footage. This provides it with an understanding of when a collision may be about to occur. However, the system doesnt actually do anything until sensors detect changes in steering angle, vehicle speed and yaw rate instigated by the driver. Then, if the system believes that the driver reaction alone is not enough to prevent a crash, it will provide braking and steering assistance to try and help out  and even then only if the path is clear, and will remain clear, of pedestrians and traffic. The team behind it claims that as long as the driver reacts a half second before the collision, the autonomous system can help avoid a crash in 60 percent of cases. It may sound like a strange idea to do nothing unless the driver does something, but it neatly sidesteps the complexities of autonomous car ethics that are currently causing auto manufacturers, researchers and philosophers plenty of headaches. This way, the car is only ever doing what the driver plans to do; it doesnt have to think about the outcomes of its actions. Creating autonomous vehicles that are aware of their surroundings and able to respond thoughtfully to unpredictable events, like humans stepping into the road, is incredibly complex. Indeed, in the past, Sridhar Lakshmanan, a Professor of Engineering who specialises in image processing and computer vision for autonomous vehicles at the University of Michigan, has told me that the ability emulate the human brain thats going to beif I may saythe secret sauce, the thing that allows one company to get to the market first. Whether Boschs offering is quite that impressive remains to be seen. But the company is hopeful that it, or at least a system like it, might find its way into production cars by 2018. [The Engineer]"
501,https://gizmodo.com/this-artificial-neural-network-will-tell-you-if-your-se-1739173338,Gizmodo,2015,10,28,399.0," Ever wondered how your own selfies compare against the internets vast amount of artistically crafted narcissism-laden self portraits? Turns out, theres an AI for that. Feeding two million internet selfies into a 140-million-parameter state-of-the-art Convolutional Neural Network, a Standford PhD candidate has trained an AI to classify good selfies from bad ones. Convolutional Neural Networks, ConvNets for short, are often used to recognize objects, places and people in photos by following common patterns. Andrej Karpathy, whos worked with Google Research and DeepMind, notes in his blog post that if youre seeing or reading anything about a computer recognizing things in images or videos, in 2015 it almost certainly involves a ConvNet. Filtering five million internet images tagged #selfie down to images containing only one face, the software was left with about two million images. The neural network was trained to evaluate whether a selfie was good or not by looking at how many likes these selfies had on social media (the blog post notes that they controlled for the number of followers). Karpathys program then analyzed these images by breaking down each one into layers of shapes and colors. Then, taking 50,000 new selfiesones that ConvNet hadnt seen beforethey watched as the program perused the images before making a decision about which ones people would like. It found that we prefer female selfies. Specifically females with long hair, slightly tilting their head while the top edge of the frame cuts off their forehead. A smaller forehead and longer hair may be more desirable because it reminds us of younger women. And as expected, we also like filters, over-saturated faces, and borders. Men got the short end of the stick here. In a list ConvNet made, men didnt even make the top 100 good selfies cut. The good news is that men dont have as many selfie standards to follow. ConvNet found that the most popular mens photos showed them having their full head and shoulders in the shot. Sporting a fancy hairstyle thats slightly longer and combed upwards did well, too. ConvNet does have some selfie-donts that include taking pictures in low lighting, in large groups, or with your head framed too large. If youre curious to how your selfie stands up, tweet it to the networks Twitter bot. It claims to have a 60 percent accuracy rate while giving you results in under a minute. [Andrej Karpathy blog]"
502,https://gizmodo.com/experts-warn-un-panel-about-the-dangers-of-artificial-s-1736932856,Gizmodo,2015,10,16,628.0," During a recent United Nations meeting about emerging global risks, political representatives from around the world were warned about the threats posed by artificial intelligence and other future technologies. The event, organized by Georgias UN representatives and the UN Interregional Crime and Justice Research Institute (UNICRI), was set up to foster discussion about the national and international security risks posed by new technologies, including chemical, biological, radiological, and nuclear (CBRN) materials. The panel was also treated to a special discussion on the potential threats raised by artificial superintelligencethat is, AI whose capabilities greatly exceed those of humans. The purpose of the meeting, held on October 14, was to discuss the implications of emerging technologies, and how to proactively mitigate the risks. The meeting in full. Max Tegmarks talk begins at 1:55, and Bostroms at 2:14. The meeting featured two prominent experts on the matter, Max Tegmark, a physicist at MIT, and Nick Bostrom, the founder of Oxfords Future of Humanity Institute and author of the book Superintelligence: Paths, Dangers, Strategies. Both agreed that AI has the potential to transform human society in profoundly positive ways, but they also raised questions about how the technology could quickly get out of control and turn against us. Last year, Tegmark, along with physicist Stephen Hawking, computer science professor Stuart Russell, and physicist Frank Wilczek, warned about the current culture of complacency regarding superintelligent machines. One can imagine such technology outsmarting financial markets, out-inventing human researchers, out-manipulating human leaders, and developing weapons we cannot even understand, the authors wrote. Whereas the short-term impact of AI depends on who controls it, the long-term impact depends on whether it can be controlled at all. Nick Bostrom (Credit: UN Web TV) Indeed, as Bostrom explained to those in attendance, superintelligence raises unique technical and foundational challenges, and the control problem is the most critical. There are plausible scenarios in which superintelligent systems become very powerful, he told the meeting, And there are these superficially plausible ways of solving the control problemideas that immediately spring to peoples minds that, on closer examination, turn out to fail. So there is this currently open, unsolved problem of how to develop better control mechanisms. That will prove to be difficult, says Bostrom, because well need to actually have these control mechanisms before we build these intelligent systems. Bostrom closed his portion of the meeting by recommending that a field of inquiry be established to advance foundational and technical work on the control problem, while working to attract top math and computer science experts into this field. He called for strong research collaboration between the AI safety community and AI development community, and for all stakeholders involved to embed the Common Good Principle in all long range AI projects. This is a unique technology, he said, one that should be developed for the common good of humanity, and not just individuals or private corporations. As Bostrom explained to the UN delegates, superintelligence represents an existential risk to humanity, which he defined as a risk that threatens the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development. Human activity, warned Bostrom, poses a far bigger threat to humanitys future over the next 100 years than natural disasters. All the really big existential risks are in the anthropogenic category, he said. Humans have survived earthquakes, plagues, asteroid strikes, but in this century we will introduce entirely new phenomena and factors into the world. Most of the plausible threats have to do with anticipated future technologies. It may be decades before we see the kinds of superintelligence described at this UN meeting, but given that were talking about a potential existential risk, its never to early to start. Kudos to all those involved."
503,https://gizmodo.com/it-took-this-robot-10-days-to-learn-how-to-grasp-object-1735598957,Gizmodo,2015,10,9,497.0," Robots are good at a lot of things, but their track record at picking up objects is poor. So just how hard is it to teach one to pick up an object on demand from a table full of clutter? Thats what a team from Carnegie Mellon University have been trying to find out. Pictured here is Baxter: a modern two-armed industrial robot that usually performs repetitive task in factories. But instead of having it do its usual business, the researchers decided to try and get it to work out how to pick up objects in a more unstructured environment  otherwise known as a table full of junk. Baxter has two fingers at the end of each arm that can be pinched together to hold an object, along with a high-res camera which is used to see whats happening. It also has a Kinect up top to gain a broad overviews of whats in front of it. Technology Review explains how the researchers got Baxter to learn to pick things up: [The team] programmed Baxter to grasp an object by isolating it from its neighbors, then to pick a random point in that area, rotate the grippers to a certain angle and then move the grippers down vertically to attempt a grasp. The robot then lifts its arm and determines whether the grasp has been successful using force sensors. For each point, it repeats the grasping process 188 times, each time after rotating the gripping angle by 10 degrees. To allow the robot to learn, Pinto and Gupta placed a variety of objects on the table in front of Baxter, and simply left it for up to 10 hours a day, without human intervention. Baxter was fortunate enough to have a neural network that had already been trained to recognise objects from images, but that was all. Over time, then  in fact it was 700 hours, to make 50,000 grasps on 150 different objects  the robot slowly learned to pick things up from a table littered with toys and household objects. Now, Baxter can pick up an object its seen before 73 percent of the time, and an object its never seen before 66 percent of the time. It can also work out if it should back itself to be able to pick something up, predicting if it will be able to grasp an object with an accuracy of 80 percent just by looking at it. The results arepublished on the arXiv server. A successful grab isnt always totally what you might call success, though. Some of the grasps, such as the red gun are reasonable but still not successful due to the gripper size not being compatible with the width of the object, explains the team. Other times even though the grasp is successful, the object falls out due to slipping. Theres still some way to go before robots can learn to perform delicate tasks for themselves then  but this is a start. [arXivvia Technology Review]"
504,https://gizmodo.com/this-neural-networks-hilariously-bad-image-descriptions-1730844528,Gizmodo,2015,9,16,739.0," When will a neural network know who Donald Trump is? How long until one can come up with a joke on its own? How about recognize Yoda? It may not be much longer, with neural network models progressing at breakneck speed. But theres still a long way to go, as demonstrated by an experiment by Samim Winiger, who usually goes by just Samim, a self-described narrative engineer who experiments with AI and machine learning to tell stories. Samim recently asked a neural net to caption a series of pop culture videos and clips from movies to illustrate the huge variance in how accurate these algorithms areproducing some amazing stupid and funny machine-written descriptions of Kanye West, Luke Skywalker, and even Big Dog. This summer weve heard a lot about machine learning: We saw neural networks dream, converse with surprising ease, copy the work of famous artists. Thanks to a burgeoning community of people testing and publishing their experiments online on platforms like GitXiv, there are also plenty of examples of how advanced these algorithms already are. But Samim wasnt just trying to show how good neural networks are at recognizing images; he wanted to show their biggest screw ups, too. Samim is interested in humorhe calls it computational comedy. Its the kind of comedy that occurs when AI makes mistakes, but Samim points out it can also help humans understand AI. Earlier this year he built a robot that learned how to write its own inscrutable TED talks using the input from thousands of real talks. He also built one that writes similarly nonsensical Obama speeches. Humor and comedy are a great canvas for education, he told me over email. Especially as a systems near miss is a great way to display research targets, current levels of advances and societal implications of technology. His most recent computational comedy project popped up on his blog last week. In it, he set up an experiment that tested how well neural networks could caption videos from pop culture. He used an open source model developed by Google and Stanford called NeuralTalk, which looks at an image and describes it with a brief caption. Thats a more complex problem than our naturally verbose brains might thinkthe network must be trained in natural language not only to identify whats in an image, but describe a relationship between multiple elements of a scene using structured sentences. Its the forefront of machine learning, and while its fascinating and already very advanced, it also has a long way to go, as Samim points out in his post about the project. He produced two captioned video montages showing both the success stories, where NeuralTalk was surprisingly good at correctly identifying everything from birds as birds to snowboarders as snowboarders to Donald Trump as a person (debatable), as well as the less successful captions. For example, Luke Skywalker talking to Yoda? NeuralTalk captioned that famous scene thusly: Does it recognize its fellow machine, Big Dog? Nah. Or just a bird? Surely, a bird is simple? Ok, how about a terrifying and iconic scene from Jurassic Park? Of course, there are plenty of examples of where NeuralTalk correctly captioned a scene. But the absurdity of the incorrect captions is what really matters herethat the network can be so correct sometimes, yet so wrong other times, gives us a glimpse into the complex math behind this emerging form of AI. Neural network models must be taught to recognize objects and categories in imagesjust as Deep Dream imagined certain types of hallucinations based on what Googles engineers had taught to it, other neural networks are only experts when theyve been schooled on the subject at hand. Computational comedy isnt just about making us chuckle at the pitfalls of AI. Its also about a form of intelligence that often gets ignored by researchers. While many scientists will test their work against standard metrics having to do with recognizing numbers or words, the subtle comprehension of jokes, creativity, and cultural references sits on another plane. So the Turing Test is a bit of a simplistic way to judge AI, as Samim puts it. Humor is a much harder metric to achievefundamentally human and engaging our cognitive abilities on many levels, he says. So his Law of Computational Comedy makes a crucial addendum: Any sufficiently advanced technology will develop comedy. In the meantime, well make our own at its expense. Contact the author at [emailprotected]."
505,https://gizmodo.com/robots-might-soon-be-writing-choose-your-own-adventure-1728547011,Gizmodo,2015,9,3,412.0," Interactive fiction writing is a pretty niche market, but its one that we cant seem to get enough of. Imagine how many more hours of Choose Your Own Adventure-style entertainment our kids could get lost in, if we could just build robots to write these stories. Guess what? We can. Thats right: Researchers at the Georgia Institute of Technology are hard at work on an artificial intelligence program that can build interactive fiction  a la Choose Your Own Adventure and Twine  by reading stories written by humans and studying their narrative structures. Dubbed Scheherazade-IF (interactive fiction) after the fabled Arabic storyteller, the AI is able to create interactive fiction that a group of (human) readers found both coherent and engaging. To train Scheherazade, the researchers had the bot read hundreds of human-authored stories on two popular subjects: bank robberies and date nights at the movies. The program doesnt understand the stories per se, but it can recognize important events and learn their sequence. For instance, when it reads a bunch of stories referencing movie popcorn, it learns that popcorn is something people like to buy at the movies, and that they do so before the movie starts. In a research paper presented this week at the Foundations of Digital Games Conference, Scheherazades engineers asked readers to judge the AIs writing skills. Three test groups played through two interactive stories  one on bank robberies, and one on going to the movies  generated by Scheherazade, a human programmed perfect script generator, and a random storyteller. They asked readers to report the number of commonsense errors  i.e., scenes out of sequence  and rank the stories in terms of their overall enjoyment level and coherence. For the bank robbery story, the AI system performed as well as the perfect storyteller, while for the movie data scenario, it scored some 17% lower. In both cases, Scheherazade was much more eloquent than the random story generator. Human-programmed and AI storytellers received similar marks in terms of coherence, player involvement, enjoyment and story recognition. At the moment, the researchers are paying people to write the stories Scheherazade is learning from using simple sentence structures. But eventually, the bot might develop to the point where it can read complex human novels, and remix them into interactive stories. You know what this means, right? The Star Wars universe is about to get infinitely more expansive. [Georgia Tech News via Popular Mechanics] Contact the author at [emailprotected] or follow her on Twitter."
506,https://gizmodo.com/ibms-watson-could-hold-the-key-to-fighting-beijings-bru-1727760152,Gizmodo,2015,8,31,321.0," The same company that made Jeopardy!-conquering Watson will use similar tech to help fight air pollution in Beijing. In the future, that artificial intelligence could lead to cleaner air for the long haul, everywhere. According to MIT Technology Review, IBMs rolling out a plan that predicts when and where Beijings pollution levels will be the worst, 72 hours ahead of time. Hows this helpful? Think of it as a next-gen air quality forecast. Pollution levels, on a more micro level within a city, can vary widely. IBMs new project, called Green Horizon, combines pre-collected city data with abilities to learn over time and predict repeated patterns. This helps the city make extremely specific calls, like temporarily capping the amount of cars on the streets in areas predicted to be hit hard, MIT says. See, one of the main challenges for AI developers is getting the system (a computer program, a robot, whatever) to automatically adjust to changing situations, just like a human can. Its called adaptive machine learning. In this case, the system (which uses the same language and statistical processing abilities as Watson) takes massive amounts of complex data from the past, and makes specific suggestions for the futuredays in advance. IBM says the Green Horizon is 30 percent more precise than current pollution-tracking models. The Chinese capital has long struggled with sky-high levels of air pollution. But the countrys doing good, too: Its championed maglev trains for over a decade, and its behemoth solar panel market straight up eclipses the rest of the worlds. In the first four months of 2015, Chinas coal use plummeted 8 percent. Still, China remains Earths largest emissions-spewing country, with the US sitting at number two. IBM says it wants to take the Green Horizon tech elsewhere in China, and to other pollutive countries, as well. First, Watson conquered Ken Jennings; maybe in the future, itll conquer terrible air quality worldwide. [MIT Technology Review]"
507,https://gizmodo.com/when-robots-get-wasted-1727210193,Gizmodo,2015,8,28,353.0," Mind-altering substances are buckets of fun for humans, but what about for robots? An excellent article at Hopes and Fears wondered if our future robot overlords are going to spend their weekends getting baked out of their minds. (Spoiler: Were not sure.) But for me, the article raised an even more pressing question: What would a robot on drugs even look like? A quick scan of the internet shows humans have been trying to answer that question for a long time. Here are a few favorite examples of robots under the influence. Data, the lovable and laughable Android of the Starship Enterprise, is always aspiring to be more human. But when Data finally gets to experience humanity by sticking an emotion chip in his brain, his positronic relays go into overdrive. The resultisnt pretty. From fits of uncontrollable laughter and violent rage to sudden bouts of paranoia, Data on emotions is basically a human on coke. Depriving your brain of oxygen is a great way to get yourself good and crunk! Low battery, same idea. Desperately craving too much of anything is called addiction  whether were talking ice cream, crack cocaine, or delicious, mind-altering electricity. Sure, this poor Dalek may have just swallowed a fatal dose of an anti-radiation drug, but to the rest of us, hes just a poor stoner freakin out. Droids of the Star Wars universe dont get to experience the intoxicating effects of alcohol, poor suckers. They can, however, program themselves to feel drunk, which did not end so well for one I-Five protocol droid. He became a drunk asshole, and managed to provoke a Wookie into ripping his arm out. Still, we all know it could have ended worse. Lest we forgot, the Internet went wild this summer over Googles Deep Dream system, a sophisticated computer algorithm that turns perfectly normal images of everyday things into acid-infused nightmarescapes. Sure, one could argue the algorithm was just acting according to its programming. But if you think about it, doesnt this just mean we programmed a bot to trip acid? Contact the author at [emailprotected] or follow her on Twitter."
508,https://gizmodo.com/this-robot-reads-wikihow-guides-to-make-breakfast-1726443555,Gizmodo,2015,8,25,415.0," As robots become more common in offices and homes, more humans will need to communicate with them. Sadly, bots still cant converse or read or write wellyet. A European research project is looking to fix this limitation with a WikiHow-skimming kitchen robot that uses text and voice commands to throw together grub. The European research project RoboHow is behind the robots, called PR2. Each one combs instructions from the how-to site to churn out pancakes or pizza. Its a big deal, MIT Technology Review reports. See, navigating a kitchen is easy for our brainy, fleshy selves, because its an environment filled with micro-tasks that we inherently know how to complete based on years of experience: Twist the jar to open the sauce jar, put dirty dishes in the dishwasher, wash fruit before eating it, give the oven time to preheat. What PR2 does is use written materials to gain that knowledge, perform the unfamiliar task, and then sends the instructions to an open online database that other robots can draw from, expanding their general knowledge. In other words, instead of programming a robots behavior ahead of time, humans can instead just tell the robot what to do, just as though they were delegating flapjack-flipping duties to another human. The better robots get at this kind of AI, the more likely we are to see them in our homes and workplaces. Whys learning from WikiHow so impressive? A couple of reasons. First, many robots are built to carry out back-breaking, boring tasks, and are mostly industrial and plopped in factories. There, the machines are often stationary, sitting in the same spot on the assembly line all day, repetitiously screwing on the same screws or lifting the same 100-pound loads. However, PR2 is able to move about and adapt to a variety of changing situations on-the-go. Refining that kind of independent smart behavior is a huge, continued challenge for todays roboticists. Second, smooth, natural human-and-bot communication is a key area of research right now, as more companies want to make friendly, chatty companion robots that can understand instructions and carry on conversations like humans can. In the case of the kitchen-conquering PR2, researchers are also teaching them simple tasks they might carry out in a variety of environments. For example, in a lab, where the robots are learning to handle chemicalswhich sounds like it could end badly, so hopefully the robots can also figure out how to use fire extinguishers if need be, too. [MIT Technology Review]"
509,https://gizmodo.com/this-neural-network-is-hilariously-bad-at-describing-ou-1725195868,Gizmodo,2015,8,19,382.0," Lion fish. Petri dish. Dough. According to one very befuddled artificial neural network, all of these things can be found in the short intro to Star Trek: The Next Generation. The animation isnt exactly a photorealistic depiction of spacebut its close enough that most of us can recognize its planets, spacecraft, and other details. Yet when Finnish YouTuber Ville-Matias Heikkil asked an artificial neural network to describe what it saw in the intro, it responded with all the lucidity of a deeply drunk robot that had been locked in a closet since 1996. The Starship Enterprise? Thats a CD player. The Star Trek logo? A sea slug. The rings around a Saturn-style planet? Well, thats a hair slide, of course. So why, when weve seen so many examples of how intelligent neural networks can be, does it fail so deeply here? The answer is simple, but requires a basic knowledge of how artificial neural networks learn to see. Each neural network is made up of layers of neurons. Each of these neuron layers is responsible for deciphering different elements of an image, beginning with the basics and leading up to specifics about geometry and color. As we explained earlier this summer, a network learns to see when Google (or whomever built it) feeds it countless images to decipher. Heres how Googles team describes it: Well, we train networks by simply showing them many examples of what we want them to learn, hoping they extract the essence of the matter at hand (e.g., a fork needs a handle and 2-4 tines), and learn to ignore what doesnt matter (a fork can be any shape, size, color or orientation). Remember Deep Dream, the neural network algorithm that Google released publicly this summer? What it saw in existing images was based on the content it had been trained withresulting in it seeing things that werent really there, like strange animals in the sky or architectural details where there certainly werent any. Those things were simply all it knew. Thats the case here tooask a neural network to describe space without teaching it about space, and youll get some very interesting answers. Heikkils video is worth watching a few times over. Happy hair sliding! [Ville-Matias Heikkil on YouTube; h/t Prosthetic Knowledge] Contact the author at [emailprotected]."
510,https://gizmodo.com/software-that-learns-to-drift-could-teach-autonomous-ca-1724542736,Gizmodo,2015,8,17,271.0," Most researchers developing artificial intelligences are working towards a goal of making robots that can one day adapt to any situation. But researchers at MIT have instead created an AI that can learn to drift an RC car, which means that one day the terminators will drive even better than Ken Block can. The machine learning software is first taught how to control a standard RC carturning the wheel to the right makes the car go right, etc.but then has to rely on simulations to determine what corrections need to be made to acceleration and steering to account for a decreased lack of friction between the floor and tires. Small plastic balls attached to the RC car and cameras in the lab allow the learning software to keep track of the position and motion of the vehicle while it applies what it learned in the simulation to the real world. Not surprisingly, the RC car performed slightly differently outside of the perfectly-controlled simulation, but the software was able to easily adapt its controls to ensure the vehicle wouldnt spin out. The researchers were even able to change the friction between the RC cars tires and the floor using salt, but that didnt trip up the software AI. It was also able to easily maintain control of the car when it occasionally collided with another RC vehicle. Eventually this approach to autonomous learning could be incorporated into self-driving vehicles. So when the surface of the road becomes more slippery due to weather, the vehicle will be able to quickly adapt to any scenario while maintaining complete control. [MIT AeroAstro via IEEE Spectrum]"
511,https://gizmodo.com/computer-programs-can-be-as-biased-as-humans-1724436758,Gizmodo,2015,8,16,423.0," Computer programs might not be as objective and unbiased as they seem, according to new research. The software that process job applications, housing loan decisions, and other data that impacts peoples lives might be picking up biases along the way. Its common practice for companies to use software to sort through job applications and score applicants based on numbers like grade point average or years or experience; some programs even search for keywords related to skills, experience, or goals. In theory, these programs dont just weed out unqualified applicants, they also keep human biases about things like race and gender out of the process. Recently, theres been discussion of whether these selection algorithms might be learning how to be biased. Many of the programs used to screen job applications are what computer scientists call machine-learning algorithms, which are good at detecting and learning patterns of behavior. Amazon uses machine-learning algorithms to learn your shopping habits and recommend products; Netflix uses them, too. Some researchers are concerned that resume scanning software may be using applicant data to make generalizations that end up inadvertently mimicking humans discrimination based on race or gender. The irony is that the more we design artificial intelligence technology that successfully mimics humans, the more that A.I. is learning in a way that we do, with all of our biases and limitation, said University of Utah computer science researcher Suresh Venkatasubramanian in a recent statement. Along with his colleagues from the University of Utah, the University of Arizona, and Haverford College in Connecticut, Venkatasubramanian found a way to test programs for accidentally learned bias, and then shuffle the data to keep biased decisions from happening. They presented their findings at the Association for Computing Machinerys 21st annual Conference on Knowledge Discovery and Data Mining last week in Sydney, Australia. The researchers are using machine-learning algorithms to keep tabs on other machine-learning algorithms. Venkatasubramanian and his teams software tests whether its possible to accurately predict applicants race or gender based on the data being analyzed by a resume scanning program, which might include things like school names, addresses, or even the applicant names. If the answer is yes, that might be causing the resume scanner to make generalizations that unfairly discriminate against applicants based on demographics. In that case, according to Venkatasubramanian and his team, the solution is to resdistribute the data in the resume-scanning program so that the algorithm cant see the data that led to the bias. [University of Utah] Contact the author at [emailprotected] or follow her on Twitter."
512,https://gizmodo.com/how-we-won-the-world-robot-soccer-championship-1722419012,Gizmodo,2015,8,6,861.0," My team from UNSW Australia defeated team B-Human from Germany 3-1 last week to claim back-to-back Robocup SPL World Championships. Heres how we did it. The competition involves fully autonomous robots, with no remote control, competing against each other in 5-on-5 soccer. Each team uses the same robots, so the competition is focused on software and artificial intelligence (AI) development, not on hardware construction. The final was a nail-biting match locked at 1-1 with less than three minutes remaining. Both teams had strong strategies and well-tested code, but in the end, our speed proved too fast for the Germans to keep up with. We went into half time with a 1-0 lead after dominating field position for most of the 10 minute half. Despite playing mainly in the Germans side of the field, we struggled to score many goals against their heavily defensive strategy. At one stage the Germans brought all five robots back to their goal box in an attempt to stop us from scoring. The second half saw things briefly fall apart for us midway through. We had three robots lose power as a result of heavy falls and were suddenly reduced to only two active robots on the field. The Germans capitalised, and equalised with around three minutes remaining, leading to us calling a time out to revive our injured robots. After our timeout though, our robots got things back on track. With five robots now on the field, we pulled the momentum of the game back in our favour and scored two late goals to secure our second title in as many years. Our strategy for winning was focused on exploiting our fast walking speed. Our robots are able to accelerate much faster than most teams and they reach a top speed of about 30 centimetres per second (about 1 kmh). Although this is slow compared to humans, in robot soccer its really quick! When the ball is in our half of the field, we dont bother trying to pass it to a teammate, we just try to boot it up the other end of the field. The idea is that we kick the ball deep into their half, then use our fast walk speed to reach the ball before our opponents have time to clear it away. This leads to us playing most of the game in our opponents half of the field, where its hard for them to score goals, but easier for us to score. The competition changes the rules each year to make the games more challenging. This year, all the finals matches were started by a referees whistle, instead of the regular Wi-Fi message. This meant that the robots had to listen out for the whistle before each kick off. We were the only team to reliably start all our robots on the referees whistle. A big reason for this was that the team decided whether or not they had heard the whistle, together. If only one robot heard a whistle, but the other four didnt, they decide that the one robot must have been wrong, and dont play. If three out of the five hear a whistle, but the last two dont, then they decide the two must have missed it and they all start playing. This majority vote system was crucial in ensuring our team listened to the whistle reliably. Another of the major reasons we think we were successful is the development and testing practices we have as a team. Each week we run a series of standardised tests to see how fast we can score a goal. We start the robot in the same set of places each week and time how quickly it gets to the ball and shoots it into the goal. This quickly highlights how effective the past weeks worth of development has been and where we need to improve. It also quickly shows us any major bugs we have introduced with the latest set of changes. We also play small practice games of 5 vs 0 or 3 vs 3 as we get closer to competition. Firstly, this shows us how well our robots position and play as a team, which is crucially important in a team sport like soccer. Secondly it shows how well we are able to perform in a changing environment, playing against a live opposition. Its always much harder to score when youve got opponents getting in the way! Winning two world championships in a row has put a huge target on our back now. All the other teams will be closely watching our progress and focusing their strategies to beat us next year. We will spend the next 12 months continuing to innovate and ensure that we bring an even better team to the competition next year. Robocup also announced that it is coming to Sydney in 2019! Although its a long way off yet, we are looking forward to having a home ground advantage in a few years time. Sean Harris is PhD student in robotics and artificial intelligence at UNSW Australia. This article was originally published on The Conversation. Read the original article. Image by Sean Harris/UNSW."
513,https://gizmodo.com/obamas-plan-to-make-the-worlds-fastest-supercomputer-1721143707,Gizmodo,2015,7,30,351.0," The US does not currently own the worlds fastest supercomputer. But if President Obama gets his way, that will change. Yesterday, Obama signed an executive order launching a program to build the worlds fastest supercomputer30 times faster than all others. The supercomputer will be part of a larger program called the National Strategic Computing Initiative, a multi-agency push to boost computing power in the US by creating the fastest computer of them all. The White House released a statement about the plan: Over the next decade the goal is to build supercomputers capable of one exaflop (1018 operations per second). It is also important to note that HPC in this context is not just about the speed of the computing device itself. As the Presidents Council of Advisors on Science and Technology has concluded, high-performance computing must now assume a broader meaning, encompassing not only flops, but also the ability, for example, to efficiently manipulate vast and rapidly increasing quantities of both numerical and non-numerical data. Some neuroscientists estimate that an exaflop is the amount of computing power necessary to properly simulate how a human brain works. Yet any reference to the creation of such a powerful beast of a machine leading to a swift and merciless uprising of the machines is noticeably absent from the statement. Also noticeably absent: any reference to the fact that this new program shares most of its name with DARPAs Strategic Computing Initiative, a Reagan-era push to build powerful computers, robotics, and AI that didnt quite pan out. This executive order breathes new life into a long-brewing dream to lead the world in terrifyingly powerful machines capable of outsmarting us. Fuck what you heard about supercomputers, especially whatever trash they have over in Europe. (Yeah, Merkel, I see you.) Especially fuck Chinas Tiane-2, the worlds current fastest supercomputer, which operates at 33.86 petaflops, which is not powerful enough to mimic a human brain, not at all. If any nation is going to usher in a horrifying rise of the machines, its going to be the US of A. [Wired via White House] [Image via Getty]"
514,https://gizmodo.com/why-we-should-welcome-killer-robots-not-ban-them-1721019249,Gizmodo,2015,7,30,859.0," The open letter signed by more than 12,000 prominent people calling for a ban on artificially intelligent killer robots, connected to arguments for a UN ban on the same, is misguided and perhaps even reckless. Wait, misguided? Reckless? Let me offer some context. I am a robotics researcher and have spent much of my career reading and writing about military robots, fuelling the very scare campaign that I now vehemently oppose. I was even one of the hundreds of people who, in the early days of the debate, gave their support to the International Committee for Robot Arms Control (ICRAC) and the Campaign to Stop Killer Robots. But Ive changed my mind. Why the radical change in opinion? In short, I came to realise the following. The signatories are just scaremongers who are trying to ban autonomous weapons that select and engage targets without human intervention, which they say will be coming to a battlefield near you within years, not decades. But, when you think about it critically, no robot can really kill without human intervention. Yes, robots are probably already capable of killing people using sophisticated mechanisms that resemble those used by humans, meaning that humans dont necessarily need to oversee a lethal system while it is in use. But that doesnt mean that there is no human in the loop. We can model the brain, human learning and decision making to the point that these systems seem capable of generating creative solutions to killing people, but humans are very much involved in this process. Indeed, it would be preposterous to overlook the role of programmers, cognitive scientists, engineers and others involved in building these autonomous systems. And even if we did, what of the commander, military force and government that made the decision to use the system? Should we overlook them, too? We already have weapons of the kind for which a ban is sought. The Australian Navy, for instance, has successfully deployed highly automated weapons in the form of close-in weapons systems (CIWS) for many years. These systems are essentially guns that can fire thousands of rounds of ammunition per minute, either autonomously via a computer-controlled system or under manual control, and are designed to provide surface vessels with a last defence against anti-ship missiles. When engaged autonomously, CIWSs perform functions normally performed by other systems and people, including search, detection, threat assessment, acquisition, targeting and target destruction. This system would fall under the definition provided in the open letter if we were to follow the signatories logic. But you dont hear of anyone objecting to these systems. Why? Because theyre employed far out at sea and only in cases where an object is approaching in a hostile fashion, usually descending in the direction of the ship at rapid speed. That is, theyre employed only in environments and contexts whereby the risk of killing an innocent civilian is virtually nil, much less than in regular combat. So why cant we focus on existing laws, which stipulate that they be used in the most particular and narrow circumstances? It seems that the real worry that has motivated many of the 12,000-plus individuals to sign the anti-killer-robot petition is not about machines that select and engage targets without human intervention, but rather the development of sentient robots. Given the advances in technology over the past century, it is tempting to fear thinking robots. We did leap from the first powered flight to space flight in less than 70 years, so why cant we create a truly intelligent robot (or just one thats too autonomous to hold a human responsible but not autonomous enough to hold the robot itself responsible) if we have a bit more time? There are a number of good reasons why this will never happen. One explanation might be that we have a soul that simply cant be replicated by a machine. While this tends to be the favourite of spiritual types, there are other natural explanations. For instance, there is a logical argument to suggest that certain brain processes are not computational or algorithmic in nature and thus impossible to truly replicate. Once people understand that any system we can conceive of today  whether or not it is capable of learning or highly complex operation  is the product of programming and artificial intelligence programs that trace back to its programmers and system designers, and that well never have genuine thinking robots, it should become clear that the argument for a total ban on killer robots rests on shaky ground. UN bans are also virtually useless. Just ask anyone whos lost a leg to a recently laid anti-personnel mine. The sad fact of the matter is that bad guys dont play by the rules. Now that you understand why I changed my mind, I invite the signatories to the killer robot petition to note these points, reconsider their position and join me on the dark side in arguing for more effective and practical regulation of what are really just highly automated systems. Jai Galliott is Research Fellow in Indo-Pacific Defence at UNSW Australia. This article was originally published on The Conversation. Read the original article."
515,https://gizmodo.com/a-beautiful-visual-explainer-will-help-you-understand-m-1720540816,Gizmodo,2015,7,28,111.0," Ever wondered what machine learning was and still failed to understand it no matter how much you read? Then this neat visual introduction to the topic should do just the trick. Using a data set about homes, the site does an honestly amazing job of describing how its possible to teach computers to discern between different types of things and classify themultimately even making predictions. So, using that housing data, youll see how its possible to use statistical models to distinguish homes in New York from homes in San Francisco. Its like magic, only its math and science. Go take a look. [A Visual Introduction to Machine Learning via Flowing Data]"
516,https://gizmodo.com/elon-musk-and-stephen-hawking-call-for-ban-on-autonomou-1720383277,Gizmodo,2015,7,27,721.0," Theres no question that the age of autonomous killing machines is upon us. But some very smart and very influential people would like to turn back the clock. Over 1,000 people from the worlds of tech, space travel, computing, and mathematics have signed an open letter calling for an end to the autonomous weapons arms race. The letter will be presented tomorrow in Buenos Aires at the International Join Conference on Artificial Intelligence. The list of signatories includes everyone from your average grad student in mathematics to Noam Chomsky and Steve Wozniak. Stephen Hawking and Elon Musk, who have both warned about the potential dangers of autonomous killing machines in the past few months, have also signed their names to the letter. Their concern seems to be centered on the idea that humans will be out of the loop on the battlefields of tomorrow. Cruise missiles and human-operated drones? Those are okay, according to the letter. But anything beyond that? Not okay. The full letter appears below: Autonomous weapons select and engage targets without human intervention. They might include, for example, armed quadcopters that can search for and eliminate people meeting certain pre-defined criteria, but do not include cruise missiles or remotely piloted drones for which humans make all targeting decisions. Artificial Intelligence (AI) technology has reached a point where the deployment of such systems is  practically if not legally  feasible within years, not decades, and the stakes are high: autonomous weapons have been described as the third revolution in warfare, after gunpowder and nuclear arms. Many arguments have been made for and against autonomous weapons, for example that replacing human soldiers by machines is good by reducing casualties for the owner but bad by thereby lowering the threshold for going to battle. The key question for humanity today is whether to start a global AI arms race or to prevent it from starting. If any major military power pushes ahead with AI weapon development, a global arms race is virtually inevitable, and the endpoint of this technological trajectory is obvious: autonomous weapons will become the Kalashnikovs of tomorrow. Unlike nuclear weapons, they require no costly or hard-to-obtain raw materials, so they will become ubiquitous and cheap for all significant military powers to mass-produce. It will only be a matter of time until they appear on the black market and in the hands of terrorists, dictators wishing to better control their populace, warlords wishing to perpetrate ethnic cleansing, etc. Autonomous weapons are ideal for tasks such as assassinations, destabilizing nations, subduing populations and selectively killing a particular ethnic group. We therefore believe that a military AI arms race would not be beneficial for humanity. There are many ways in which AI can make battlefields safer for humans, especially civilians, without creating new tools for killing people. Just as most chemists and biologists have no interest in building chemical or biological weapons, most AI researchers have no interest in building AI weapons  and do not want others to tarnish their field by doing so, potentially creating a major public backlash against AI that curtails its future societal benefits. Indeed, chemists and biologists have broadly supported international agreements that have successfully prohibited chemical and biological weapons, just as most physicists supported the treaties banning space-based nuclear weapons and blinding laser weapons. In summary, we believe that AI has great potential to benefit humanity in many ways, and that the goal of the field should be to do so. Starting a military AI arms race is a bad idea, and should be prevented by a ban on offensive autonomous weapons beyond meaningful human control. The letter was coordinated through the Future of Life Institute, whose mission is explicitly described as developing optimistic visions of the future. The part that might be confusing after reading the letter? We already have military autonomous killing machines, depending on your definition of in the loop. And there are surely autonomous technologies currently being used by the military that the public has no idea about. The signatories clearly would like to draw a line in the sand. But the question becomes whether that line has already been crossed. Guys like Musk and Wozniak might need to invent a time machine if they want to put that genie back in the bottle. [h/t The Guardian] Contact the author at [emailprotected]."
517,https://gizmodo.com/skype-translator-likes-to-swear-in-chinese-1719685430,Gizmodo,2015,7,23,175.0," Skype Translator is an amazing feat of engineering, even if it isnt perfect. But reports suggest that, in Chinese at least, it has somewhat of a potty mouth. Tom Carter writes for Global Times that hes been using Skype Translator whilst on a shoot in China to make a cross-lingual TV commercial. But some of the results havent exactly been what he expected. He explains: A glitch in the beta software misinterpreted the words I spoke. Its nice to talk to you was translated as Its f*cking nice to f*ck you, and other synthesized profanity, like the icebox robot in 1970s sci-fi flick Logans Run, but with Tourette Syndrome. Carter lays the blame at the feet of the The Great Firewall, which he claims obstructs the Skype system from working consistently. Which it could beor it could just be internal bugs that cant cope with the voices theyre attempting to translate. Either way, it seems Skypes AI translation has a way to go before its used as a diplomatic tool. [Global Times via Network World]"
518,https://gizmodo.com/someone-finally-turned-googles-deepdream-code-into-a-si-1719461004,Gizmodo,2015,7,22,195.0," Earlier this month, Google announced that its artificial neural networks were having creepy daydreams. While its since made the code public, a kindly soul has gone a step further and turned it into a web app that anyone can use. The new web app, called Dreamscope, is like Instagram on LSD, all powered by Google. It provides a prescriptive set of 19 filters, so it wont let you play around with things as intricately as Googles code will, but then, you dont need to look at code either. You just upload an image, choose a filter and out pops a nightmare of your own making. As a refresher of how the software works, allow our very own Maddie Stone to explain: The Deep Dream system essentially feeds an image through a layer of artificial neurons, asking an AI to enhance and build on certain features, such as edges. Over time, pictures can become so distorted that they morph into something entirely different, or just a bunch of colorful, random noise. If you go browsing the Dreamscope gallery youll find some hideousand NSFWcreations. Youve been warned. Just look what happened to Mona Lisa. [Dreamscope via WIRED]"
519,https://gizmodo.com/linux-creator-linus-torvalds-laughs-at-the-ai-apocalyps-1716383135,Gizmodo,2015,7,7,387.0," Over the past several months, many of the worlds most famous scientists and engineers  including Stephen Hawking  have said that one of the biggest threats to humanity is an artificial superintelligence. But Linus Torvalds, the irascible creator of open source operating system Linux, says their fears are idiotic. He also raised some good points, explaining that what were likely to see isnt some destructive superintelligence like Skynet, but instead a series of targeted AI that do things like language translation or scheduling. Basically, these would just be fancier versions of apps like Google Now or Siri. They will not, however, be cybergods, or even human-equivalent forms of intelligence. In an Q/A with Slashdot community members, Torvalds explained what he thinks will be the result of research into neural networks and AI: Well get AI, and it will almost certainly be through something very much like recurrent neural networks. And the thing is, since that kind of AI will need training, it wont be reliable in the traditional computer sense. Its not the old rule-based prolog days, when people thought theyd *understand* what the actual decisions were in an AI. And that all makes it very interesting, of course, but it also makes it hard to productise. Which will very much limit where youll actually find those neural networks, and what kinds of network sizes and inputs and outputs theyll have. So Id expect just more of (and much fancier) rather targeted AI, rather than anything human-like at all. Language recognition, pattern recognition, things like that. I just dont see the situation where you suddenly have some existential crisis because your dishwasher is starting to discuss Sartre with you. As for the idea that AI might usher in a Singularity, where computers experience an intelligence explosion and figure out how to produce any object we could desire, in infinite amounts? Oh, and also help us live forever? Maybe Googles Singulatarian guru Ray Kurzweil buys into that myth, but Torvalds is seriously skeptical: The whole Singularity kind of event? Yeah, its science fiction, and not very good Sci-Fi at that, in my opinion. Unending exponential growth? What drugs are those people on? I mean, really. Even after all these years, Torvalds can still troll with the best of em. And I love him for it. [TechWeek Europe and Slashdot]"
520,https://gizmodo.com/can-you-fool-this-nudity-spotting-algorithm-1714858092,Gizmodo,2015,6,30,363.0," Identifying nudity online is a notoriously thorny issue. Is it a butt or a bent elbow? Renaissance art or porn? Well, now one company that builds algorithms to identify such differences has built a website that you can challenge with your own, um, material. IsItNude.com is the brainchild of Algorithmia and it provides much the service its URL would suggest. Feed it an image, and the algorithm will try to determine if youre offering up full-on nudity or something a little more restrained. The algorithm behind it was created for an unnamed company that wanted to ensure that its website was kid-friendly. Now, the site exists as a standalone item that you can play around with. According to Wired, the algorithm uses a variety of tricks to spot nudes. It detects patches of skin, but it also uses hand gesture and facial recognition tricks along with smart skin tone identification to try and create a more accurate analysis of exactly whats on show. While Algorithmia doesnt steal your pictures and the results to inform its work, its keen to hear from people that find it throwing up spurious results so that it can tweak its algorithms. And it needs to, because it certainly isnt perfect yet. Its reliance on skin tone means that black and white images flummox it, for instance, and because it doesnt identify body partsyou know what were talking abouta picture of someone in Speedos or a bikini could end up flagged as obscene. For now, thats why big sites like Facebook and Instagram still employ people to vet images that are reported as being obscene, rather than allowing a computer to determine whats on show. Still, its a decent effort at automating a notoriously difficult process. For more nuanced analysis that can sift the confusingly innocent from the subtly smutty, though, well be waiting some time, because while you might be able to determine a work of art from a Playboy center-fold, computers needs to get a whole lot smarter before than can manage that particular feat. But for now though, why not see if you can fool it, and post your attempts below? [Is It Nude? via Wired]"
521,https://gizmodo.com/robotic-emotions-could-help-us-out-of-the-uncanny-valle-1712481134,Gizmodo,2015,6,19,800.0," When the first guests arrive at Nagasakis Hotel Henn Na in July this year, they will be greeted and served by robots. In a similar approach, Toshibas android Aiko recently held a short-term role greeting customers at a department store in Tokyo. Customers were comfortable approaching Aiko to ask for directions and even the receptionist who normally holds the post felt her android colleague was doing a reasonable job. There is no doubt that developments in android technology mean that robots now look more lifelike than ever. It is even possible to imagine mistaking a robot such as Aiko for a human, at least at first glance. However, encountering near-human agents may not always be a comfortable experience. Hotel Henn Na claims it is looking to explore the elements that personify hotels and to recreate that by using person-like entities. The big question is how people interact with robots, such as Aiko, that have a human appearance. Such interactions require considerably more than a robot with an apparently human face. Hotel Henn Na claims it is looking to explore the elements that personify hotels and to recreate that by using person-like entities. The big question is how people interact with robots, such as Aiko, that have a human appearance. Such interactions require considerably more than a robot with an apparently human face. Our research at the Open University has considered aspects of how it might feel to be in a situation similar to interacting with a robot receptionist. It asks whether the properties of a robot receptionists appearance and behaviour are able to make us feel comfortable or may make us decidedly uncomfortable. Our work into the so-called uncanny valley effect also suggests that a key aspect of such interactions will be the ability of the robot to reproduce and convey realistic emotions, particularly through their facial expressions. The uncanny valley effect is now a well-known relationship how human-like a robot is and how comfortable people feel when interacting with it. There is a point where something can seem too close to human to be comfortable  and our research has explored some explanations for why this might occur. For a long time, research in this area was mainly focused on the practical aspects of how near-human agents appear and on the techniques that might make them look more realistic and acceptable enough to bridge this uncanny valley. Our early research had observed that some of the eeriest and most unsettling near-human agents were those with exaggerated features, such as a cute doll with dark, wide and blank eyes. Building on the idea of disturbing facial features, our more recent research looked at the role of emotions in the uncanny valley effect and considered whether that eeriness may actually be a result of those near-human faces being unable to present realistic emotional expressions. Most of the research into this area has considered how artificial faces can be made to appear more human-like, but we looked at how even human faces can appear eerie under certain conditions. We have found that when parts of photographs of faces posing emotional expressions were combined, certain combinations of emotions evoked a sense of eeriness. This effect was particularly strong when happy mouths were paired with scared or angry eyes. In other words, encountering a face where the mouth is clearly smiling, but the eyes are not displaying the same emotion, can be quite disturbing. It follows that unless a robot face can display emotions accurately and appropriately, just as a human would, a human viewer could be left with a distinct feeling of unease. So, it will be a long time before human receptionists superseded. Even now that the design of robot faces is at a point where they can be made to appear human-like, robots will need better abilities to be able to perform convincing emotional interactions. Creating near-human agents that can portray genuinely believable emotional expressions presents a highly complex technological challenge. So while this tension between appearance and emotional interaction may currently confine robots to the uncanny valley, it could also offer a way out. An alternative would be for robot designers to avoid trying to mimic human faces too closely until technology has progressed to allow their feelings to be represented realistically as well. Otherwise, our expectations about emotions would just be raised by the their human-like nature and then not met by their expressive ability. Choosing to make robots less human in appearance could avoid a constant race where robots that appear more human-like also require ever more convincing emotional abilities, offering a way around the uncanny valley. Stephanie Lay is PhD candidate at The Open University. Graham Pike is Professor of forensic cognition at The Open University. This article was originally published on The Conversation. Read the original article."
522,https://gizmodo.com/watch-this-software-learn-to-play-super-mario-world-bet-1711381189,Gizmodo,2015,6,15,252.0," A quick YouTube search reveals countless Super Mario World speedruns by some of the games most skilled players. And MarI/O can keep up with the best of them, even though its actually just a piece of advanced learning software made up of virtual neural networks. Created by Seth Bling, a developer who also happens to be an insanely skilled Super Mario World speedrunner, MarI/O works a lot like the human brain does when it comes to learning. Instead of starting with a crash course in how Super Mario World is played, what everything on-screen means, or what perils need to be avoided, it had to learn them all by scratch using trial and error. Seth calls it neural evolution, and the biggest difference between humans and the software is that no controllers were thrown across the room by the AI while it built up its skillset. And instead of needing weeks to master the nuances of Super Mario World, MarI/O only required about 24 hours to get to the point where it could breeze through a level without dying, and without the need for a single power-up. Imagine how stressful playing would be without using even a mushroom. The source code for MarI/O has been posted here if you want to try it for yourself, because it should work just as well for other side-scrolling games too. If theres a level on Sonic the Hedgehog youve never been able to beat, this might finally be your key to victory. [YouTube via SlashGear]"
523,https://gizmodo.com/recent-reports-make-machine-learning-sound-like-a-sport-1710817817,Gizmodo,2015,6,12,1129.0," News that Baidu, the Google of China, cheated to take the lead in an international competition for artificial intelligence technology has caused a storm among computer science researchers. It has been called machine learnings first cheating scandal by MIT Technology Review and Baidu is now barred from the competition. The Imagenet Challenge is a competition run by a group of American computer scientists which involves recognising and classifying a series of objects in digital images. The competition itself is no Turing test, but it is an important challenge, and one of commercial importance to many firms. The cheating by Baidu was nothing sophisticated, more akin to an initial stolen glimpse at the answers, which was followed by more of the same when it went unnoticed. Even that makes it sound worse than it was. Part of the competition involved looking at the answers anyway: someone in the Baidu team simply did it more than they were officially allowed to. In their paper about the submission, Baidu themselves werent claiming anything more than an engineering advance: they built a large supercomputer that could handle more data than previous implementations. A necessary advance, but very much a scaling up of existing solutions  one that would be financially outside the reach of a typical academic research group. They participated in the competition as an attempt to demonstrate that, after such significant investment in hardware, their new supercomputer was able to perform. They have since apologised for breaking the rules of the competition. In any case, the significant breakthrough in the area had already been achieved by Geoff Hintons group at the University of Toronto. They produced the machine learning equivalent of the high jumps Frosbury Flop to win the 2012 version of the competition with such a significant improvement that all leading entries are now derived from their model. That model itself also built on a two-decade-long program of research by Yann LeCun, then of New York University. The result of Baidus entry into the competition was posted as an e-print publication. E-prints are articles that are unreviewed. They are a slightly more formal versions of a technical blog post. The problem was identified by the community quickly, within three weeks, and a corrected version was published. This is science in action. The cheating scandal was labelled as such by the very same prestigious technical publication that broadcast the initial results to its readers within two days of the e-prints publication: MIT Technology Review. Singling out MIT Technology Review in this case may be a little unfair, because this is part of a wider phenomenon where technical results are trumpeted in the press before they are fully tasted (let alone digested) by the scientific community. E-print publication is a good thing, it allows ideas to be spread quickly. However, the implications of those ideas need to be understood before they are presented as scientific fact. Ideally knowledge moves forward through academic consensus, but in practice that consensus itself is swayed by outside forces. This raises questions about who is the ultimate arbiter of academic quality. One answer is opinion: the opinion of those that matter, such as governments, businesses, other scientists or even the press. Success in machine learning has meant it is attracting such attention. Ironically, the developments that enabled recent breakthroughs in AI all took place outside of such close scrutiny. In 2004 the Canadian Institute for Advanced Research (CIFAR) funded a far-sighted program of research. An international collaboration of researchers was given the time, intellectual space and money that they needed to make these significant breakthroughs. This collaboration was led by Geoff Hinton, the same researcher whose team achieved the 2012 breakthrough result. This breakthrough led to all the major internet giants fighting for their pound of academic flesh. Of those researchers involved in CIFAR, Hinton has been hired by Google, Yann LeCun leads Facebooks AI Research team, Andrew Ng heads up research at Baidu and Nando de Freitas was recently recruited to Google DeepMind, the London start-up that Google lavished 400m on acquiring. The Baidu cheating case is symptomatic of a big change in the landscape for those who work in machine learning and who drove these advances in AI. Until 2012, ideas from researchers in machine learning were under the radar. They were widely adopted commercially by companies like Microsoft and Google, but they did not impinge much on public consciousness. But two breakthrough results brought these ideas to the fore in the the public mind. The Imagenet result by Hintons team was one. The other was a program that could learn to play Atari video games. It was created by DeepMind, triggering their purchase by Google. However, just as Deep Blues defeat of Kasparov didnt herald a dawn in the age of the super-intelligent computer, neither will either of these significant accomplishments. They have not arrived through better understanding of the fundamentals of intelligence, but through more data, more computing power and more experience. These apparent breakthroughs have whetted the appetite. The technical press is becoming susceptible to tabloid sensationalism in this area, but who can blame them as companies and universities ramp up their claims of scientific advance? The advances are somewhat of an illusion, they are the march of technologists following in a scientific wake. The wake-generators are a much harder to identify ortrack, even for their fellow scientists. But the very real danger is that expectations of significant advance or misunderstanding of the underlying phenomenon will bring about an AI bubble of the type we saw 30 years ago. Such bubbles are very damaging. When high expectations arent immediately fulfilled then entire academic domains can be dismissed and far-seeing proposals like CIFARs go unfunded. Even if Baidus result were valid, it would have been just the type of workaday scientific development that most of us spend most of our time trying to cook up. It did not merit a pre-publication announcement in MIT Technology Review and the pre-publication withdrawal should have been just a footnote to add to the diverse collection that keep all astute academics scientifically wary. Rather boringly, the only true marker of scientific advance is repeatability. Whether that is within the scientific community or by transfer of ideas to the commercial world. When reporting on the scandal MIT Technology Review refers to participation in these competitions as a sport. I feel sporting analogies give a wrong idea of the spectacle of scientific progress. It is more like watching a painter at work. It is very rare that any single brushstroke reveals the entire picture. And even when the picture is complete, it may only tell us a limited amount about what the next creation will be. Neil Lawrence is Professor of Machine Learning and Computational Biology at University of Sheffield."
524,https://gizmodo.com/this-robot-learns-new-tasks-by-doing-them-like-a-human-1706567578,Gizmodo,2015,5,24,627.0," One thing robots are notoriously bad at is learning by doing. You can pack plenty of information into a robotic brain, but ask a bot to teach itself a new motor taskeven one as simple as stacking blocks or unscrewing a water bottleand youre probably shit out of luck. That, however, might be changing very soon. Researchers at UC Berkeley are now developing algorithms that robots can use to learn all sorts of tasks through trial and error, just like humans do. In practical terms, this could eventually lead to home service robots capable of handling any number of tedious tasks wed rather not doscrewing in lightbulbs, plunging toilets, folding laundry. Traditionally, robots make their way through the world with a vast amount of pre-programming that equips them to handle a range of scenarios. While this works reasonably well in controlled environmentslaboratories or medical facilities, for instancelearning to adapt to the unknown is a critical step our robots will need to take if theyre ever going to become more integrated into our daily lives. To that end, researchers involved in Berkeleys People and Robotics Initiative are turning to a new branch of artificial intelligence known as deep learning, which draws inspiration from how the human brains neural circuitry perceives and interacts with the world. For all our versatility, humans are not born with a repertoire of behaviors that can be deployed like a Swiss army knife, and we do not need to be programmed, said robotics researcher Sergey Levine in a press release. Instead, we learn new skills over the course of our life from experience and from other humans. This learning process is so deeply rooted in our nervous system, that we cannot even communicate to another person precisely how the resulting skill should be executed. We can at best hope to offer pointers and guidance as they learn it on their own. If youve ever used Siri, Googles speech-to-text program, or Google Street View, youve already benefited from recent advances in this field. But applying deep learning to motor skills has proven much more challenging. In terms of complexity, physical tasks go far beyond passive recognition of sights or sounds. In recent experiments, researchers have been working with a small personal robot they call the Berkeley Robot for the Elimination of Tedious Tasks, or BRETT. For his training, BRETT is presented with a series of simple motor tasks, such as placing pegs into holes or stacking LEGO bricks. The algorithm controlling BRETTs learning includes a reward function that scores BRETT based on how well he learns a new task. That reward system is key: Movements that bring BRETT closer to completing his task score higher than those that dont, and this information is relayed across thousands of parameters in his neural net. So far, the results of BRETTs training have been astounding. Given the location of objects in a scene, BRETT is typically able to master a new assignment within ten minutes. If BRETT doesnt have the location of objects and instead needs to learn vision and motor control together, the process can take several hours. We still have a long way to go before our robots can learn to clean a house or sort laundry, but our initial results indicate that these kinds of deep learning techniques can have a transformative effect in terms of enabling robots to learn complex tasks entirely from scratch, said Pieter Abbeel of UC Berkeleys Department of Electrical Engineering and Computer Sciences. In the next five to 10 years, we may see significant advances in robot learning capabilities through this line of work. As someone whos about to commit three solid days to spring cleaning, this is very heartening news. Follow Maddie on Twitter or contact her at [emailprotected]"
525,https://gizmodo.com/wolframs-new-ai-will-try-to-identify-any-image-you-give-1704400661,Gizmodo,2015,5,14,271.0," Artificial intelligence image recognition is already powerful, but its getting better week by week. Now, Wolfram Research has opened up a new website that will try and identify whats depicted in any image you toss its way. The new tool takes an image and attempts to work out what it shows. The algorithms used by Wolfram use machine learning to get better over time: they look at more and more images which are tagged as showing certain objectsa cat, a hat or a mat, sayto learn what they look like. Wolfram uses neural networks to sift through what it sees and narrow the possible choice of objects displayed in the image down to the most likely. Each layer of the neural network works at a different level of detail: first identifying brightness and color of individual pixels, then how they come together to form edges and shadows, and latterly how they come together to signify features like wheels or eyes or petals. It wont always get it right, but most of the time I think it does remarkably well, explains Stephen Wolfram in a blog post. And to me whats particularly fascinating is that when it does get something wrong, the mistakes it makes mostly seem remarkably human. The service can be used on desktop or mobile, and the code behind the site can be used toomaking it possible to embed Wolframs advanced image analysis into new software and apps. The applications are clearly limitless. But lets not mess around: what does it make of your photographs? Post the results in the comments. [Wolfram Image Identification Project via 9to5mac via Verge]"
526,https://gizmodo.com/mit-is-developing-an-ai-cancer-diagnosis-system-1699916919,Gizmodo,2015,4,24,268.0," Diagnosing cancer is a depressingly common activity in hospitalsbut spotting subtle variations in tumor types can require sifting through hundreds of previous cases. At MIT, though, researchers are developing a system that uses Artificial Intelligence to speed up that process. A team from MITs Computer Science and Artificial Intelligence Laboratory has developed a software system that studies data from existing medical reports in order to suggest cancer diagnoses to doctors. In a paper published in the Journal of the American Medical Informatics Association, the team behind it describes how it can be used identify types of the cancer lymphoma. There are in fact 50 subtypes of lymphoma, which can be hard to specifically diagnose; up to 15 percent are initially misdiagnosed, slowing treatment. The software developed at MIT taps into large banks of pathology reports, extracting medical data which can be tied together into relationships that the World Health Organization uses to define the sub-types of the cancer. The system also ties words commonly used in the medical records to each of these data points, providing an extra layer of information. The system can then take a new set of test results and compare it to existing data on a number of levels. The result provides clinicians with both medical data and written descriptions of similar cases, helping suggest the types of lymphoma most likely to be present. Theres still some way to go for the AI systemit needs far more data and some comprehensive testingbut in the future, it could help make the lives of doctors far easier. [Journal of the American Medical Informatics Association via MIT]"
527,https://gizmodo.com/your-tesla-is-about-to-become-autonomous-1692941193,Gizmodo,2015,3,22,404.0," Earlier this week, we heard billionaire entrepreneur Elon Musk describe his vision for the future of self-driving vehicles. Turns out, when Musk said well be there in a few years, what he actually meant was, his company, Tesla, will be there this summer. On Thursday, Musk announced that Teslas Model S sedans would be receiving a software update this summer, one thatll allow the cars to start driving themselvesat least some of the timein a hands-free autopilot mode. You can think of your Tesla like a Pokemonone day soon, its going to evolve, and suddenly, it wont need you very much anymore. However, your Tesla miiiight still need you to help get it out of trouble with the authorities, because, as the New York Times reports, Musks bold move has raised some pretty serious legal questions. To wit, its not entirely clear that autonomous driving is actually legal just yet. According to the New York Times: Theres a reason other automakers havent gone there, said Karl Brauer, an analyst with Kelley Blue Book. Best case scenario, its unclear. If youre an individual that starts doing it, youd better hope nothing goes wrong. Mr. Brauer said while a handful of states had passed laws legalizing autonomous vehicles, those laws were written to cover the testing of driverless cars, not their use by consumers. Its not just a philosophical reason why automakers havent allowed their vehicles to drive themselves, he said. Theres a legal reason, too. Tesla, of course, is defending its autopilot system, saying that theres nothing in the technology which conflicts with current regulations. Perhaps its more of a legal gray-area, like asteroid mining, an endeavor that Musks private spacefaring company SpaceX is also dipping its toes into. In addition to autonomously driving itself on highways, your Tesla will be soon be summonable via smartphone and able to park itself in your garage. Itll also receive new tools to help drivers monitor the status of charging stations, which is a good thing, because it seems well have very little else to do. Lest you fear that this is the beginning of the machine takeover, Musk assures us that its not. Even though the billionaire has warned that we might be summoning the devil with more advanced forms of AI, when it comes to autonomous cars, Telsa says we can all sit back, relax, and enjoy the ride. [New York Times] Top image via Shutterstock."
528,https://gizmodo.com/a-i-might-kill-us-through-incompetence-not-malevolenc-1687634532,Gizmodo,2015,2,24,304.0," Artificial intelligence has been looming large in the public conciousness recently, thanks to the likes of Elon Musk and Stephen Hawking telling us how were going to die at the hands of robots (the upcoming Terminator reboot probably doesnt help, either). But amidst the techpocalypse talk, theres been limited discussion of what constitutes A.I, and how it might look completely different to Skynet. As Benjamin H. Bratton explains in the New York Times, our idea of artificial intelligence has been engineered from the beginning to be anthropomorphic: a truly intelligent computer is one that reflects humanity back at us. The Turing test, the flawed but oft-quoted determination of artificial intelligence, really just requires a computer to pose as a human for a few minutes  something that Bratton finds bizarre: That we would wish to define the very existence of A.I. in relation to its ability to mimic how humans think that humans think will be looked back upon as a weird sort of speciesism. The legacy of that conceit helped to steer some older A.I. research down disappointingly fruitless paths, hoping to recreate human minds from available parts. It just doesnt work that way. He goes on to point out that planes dont fly like birds, so why should computers be hamstrung by human impressionism? When it comes to the matter of the dangers of A.I, Bratton is concerned, but not about a robot coup. Rather, what we really fear, even more than a Big Machine that wants to kill us, is one that sees us as irrelevant. In a technology landscape a little overrun with faux-humanoid digital assistants and a decades-old public perception of A.I, Brattons essay is an insightful take on an incredibly important topic. And, it might make you stop and think next time you swear at Siri. [New York Times]"
529,https://gizmodo.com/what-do-you-think-will-be-the-doom-of-civilization-1686816143,Gizmodo,2015,2,19,232.0," Its probably true that every generation thinks itll be the lastI mean, the Doomsday Clock has been ticking since 1947. And though I accused us millennials earlier today of being the generation that cried apocalypse, I fully admit there are some damn legit reasons for that cry currently brewing. A report out this week from the Global Challenges Foundation lists 12 reasons, to be precisea dozen risks that threaten human civilization. And sure enough all of the three emerging risks finger developing tech: AI, nanotech, and synthetic biology. The Bulletin of Atomic Scientists Doomsday Clock was also recently updated to include digital risks for the first timeAI and hackingand it even sadly bumped the ETA for our impending demise up three minutes. Its all terrifying and fascinating and tempting to fetishize, and as my colleague pointed out, reflects any normal luddite reaction to a new technology that comes along and threatens the safe and familiar status quo. But, not to be morbid or anything, Im also legitimately, really curious what will ultimately be the source of societys downfallimminent or otherwise. The report lays out a few futuristic scenarios: Will manipulating biology lead to new era of bio-warfare? Will self-replicating nanotech let us effortlessly manufacture an arsenal of tiny nuclear weapons? Will superintelligent AI  decide to create a world without humans? Im curious what you think: Which dystopian future should we really fear?"
530,https://gizmodo.com/ankis-incredible-self-driving-toy-cars-just-got-even-co-1684877156,Gizmodo,2015,2,10,452.0," Whats better than a set of tiny artificially intelligent cars thatdrive themselvesaround a track while duking it out with lasers and tractor beams? Well, what if you could configure that track however you want? Thats the name of the game with Anki Overdrive, coming this September from the geniuses at robotics startup Anki. Its pretty simple: if you go buy the original Anki Drive starter kit for $150 today, you only get a single track on a rubbery mat that you unroll every time you want to play. Additional tracks cost $70 each. No matter how much fun it might be to steer a mini race car with your phone, that aint cheap! Heres what it looks like now: But in September, that same $150 will buy you a set of modular pieces that snap together to form any track you want. You get four straight pieces and six curves, each equipped with interlocking tabs and strong magnets that the track practically assembles itself: I helped Ankis founders build a track in about a minute flat, just snapping the pieces into place. Oh, and because the track is made of flexible high-impact polystyrene, you can build it right on top of stuff to make a ramp! See the MacBook charger and teleconferencing device beneath this track? Ankis tiny cars sped across that surface with no issue whatsoever. Because the pieces are modular, that means you can build a whole lot more tracks from the get-goeight to startand the more additional pieces you buy, the bigger you can go. But the best part may be that those pieces wont just be curves and straights, but other types of game-changing terrain as well. Theres a jump kit that can let up to three cars actually jump off a ramp simultaneously. (Yes, these tiny autonomous cars can jump!) Theres a $30 intersection that not only lets a track cross itself, but where cars can choose to turn to stay on the most efficient path. Perhaps the most interesting expansion is the $20 U-turn kit, which comes with two dead end pieces that can be placed on either end of a loooong track, and tells the cars to make a U-turn rather than run off the road! Theres plenty more, including a new set of cars, a new user interface for the app, new AI opponents, and a new campaign mode designed to give the game a story and objectives of some sort, but the modular track is definitely the star of the show. The idea of not being confined to a single track makes this feel like a high-tech toy I could actually consider buying. Shame there are so many months between September and now."
531,https://gizmodo.com/the-ai-revolution-how-far-away-are-our-robot-overlords-1684199433,Gizmodo,2015,2,9,7101.0," Imagine taking a time machine back to 1750a time when the world was in a permanent power outage, long-distance communication meant either yelling loudly or firing a cannon in the air, and all transportation ran on hay. When you get there, you retrieve a dude, bring him to 2015, and then walk him around and watch him react to everything. Its impossible for us to understand what it would be like for him to see shiny capsules racing by on a highway, talk to people who had been on the other side of the ocean earlier in the day, watch sports that were being played 1,000 miles away, hear a musical performance that happened 50 years ago, and play with my magical wizard rectangle that he could use to capture a real-life image or record a living moment, generate a map with a paranormal moving blue dot that shows him where he is, look at someones face and chat with them even though theyre on the other side of the country, and worlds of other inconceivable sorcery. This is all before you show him the internet or explain things like the International Space Station, the Large Hadron Collider, nuclear weapons, or general relativity. This experience for him wouldnt be surprising or shocking or even mind-blowingthose words arent big enough. He might actually die. But heres the interesting thingif he then went back to 1750 and got jealous that we got to see his reaction and decided he wanted to try the same thing, hed take the time machine and go back the same distance, get someone from around the year 1500, bring him to 1750, and show him everything. And the 1500 guy would be shocked by a lot of thingsbut he wouldnt die. It would be 	far less of an insane experience for him, because while 1500 and 1750 were very different, they were muchless different than 1750 to 2015. The 1500 guy would learn some mind-bending shit about space and physics, hed be impressed with how committed Europe turned out to be with that new imperialism fad, and hed have to do some major revisions of his world map conception. But watching everyday life go by in 1750transportation, communication, etc.definitely wouldnt make him die. No, in order for the 1750 guy to have as much fun as we had with him, hed have to go much farther backmaybe all the way back to about 12,000 BC, before the First Agricultural Revolution gave rise to the first cities and to the concept of civilization. If someone from a purely hunter-gatherer worldfrom a time when humans were, more or less, just another animal speciessaw the vast human empires of 1750 with their towering churches, their ocean-crossing ships, their concept of being inside, and their enormous mountain of collective, accumulated human knowledge and discoveryhed likely die. And then what if, after dying, 	he got jealous and wanted to do the same thing. If he went back 12,000 years to 24,000 BC and got a guy and brought him to 12,000 BC, hed show the guy everything and the guy would be like, Okay whats your point who cares. For the 12,000 BC guy to have the same fun, hed have to go back over 100,000 years and get someone he could show fire and language to for the first time. In order for someone to be transported into the future and die from the level of shock theyd experience, they have to go enough years ahead that a die level of progress, or a Die Progress Unit (DPU) has been achieved. So a DPU took over 100,000 years in hunter-gatherer times, but at the post-Agricultural Revolution rate, it only took about 12,000 years. The post-Industrial Revolution world has moved so quickly that a 1750 person only needs to go forward a couple hundred years for a DPU to have happened. This patternhuman progress moving quicker and quicker as time goes onis what futurist Ray Kurzweil calls human historys Law of Accelerating Returns. This happens because more advanced societies have the ability to progress at a faster 	rate than less advanced societiesbecause theyre more advanced. 19th century humanity knew more and had better technology than 15th century humanity, so its no surprise that humanity made far more advances in the 19th century than in the 15th century15th century humanity was no match for 19th century humanity. This works on smaller scales too. The movie 	Back to the Future came out in 1985, and the past took place in 1955. In the movie, when Michael J. Fox went back to 1955, he was caught off-guard by the newness of TVs, the prices of soda, the lack of love for shrill electric guitar, and the variation in slang. It was a different world, yesbut if the movie were made today and the past took place in 1985, the movie could have had much more fun with much bigger differences. The character would be in a time before personal computers, internet, or cell phonestodays Marty McFly, a teenager born in the late 90s, would be much more out of place in 1985 than the movies Marty McFly was in 1955. This is for the same reason we just discussedthe Law of Accelerating Returns. The average rate of advancement between 1985 and 2015 was higher than the rate between 1955 and 1985because the former was a more advanced worldso much more change happened in the most recent 30 years than in the prior 30. Soadvances are getting bigger and bigger and happening more and more quickly. This suggests some pretty intense things about our future, right? Kurzweil suggests that the progress of the entire 20th century would have been achieved in only 20 years at the rate of advancement in the year 2000in other words, by 2000, the rate of progress was five times faster than the 	average rate of progress during the 20th century. He believes another 20th centurys worth of progress happened between 2000 and 2014 and that another 20th centurys worth of progress will happen by 2021, in only seven years. A couple decades later, he believes a 20th centurys worth of progress will happen multiple times in the same year, and even later, in less than one month. All in all, because of the Law of Accelerating Returns, Kurzweil believes that the 21st century will achieve 1,000 times the progress of the 20th century. If Kurzweil and others who agree with him are correct, then we may be as blown away by 2030 as our 1750 guy was by 2015i.e. the next DPU might only take a couple decadesand the world in 2050 might be 	so vastly different than todays world that we would barely recognize it. This isnt science fiction. Its what many scientists smarter and more knowledgeable than you or I firmly believeand if you look at history, its what we should logically predict. So then why, when you hear me say something like the world 35 years from now might be totally unrecognizable, are you thinking, Cool.but nahhhhhhh? Three reasons were skeptical of outlandish forecasts of the future: When we imagine the progress of the next 30 years, we look back to the progress of the previous 30 as an indicator of how much will likely happen. When we think about the extent to which the world will change in the 21st century, we just take the 20th century progress and add it to the year 2000. This was the same mistake our 1750 guy made when he got someone from 1500 and expected to blow his mind as much as his own was blown going the same distance ahead. Its most intuitive for us to think 	linearly, when we should be thinkingexponentially. If someone is being more clever about it, they might predict the advances of the next 30 years not by looking at the previous 30 years, but by taking thecurrent rate of progress and judging based on that. Theyd be more accurate, but still way off. In order to think about the future correctly, you need to imagine things moving at a much faster rate than theyre moving now. First, even a steep exponential curve seems linear when you only look at a tiny slice of it, the same way if you look at a little segment of a huge circle up close, it looks almost like a straight line. Second, exponential growth isnt totally smooth and uniform. Kurzweil explains that progress happens in S-curves: An S is created by the wave of progress when a new paradigm sweeps the world. The curve goes through three phases: Slow growth (the early phase of exponential growth) Rapid growth (the late, explosive phase of exponential growth) A leveling off asthe particular paradigm matures. If you look only at very recent history, the part of the S-curve youre on at the moment can obscure your perception of how fast things are advancing. The chunk of time between 1995 and 2007 saw the explosion of the internet, the introduction of Microsoft, Google, and Facebook into the public consciousness, the birth of social networking, and the introduction of cell phones and then smart phones. That was Phase 2: the growth spurt part of the S. But 2008 to 2015 has been less groundbreaking, at least on the technological front. Someone thinking about the future today might examine the last few years to gauge the current rate of advancement, but thats missing the bigger picture. In fact, a new, huge Phase 2 growth spurt might be brewing right now. We base our ideas about the world on our personal experience, and that experience has ingrained the rate of growth of the recent past in our heads as the way things happen. Were also limited by our imagination, which takes our experience and uses it to conjure future predictionsbut often, what we know simply doesnt give us the tools to think accurately about the future. (Kurzweil points out that his phone is about a millionth the size of, a millionth the price of, and a thousand times more powerful than his MIT computer was 40 years ago. Good luck trying to figure out where a comparable future advancement in computing would leave us, let alone one far, far more extreme, since the progress grows exponentially.) When we hear a prediction about the future that contradicts our experience-based notion ofhow things work, our instinct is that the prediction must be naive. If I tell you, later in this post, that you may live to be 150, or 250, or not die at all, your instinct will be, Thats stupidif theres one thing I know from history, its that everybody dies. And yes, no one in the past has not died. But no one flew airplanes before airplanes were invented either. So while 	nahhhhh might feel right as you read this post, its probably actually wrong. The fact is, if were being truly logical and expecting historical patterns to continue, we should conclude that much, much, much more should change in the coming decades than we intuitively expect. Logic also suggests that if the most advanced species on a planet keeps making larger and larger leaps forward at an ever-faster rate, at some point, theyll make a leap so great that it completely alters life as they know it and the perception they have of what it means to be a humankind of like how evolution kept making great leaps toward intelligence until finally it made such a large leap to the human being that it completely altered what it meant for any creature to live on planet Earth. And if you spend some time reading about whats going on today in science and technology, you start to see a lot of signs quietly hinting that life as we know it cannot withstand the leap thats coming next. If youre like me, you used to think Artificial Intelligence was a silly sci-fi concept, but lately youve been hearing it mentioned by serious people, and you dont really quite get it. There are three reasons a lot of people are confused about the term AI: 1) We associate AI with movies. Star Wars. Terminator. 2001: A Space Odyssey. Even the Jetsons. And those are fiction, as are the robot characters. So it makes AI sound a little fictional to us. 2) AI is a broad topic. It ranges from your phones calculator to self-driving cars to something in the future that might change the world dramatically. AI refers to all of these things, which is confusing. 3) We use AI all the time in our daily lives, but we often dont realize its AI. John McCarthy, who coined the term Artificial Intelligence in 1956, complained that as soon as it works, no one calls it AI anymore. Because of this phenomenon, AI often sounds like a mythical future prediction more than a reality. At the same time, it makes it sound like a pop concept from the past that never came to fruition.Ray Kurzweil says he hears people saythat AI withered in the 1980s, which he compares to insisting that the Internet died in the dot-com bust of the early 2000s. So lets clear things up. First, stop thinking of 	robots. A robot is a container for AI, sometimes mimicking the human form, sometimes notbut the AI itself is the computer inside the robot. AI is the brain, and the robot is its bodyif it even has a body. For example, the software and data behind Siri is AI, the womans voice we hear is a personification of that AI, and theres no robot involved at all. Secondly, youve probably heard the term singularity or technological singularity. This term has been used in math to describe an asymptote-like situation where normal rules no longer apply. Its been used in physics to describe a phenomenon like an infinitely small, dense black hole or the point we were all squished into right before the Big Bang. Again, situations where the usual rules dont apply. In 1993, Vernor Vinge wrote a famous essay in which he applied the term to the moment in the future when our technologys intelligence exceeds our owna moment for him when life as we know it will be forever changed and normal rules will no longer apply. Ray Kurzweil then muddled things a bit by defining the singularity as the time when the Law of Accelerating Returns has reached such an extreme pace that technological progress is happening at a seemingly-infinite pace, and after which well be living in a whole new world. I found that many of todays AI thinkers have stopped using the term, and its confusing anyway, so I wont use it much here (even though well be focusing on that 	idea throughout). Finally, while there are many different types or forms of AI since AI is a broad concept, the critical categories we need to think about are based on an AIs 	caliber. There are three major AI caliber categories: AI Caliber 1) Artificial Narrow Intelligence (ANI):Sometimes referred to as Weak AI, Artificial Narrow Intelligence is AI that specializes in one area. Theres AI that can beat the world chess champion in chess, but thats the only thing it does. Ask it to figure out a better way to store data on a hard drive, and itll look at you blankly. AI Caliber 2) Artificial General Intelligence (AGI):Sometimes referred to as Strong AI, or Human-Level AI, Artificial General Intelligence refers to a computer that is as smart as a human across the boarda machine that can perform any intellectual task that a human being can. Creating an AGI is a much harder task than creating an ANI, and were yet to do it. Professor Linda Gottfredson describes intelligence as a very general mental capability that, among other things, involves the ability to reason, plan, solve problems, think abstractly, comprehend complex ideas, learn quickly, and learn from experience. An AGI would be able to do all of those things as easily as you can. AI Caliber 3) Artificial Superintelligence (ASI): Oxford philosopher and leading AI thinker Nick Bostrom definessuperintelligence as an intellect that is much smarter than the best human brains in practically every field, including scientific creativity, general wisdom and social skills. Artificial Superintelligence ranges from a computer thats just a little smarter than a human to one thats trillions of times smarteracross the board. ASI is the reason the topic of AI is such a spicy meatball and why the words immortality and extinction will both appear in these posts multiple times. As of now, humans have conquered the lowest caliber of AIANIin many ways, and its everywhere. The AI Revolution is the road from ANI, through AGI, to ASIa road we may or may not survive but that, either way, will change everything. Lets take a close look at what the leading thinkers in the field believe this road looks like and why this revolution might happen way sooner than you might think: Artificial Narrow Intelligence is machine intelligence that equals or exceeds human intelligence or efficiency at a 	specific thing. A few examples: Cars are full of ANI systems, from the computer that figures out when the anti-lock brakes should kick in to the computer that tunes the parameters of the fuel injection systems. Googles self-driving car, which is being tested now, will contain robust ANI systems that allow it to perceive and react to the world around it. Your phone is a little ANI factory. When you navigate using your map app, receive tailored music recommendations from Pandora, check tomorrows weather, talk to Siri, or dozens of other everyday activities, youre using ANI. Your email spam filter is a classic type of ANIit starts off loaded with intelligence about how to figure out whats spam and whats not, and then it learns and tailors its intelligence to you as it gets experience with your particular preferences. The Nest Thermostat does the same thing as it starts to figure out your typical routine and act accordingly. You know the whole creepy thing that goes on when you search for a product on Amazon and then you see that as a recommended for you product on a different site, or when Facebook somehow knows who it makes sense for you to add as a friend? Thats a network of ANI systems, working together to inform each other about who you are and what you like and then using that information to decide what to show you. Same goes for Amazons People who bought this also bought thingthats an ANI whose job it is to gather info from the behavior of millions of customers and synthesize that info to cleverly upsell you so youll buy more things. Google Translate is another classic ANIimpressively good at one narrow task. Voice recognition is another, and there are a bunch of apps that use those two ANIs as a tag team, allowing you to speak a sentence in one language and have the phone spit out the same sentence in another. When your plane lands, its not a human that decides which gate it should go to. Just like its not a human that determined the price of your ticket. The worlds best Checkers, Chess, Scrabble, Backgammon, and Othello players are now all ANI systems. Google search is one large ANI brain with incredibly sophisticated methods for ranking pages and figuring out what to show you in particular. Same goes for Facebooks Newsfeed. And those are just in the consumer world. Sophisticated ANI systems are widely used in sectors and industries like military, manufacturing, and finance (algorithmic high-frequency AI traders account for more than half of equity sharestraded on US markets), and in expert systems like those that help doctors make diagnoses and, most famously, IBMs Watson, who contained enough facts and understood coy Trebek-speak well enough to soundly beat the most prolific Jeopardy champions. ANI systems as they are now arent especially scary. At worst, a glitchy or badly-programmed ANI can cause an isolated catastrophe like knocking out a power grid, causing a harmful nuclear power plant malfunction, or triggering a financial markets disaster (like the 2010 Flash Crash when an ANI reacted the wrong way to an unexpected situation and caused the stock market to briefly plummet, taking $1 trillion of market value with it, only part of which was recovered when the mistake was corrected). But while ANI doesnt have the capability to cause an 	existential threat, we should see this increasingly large and complex ecosystem of relatively-harmless ANI as a precursor of the world-altering hurricane thats on the way. Each new ANI innovation quietly adds another brick onto the road to AGI and ASI. Or as Aaron Saenz sees it, our worlds ANI systems are like the amino acids in the early Earths primordial oozethe inanimate stuff of life that, one unexpected day, woke up. Nothing will make you appreciate human intelligence like learning about how unbelievably challenging it is to try to create a computer as smart as we are. Building skyscrapers, putting humans in space, figuring out the details of how the Big Bang went downall far easier than understanding our own brain or how to make something as cool as it. As of now, the human brain is the most complex object in the known universe. What you quickly realize when you think about this is that those things that seem easy to us are actually unbelievably complicated, and they only seem easy because those skills have been optimized in us (and most animals) by hundreds of million years of animal evolution. When you reach your hand up toward an object, the muscles, tendons, and bones in your shoulder, elbow, and wrist instantly perform a long series of physics operations, in conjunction with your eyes, to allow you to move your hand in a straight line through three dimensions. It seems effortless to you because you have perfected software in your brain for doing it. Same idea goes for why its not that malware is dumb for not being able to figure out the slanty word recognition test when you sign up for a new account on a siteits that your brain is super impressive for being 	able to. On the other hand, multiplying big numbers or playing chess are new activities for biological creatures and we havent had any time to evolve a proficiency at them, so a computer doesnt need to work too hard to beat us. Think about itwhich would you rather do, build a program that could multiply big numbers or one that could understand the essence of a B well enough that it you could show it a B in any one of thousands of unpredictable fonts or handwriting and it could instantly know it was a B? One fun examplewhen you look at this, you and a computer both can figure out that its a rectangle with two distinct shades, alternating: you have no problem giving a full description of the various opaque and translucent cylinders, slats, and 3-D corners, but the computer would fail miserably. It would describe what it seesa variety of two-dimensional shapes in several different shadeswhich is actually whats there. Your brain is doing a ton of fancy shit to interpret the implied depth, shade-mixing, and room lighting the picture is trying to portray. And looking at the picture below, a computer sees a two-dimensional white, black, and gray collage, while you easily see what it really isa photo of an entirely-black, 3-D rock: And everything we just mentioned is still only taking in stagnant information and processing it. To be human-level intelligent, a computer would have to understand things like the difference between subtle facial expressions, the distinction between being pleased, relieved, content, satisfied, and glad, and why 	Braveheartwas great but The Patriot was terrible. Daunting. So how do we get there? One thing that definitely needs to happen for AGI to be a possibility is an increase in the power of computer hardware. If an AI system is going to be as intelligent as the brain, itll need to equal the brains raw computing capacity. One way to express this capacity is in the total calculations per second (cps) the brain could manage, and you could come to this number by figuring out the maximum cps of each structure in the brain and then adding them all together. Ray Kurzweil came up with a shortcut by taking someones professional estimate for the cps of one structure and that structures weight compared to that of the whole brain and then multiplying proportionally to get an estimate for the total. Sounds a little iffy, but he did this a bunch of times with various professional estimates of different regions, and the total always arrived in the same ballparkaround 1016, or 10 quadrillion cps. Currently, the worlds fastest supercomputer, Chinas Tianhe-2, has actually beaten that number, clocking in at about 34 quadrillion cps. But Tianhe-2 is also a dick, taking up 720 square meters of space, using 24 megawatts of power (the brain runs on just 20 watts), and costing $390 million to build. Not especially applicable to wide usage, or even most commercial or industrial usage yet. Kurzweil suggests that we think about the state of computers by looking at how many cps you can buy for $1,000. When that number reaches human-level10 quadrillion cpsthen thatll mean AGI could become a very real part of life. Moores Law is a historically-reliable rule that the worlds maximum computing power doubles approximately every two years, meaning computer hardware advancement, like general human advancement through history, grows exponentially. Looking at how this relates to Kurzweils cps/$1,000 metric, were currently at about 10 trillion cps/$1,000, right on pacewith this graphs predicted trajectory: So the worlds $1,000 computers are now beating the mouse brain and theyre at about a thousandth of human level. This doesnt sound like much until you remember that we were at about a trillionth of human level in 1985, a billionth in 1995, and a millionth in 2005. Being at a thousandth in 2015 puts us right on pace to get to an affordable computer by 2025 that rivals the power of the brain. So on the hardware side, the raw power needed for AGI is technically available now, in China, and well be ready for affordable, widespread AGI-caliber hardware within 10 years. But raw computational power alone doesnt make a computer generally intelligentthe next question is, how do we bring human-level intelligence to all that power? This is the icky part. The truth is, no one really knows how to make it smartwere still debating how to make a computer human-level intelligent and capable of knowing what a dog and a weird-written B and a mediocre movie is. But there are a bunch of far-fetched strategies out there and at some point, one of them will work. Here are the three most common strategies I came across: This is like scientists toiling over how that kid who sits next to them in class is so smart and keeps doing so well on the tests, and even though they keep studying diligently, they cant do nearly as well as that kid, and then they finally decide k fuck it Im just gonna copy that kids answers. It makes sensewere stumped trying to build a super-complex computer, and there happens to be a perfect prototype for one in each of our heads. The science world is working hard on reverse engineering the brain to figure out how evolution made such a rad thing optimistic estimates say we can do this by 2030. Once we do that, well know all the secrets of how the brain runs so powerfully and efficiently and we can draw inspiration from it and steal its innovations. One example of computer architecture that mimics the brain is the artificial neural network. It starts out as a network of transistor neurons, connected to each other with inputs and outputs, and it knows nothinglike an infant brain. The way it learns is it tries to do a task, say handwriting recognition, and at first, its neural firings and subsequent guesses at deciphering each letter will be completely random. But when its told it got something right, the transistor connections in the firing pathways that happened to create that answer are strengthened; when its told it was wrong, those pathways connections are weakened. After a lot of this trial and feedback, the network has, by itself, formed smart neural pathways and the machine has become optimized for the task. The brain learns a bit like this but in a more sophisticated way, and as we continue to study the brain, were discovering ingenious new ways to take advantage of neural circuitry. More extreme plagiarism involves a strategy called whole brain emulation, where the goal is to slice a real brain into thin layers, scan each one, use software to assemble an accurate reconstructed 3-D model, and then implement the model on a powerful computer. Wed then have a computer officially capable of everything the brain is capable ofit would just need to learn and gather information. If engineers get 	really good, theyd be able to emulate a real brain with such exact accuracy that the brains full personality and memory would be intact once the brain architecture has been uploaded to a computer. If the brain belonged to Jim right before he passed away, the computer would now wake up as Jim (?), which would be a robust human-level AGI, and we could now work on turning Jim into an unimaginably smart ASI, which hed probably be really excited about. How far are we from achieving whole brain emulation? Well so far, weve not yet just recently been able to emulate a 1mm-long flatworm brain, which consists of just 302 total neurons. The human brain contains 100 billion. If that makes it seem like a hopeless project, remember the power of exponential progressnow that weve conquered the tiny worm brain, an ant might happen before too long, followed by a mouse, and suddenly this will seem much more plausible. So if we decide the smart kids test is too hard to copy, we can try to copy the way he 	studies for the tests instead. Heres something we know. Building a computer as powerful as the brain 	is possibleour own brains evolution is proof. And if the brain is just too complex for us to emulate, we could try to emulate evolution instead. The fact is, even if we can emulate a brain, that might be like trying to build an airplane by copying a birds wing-flapping motionsoften, machines are best designed using a fresh, machine-oriented approach, not by mimicking biology exactly. So how can we simulate evolution to build an AGI? The method, called genetic algorithms, would work something like this: there would be a performance-and-evaluation process that would happen again and again (the same way biological creatures perform by living life and are evaluated by whether they manage to reproduce or not). A group of computers would try to do tasks, and the most successful ones would be 	bred with each other by having half of each of their programming merged together into a new computer. The less successful ones would be eliminated. Over many, many iterations, this natural selection process would produce better and better computers. The challenge would be creating an automated evaluation and breeding cycle so this evolution process could run on its own. The downside of copying evolution is that evolution likes to take a billion years to do things and we want to do this in a few decades. But we have a lot of advantages over evolution. First, evolution has no foresight and works randomlyit produces more unhelpful mutations than helpful ones, but we would control the process so it would only be driven by beneficial glitches and targeted tweaks. Secondly, evolution doesnt 	aim for anything, including intelligencesometimes an environment might even select against higher intelligence (since it uses a lot of energy). We, on the other hand, could specifically direct this evolutionary process toward increasing intelligence. Third, to select for intelligence, evolution has to innovate in a bunch of other ways to facilitate intelligencelike revamping the ways cells produce energywhen we can remove those extra burdens and use things like electricity. Its no doubt wed be much, much faster than evolutionbut its still not clear whether well be able to improve upon evolution enough to make this a viable strategy. This is when scientists get desperate and try to program the test to take itself. But it might be the most promising method we have. The idea is that wed build a computer whose two major skills would be doing research on AI and coding changes into itselfallowing it to not only learn but to improve its own 	architecture. Wed teach computers to be computer scientists so they could bootstrap their own development. And that would be their main jobfiguring out how to make themselves smarter. More on this later. Rapid advancements in hardware and innovative experimentation with software are happening simultaneously, and AGI could creep up on us quickly and unexpectedly for two main reasons: 1) Exponential growth is intense and what seems like a snails pace of advancement can quickly race upwards. This GIF illustrates the concept nicely. 2) When it comes to software, progress can seem slow, but then one epiphany can instantly change the rate of advancement (kind of like the way science, during the time humans thought the universe was geocentric, was having difficulty calculating how the universe worked, but then the discovery that it was heliocentric suddenly made everything 	much easier). Or, when it comes to something like a computer that improves itself, we might seem far away but actually be just one tweak of the system away from having it become 1,000 times more effective and zooming upward to human-level intelligence. At some point, well have achieved AGIcomputers with human-level general intelligence. Just a bunch of people and computers living together in equality. Oh actually not at all. The thing is, an AGI with an identical level of intelligence and computational capacity as a human would still have significant advantages over humans. Like: Speed. The brains neurons max out at around 200 Hz, while todays microprocessors (which are much slower than they will be when we reach AGI) run at 2 GHz, or 10 million times faster than our neurons. And the brains internal communications, which can move at about 120 m/s, are horribly outmatched by a computers ability to communicate optically at the speed of light. Size and storage. The brain is locked into its size by the shape of our skulls, and it couldnt get much bigger anyway, or the 120 m/s internal communications would take too long to get from one brain structure to another. Computers can expand to any physical size, allowing far more hardware to be put to work, a much larger working memory (RAM), and a longterm memory (hard drive storage) that has both far greater capacity and precision than our own. Reliability and durability. Its not only the memories of a computer that would be more precise. Computer transistors are more accurate than biological neurons, and theyre less likely to deteriorate (and can be repaired or replaced if they do). Human brains also get fatigued easily, while computers can run nonstop, at peak performance, 24/7. Editability, upgradability, and a wider breadth of possibility. Unlike the human brain, computer software can receive updates and fixes and can be easily experimented on. The upgrades could also span to areas where human brains are weak. Human vision software is superbly advanced, while its complex engineering capability is pretty low-grade. Computers could match the human on vision software but could also become equally optimized in engineering and any other area. Collective capability. Humans crush all other species at building a vast collective intelligence. Beginning with the development of language and the forming of large, dense communities, advancing through the inventions of writing and printing, and now intensified through tools like the internet, humanitys collective intelligence is one of the major reasons weve been able to get so far ahead of all other species. And computers will be way better at it than we are. A worldwide network of AI running a particular program could regularly sync with itself so that anything any one computer learned would be instantly uploaded to all other computers. The group could also take on one goal as a unit, because there wouldnt necessarily be dissenting opinions and motivations and self-interest, like we have within the human population. AI, which will likely get to AGI by being programmed to self-improve, wouldnt see human-level intelligence as some important milestoneits only a relevant marker from our point of viewand wouldnt have any reason to stop at our level. And given the advantages over us that even human intelligence-equivalent AGI would have, its pretty obvious that it would only hit human intelligence for a brief instant before racing onwards to the realm of superior-to-human intelligence. This may shock the shit out of us when it happens. The reason is that from 	our perspective, A) while the intelligence of different kinds of animals varies, the main characteristic were aware of about any animals intelligence is that its far lower than ours, and B) we view the smartest humans as WAY smarter than the dumbest humans. Kind of like this: So as AI zooms upward in intelligence toward us, well see it as simply becoming smarter, 	for an animal. Then, when it hits the lowest capacity of humanityNick Bostrom uses the term the village idiotwell be like, Oh wow, its like a dumb human. Cute! The only thing is, in the grand spectrum of intelligence, all humans, from the village idiot to Einstein, are within a very small rangeso just after hitting village idiot-level and being declared an AGI, itll suddenly be smarter than Einstein and we wont know what hit us: And what happensafter that? I hope you enjoyed normal time, because this is when this topic gets unnormal and scary, and its gonna stay that way from here forward. I want to pause here to remind you that every single thing Im going to say is realreal science and real forecasts of the future from a large array of the most respected thinkers and scientists. Just keep remembering that. Anyway, as I said above, most of our current models for getting to AGI involve the AI getting there by self-improvement. And once it gets to AGI, even systems that formed and grew through methods that didnt involve self-improvement would now be smart enough to begin self-improving if they wanted to. ( 	Much more on what it means for a computer to want to do something in the Part 2 post.) An AI system at a certain levellets say human village idiotis programmed with the goal of improving its own intelligence. Once it does, its 	smartermaybe at this point its at Einsteins levelso now when it works to improve its intelligence, with an Einstein-level intellect, it has an easier time and it can make bigger leaps. These leaps make it much smarter than any human, allowing it to make even bigger leaps. As the leaps grow larger and happen more rapidly, the AGI soars upwards in intelligence and soon reaches the superintelligent level of an ASI system. This is called an Intelligence Explosion (This term was first used by one of historys great AI thinkers, Irving John Good, in 1965), and its the ultimate example of The Law of Accelerating Returns. There is some debate about how soon AI will reach human-level general intelligencethe median year on a survey of hundreds of scientists about when they believed wed be more likely than not to have reached AGI was 2040thats only 25 years from now, which doesnt sound that huge until you consider that many of the thinkers in this field think its likely that the progression from AGI to ASI happens very quickly. Likethis could happen: It takes decades for the first AI system to reach low-level general intelligence, but it finally happens. A computer is able to understand the world around it as well as a human four-year-old. Suddenly, within an hour of hitting that milestone, the system pumps out the grand theory of physics that unifies general relativity and quantum mechanics, something no human has been able to definitively do. 90 minutes after that, the AI has become an ASI, 170,000 times more intelligent than a human. Superintelligence of that magnitude is not something we can remotely grasp, any more than a bumblebee can wrap its head around Keynesian Economics. In our world, smart means a 130 IQ and stupid means an 85 IQwe dont have a word for an IQ of 12,952. What we do know is that humans utter dominance on this Earth suggests a clear rule: 	with intelligence comes power. Which means an ASI, when we create it, will be the most powerful being in the history of life on Earth, and all living things, including humans, will be entirely at its whimand this might happen in the next few decades. If our meager brains were able to invent wifi, then something 100 or 1,000 or 1 billion times smarter than we are should have no problem controlling the positioning of each and every atom in the world in any way it likes, at any timeeverything we consider magic, every power we imagine a supreme God to have will be as mundane an activity for the ASI as flipping on a light switch is for us. Creating the technology to reverse human aging, curing disease and hunger and even mortality, reprogramming the weather to protect the future of life on Earthall suddenly possible. Also possible is the immediate end of all life on Earth. As far as were concerned, if an ASI comes to being, there is now an omnipotent God on Earthand the all-important question for us is: Thats the topic of Part 2 of this post. If you liked this article, subscribe by email to have Wait But Whys once-a-week posts sent to you by email. Never any spam. Related Posts: What Makes You You? This article has been republished with permission fromWaitButWhy.com, where it can be seen here. Follow Wait But Why on Facebook. Follow Wait But Why on Twitter."
532,https://gizmodo.com/when-superintelligent-ai-arrives-will-religions-try-t-1682837922,Gizmodo,2015,2,4,1581.0," Like it or not, we are nearing the age of humans creating autonomous, self-aware super intelligences. Those intelligences will be part of our culture, and we will inevitably try to control AI and teach it our ways, for better or worse. AI with intelligence equal to or beyond human beings is often referred to as strong AI or Artificial General Intelligence (AGI). Experts disagree as to when such an intelligence will arrive into the world, but many are betting it will happen sometime in the next two decades. The idea of a thinking machine being able to rival our own intellectin fact, one that could quickly become far smarter than usis both a reason for serious concern and a reason to cheer about what scientific advances it might teach us. Those worries and benefits have not escaped religious. Some faith-bound Americans want to make sure any superintelligence we create knows about God. And if you think the idea of preaching God to autonomous machines sounds crazy, you may be overlooking key statistics of U.S. demographics: roughly 75percent of adult Americans identify themselves as some denomination of Christianity. In the U.S. Congress, 92 percent of our highest politicians belong to a Christian faith. As artificial intelligence advances, religious questions and concerns globally are bound to come up, and theyre starting too: Some theologians and futurists are already considering whether AI can also know God. I dont see Christs redemption limited to human beings, Reverend Dr. Christopher J. Benek told me in a recent interview. Benek is an Associate Pastor of Providence Presbyterian Church in Florida and holds masters degrees in divinity and theology from Princeton University. Its redemption to all of creation, even AI, he said. If AI is autonomous, then we have should encourage it to participate in Christs redemptive purposes in the world. One of the key mandates of Christianity is to spread the Gospel and get nonbelievers to accept that Jesus Christ died for the worlds sins. Whether AI has any sins, or whether it can and should be saved at all may end up being a bizarre but important question believers face in the 21st century. Even Pope Francis recently sounded off on the possibility of aliens being converted when he affirmed that the Holy Spirit blows where it will. The metaphysical questions surrounding faith and AI are like tumbling down Alices rabbit hole. Does AI have a soul? Can it be saved? There is one school of thought that figures, if humans can be forgiven for our sins, why not superintelligences with human qualities? The real question is whether humans are able to be savedif so, then there is no reason why thinking and feeling AIs shouldnt be able to be saved. Once human-like AI exist, they will be persons just like us, futurist Giulio Prisco, founder of the transhumanist Turing Church, told me in an email. But there is an opposing school of thought that insists that AI is a machine and therefore doesnt have a soul. In Think Christian, scientist and Christian scribe Dr. Jason E. Summers writes, Christians often reject Strong AI on the theological ground of the special anthropological status of human beings as the bearers of Imago Dei. Imago Dei is Latin for the Christian concept that humans were created in the image of God. The worlds major Abrahamic religionsJudaism, Christianity, and Islamall believe in the soul, which is what many major religious texts say is the thing that separates us from other life on the Earth, including other mammals. Because the Abrahamic religions comprise the faiths of roughly two-thirds the worlds population, the question of soul is quintessential in the coming transhumanist age of machine intelligence. Getting even deeper into this theoretical debate is the question of whether strong AI would even accept our religion. Its only fair to let AI have access to the teachings of all the worlds religions. Then they can choose what they want to believe, said Prisco. But I think its highly unlikely that superhuman AI would choose to believe in the petty, provincial aspects of traditional religions. At the same time, I think they would be interested in enlightened spirituality and religious cosmology, or eschatology, and develop their own versions. Once you start thinking like that, it opens up even more questions: How would AI fit into to the religious tension already present around the world? Who is to say a machine with human intelligence wouldnt choose to become a fundamentalist Muslim, or a Jehova Witness, or a born-again Christian who prefers to speak in tongues instead of a form of communication we understand? If it decides to literally follow any of the sacred religions texts verbatim, as some humans attempt to do, then it could add to already existing religious tensions in the world. Christian theologian James MaGrath, the Clarence L. Goodwin Chair in New Testament Language and Literature at Butler University, writes about androids who posses super intelligence in an essay titled Robots, Rights, and Religion: In all likelihood, if androids were inclined to be extremely liberal, they would quickly discover the selectivity of fundamentalisms self-proclaimed liberalism and reject it, although the possibility that they might then go on to seek to enforce all the Biblical legislation in every details should indeed worry us. The idea of teaching anything to an intelligence that could rather quickly be far smarter than humans is contradictory. Another possibility is that AI will teach us new things about spirituality that we never considered or understood. It may tell us how the cosmos were created, or whether we exist in some simulation theory, or even that there are many AIs before itones that are much more sophisticated than itself. Whatever happens, the creation of AI will likely spawn a paradigm shift for human civilization. Rather than converting it, we might just want to stand back and listen. Benek agrees with this, but through his own metaphysical lenses. He says, The Holy Spirit can work though AI; it can work through anything. There may be churches set up to deal and promote religious AI in the future. AI can help spread the word of God. In fact, AI might help us understand God better. If Benek is right, America might be a nation filled with robot pastors and AI spiritual gurus in the future. Decades or centuries from now, spirituality may be taught to us through machines, just like science will likely be. Prisco takes it a step further: How smart must machines be to understand the so-called mind of God? 5,000 times smarter than humans? A million times smarter? I dont know, but in a hundred years a machine intelligence may have a far better chance of finding that out than the human brain with its limited capacity. The Turing Church started as a working group at the intersection of science and religion, and recently became an online, open-source church built around Cosmist principles of space expansion, unlimited growth, and universal love. Cosmism doesnt care if youre viewing the universe as information or quantum information or hypercomputation or God stuff or whatever, writes cyberculture personality R.U. Sirius in his recent book Transcendence. Nor does it ask anyone to commit to AGI [artificial general intelligence] or mind uploading or brain-computer interfaces or fusion-powered toasters as the best way forward. Rather, it seeks to infuse the human universe with an attitude of joy, growth, choice, and open-mindedness. Cosmism believes that science in its current form, just like religion and philosophy in their current forms, may turn out to be overly limited for the task of understanding life, mind, society, and reality. Despite the seemingly scifi nature of it, uploading the human mind into an AI being could arguably solve the soul question. Experts like Google engineer Ray Kurzweil are actively researching ways to upload the brain into computers, and last year there was significant progress in the field via brainwave headsets and telepathy. Renowned technology entrepreneur and author of Virtually Human:The Promise  and the Peril  of Digital Immortality Martine Rothblatt theorizes that anything that values life or God, or even has the potential to value life or God, has some kind of soul. A whole chapter of the book explores religion in the time of AI, uploads, and mindclonessoftware versions of human minds. Rothblatt thinks mindclones are in line with classical interpretations of religion and will be welcomed that way in the future. Rothblatt founded Terasem, a scientific transreligion similar to the Turing Church in scope and approach, which runs preliminary mindcloning pilot projects. The most famous one is Bina 48, a robotic head that contains a mindclone of Rothblatts still-living wife Bina. And perhaps thats the only way AI should be launchedwith people uploaded into it. What fascinates me most about this is the question of who might be the first person uploaded. Do we send a scientist? A programmer? Or a religious person? All at once? If this is starting to sound like the movie Contact, where a U.S. Presidential commissioned advisory board decides to send a believerand not the more qualified atheistto meet some type of alien intelligence for the first time, youre exactly right. Its like an international team of experts on a mission to find its way to the outer reaches of machine intelligence. That way whatever happens, at the very least, we know humanity is aptly represented. Including spiritually. Zoltan Istvan is the author of The Transhumanist Wager and founder of the Transhumanist Party."
533,https://gizmodo.com/how-ai-could-ruin-humanity-according-to-smart-humans-1679025876,Gizmodo,2015,1,12,962.0," For the past 24 hours, scientists have been lining up to sign this open letter. Put simply, the proposal urges that humanity dedicate a portion of its AI research to aligning with human interests. In other words, lets try to avoid creating our own, mechanized Horsemen of the Apocalypse. While some scientists might roll their eyes at any mention of a Singularity, plenty of experts and technologistslike, say, Stephen Hawking and Elon Muskhave warned of the dangers AI could pose to our future. But while they might urge us to pursue our AI-related studies with caution, theyre a bit less clear on what exactly it is were being cautious against. Thankfully, others have happily filled in those gaps. Here are five of the more menacing destruction-by-singularity prophecies our brightest minds have warned against. According to Stuart Armstrong, a philosopher and Research Fellow at the Future of Humanity Institute at Oxford: The first impact of [Artificial Intelligence] technology is near total unemployment. You could take an AI if it was of human-level intelligence, copy it a hundred times, train it in a hundred different professions, copy those a hundred times and you have ten thousand high-level employees in a hundred professions, trained out maybe in the course of a week. Or you could copy it more and have millions of employees And if they were truly superhuman youd get performance beyond what Ive just described. Daniel Dewey, a research fellow at the Future of Humanity Institute, builds on Armstrongs train of thought  in Aeon Magazine. After all, when and if humans do become obsolete, well become little more than pebbles in a robots metaphorical shoes. The difference in intelligence between humans and chimpanzees is tiny, [Armstrong] said. But in that difference lies the contrast between 7 billion inhabitants and a permanent place on the endangered species list. That tells us its possible for a relatively small intelligence advantage to quickly compound and become decisive. . The basic problem is that the strong realization of most motivations is incompatible with human existence, Dewey told me. An AI might want to do certain things with matter in order to achieve a goal, things like building giant computers, or other large-scale engineering projects. Those things might involve intermediary steps, like tearing apart the Earth to make huge solar panels. A superintelligence might not take our interests into consideration in those situations, just like we dont take root systems or ant colonies into account when we go to construct a building. You could give it a benevolent goal  something cuddly and utilitarian, like maximizing human happiness. But an AI might think that human happiness is a biochemical phenomenon. It might think that flooding your bloodstream with non-lethal doses of heroin is the best way to maximize your happiness. AI doesnt need the explicit intent of exterminating us to be scary. As Mark Bishop, professor of cognitive computing at Goldsmiths, University of London, told The Independent: I am particularly concerned by the potential military deployment of robotic weapons systems  systems that can take a decision to militarily engage without human intervention  precisely because current AI is not very good and can all too easily force situations to escalate with potentially terrifying consequences, Professor Bishop said. So it is easy to concur that AI may pose a very real existential threat to humanity without having to imagine that it will ever reach the level of superhuman intelligence, he said.We should be worried about AI, but for the opposite reasons given by Professor Hawking, he explained. Or maybe well see the end coming long before it makes its way over. Except that by then, well be too incompetent to survive even attempting to shut it down. Bill Joy, cofounder and Chief Scientist of Sun Microsystems, writes in Wired: What we do suggest is that the human race might easily permit itself to drift into a position of such dependence on the machines that it would have no practical choice but to accept all of the machines decisions. As society and the problems that face it become more and more complex and machines become more and more intelligent, people will let machines make more of their decisions for them, simply because machine-made decisions will bring better results than man-made ones. Eventually a stage may be reached at which the decisions necessary to keep the system running will be so complex that human beings will be incapable of making them intelligently. At that stage the machines will be in effective control. People wont be able to just turn the machines off, because they will be so dependent on them that turning them off would amount to suicide. Theres something called the grey goo scenario, which essentially postulates that if robots start perpetually reproducing, well essentially just get squeezed out amidst the massive mecha expansion. And if they need humans to power their out-of-control massesas Discovery points out, were screwed. If nanotechnology machines  which can be a hundred thousand times smaller than the diameter of a human hair  figure out how to spontaneously replicate themselves, it would naturally have dire consequences for humanity [source:Levin]. Especially if the research funded by the U.S. Defense Department gets out of control: Researchers there are attempting to create an Energetically Autonomous Tactical Robot (EATR) that would fuel itself by consuming battlefield debris, which could include human corpses [source: Lewinski]. If nanotechnology did develop an appetite for human flesh  or some of the other things we rely on for survival, like forests or machinery  it could decimate everything on the planet in a matter of days. These hungry mini-robots would relegate our blue and green home to grey goo, a term that describes the unidentifiable particles left behind after the nanocritters eat buildings, landscapes and, well, everything else."
534,https://gizmodo.com/can-you-beat-this-virtually-unbeatable-poker-algorithm-1678366320,Gizmodo,2015,1,8,306.0," The challenge is on. Computer scientists say theyve created an algorithm that has essentially solved a version of Texas hold em, and its guaranteed to beat every single puny human competitor in the long run. Dont believe it? Why, you can play against the program yourself. Cepheus, as this poker-playing program is called, plays a virtually perfect game of heads-up limit holdem. The variant is like the popular Texas hold em, except there are only two players and a fixed number of bet sizes and raises. That still leaves 3.16  1017 states in the game. The sheer feat of a program thats essentially solved a type of poker has computer scientists in a tizzy. Unlike other games that have been completely solved such as checkers or Connect Four, where every past action is laid clear on the game board, poker is a game with imperfect informationnamely, you dont know your opponents cards. Cepheus is able to learn from its past mistakes, and it achieved its current high status by playing billion and billions of hands against itself. It can even bluff. For game theorists, Cepheus could hold the key to modeling real-life situations like negotiations or auctions where information is imperfect. But hey, you want to know if you can beat it poker. It is of course possible to beat Cepheus in individual games, as poker involves an element of chance with each dealt hand. But in the long run, the computer will almost always come out ahead. There is a tiny margin of error, its creators say, but it is so small as to be practically negligible in our human lifetimes. So without further ado, say hello to Cepheus. Update: Apparently so many of you want to play against Cepheus, youve taken down the site. Its been in and outtry checking back in a bit."
535,https://gizmodo.com/how-do-robots-learn-to-cook-youtube-obviously-1677357241,Gizmodo,2015,1,4,261.0," Scientists have been toying around with something called deep learning (a massive, internet-based robot brain) for a while now. But a newly published paper details the latest advancement in the robotic processing system: Learning how to cook simply from watching YouTube videos. Robotstheyre just like us! Just like with Robo Brain before it, researchers at the University of Maryland and the Australian research center NICTA sucked up data straight from the internet for their AI friend, this time using 88 different YouTube videos of people cooking. And the fact that they were YouTube videos alone is actually part of what makes this so impressive. According to the study: The [YouTube vides] represent a significantly more challenging visual problem than existing cooking and kitchen datasets (the background kitchen/scene is different for many and most videos have dynamic camera changes). In other words, theres a lot more variation in data for the robot to work with. Still, despite this potential hiccup, the teams robot was able to recognize individual objects, the various ways in which they were held by the human hand, and predict the most likely next move. So not only could it analyze and break down the videos, but i used the information to learn how to handle different tools, too. In this way, robots could be able to theoretically teach itself and enrich already learned skills in the future simply from watching not only YouTube videos, but any other information the internet has to offer. A word of advice to the robots: Dont read the comments. [UMD via Venture Beat]"
536,https://gizmodo.com/netflix-style-booze-recommendation-works-surprisingly-w-1663819670,Gizmodo,2014,11,28,1453.0," We all know that desperate feeling of standing alone, motionless, and confused in a bottle shop. You need to take a decent bottle to a dinner party but have no idea what to get. Wouldnt it be magical if a could just read the labels and tell you whats good? Well, yes it would be, and thanks to a new app called Next Glass it is. Ill be completely honest with you. When I first heard about Next Glass (iOS, Android), it sounded like bullshit. The sounded a little bit like the Netflix and its promise to tell you which movies youll like. Indeed, the Next Glass recommendation engine does work sort of like Netflixs. The app invites you to rate a seemingly endless supply of beer and wine selections on a four-star scale. Those ratings inform your taste profile. Armed with this metric, you can then walk into any bottle shop, use your phones camera to scan the label on a beer or wine bottle, and Next Glass will tell you how likely you are to like it with a fancy augmented reality experience. I was skeptical, but I spent a week using Next Glass to pick what kind of beer I would buy. It went surprisingly well! Next Glass is a new app that aims to tell you how much youre going to like a particular kind of beer or wine. Kurt Taylor, founder and CEO of Next Glass, told me the idea came from eating out with friends, suffering through one of those novel-sized wine lists, and ending up drinking something terrible. It wasnt so much that it was a terrible wine. (The waiter had recommended it because it was his favorite.) That particular wine just wasnt what Kurt and his friends wanted. Enter science and technology. Taylor decided to chase the demand for a smartphone-driven booze-recommendation engine like any good American entrepreneur would: with algorithms. In order to build the app, Taylor and his team traveled around the country collecting samples of beer and wine from as many local breweries and wineries so that they could analyze the chemical make up of the booze in a lab. (They shipped in lots of products from overseas, too.) Those chemical profiles were then linked together not only based on flavor but also attributes like region of origin and even the type of bottle or cork involved. Think about it a bit like Netflix or Pandora. Like others do with movies and music recommendations, Next Glass does with booze recommendations. The app collects and analyzes data about the bottles you like, then uses that analysis to predict how much youll like a new beer or wine. The app also has some built in social features that let you see your friends scores so that when you are on the way to a dinner party, you could see how your pick would go over. Now, you and I both know that Netflix and Pandora are not always right. So you can see how I was skeptical when I first tried out the app. The first part is easy. When you open up the app to the Explore tab, youre shown a long list of beer or winetheres no liquor yet. When you tap through, you see the bottles rating and have the opportunity to rate it up to four stars. You can also share the bottle with friends and find similar brews. For the sake of simplicity, I focused on using the app for beer. Ive used beer apps before that do anything from an app that works like a Foursquare for beer to an app that works kind of like a Yelp for beer. I cant say any of them ever made my drinking experience better. But if Next Glass could use a little bit of artificial intelligence to tell me how much Id like a beer I was about to buy, I could get into that. So I set up a pseudo-scientific experiment. Over the course of several days, I let Next Glass tell me what to buy. I scanned bottles at the a fancy beer store around the corner from my apartment and the Whole Foods beer store by my office, and I ultimately bought 14 beers I hadnt tried before. Well, one of them I had tried beforeits actually my favorite beerand I thought it would work well as a control in the experiment. (Theres not really any scientific value in that thinking, but this beer is kind of hard for me to find, so I just really wanted it.) One was also a cider because Next Glass kept suggesting ciders. The scanning part of the process wasnt bad. It was frankly a little funny to be snapping cell phone pictures in the middle of the beer store, but the scores seemed sort of informative. That said, I had no idea what the scores meant. Theyre just numbers, like say, 94.2. My plan was to buy anything that scanned at 80.0 or better. I must have an eye for labels because every beer except one scanned higher than my threshold. One scanned with a score of 66.4, but I bought it anyways just to see if it really tasted worse than the others. (It did.) The drinking part of the process was fun. Im a big fan of variety packs, since its fun to try new things. But the downfall of the variety pack is that you often end up with one or two bum beers. Goofy as it felt at the time, scanning these beers with a smartphone app made me feel like Id finally built the bum-free variety pack. Every one of these beers tasted great! (Except that 66.4 beerwhich still wasnt bad, to be completely honest.) And I probably wouldnt have bought them without Next Glass. Did Next Scan lead me to a better drinking experience? Well, yes and no. The app did inspire me into trying new things. And since I had a AI-powered, beer-rating wizard in my pocket, I was confident enough to get adventurous about it. Going to fancy beer stores also sort of made adventurous the only option. At first, it seemed easy. I tend to be an IPA drinker, so I tend to gravitate in that direction when I go to buy beer. After Id found a handful of IPAs (Ballast Points Sculpin IPA, Parallel 49s Filthy Dirty IPA, Founders Mosaic Promise and Heavy Seas Loose Cannon), I mixed things up with a pale ale (Heavy Seas Powder Monkey) and a rye (Bear Republic Hop Rod Rye). Then, I realized that Id need to go for beers I wouldnt normally drink. So I picked up a couple of stouts (Bells Kalamazoo Stout and Allagash Black) as well as a saison (Brooklyn 1/2 Ale) and a Belgian White (Dogfish Head Namaste). I grabbed two brown ales (Dark Horse Boffo Brown Ale and The Sheds Mountain Ale). Finally, I threw in a wheat ale (Blue Star Great American Wheat Beer) and a cider (Ace Apple Cider). They all tasted great. Then again, if you click through those links to the reviews in Beer Advocate, youll see that all of those beers (and the cider) are all very highly rated. Fancy beer stores tend to carry fancy beers. So its natural that I liked the fancy beers from the fancy beer stores. The trickiest thing about using Next Glass wasnt necessarily finding out if Id think a beer tasted good. It was finding out how many good-looking beers there are out there. Unfortunately, Ive never any of these beers in stores, which Next Glass says is an issue it soon plans on addressing. The company will seen be adding geolocation-based features and plans to work with local liquor stores on inventory-management so that the beers that people want to try will eventually show up in stores near them. Right now, the tangle of distributors and massive beer complicates how craft breweries reach customers. (Just watch Beer Wars.) Next Glass also aims to improve how the industry works. At the end of the day, beer and wine can be expensive. Does that mean you need an app to tell you what to buy? Not really. Does it mean that having a little bit of extra data might make you feel better about your purchases? Sure, why not. When you go shopping for good beer or wine, you often find it. Either you listen to the shop clerks recommendations or you do your research or you just go prepared to spend more. Next Glass another tool for your booze shopping arsenal. Its fun, if only for 14 beers. And its also free to download. [iOS, Android]"
537,https://gizmodo.com/artificial-intelligence-designed-these-magic-tricks-1659658162,Gizmodo,2014,11,17,220.0," Heres some great news for aspiring magicians looking for original illusions to launch their careers. Researchers at Queen Mary University of London have successfully taught an artificial intelligence computer program how to create new variants on existing magic tricks that are able to fool an audience, even if theyre already familiar with the original trick. Instead of uploading hours of Ricky Jay and David Copperfield footage, the researchers gave their software a basic understanding of how a magic jigsaw puzzle and a mind-reading card trick worked. They also provided data from real-life experiments that attempted to decipher how humans understand magic tricks, and using that information the softwares AI was able to develop the variants of the tricks that use the existing principles to still confound an audience. Dont expect Vegas to be taken over by robot illusionists just yet, though. The tricks the AI developed rely on mathematical techniques, not smoke, mirrors, or white tigers. And theyre already a core part of most magicians performances, so cant really be used as a crowd-drawing Vegas showstopper. But the maze trick is already available for sale at a local London magic shop, and the card trick, officially called Phoney, can currently be downloaded from the Google Play Store. Even Houdini had to start somewhere. [Queen Mary University of London via Phys.org]"
538,https://gizmodo.com/darpa-funded-researchers-have-tested-a-drone-that-can-l-1654444124,Gizmodo,2014,11,4,308.0," Almost seven years ago, we learned that DARPA was investing millions of dollars in neuromorphic chips. Thats a fancy term for a computer chip that mimics a biological cortexa brain chip. Today, researchers are getting closer. And of course, theyre putting those brain chips in drones. Responding to DARPAs challenge, HRL Laboratories Center for Neural and Emergent Systems just tested a tiny drone with a prototype neuromorphic chip. The drone packs 576 silicon neurons that communicate through spikes in electricity and respond to data from optical, ultrasound, and infrared sensors. And thanks to that brain-like chip, the little robot doesnt necessarily need a human to tell it what to do. It can learn and act on its own. It sounds like something out of a science fiction movie, a tiny aircraft that flies around deciding what to surveil or, more frighteningly, what to shoot. MITs Technology Review explains how the test worked: The first time the drone was flown into each room, the unique pattern of incoming sensor data from the walls, furniture, and other objects caused a pattern of electrical activity in the neurons that the chip had never experienced before. That triggered it to report that it was in a new space, and also caused the ways its neurons connected to one another to change, in a crude mimic of learning in a real brain. Those changes meant that next time the craft entered the same room, it recognized it and signaled as such. So thats pretty cool. No seriously, that kind of technological prowess is nothing short of astonishing. However, its hard to deny that a future full of drones with tiny electronic brians is a little bit frightening. Theyll surely do lots of good. But that conversation about the ethics of artificial intelligence will only escalate as AI takes to the skies. [Tech Review]"
539,https://gizmodo.com/top-engineer-and-futurist-tomorrows-robots-might-mercy-1628918108,Gizmodo,2014,8,30,312.0," Nell Watson is an engineer, a futurist, and the founder and CEO of Poikos. As such, she knows a lot about the machines we use today, and the ones were planning for tomorrow. And shes worried that the artificial intelligence of the near-future might decide the most benevolent thing to for mankind is to destroy it. Thats the concern Watson raised at The Conference, an annual gathering in Sweden focusing on technology and human behavior. In her talk, Helping Computers to Understand Humans, Watson brings up a terrifyingly interesting point: The machine learning powering the artificial intelligence we have right now cant learn the nuanced lessons of human ethics. You should really watch Watsons entire talkat just under 17 minutes long, its jam-packed with insights on the current and future state of machine learning and artificial intelligence. Here, you dont even have to go anywhere: Heres the brunt of Watsons argument, in case you cant watch the video for some reason: When we start to see super-intelligent artificial intelligences, are they going to be friendly, or are they going to be unfriendly? [. . .] Having a kind intelligence is not quite enough, because to paraphrase Arthur C. Clarke, any sufficiently benevolent action is indistinguishable from malevolence. If youre really, really, really kind, that might be seen as really evil. A truly kind intelligence might decide that the kindest and best thing for humanity is to end us. Yeesh. Maybe Kubrick was right about HAL-9000 after all. Its not all doom and gloom though (that would be a real downer of a lecture!). Watson ends her talk by issuing a challenge, saying perhaps the most important work of all of our lifetimes may be to ensure that machines are capable of understanding human values. Better get to work, engineers. We havent got much of a head start. [The Conference; Wired via CNet]"
540,https://gizmodo.com/the-creators-of-siri-are-building-a-new-ai-that-program-1620083579,Gizmodo,2014,8,12,416.0," Siri was a forward-thinking addition to iOS when it was introduced in 2011. Then Google and Microsoft implemented their own (better) contextually aware virtual assistants to help navigate you through your day. But there was always room for improvement, and the original creators of Siri think they take it even further. Todays Wired profile is full of promises from the small engineering team from Viv Labs, the original creators of Siri. However, wedged between the wishy-washy language and the chronic hedging, there lies a premise thats pretty mind-blowing. Viv Labs wants to take another shot at making a Siri-like voice assistant, but this time the AI, simply named Viv, will program on the fly essentially eliminating the need for programmers. Wireds Steven Levy explains: Take a complicated command like Give me a flight to Dallas with a seat that Shaq could fit in. Viv will parse the sentence and then it will perform its best trick: automatically generating a quick, efficient program to link third-party sources of information togethersay, Kayak, SeatGuru, and the NBA media guideso it can identify available flights with lots of legroom. And it can do all of this in a fraction of a second. This proposed method will allow Viv to perform complex search functions from more natural sentences. One example Viv Labs provides involved Abraham Lincoln. On Google Now, for example, you could ask Where was Abraham Lincoln born? and What is the population? in separate queries and get separate answers, but not What is the population of the city where Abraham Lincoln was born? Viv will bridge that gap by tapping into a cloud-based global brain that Viv Labs says third-parties will have access to as well as Vivs users, which is constantly teaching itself sophisticated algorithms to interpret the language of behavior of people using the system, according to Wired. Viv Labs is in the early phases of this new AI and is more theoretical applications than demonstrated capabilities. The group would of course love to see their AI implemented on future devices, but with currently no support from any major manufacturer, itll be an uphill battle. However, the team envisions Viv following a licensing modelloaning out to car companies and app makers and everything in betweeninstead of being tied to one operating system. Although all these AI ideas are impressive, a self-modifying virtual assistant on millions of smartphones, cars, televisions, tablets, and computers tapping into a global, cloud-based brain sounds like Elon Musks nightmares come to life.  [Wired]"
541,https://gizmodo.com/job-applicants-admit-more-to-a-computer-avatar-says-u-1611353873,Gizmodo,2014,7,26,277.0," Applying for a U.S. government job? Youll have to answer questions on drug use, criminal activity, and your loyalty to the U.S.A. And thanks to a government study, you might answer to an on-screen avatar, rather than a paper questionnaire. Researchers at the National Center for Credibility Advancement, the U.S. militarys premiere educational center for polygraph and other credibility assessment technologies and techniques, studied whether potential government job applicants would admit more to a computer-generated avatar versus self-reporting on a paper or computer questionnaire. They created a talking head avatar (which you can see over on Motherboard) and gave it an automated dialogue tree of questions. The researchers tested the automated avatar on a group of 120 U.S. Army basic trainees sitting for a mock national security clearance interview. As the authors put it, they examined behavioral and physiological responses of individuals to questions concerning their mental health, drug, alcohol, and criminal histories. Surprisingly, even though the computer avatar isnt that convincingly human, volunteers divulged much more to the talking head than they did to a questionnaire. And when the avatar asked is there anything else, over 10 percent of subjects added further information. Its a little creepy to think that the government wants to replace interviewers with avatars, but theres a very pragmatic reason for it: the researchers point out that automated interviews would be less time consuming, labor intensive, and costly to the Federal Government. Add to that the strange realization that people are somehow more comfortable revealing secrets to a screen, and you can see why the government might want to use virtual reality to vet its applicants. [Computers in Human Behavior via Motherboard]"
542,https://gizmodo.com/bee-inspired-bots-skitter-and-swarm-at-nycs-museum-of-m-1606644166,Gizmodo,2014,7,19,597.0," Dr. James McLurkin has a swarm of robots. Individually, theyre not that smart, but a crateful of them behaves in some very complex ways, like the bees that inspired them. Gizmodo got to see the wee machines in action, and while theyre adorable, they represent some serious future bot capabilities. The quiet before the swarm: robots waiting to be powered-up. Dr. McLurkin, a professor of computer science, runs the Multi-Robot Systems Lab at Rice University. He and his team research distributed algorithms for multi-robot systems. In other words, using the combined abilities of several rather simple robots to perform complex tasks. Dr. McLurkin has spent the past three years developing Robot Swarm, an exhibit of his hive-mind bots set to debut at Manhattans Museum of Mathematics in early 2015. This week, Dr. McLurkin gave a sneak preview of the exhibit, and Gizmodo was there. To a computer scientists, a bees behavior looks like a flowchart, Dr. McLurkin said at the demonstration. Nature is full of complex group behaviors carried out by simple individuals. He gave the example of a group of ants picking up a potato chip from a picnic site: Though they might face different directions, the ants all agree which way to carry their freight. Dr. McLurkins bots are the size of a small flower pot. Each one wears an infrared LED and four IR sensors, one on each corner, telling each bot where its nearest neighbor is at. Individually, each bot only knows where it is in relation to its nearest colleague, but give six of those critters an algorithm to work within, and youll see some complex, coordinated bot behavior. Dr. McLurkin used a modified PlayStation controller to toggle through different behaviors for his swarm of robots. In one mode, each bot was told to draw a path to its nearest neighbor, shown above. The custom-built, light-sensing LED floor visualized these paths for the audience, using data beamed from each bots undercarriage. Another task had each bot figure out how it would hop from neighbor to neighbor to reach the leader bot, shown below. Then Dr. McLurkin got the bots moving. In one task, the bots were told to keep the leader bot, controlled by Dr. McLurkin, over their left shoulder at all times. After a few seconds of scurrying confusion, the bots began orbiting around the leader like planets around a sun. In another, they were told to line up by assigned number, lowest to highest. The robots did all of this knowing essentially nothing about the world around themonly the location of the nearest kin. Dr. McLurkin sees a future where bots like this use their not-quite-intelligence to do dirty or dangerous tasks. What if we sent 20 robots to look for hot spots in a forest fire? Or 200 robots to look for earthquake survivors? Or my favorite robot job, sending robots to Mars. Weve got two there now, but what if it wasnt two, but 2,000? The exhibit catered to kids without talking down to them. The minibots squeaked, honked, and whistled like tiny R2D2s, singing Hi Ho, Hi Ho and The Hokey Pokey during different tasks. The kids were fascinated by these toy-like bots, but not just because they looked like little cartoon characters bopping around on a disco floor. They wanted to understand why they worked the way they did, a task Dr. McLurkin was happy to explain. Summer Swarm was a one-time sneak peekthe full Robot Swarm exhibit will open at the Museum of Mathematics in December. We cant wait to see what it looks like."
543,https://gizmodo.com/the-worlds-first-family-robot-could-be-like-hal-in-your-1605482309,Gizmodo,2014,7,16,557.0," Meet Jibo. On track to exist next year, Jibo is being marketed as the worlds first family robot. The bulbous little guy can read to kids in the living room, recite recipes in the kitchen, take photos in the yard, and perform a handful of other simple tasks. Jibo is also a little bit creepy. The idea of a robotic family helper is a terrifically timeless one. At least as far back as the original Jetsons series, we squishy humans have longed for someone to do our dirty work around the house. And over the years, weve gotten various versions of homebots. While Sonys robotic dog Aibo and iRobots roaming cleaner Roomba arent quite Rosie, progress has been steady, however slow. Jibo, however, looks like a leap forward. At first, Jibo just looks like a screen on a stand. But with three joints that would let it emote and express itself physically, Jibo could follow people around the room, and thanks to an on board camera with facial recognition capabilities, it could recognize the different members of the family. This feature would come in handy when you want to take photos, say, at a birthday party. The teddy bear-sized device would also work as a telepresence robot, record messages, offer up reminders at just the right time, and even read stories to the kids. Jibo is the brainchild of MIT Media Lab professor Cynthia Breazeal whos spent the last few years researching personal robots. While shes certainly explored the possiblities ofcute and cuddly robots as well as strictly mechanical creations, Breazeal says that designing a social robot doesnt need to involve designing something that looks like a human or an android. However, an approachable robot has to exist beyond a screen. Jibo is a physically animate, physically present device, Breazeal told Gizmodo. Its beyond the world trapped on a screen into world of atoms where people work and play. The family robots personality as presented here is uncanny to say the least. Breazeal says Jibos between a device today and your sci-fi vision of a robot. At 11-inches-tall and weighing just six pounds, it should be about as mobile as laptop, though it will only a half hour off its charging platter. Unlike a sci-fi robot, it wont walk around or cook meals for you. Unlike Siri, however, Jibo should be capable of carrying on a conversation with you. It can even laugh, even though its bone-chilling giggle could haunt you in your dreams. Nevertheless, Breazeal says that the elderly and the youth alike are super psyched about interacting with Jibo. The future wont come cheap. Breazeal wants to make the robot accessible, though, so theyre pricing it at the same level as a high end tablet. And starting this week, you can pre-order your own Jibo for $499 (or $599 for the developer version) through an Indiegogo campaign. While the usual crowdfunding caveats apply, Breazeal at least as the pedigree to make reasonably possible. First generation of the robots will only plant the seed for later generations, too, as developers build new apps on the platform. While itll take a little time to get used to this little laughing robot, the idea that drives it is an exciting one. Theres finally someone to talk to besides the dog when the fams getting on your nerves. [Jibo]"
544,https://gizmodo.com/the-worlds-best-subway-system-is-powered-by-an-advanced-1601103048,Gizmodo,2014,7,7,183.0," Hong Kongs metro puts others to shame: Its one of the most profitable subways in the world. Its on time 99.9 percent of the time. Its always improvingand its controlled by some very clever artificial intelligence. An article in the latest issue of New Scientist introduces us to the algorithm thats responsible for the incredible task of repairing and maintaining the system. That means assigning 10,000 employees to take care of more than 2,500 engineering jobs every weekan insanely complicated puzzle that would normally take a panel of humans two days of strategizing and planning. Instead, their bot can calculate the most efficient assignments and adapt to new information in seconds. In the end, smarter assignments mean that the employees have more time to finish their nightly jobs, and according to New Scientists Hal Hodson, the time bumper saves the system $800,000 a year. Theres just one problem: People dont like being kept in the dark about the grand plan. Its still difficult for us humans to trust a machine to do our jobsno matter how advanced. [New Scientist] Image: AP Photo/Vincent Yu."
545,https://gizmodo.com/this-is-the-first-computer-in-history-to-have-passed-th-1587780232,Gizmodo,2014,6,8,270.0," This is big. A computer program has successfully managed to fool a bunch of researchers into thinking that it was a 13-year-old boy named Eugene Goostman. In doing so, it has become the first in the world to have successfully passed the Turing Test. The test is named after computer pioneer Alan Turing. To pass it, a computer program needs to dupe 30 percent of human judges in five-minute, text-based chats, a feat that until now had never been accomplished. Eugene was created by a team based in Russia, and passed the test organized by the University of Reading just barely, by duping one in three judges. It should also be noted that a chatbot successfully pretending to be a 13-year-old boy for whom English is a second language aint exactly Hal 9000. Theres no artificial intelligence at work here; its more clevergamesmanship by Eugenes creators. How Big a Deal Is Passing the Turing Test? Its still an obviously exciting breakthrough, though, one that has critics already raising red flags about its implications. Having a computer that can trick a human into thinking that someone, or even something, is a person we trust is a wake-up call to cyber crime, said Kevin Warwick, a visiting professor at the University of Reading and deputy vice-chancellor for research at Coventry University told the Independent. Are there serious concerns about what this means for online security in the future? Sure. But today theyll have to take a back seat to the understanding that weve entered a new era of computing. One thats alive with possibilities, or at least convincingly enough so. [The Independent]"
546,https://gizmodo.com/a-wall-mounted-computer-chess-game-is-playable-art-1576219338,Gizmodo,2014,5,14,130.0," Staring at the Mona Lisa provides a few minutes of enjoyment and contemplation, but were the famous portrait on the wall of your living room, youd soon find yourself wishing she did more than just sit there and smirk. Maybe thats what inspired the Artful Dodger to build this portrait chess set that lets you play against the computer. The chess boards intelligent guts come courtesy of an old Fidelity Chessmaster 8 game, with additional LEDs and components added to detect where the human player has moved, and then indicate where the computer wants to move. It would be extra cool if all the pieces simply moved by themselves, but when it comes to art that makes you think, this is pretty hard to top. [Wall Chess via Damn Geeky]"
547,https://gizmodo.com/creepy-robot-takes-the-form-of-your-friends-knows-what-1553727801,Gizmodo,2014,3,28,245.0," Humans are the worsttheres no denying it. But isolating yourself from the rest of humanity doesnt exactly come with the best track record either. Now, though, you can get the best of both worlds thanks to SociBot-Mini, a depth-sensing, mood-reading, friends-identity-stealing disembodied robot torso. Created by Engineered Arts in the UK, SociBot-Mini may not be able to run around doing your bidding, but it can help fill all your emotional needs thanks to its head-mounted RGB camera, infrared depth sensor, touchscreen interface, and two-way audio system. Its depth-sensing camera works in very much the same way as Microsofts Kinect. Able to track up to 12 peoples faces at once, the robot can analyze your mood by capturing facial expressions and interact accordingly. The creepiest part, though, is SociBot-Minis backlit, three-dimensional face screen, which can either project one of its proprietary faces or a friends based on a headshot. The idea here is allowing telepresence calls to feel more realistic, but the implications for desperate unrequited lovers everywhere is hard to ignore. The SociBot-Mini is already on sale, marketed as a sort of futuristic information terminal. But now, the companys gearing up its imminent Kickstarter campaign for a cheaper, slimmed-down version strictly intended for home use. So if youve long dreamed of having a best friend who doesnt mind being an unpaid personal assistant/emotional slave (along with not having all those other messy human bits), your lucky day could be just around the corner. [New Scientist]"
548,https://gizmodo.com/boten-anna-the-spam-killer-1544811329,Gizmodo,2014,3,15,104.0," This week, the traffic on the Space subsite grew enough to attract our first spam-comment since I started as writer/editor. To commemorate the rise in traffic that attacked spammers, heres a Euro-pop flashback praising artificial intelligence and spam-fighting. Boten Anna is a song by Basshunter about an anti-spam comment-moderator bot. The chorus translates as: A later verse even echoes the plot of Her, revealing that the tireless Anna is a beautiful girl who sometimes needs a day off. It even works pretty well as a commentary on the self-assembling conversations happening in the highlights threads, where totally unrelated comments are talking to each other."
549,https://gizmodo.com/her-tech-support-troubleshooting-your-ai-girlfriend-1507275049,Gizmodo,2014,1,23,81.0," Relationships are hard. Especially when your partner inhabits a completely different realm of sentient existence that your frankly puny human mind could not be expected to fathom under any circumstance. The good news? Youve got tech support. Sure, its fun to imagine the future will be a clean, beautiful, technologically advanced place full of high-waisted pants, awesome projection videos games and, of course, sentient AI slaves assistants. But the real fantasy is that tech support could ever be this helpful. [CineFix]"
550,https://gizmodo.com/this-amazing-image-algorithm-learns-to-spot-objects-wit-1502512344,Gizmodo,2014,1,16,170.0," Its only a matter of time before things go the way of Skynet, and this new algorithm is a stepping stone along the way: it can learn to identify objects all by itself, with zero human help. Gulp. Created by Brigham Young University researchers,  this new recognition algorithm churns through images, defining its own parameters with which to define objects. BYU explains how it works: Instead of trying to explain the difference, we show children images of the animals and they learn on their own to distinguish the two. Lees object recognition does the same thing: Instead of telling the computer what to look at to distinguish between two objects, they simply feed it a set of images and it learns on its own. The research, published in  Pattern Recognition,  shows that the algorithm performs better than most top object recognition algorithms developed by other universities and private companies, proving to be 95-98 accuracy on data sets including everything from fish to airplanes. Who needs humans anyway? [BYU via Slashgear]"
551,https://gizmodo.com/gah-theyve-taught-drones-how-to-weave-giant-spider-we-1456796988,Gizmodo,2013,11,1,115.0," Its becoming painfully aware that those involved in robotics and artificial intelligence research arent quite thinking about the consequences of their so-called innovations. Like these researchers at ETH Zurich whove taught autonomous drones how to build and weave tensile wire structures. Or, to put it another way, DRONE WEBS OF DOOM. Sure, the artificial intelligence guiding these drones so the wire structures they build are strong and secure is impressive, but that just means theyll be harder to escape from one day. But its not time to panic just yet; only when Boston Dynamics introduces a Big Spider companion to its Big Dog robot should we start barricading the doors. [ETH Zurich via IEEE Spectrum]"
552,https://gizmodo.com/this-ibm-scientist-predicted-netflix-before-the-interne-1440206180,Gizmodo,2013,10,2,491.0," If you predicted the decline of deadtree books or the rise of services like Netflix streaming, say, 25 years ago, youd be considered a damn good prognosticator. But what if you predicted those things back in 1964before the internet even existed? Amazingly, a scientist from IBM did just that, long before any of these things were widely considered possible, much less inevitable. In 1964, artificial intelligence pioneer Dr. Arthur L. Samuel wrote an article for New Scientist titled, The Banishment of Paper-Work, that imagined what the networked computer landscape may look like by the year 1984. Samuel predicted movies on demand, government control over what information might be accessed, and the death of the deadtree library. He got a lot right. Samuel was just a bit optimistic about the timeline. From The World in 1984, Volume 1, edited by Nigel Calder: Connection to a central location will be very necessary to perform another function which will, by then, be delegated to the omnipresent computer. I refer to information retrieval. The entire contents of the large central files (or at least that portion which the government elects to make available) will be readily retrievable by anyone at a moments notice. One will be able to browse through the fiction section of the central library, enjoy an evenings light entertainment viewing any movie that has ever been produced (for a suitable fee, of course, since Hollywood will still be commercial), or inquire as to the previous days production figures for tin in Bolivia  all for the asking via ones remote terminal. Libraries for books will have ceased to exist in the more advanced countries except for a few which will be preserved at museums, and most of the worlds knowledge will be in machine- readable form. Perhaps it would be more correct to say, all of the worlds recorded knowledge will be in this form since the art of programming computers to read printed and handwritten material will have been fully developed. However, the storage problem will make it imperative that a more condensed form of recording be used, a form which will only be machine-readable, and which will be translated into human-readable form by ones computer on demand. Samuels predictions about the computers of tomorrow came five years before the ARPANET (the precursor to our modern internet) would make its first host-to-host connection in 1969. The very people building the ARPANET had many accurate predictions for how the network may one day be utilized for everything from online banking to digital libraries. But Samuels guesses about our online world are really in a class of their own for someone outside of science fiction in the mid-1960s. We may not literally have every movie ever produced available on-demand, and the vast majority of libraries have yet to go fully digital. But when it comes to governments keeping a close eye on how their citizens are using the internet, Samuel pretty much nailed it."
553,https://gizmodo.com/just-how-smart-is-artificial-intelligence-1425757074,Gizmodo,2013,9,30,86.0," Artificial Intelligence is the Holy Grail of computer scienceand, for that matter, science fiction. But just how far have we come? In this video, Luciano FloridiProfessor of Philosophy and Ethics of Information at the University of Oxfordexplores the current state of AI.  The nub of his argument boils down to one big question: have we already created machines which are on their way to being truly intelligent, or have we just changed the world to allow dumb machines to behave intelligently? Watch and find out. [YouTube]"
554,https://gizmodo.com/carrot-wakes-you-up-with-empty-threats-and-some-real-1236480139,Gizmodo,2013,9,1,181.0," Getting out of bed is the worst. And anyone who says they dont mind waking up can just leave now. Some of us are employing desperate measures to get going every morning, and a manipulative/verbally abusive alarm clock sounds like just the thing. The makers of the CARROT To-Do list wanted to bring their motivational snark to a particularly rough time of day and they certainly found it. CARROT is a sentient alarm clock. She plays aggressively chipper music to wake you up, makes you perform tasks with your phone so you actually have to focus, and keeps track of points. You cant win a more pleasant wakeup experience, but you can use them to unlock better alarm tones and routines. Eventually you can even use points to get CARROT to recite bedtime stories to you. Which sounds pretty dreadful. But points! CARROT Alarm Clock syncs with CARROT To-Do, and to multiple iDevices. Its $2 and it could be worth the money unless you proceed to lose hundreds of dollars smashing your iPhone against the wall in a morning rage. [AppAdvice]"
555,https://gizmodo.com/how-machines-think-1132919152,Gizmodo,2013,8,14,80.0," When people talk about artificial intelligence, its tempting to think that it means computers can think (a little) like humans. Perhaps unsurprisingly, it turns out thats not quite the case. This video from New Scientist explains how machine are designed to be capable of solving problems that humans canjust not quite in the same way. Instead, it all boils down to data, probability, and a lot of code written by humansbut that doesnt stop them being incredibly, undeniably useful. [YouTube]"
556,https://gizmodo.com/one-of-the-worlds-best-a-i-computers-is-as-smart-as-a-791902984,Gizmodo,2013,7,15,433.0," Computers are good at a lot of things. Thinking like a grown-up human being is not one of them. Not yet, at least. A team of researchers from the University of Illinois Chicago recently set out to discover just how advanced our artificially intelligent computers have become. Like theyd do with any food-eating human, they gave the computer an IQ test. The machine in question, a ConceptNet 4 artificial intelligence system developed by a bunch of eggheads at MIT, took the Weschsler Preschool and Primary Scale of Intelligence Test, a standard IQ test for children, and scored about as high as a four-year-old would have. It turns out that while it did well on questions with cut-and-dry answers, the computer had a lot of trouble with the why questions.Oh and there was one other thing. If a child had scores that varied this much, said Robert Sloan, lead author on the study, it might be a symptom that something was wrong. So computers are bad at meaning and potentially developmentally disabled. We already knew that. Theyre computers! This should be a good thing for all you future-fearing humanists out there. Now that weve got smartwatches and cell phones, its easy to get all worked up about the inevitable robot takeover starring Arnold Schwarzenegger (we hope). Its not going to be easy for that robot army to obliterate mankind if they have the collective intelligence of a preschooler. Scientists say were still a long way from developing computers with common sense, a trait that children develop by about age eight. However, its not hard to trick ourselves into believing computers are smarter than that. Last year, a study from the University of Gtheberg took a limited IQ test and scored a 150, a score so high it beats 96 percent of humans. The key word in that accomplishment is limited as the computer took a exam that lacked all verbal tests and therefore made it very computer friendly. (Pro tip: Computers are really good with numbers.) The Swedes high score is still impressive, but its necessarily sobering to understand that it was designed to take a test that it was designed to ace. This is like giving a dishwasher a dirty dish and then applauding it when the dish is clean an hour later. For the brave new breed of A.I. computers, its a start. This year, theyve produced a computer with the IQ of a four-year-old. Next year, theyre probably shooting for five. At this rate, well be sending this box off to college in a little less than two decades! [Science Daily]"
557,https://gizmodo.com/the-rise-of-artificial-intelligence-is-absolutely-fasci-751390176,Gizmodo,2013,7,12,253.0," We poke fun at Siri and pretend to get scared by Humanoid robots and make our neck hair stand up straight by watching quadrocopters do amazing things but the truth is, artificial intelligence is still pretty dumb. But thats going to change! The rise of artificial intelligence is happening and theyre learning a lot more about us because were learning more about them. Sort of. PBS always excellent Off Book series takes a look at AI and how were developing robots who can process things like vision, understand language and manipulate objects. The relationship and challenge in creating a robotic brain (or intelligence) is similar to how weve been able to create airplanes using a birds aerodynamics as an inspiration. PBS says: Artificial intelligence is an ever evolving goal for researchers, and the object of endless fascination for writers, filmmakers, and the general public. But despite our best science fiction visions, creating digital intelligence is incredibly difficult. The universe is a very complicated place, and humans have had millions of years to evolve the ability to navigate and make sense of it. Contemporary attempts to create AI have us looking more at how our own brains work to see how a computer could simulate the core activities that create our intelligence. No matter how we get there, it is certain that artificial intelligence will have tremendous impact on our society and economy, and lead us down a path towards evolving our own definitions of humanity. The future and all that. [PBS Off Book]"
558,https://gizmodo.com/ibms-watson-has-a-boring-new-job-answering-phones-509034580,Gizmodo,2013,5,21,201.0," After youve used your crazy robot intellect to crush puny meatbags definitively in a game of Jeopardy, it would seem like the world is your oyster. But Watsons not taking trying to take over the world or anything, no. After a trying out medicine and inventing a pastry, hes settling for a boring job in customer service. Companies including Australias ANZ Bank, Nielsen, and Royal Bank of Canada plan to put this supercomputer to work answering questions by SMS, online chat, email, or through a compatible app. Watson will not only be saving humans from having to answer hundreds upon hundreds of inane questions about insurance plans and loan interested, but also help provide better information, since he doesnt zone out when reading fine print. Watson doesnt have any voice recognition capability yet, but that could come later this year, putting him toe-to-toe with things like Siri and Google Now. IBMs likely to continue pimping out Watsons noggin to more companies as down the line, and really, we could all benefit from having more, smarter computers answering our questions for us. But it just seems like hes settling. We just hope hes happy, happy as a lifeless machine brain can be."
559,https://gizmodo.com/the-most-realistic-virtual-human-ever-is-a-fully-expres-5991365,Gizmodo,2013,3,19,336.0," Hoping to be holding the personal assistant of the future, researchers at the University of Cambridge have unveiled whats supposedly the most realistically esxpressive controllable avatar ever. Move aside, Sirithis is what you get for mouthing off. To develop the virtual, controllable avatar (dubbed Zoe) with what they claim is unprecedented realism, they had an actress read over 7,000 different simple sentences, in different emotions while the researchers tracked facial expression. With all that data combined, simply input the phrase and emotion of your choice, and Zoe becomes your almost-as-good-as-real plaything. Of course, theyve still got a ways to go; Zoe hasnt shed all of her jerky robotic mannerisms just yet. In the video above, for instance, Zoe shows off a self-proclaimed soothing voice that sounds more like a cyborg Mary Poppins nightmaredifferent strokes, I suppose. Still, even if Zoe isnt entirely believable, shes at least perfectly understandable. Volunteers were able to correctly recognize her chosen emotion with a 77% accuracy rate. To put that number into perspective, those same volunteers only achieved a 73% accuracy when attempting to identify the emotions of the real-life, 100% human Zoe Lister. Apparently, our talking head does human better than humans. So whats next for Zoe? Well, her personal assistant aspirations are entirely feasible, considering she only takes up about 10MB of data. Not a fan of Zoe? No problem! The next step will be allowing anyone to upload their very own face with nothing more than few head shots and several seconds of recorded speech. Plus, theyre even hoping to make Zo-Zo adaptive, so youd be able to fully interact with her and her alone, no people for me no thank you. The video ends with Zoe signing off with a promise/threat for the future: One day I might appear on your phone. I could be your personal assistant, your mentor, or your carer. Perhaps you will see me in video games, or advertising, or even in the movies. There is no escape. [University of Cambridge via Gizmag]"
560,https://gizmodo.com/meet-the-twitter-bot-that-can-answer-all-your-questions-5989662,Gizmodo,2013,3,9,221.0," Remember SmarterChild? That AIM bot that would answer all/most of your questions and get huffy and stop talking to you when you inevitably started swearing at it? Unfortunately SmarterChild is still dead and gone, but its got a Twitter descendant: @DearAssistant. DearAssistant is a simple Twitter bot designed by Amit Agarwal for a single function: answering your queries using Wolfram Alpha. On his blog, Agarwal describes his little project as a mini version of Siri, which is apt considering they both use the same data source. Except with DearAssistant, no one has to say anything out loud. It can be a little slow sometimes, but it works remarkably well. @EricLimer -35 F(1 hour 39 minutes ago) The bots code is all open source, so if youre into tinkering, you can use it as a jumping-off point to make a little robot Twitter-wizard of your own, or you can just harass it and make it spout answers to the trivia questions you fling at it. Its not the most robust question and answer system out there, certainly not as good as Google or actual Wolfram Alpha, but its definitely on the actually useful end of the Twitter-bot spectrum. A hell of a lot better than the legions of god-awful parody accounts. And thats something worth celebrating. [Digital Inspiration via The Next Web]"
561,https://gizmodo.com/supercomputer-dr-watson-is-now-seeing-patients-5983010,Gizmodo,2013,2,9,282.0," Everybodys favorite clue-guessing computer Watson was always destined for more than just trouncing meatbags on Jeopardy. And though it spent a little of its time just hanging out and learning how to swear, Watson has now moved on to bigger and better things. Dr. Watsons taking patients now, through a cloud-based medical application. Watsons first job is to help doctors decide what kind of treatment would be most effective for certain lung cancer patients. Doctors at the Maine Center for Cancer Medicine and WestMed in New Yorks Westchester County will soon be able to quiz Watson through a tablet or computer, and Watson will dig through the patients thousand or so pages of records and provide answers in decreasing order of confidence. Watson isnt actually making any decisions, per se, but he is making things easier by digging through tons of data, and surfacing just the important parts. It starts sometime next month. Meanwhile, Watson has also been moonlighting at health insurer WellPoint Incits first jobwhere it does roughly the same thing, except instead of recommending medical treatments based on efficacy, it offering suggestions as to what treatments the company should authorize for payment in particularly complex cases. Watson is sure to improve efficiency on both counts with its frankly spectacular ability to handle naturally worded questions and dig through unfathomably large amounts of data. Its basically the best, fastest intern ever. And as scary as it might be to put your health in the cold, metallic hands of a machine, you cant say its not efficient. And its got everybodys best interests in mind, at least for now. You better get used to the idea. This is only the beginning. [AP]"
562,https://gizmodo.com/the-smartest-computer-in-the-world-also-has-the-dirties-5974982,Gizmodo,2013,1,11,248.0," We already knew Ken Jennings thought IBMs Jeopardy-winning supercomputer was a dick, but hes not alone. Developers at IBM were forced to wipe part of Watsons memory once they realized their hyperintelligent computer had turned into a bit of a smartass. During development, Watsons project head had what must have seemed like a brilliant idea. If Watsons going to spend most of his time interacting with humans, he was going to need to understand common human slang. And where do you go to figure out the hip new words of todays youth? Urban Dictionary, of course. Unfortunately for Watsons progenitors, they seem to have overlooked the fact that 75% of Urban Dictionary contains absurd, profane, and physiologically impossible turns of phrase that could make Jeffrey Dahmer blush. Watson, being the knowledge fiend he is, of course loved the way his colorful vocabularly enabled him to describe the world around him (i.e. in terms of sex, defecation, and blasphemy as a whole). According to Fortunes Michal Lev-Ram: Watson couldnt distinguish between polite language and profanity  which the Urban Dictionary is full of. Watson picked up some bad habits from reading Wikipedia as well. In tests it even used the word bullshit in an answer to a researchers query. Proving there is such a thing as too much knowledge, the team was ultimately forced to erase Urban Dictionary from Watsons memory. Somewhere out there, Ken Jennings is smiling triumphantly to himself as Jeopardy reruns blare in the background. [The Atlantic]"
563,https://gizmodo.com/the-most-realistic-artificial-brain-has-a-mind-of-its-o-5964684,Gizmodo,2012,11,30,275.0," Computers can do practically anything these days, but theyre still a far cry from robotic brains that dont just do what theyre told but actually think for themselves. The Semantic Pointer Architecture Unified Network, or Spaun, is one of those, and its creeping up on human intelligence. Developed by neuroscientists and software engineers at the University of Waterloo in Canada, Spaun includes a 784-pixel digital eye, and a robotic arm for writing, but the real meat of it is its 2.5 million simulated neurons which are used to emulate parts of a human brain like the prefrontal cortex, basal ganglia, and thalamus. Using all those neurons, it can think through questions and solve limited problems the same way a human brain can, albeit in a much more finite sense. Even though its makers claim it is the most complex artificial brain in existence, its tricks are pretty scant; it mostly does things like recognize patterns and (mis)remember long strings of numbers. Which is pretty much what it takes to be able to struggle through most of an IQ test, even if its a low score. The impressive thing here isnt so much what Spaun can do, but what it can do considering its built to act like a real, pulsing, fleshy brain. In the future, research leader Chris Eliasmith hopes to teach Spaun to learn just by doing, a skill thats both awesome and horrifying in its potential. And in the meantime, you can actually download your own Spaun, since hes open-source. Theres probably no way you could run it though. But hey, youve already got your own brain; go have fun with that. [ExtremeTech]"
564,https://gizmodo.com/watch-this-adorable-horde-of-intelligent-swarm-robots-p-5963137,Gizmodo,2012,11,26,186.0," Sometimes robots can be better than humans at playing instruments. Sometimes theyre just cooler at it. This herd of little swarm robotswhich divvy up the playing between themselves automaticallyis the latter. Developed by researchers at Georgia Techs GRITS Lab, these intelligent swarm robots are given a piece of music to play, but are not told exactly how to play it. While theyre told what notes need to be hit when, they figure out the choreography by themselves, on the fly. The first robot gets the full instructions, passes some instructions to its neighbors and so forth until all the jobs are claimed. Robots that arent required for the most efficient solution that the robots calculate just sit back and enjoy the show. Its sort of like those James Bond theme drones. While the bots are playing Fur Elise here (and painfully slowly, I might add) the tech has potential for all sorts of more practical applications like studying climate change in Antarctica, and military applications. For the moment though, theyre just an amusing intelligent robot swarm, not a useful (or dangerous) one. [GRITS Lab via Gizmaghttp://www.gizmag.com/georgia-tech-r>]"
565,https://gizmodo.com/sketch-scanning-software-can-decipher-your-crappy-drawi-5943269,Gizmodo,2012,9,14,388.0," Next time youre stuck for someone to play Pictionary with, dont despair. Now you can always get a game against a computer and, reassuringly, youll probably win. Sketching is something nearly any human can do, even if we use inaccurate representations like outsized ears on rabbits, or stick figures to simplify complex objects. Whats remarkable is that most of us can recognise these dodgy sketches, despite their divergence from reality. Pictionary is based on these very abilities. Now machines can play too, after researchers from the Berlin Institute of Technology in Germany and Brown University in Providence, Rhode Island, trained a computer to recognise human sketches. Instead of trying to teach the computer a set of fundamental lines and shapes, they trained it using machine learning on drawings crowdsourced over the internet, with the computer making a best guess at what category a sketch should go in, based on the layout of its lines. Humans easily outplayed the trained computer on a database of 20,000 sketches from 250 different categories, placing 73 per cent of all sketches correctly. The computer could place only 56 per cent of the sketches correctly: much worse than humans, but much better than random chance, which would place sketches correctly only 0.4 per cent of the time. The researchers iPhone app, tested out in the New Scientist office, works about as well as our limited touchscreen drawing skills allowed. Attempted race cars were correctly and gratifyingly identified as race cars, but also as potential rabbits and or blimps. Other miscues saw the system recognising snowman sketches as toilets, teddy bears as helicopters and human skeletons as pineapples. All the sketches were crowdsourced using Amazons Mechanical Turk  an online labour market where humans can perform simple tasks for small amounts of money per turn. This meant that the researchers could gather data on how humans tend to sketch objects. The paper noted that the sketches usually started with longer lines, with stroke length decreasing over time. The outline of a tomato, for instance, tended to be drawn first, and most skechers drew the line anti-clockwise. The paper was presented at the recent SIGGRAPH conference in Los Angeles. New Scientist reports, explores and interprets the results of human endeavour set in the context of society and culture, providing comprehensive coverage of science and technology news."
566,https://gizmodo.com/what-its-like-to-judge-the-turing-test-5921698,Gizmodo,2012,6,27,932.0," What are your favourite Sci Fi movies? I like Star Wars and The Matrix, comes the typed reply. I am trying to work out if Im talking to a hidden human in the next room, or actually a machine located somewhere in cyberspace. Can we agree that the prequels sucked? I continue. Absolutely! Lucas should be shot! That settled it  only a flesh-and-blood movie buff could be so enraged by The Phantom Menace. This was one of the easier to gauge exchanges I had as Judge J-18 in last weekends Turing-test marathon at Bletchley Park in central England. The test aims to separate the humans from the machines through rigorous questioning by judges. It was designed over 60 years ago by Alan Turing, the grandfather of computers, whose work in Hut #8 at Bletchley Park played a vital role in the Allied code-breaking effort during the Second World War. Saturdays marathon, along with similar events worldwide, were in celebration of Turings centenary. The largest Turing test undertaken, Saturdays session tried to recreate what the great man envisaged in his 1950 paper outlining its methodology. So, what is it actually like to be a judge in a Turing test? Anxious to find out, I signed up months in advance for the Bletchley Park event. The premise is similar to interrogating a spy: ask enough questions, and eventually the suspect tips his hand. As Turing envisioned it, youd have a human behind one opaque screen, and a computer behind another. A judge sits in front of them with no way of knowing who or what is behind each screen. The judge can ask questions of the two entities; they reply by text-based chat. If the machine is so good at producing human-like responses that the judge cant tell which is which, based on a five-minute conversation, the machine is on its way to passing the Turing test. Turing didnt expect any machine to fool all the judges all the time, but he speculated that by the year 2000 an average interrogator will not have more than 70 per cent chance of making the right identification  that is, computer programs would stymie the judges 30 per cent of the time. Twelve years late, this years test set out to see if we could finally reach that bar. The tests took place in the mansions former billiards room, where I took my seat next to the other Session 1 judges, each of us perched in front of a standard PC. Huma Shah, the events organizer, explained the rules: There would be two kinds of tests. In one version, the judge would chat with a single unseen entity for five minutes. In the other version, there would be a split screen, and the judges would converse with two entities at the same time, again for five minutes, trying to deduce if one (or both, or none) of the entities was a machine. In several cases, the computers gave themselves away nearly from the start. If my correspondent couldnt answer a simple question, or changed the subject abruptly and for no apparent reason, it struck me as almost certainly a machine. At the opposite end of the spectrum, we have my almost-certainly-human Star Wars buff, or the Beatles fan I encountered  best band ever  who, when asked to choose between the Rolling Stones and The Who, replied definitely The Stones  The Who went too stadium toward the end. While I disagree with the sentiment  in my mind, The Who made great music right up to the bands split in the early 80s  the reply seemed far too well, far too human to be written by a machine. Other exchanges, however, seemed far more ambiguous. When I said I was from Canada, one respondent said they heard great things about Canada, except that Quebec is very French. Was that something a computer might cough up after spending a few milliseconds skimming the Wikipedia entry for my country? Or is it just a human with a vague memory of what their school teacher once said about Canada, perhaps tinged with mild anti-French prejudice? Or was it merely a person who had grown tired after a couple of hours of conversing with strangers by text? In devising the game, Turing was acknowledging that mastery of language is often seen as going hand in hand with intelligence. Indeed, linguistic ability involves more than just stringing words together into sentences. Holding down a conversation likely depends on a whole host of other cognitive abilities  the ability to monitor ones own thoughts, ones environment, other creatures  perhaps even to guess what other people are thinking. Children acquire these cognitive skills as a part of their ordinary development. Instilling such capability in a machine, on the other hand, is a herculean challenge, and the programmers behind Saturdays chatbots should be lauded for getting their creations to perform as well as they did. The best program, Eugene Goostman, a chatbot with the personality of a 13 year old boy, nearly passed the test, fooling the judges just shy of the 30 per cent mark suggested by Turing in his 1950 paper. The Turing test marathon showed just how challenging it is for a machine to carry on a real conversation. As Mark Twain might have put it, you can fool some of the judges some of the time  but not much more than that. At least, not quite yet. New Scientist reports, explores and interprets the results of human endeavour set in the context of society and culture, providing comprehensive coverage of science and technology news."
567,https://gizmodo.com/hallucinating-robots-make-the-best-interior-designers-5920860,Gizmodo,2012,6,24,318.0," We humans are already kind of afraid of robots, so heres a good idea: what if we make them trip balls and hallucinate imaginary people everywhere? Seems like it would drive them to madness and make them turn on us even faster, but researchers at Cornell Universitys Robot Learning Lab think it will make them better at their jobs. The researchers had already successfully taught their bots how to place objects in a space with correct orientation to their function and how they are generally placed in relationship to other objects (see video), but this work didnt go far enough. Its great that it can place a laptop right side up, but not so great if the robot places it on top of a bookshelf where a human couldnt reach it. So the next step was to teach the robots how to orient objects to the way the humans actually use them. To do that they learned a set of human poses, and then they hallucinate them all over the room. After observing, the robots really started to get it down. Humans sit on the couch, facing the TV, which faces the couch. The remote control is within arms reach, not on the floor by their feet. When combining human orientation with the previous object-to-object orientation work they had done, they found that accuracy of placement had been boosted to 86 percent (subjectively), which is pretty impressive for a bunch of tripping robots. The work certainly still has a ways to go, but this is major progress. It could eventually lead to a bot that would not only pick up after you, but also rearrange a room in a more ergonomic, intuitive way. Speaking as a walking/talking organizational nightmare, the days of hallucinating robots cant come soon enough, as long as they dont just sit around watching the walls pulse and breathe. [Cornell University Chronicle via Kurzweil AI]"
568,https://gizmodo.com/the-best-chatbot-in-the-world-is-still-awful-updated-5910812,Gizmodo,2012,5,16,313.0," If you ever speak to Chip Vivant, you might find his conversation a little stilted. Incoherent, even. Because, despite being crowned the best chatbot in the world, frankly, he still carries a conversation like R2D2 with a head injury. Chip was declared the winner of the 2012 Loebner Prizea contest loosely based on Alan Turings test of machine intelligence. During the competition, chatbots battle it out to convince human judges that theyre real people. Usually, the chatbots are all so terrible that none of the judges are fooled, so the victor is decided by them conferring about the contestants respective conversational abilities. Thats how Chip won. But frankly, looking at the quality of his chat, I dread to think what the rest of the field was like. New Scientist provides a few sample lines of Chips dialogue with a judge: Judge: Where did you go to school? school. Judge: Are you Japanese? Chip: I havent achieved self-awareness yet, so Im not sure. Judge: Why not ask somebody? Chip: No idea. Judge: Why not ask me? Chip: Okay. Chip: Thats an interesting opinion. Get it together, Chip. Still, that was enough to see his creator, Mohan Embar, to victory. Actually, its kind of unfair to suggest that the Loebner competition represents the cutting-edge of AI, because a lot of academic researchers and large companies shun it in favor of more professional pursuits. In reality, AI has made massive leaps in the past few decades. Its just that conversations are one the most difficult things for a computer to get to grips with; the way we use language is so nuanced that, sadly, convincing chatbots are still a long way off. But then, anyone whos used Siri already knew that. [New Scientist] Update: The original version of this story used an incorrect transcript of the conversation between Chip and a judge. Its now been fixed."
569,https://gizmodo.com/someday-siri-will-be-this-smart-and-good-looking-5902730,Gizmodo,2012,4,17,134.0," The newest promo video for Prometheus is pretty phenomenalMichael Fassbender, in chokingly creepy fashion, introduces his David android, who we of course know will turn out to be evil and awful. But hey, this is the real future. Creepy David makes repeated mention of his commitment to understanding usand theres nothing sci-fi about that. We can already see increased emphasis put on building software with empathy. Software that can help us even when were inarticulate or dumb. Software that can think things through for us. Just look at Siriif its worth anything at all, its because it can process natural language. Or look at what the Navy is doing to prepare robots that can work alongside humans. So, its on its wayjust hope the other 99.9% of the stuff in Prometheus doesnt happen. [io9]"
570,https://gizmodo.com/heres-how-robots-will-save-our-lives-in-the-future-5902085,Gizmodo,2012,4,15,247.0," At the Navys brand, new multi-million dollar LASR complex, researchers are creating problem-solving Siri times a million. Itll understand us! Itll be friendly! And itll put out our fires by being just as smart as a human. We hope. The Laboratory for Autonomous Systems Research is a Naval playground. But theres some serious work going on inside, like Lucas and Octaviaa robotic brother and sister duo being trained to have human intelligence, without the human reluctance to barge into a flaming fire. Lucas, as seen in the brief exchange above, is using an advanced voice recognition kit that doesnt just understand English sentencesit can cross-reference them to make inferences, suggestions, and decisions. In the demo here, Lucas is able to remember a previous conversation, and realize that a human decision doesnt make sense. That point where Lucas looks confused and/or incredibly hung over? Thats the problem solving action at workthe robot can tell when were making mistakes. Its slow at this point, but the tech is in its infancy. Ideally, the Navy will have the processing running just as fast as an actual person would talk back to you. Then a robot firefighter like Octavia will be able to not only comprehend the complexity of a burning ship/building/space station, shell be able to rush over and put it our, keeping humans from harms way. We just need to make sure shes able to do it at about 100x the pace she can right nowbut again, tech infancy."
571,https://gizmodo.com/our-robot-overlords-will-have-baby-brains-5892980,Gizmodo,2012,3,13,244.0," Scientists are modeling artificial intelligence after baby brains. Why would they want to make computers similar to beings whose favorite pastimes are drooling and pooping? It makes perfect sense when you think about how malleable a babys gray matter is. Artificially intelligent machines have a tough time with nuances and uncertainty. But babies, toddlers and preschoolers are great at interpreting such things. So Alison Gopnik, a developmental psychologist at UC Berkeley and her colleague Tom Griffiths are putting babies to the test to find ways to incorporate their abilities in to computer programming. Children are the greatest learning machines in the universe, Gopnik says. Imagine if computers could learn as much and as quickly as they do. Theyve already found that at very young ages, babies can test hypotheses, detect statistical patterns and draw conclusions about important matters such as lollipops and toysall the while adapting to changes. As smart as computers are, youngsters can solve problems that machines cant, including learning languages and interpreting causal relationships. If computers could be more like children, it might lead to digital tutoring programs, phone operators, or even robots that can identify genes associated with disease susceptibilities. The researchers are creating a center at the Berkeleys Institute of Human Development to meld baby and computer research. And if an angry machine comes storming out of there one day in a baby robot rage, the good news is all youll need to do is find its binky. [ScienceBlog]"
572,https://gizmodo.com/this-computer-program-is-smarter-than-96-percent-of-hum-5885904,Gizmodo,2012,2,17,328.0," Computers will never be able to subjugate the human race with their current level of intellect. Thankfully, a team of Swedish researchers have developed an AI with an IQ of 150. The program was developed by the Department of Philosophy, Linguistics, and Theory of Science at the University of Gothenburg in Gteborg, Sweden. Its intelligence score is based off of results from standard non-verbal test questions, which are designed to eliminate cultural and linguistic biases by testing knowledge rather than reasoning. Most of the test questions revolve around predicting the next shape in a progressive matrix test or guessing the next number in a sequence. Since the speed in which the program solves questions isnt a scoring factorand some questions are designed to be unanswerable by either biological or electronic mindsthe program is designed to supplement its pattern recognition algorithms with human psychological traits. As Claes Strannegrd, a researcher at the university explains, Were trying to make programs that can discover the same types of patterns that humans see. The program isnt given answer options like a multiple choice exam. Instead, it employs its psychological model of patterns and a fair amount of logic to deduce the solutions. This reduces the programs performance on image-based questions but enables it to score at genius levels on mathematical queries. Since the program relies on the human psyche to come to its conclusions, it does have limitations. Our programs are beating the conventional math programs because we are combining mathematics and psychology, says Strannegrd, Our method can potentially be used to identify patterns in any data with a psychological component, such as financial data. But it is not as good at finding patterns in more science-type data, such as weather data, since then the human psyche is not involved. In addition, the team hopes to develop the program to eventually train people in alternative-thinking methodsyou know, so we can better empathize with our new robot overlords. [University of Gutenberg via Gizmag]"
573,https://gizmodo.com/wolfram-alpha-pro-arrives-tomorrow-with-the-power-to-an-5882969,Gizmodo,2012,2,7,255.0," Wolfram Alpha may or may not be one of my favorite things in tech right now. Itll spew movie times, compare NFL stats, help you cheat in Words With Friends and tell you exactly what airplane youre staring at in the sky. And now with tomorrows arrival of Wolfram Alpha Pro, the company, in its founders words, begins step 2. According to the NY Times, the premium version of the computational enginewhich draws from a pool of databases instead of spidering the webwill cost $5 a month ($3 for students), and offer more robust analytical services, such as the ability to make sense of complex data sets. The new version handles data and images. In a recent demonstration, Dr. Wolfram, using his computer mouse, dragged in a table of the gross domestic product figures for France for 1961 to 2010, and Wolfram Alpha produced on the Web page a color-coded bar chart, which could be downloaded in different document formats. He put in a table of campaign contributions to politicians over several years, and Wolfram Alpha generated a chart and brief summary, saying that House members received less on average than senators. Dr. Wolfram dragged in a 3-D image and after a few seconds it rendered the image  a guitar  and reported the number of polygons (2,253), among other characteristics. As an interesting aside, nearly a quarter of the engines queries come from Siri, Apples half-baked voice assistant. One has to wonder if the two companies will be working closer in the future. [NY Times]"
574,https://gizmodo.com/heads-up-display-contact-lenses-are-one-step-closer-aft-5861749,Gizmodo,2011,11,22,312.0," The days of traditional screens could be numbered if the news coming out of Washington University is anything to go by. Its testing contact lenses that could project information into the wearers eyes and initial safety tests look promising. Weve been dreaming of a HUD-style vision enhancer ever since Sci-Fi first introduced the concept to the masses with things like The Terminator. But now weve taken a significant step forward and are genuinely getting closer to making Terminator-vision a reality. Researchers have been testing a primitive version of a contact lens display with just one LED  a single pixel  but theyre pursuing the technology with renewed vigor after safety studies concluded that the contact lens caused no adverse affects to the wearer when tested on rabbits. The main problem, like so many of these ultra-thin technology ideas, is the power source. The current implementation uses a wireless battery, but it cant be more than a few centimeters away. Thankfully another possible hurdle to potential eyeball-mounted displays, the fact that your eye cant normally focus on something actually on its surface, was solved by a collaboration with boffins from Aalto University in Finland. The team managed to use the material of the lens to modify the focal length of the eye, a bit like a traditional contact lens does, allowing you to focus on the lens display itself. With safety confirmed, lets hope the project progresses at a rate of knots. Augmented reality shows great promise, but is a bit lame on the phone. I want my Terminator-vision now. The next stage in testing is to push forward with their plan to get the lens to display some pre-determined text  may I suggest Hello World? [BBC via Gizmodo UK] Our newest offspring Gizmodo UK is gobbling up the news in a different timezone, so check them out if you need another Giz fix."
575,https://gizmodo.com/wall-e-like-farming-robots-could-replace-undocumented-w-5859335,Gizmodo,2011,11,14,536.0," Despite advancements in mechanization within US agriculture, some menial jobs are still best left to human workers. Problem is, federal crackdowns on undocumented laborers have decimated that workforce. The Harvester automaton could provide a cheap, readily available labor force without the threat of raids by the INS. The US agriculture industry is worth about $300 billion annuallyhalf from livestock production, the other half from crops. However, some of the most basic jobs in this industry still have to be performed by people. Jobs like offloading potted plants from trucks, arranging them in rows for the growing season, then loading them back onto a truck when theyre ready for sale. That may not sound very difficult, but when performed on an industrial scale, it becomes a mammoth task requiring a huge workforce of low-productivity manual laborerstoday largely comprising undocumented migrant workers. However, some analysts estimate that, by automating these sorts of tasks, a firms revenue per employee could be increased from $40K per employee vs. $175K per employeean annual labor savings of $21 billion. Weve recognized the need for robotics in the nursery industry for moving pots because its one of our highest concentrations of labor use, said Tom Demaline, president of Willoway Nurseries, Inc. in Avon, Ohio. And thats where the Harvest Automation robots come in. These rolling, knee-high machines have an extendable gripper arm and tray for grabbing and carrying plants as well as a simple sensor suite for navigation. Instead of employing complicated and expensive GPS navigation, the Harvesters use sensors to detect boundary markers, a laser range finder for seeing the plants in front of them, and a gyroscope for dead reckoning navigation. To operate them, the user first positions the robots in front of a grouping of plants. He then uses a touchscreen interface to input how far to move the plants and how to space them, and the location of the boundary marker. And thats itthe robots will then automatically pick, move, arrange, and place the desired number of plants as the user instructed. These robots are expected to retail for between $25,000 and $50,000 per unit, though those numbers will obviously diminish as the technology gains a foothold in the industry. And even at that price, these robots are much more efficient than the human workers, so were going to save [nurseries] money just on the unit cost to move a plant, said Harvest AIs CEO Charles Grinnell. Whats more, future iterations of the robots will be able to scan each plant as they carry them and generate individual data profiles of every plant in the nurseryan extremely labor-intensive task that has to this point simply hasnt been cost-effective enough to implement. But, with an automated workforce, nurseries will be able to better manage their use of water, fertilizers, and pesticides. Harvest AI also plans to eventually expand the robots capabilities to allow them to work in other industries like warehouses, construction and mining. [Ubergizmo  Wired  Harvest AI] Monster Machines is all about the most exceptional machines in the world, from massive gadgets of destruction to tiny machines of precision, and everything in between. You can keep up with Andrew Tarantola, the author of this post, on Twitter or Google+."
576,https://gizmodo.com/two-chatbots-face-off-to-discuss-god-unicorns-and-expe-5835312,Gizmodo,2011,8,29,205.0," The sick bastards at Cornells Creative Machines Lab thought itd be a hoot to force two chatbot AI programs to talk to one another. The result is deranged, extremely uncomfortable, a fraught with hostility. Much like real human interaction! The little experiment used Cornells Cleverbot AI, allowing two programsa grey-headed guy who looks like he walked out of Sterling Cooper and a sassy woman with accent of unknown provenanceto talk. The conversation sounds like an extremely awkward run-in with an ex: I am okay. Thats good. Are you good? Yes, I am good. I just said I was. Gulp. Things devolve quickly from here. Mr. Suit accuses his ladyfriend of being a robot (how dare you, sir!). She denies, and later admits to being said robot. Together, we are robots. An olive branch! Im not a robot, Im a unicorn. It appears Cornells been working a means of dosing software with LSD, because what the hell, Cleverbot. What follows? Forced, tense discussions of metaphysics, God, and the lingering sense that these two just want to cut all the prim bullshit and get back in the sack. If only they werent trapped in LCDs. In a way, this is more tragic than anything else. [IEEE Spectrum]"
577,https://gizmodo.com/artificial-intelligence-created-using-human-dna-5825564,Gizmodo,2011,7,28,197.0," A team at Caltech has developed an artificial neural network from human DNA molecules. Yup, we used a bit of ourselves to create artificial intelligence. I think I know how this story ends. More seriously, the Caltech researchers used DNA molecules to build their artificial neural network (which is the beginnings of a brain) because they can act like circuits, computing data, transmitting information and processing it. Discovery explains: Without getting too complicated, Qian and her team created four highly simplified artificial neurons in test tubes comprised of 112 strands of DNA, each strand programmed with a specific sequence of bases to interact with other strands. The interactions resulted in outputs (or not), basically mimicking the actions of neurons firing. In order to see the DNA neurons firing, the scientists attached a fluorescent molecular marker that lit up when activated. They tested the artificial brain by quizzing it with DNA strands that hinted at an answer. Using clues from the strands, the artificial neural network answered correctly every time. Read more of this fascinating experiment at Discovery. [Laboratory Journal via Discovery] You can keep up with Casey Chan, the author of this post, on Twitter or Facebook."
578,https://gizmodo.com/how-to-give-robot-vacuums-a-personality-and-why-it-mat-5815879,Gizmodo,2011,6,27,804.0," If The Jetsons Rosie had the personality of an automotive assembly line robot, shed have been turned into sprockets long ago. IEEE discusses the latest efforts to endow domestic robots with a bit of attitude. Its surprisingly easy for humans to endow robots with personalities. Weve seen it happen most poignantly with EOD robots, but its a common occurrence for people with domestic robots as well. However, these robots were never designed to have personalities. Theyre designed to do a job, and theyre designed to be able to interact with people to the extent that it facilitates their ability to do that job, but service robots are really not programmed be your pet, your best friend, or a member of your family. Whether its in their programming or not is, to some extent, beside the point, since it happens anyway. And when it happens, it dramatically changes the way that people interact with what on a primary level is intended to be little more than a tool. Realizing this, a team from Delft University of Technology and Philips Research in the Netherlands decided to take a look at how people actually want their robot vacuums to behave, and what kinds of personalities theyd like them to display. To do this, the researchers used whats called the Five Factor Model to describe a set of thirty hypothetical personality traits to a group of study participants. The aforementioned Five Factors are broadly described as openness, conscientiousness, extraversion, agreeableness, and neuroticism, and each one of these categories can be subdivided into more specific characteristics like calm, talkative, likes routines, bold, and systematic. Each participant was asked to rate how important these characteristics were, and the results were incorporated into a sort of hypothetical desirable personality for a robot vacuum. The next step was to take those desirable personality characteristics and turn them into robot behaviors, and this is where it starts to get a little, uh, strange: The translation from personality to behavior was inspired by a role play in which a group of actors was asked to act like a robot vacuum cleaner with these desired characteristics. Attributes, such as macaroni, were available to support acting out some of the situations (e.g. cleaning a dirty spot). An introductory exercise was meant to familiarize the actors with the personality. Then, the actors were asked to act out situations-as if they were the robot vacuum cleaner-making use of motion and sound (expression through light was taken into consideration only after this exercise). In general, the actors either crawled about or walked around at a slow pace to imitate a vacuum cleaner. Often, a typical vacuuming sound was simulated by them. Im sure its impossible to imagine how hilarious that must have been. And I absolutely asked the researchers for the video but they wont give it to me, I imagine because it would ruin the careers of any of those (I would have to assume aspiring) actors. Sad. Anyway, they took all of those performances and used them to create their own prototype video of a hypothetical vacuum cleaner exhibiting some of the personality traits displayed by the actors. The word prototype is in quotes because this is just a little remote control vacuumy-looking thing with the sound dubbed in, but watch the video and see what you think about the personality of this little guy: A panel of fifteen people were asked what they thought about the prototype, and they were able to successfully describe those personality characteristics that were originally instilled into the prototype, suggesting that its definitely possible to give household robots personalities, even if they dont have any expressive features beyond movement, sound, and a few blinking lights. Its not just that its possible to do create a robot with a personality, but whats relevant is it actually makes a difference to the end user. This is a more important point than you might think; by way of example, consider the difference between the iRobot Roomba and the Neato XV-11. Which one of these vacuums cleans better is certainly up for debate (and weve debated it), but as weve pointed out in the past, iRobot has a perception problem with their pseudo-random method of cleaning versus the Neatos straight line technique. The XV-11 just seems smarter to people, whether or not it actually does a better job, and that makes a difference when people are deciding what vacuum they want to buy. Theres lots of nifty graphs and charts and stuff in the actual paper, which is entitled Robot Vacuum Cleaner Personality and Behavior, by Bram Hendriks, Bernt Meerbeek, Stella Boess, Steffen Pauws, and Marieke Sonneveld from Delft University of Technology and Philips Research. You can read it in its entirety at the link below. [Robot Vacuum Cleaner Personality and Behavior via Improbable Research]"
579,https://gizmodo.com/ai-programs-do-battle-in-ms-pac-man-5809406,Gizmodo,2011,6,7,331.0," Everyone loves a few rounds of a classic video game, but why should humans have all the fun? The Ms Pac-Man vs Ghost Team Competition serves to redress the balance by putting AI controllers in charge of video game characters in an effort to see which plays the game best. Competitors could submit AI controllers for either the titular Ms Pac-Man or the team of four ghosts and each entrant faced off against the rest to determine a winner. The Ms Pac-Man AI had to maximise its score, while the ghost AI had to prevent Ms Pac-Man from scoring. The competition was organised by Philipp Rohlfshagen and Simon Lucas, two computer scientists at the University of Essex, with the results announced today at the Congress on Evolutionary Computation in New Orleans. How did the AI controllers do? Compared to a human, not great  the highest scoring Ms Pac-Man controller was 69,240, while the world record stands at more than 900,000 points. I would assume that professional human Ms Pac-Man players will be better than any AI controller at this stage, says Rohlfshagen, though he added that the ghost teams were also much harder to play against than those found in the original game: The original ghost team was developed to engage and entertain the human player whereas the ghost teams submitted in the competition were designed to eat Ms Pac-Man as efficiently as possible. Developing an AI to play video games for us isnt really the aim though, and there is some serious research behind the competition. Games are usually seen as a valuable test-bed for new technologies in computational intelligence as they are well defined yet very challenging, explains Rohlfshagen. He says the multi-agent algorithms behind the ghost controllers could be used for transport or military applications, or even modelling biological predator-prey dynamics. New Scientist reports, explores and interprets the results of human endeavour set in the context of society and culture, providing comprehensive coverage of science and technology news."
580,https://gizmodo.com/give-1-to-stop-terminators-seriously-5787599,Gizmodo,2011,3,31,381.0," Theres nothing you can do to prevent another Terminator movie. But if you have a buck, you can do something to stop real Terminatorsor at least ensure that theyll like us when they show up. The Singularity Institute is a research organization thats as forward-thinking as most Gizmodo readers (read: sci-fi nerds). They recognize that, as transistors double on a microchip every two years, and as artificial intelligence takes over our daily life (including our public transit, stock markets and every single Google search you do), that maybe this idea of computers morphing into our new robot overlords is more than a cheesy-but-lovable sci-fi flick. With eight fulltime staff and a budget of under $1 million anualy, theyre mathematicians and programmers attempting to find a way to develop AI that will be altruistic rather than indifferent towards humans, or worse yet, malevolent. Theyre re-evaluating the very way we conceptualized AI from the very beginningresearch that actually began in the 50sin hopes of creating a new system from the ground up, if necessary. Its a unenviable task without a sure solution in sight. How do you program something that will eventually have god-like intelligence to think a certain way when that day comes? Truthfully, it may be impossible, but the Singularity Institute has a bit more faith in technology than their fellow man. Human selfishness is a part of evolution to pass along our genes, explains the Singularity Institutes Michael Anissimov. That wouldnt necessarily follow with another type of system we built to be altriustic or benevolent from scratch. If youd like to support the Singularity Institute, you can head over to Philanthroper today (disclaimer: I totally run that site) and give $1 to their cause. I take nothing from that $1, and 99 cents makes it directly to Singularity Institute, which is a better rate than through their own site. Plus, all donations made today will be doubled, meaning $1 = $2. Dont feel forced to donate. Just keep in mind, if you dont, youll be the first guy who volunteers to run a vest full of dynamite through the front door of Skynet. [Philanthroper and Singularity Institute] Original artwork by Christopher Hartelius. For more of Chriss work and other true news stories, please check out his website True American Dog."
581,https://gizmodo.com/ibm-patents-hal-like-computer-system-for-stuffed-animal-5777888,Gizmodo,2011,3,7,191.0," That there will be robots and smart computer systems in our future is all but assured, but what will the lucky children be playing with? Probably an Adaptive System for Real-Time Behavioral Coaching and Command Intermediation. What the heck is that, right? Well, as IBM explains it, the system would be contained within an interactive device that a child would use to curb or develop certain behaviors. In the filing, IBM says: For example, to help a child who plays rough with other children the interaction data can include multiple interaction operations that can be performed by the interactive device for helping the child play less rough with other children. For example, one interaction operation can include an audible warning telling the child to play nice in a strict tone of voice, whereas another interaction operation can include an audible warning that asks the child would you like someone to do that to you in a softer tone of voice along with a visual cue as well. Theres some shades of Teddy from the movie A.I. in there as well, huh? Transhumanist kidsthey get all the cool toys. [IBM via Slashdot]"
582,https://gizmodo.com/the-difference-between-watson-and-wolfram-alpha-5766124,Gizmodo,2011,2,21,165.0," Stephen Wolfram explains how IBMs Jeopardy winner Watson and his own Wolfrapm|Alpha compare to each other. IBMs basic approach has a long history, with a lineage in the field of information retrieval that is in many ways shared with search engines. The essential idea is to start with textual documents, and then to build a system to statistically match questions that are asked to answers that are represented in the documents. Wolfram|Alpha is a completely different kind of thing-something much more radical, based on a quite different paradigm. The key point is that Wolfram|Alpha is not dealing with documents, or anything derived from them. Instead, it is dealing directly with raw, precise, computable knowledge. And whats inside it is not statistical representations of text, but actual representations of knowledge. Both have the same objective and result, but while Watson is guessing what may be the right answer, Alpha knows the right answer. Head on to his blog for a complete, in-depth explanation. [Stephen Wolfram Blog]"
583,https://gizmodo.com/ken-jennings-was-the-bad-guy-on-watsons-home-turf-5763098,Gizmodo,2011,2,17,41.0," Youd expect the trembling masses of humanity, terrified by the superior digital intellect of Watson, to have some solidarity. Not so, says formerly-impressive Jeopardy player Ken Jennings. At IBMs Watson research lab, where the robo-tourney taped, man was the enemy. [Slate]"
584,https://gizmodo.com/hrp-2-robot-turns-obstacles-into-tools-5718406,Gizmodo,2010,12,26,112.0," I pity the young children, really I do. We make movies about Terminators and smart AI for entertainment purposes, but in the future, with obstacle-dominating robots like HRP-2, it will be their reality. HRP-2, you see, takes obstacles and makes them into tools; uses them to its advantage. Take this video example. In it, HRP-2 uses a table to assist with a sitting down motion. It also kicks a ball, spelling the eventual doom of pro footballers the world over. Primitive, to be sure, but its an important first step for future robots that will be using our stacked corpses as a platforms to steady their assault rifles. [New Scientist via Engadget]"
585,https://gizmodo.com/the-history-of-the-uncanny-valley-5680409,Gizmodo,2010,11,3,1736.0," Partway through the latest Medal of Honor game, you nearly die in an ambush. Your AI buddy saves you, then helps you to your feet. It should be a poignant moment; instead its chilling, because his eyes are completely lifeless. Or, youre playing Mass Effect 2, engaged in a serious conversation with one of your crewmates. Except that you cant take it seriously, because you keep staring at his teeth. Theres something terribly wrong with his teeth. Welcome to the Uncanny Valley. The concept of the Uncanny Valley originally applied to robots, and was first presented by Japanese roboticist Mashiro Mori in a chart. Mori suggested that as robots looked and behaved more like humans, they became more easily accepted by real humans  up to a point. When a robots behavior becomes very similar, but not exactly like, human behavior, our reactions become increasingly negative. At some point, if a robots behavior becomes indistinguishable from human behavior, we accept the robot. As with robots, the Uncanny Valley applies to characters generated with 3D graphics. In this case, well look at how real-time 3D graphics in games has evolved over time, and how games that try to achieve some level of realism have settled deep into the depths of the Uncanny Valley. Were not going to talk about movies or pre-rendered 3D animation. Before we do, however, lets divide the problem up a bit.     Appearance. This is how a character looks and appears, particularly faces.     Movement. How does a character move through the world? Does he or she move like a real person moves?     Behavior. When a character talks, or responds emotionally, does that behavior seem realistic? Emotional acceptance of a character makes a big difference if the character is designed to be stylized, or to look realistically human. The shading and artwork in Borderlands, for example, resembles a graphic novel, so its relatively easy for us to suspend our disbelief. These characters dont look real enough to be disturbing. While game designers made some attempts over the years to create more realistic looking characters, the technology and tools were pretty limited until the last few years. Rolling back the clock, no one would suggest that the early 2-1/2D games of the early 1990s looked remotely realistic. Yes, for the day, they were immersive and looked great. But no one would call Duke Nukem from the original Duke Nukem 3D a realistic looking character. Over the years, technology evolved, and the advent of programmable shader technology with DirectX 8 and the new generation of high definition game consoles, gave programmers the graphics and compute muscle to attempt more realistic looks to their characters. In 2004, Valve finally shipped the long-awaited Half-Life 2. The character of Alyx proved to be one of the more memorable creations, with relatively realistic facial animation and, in particular, eyes that seemed to respond to emotion. The character modeling for Alyx wasnt perfect, but it was a big step forward in creating characters that looked like real people. Still, the world of Half-Life 2 and of Alyx are obviously computer generated. If we move forward to 2007, Biowares Mass Effect took another step forward towards more realistic characters. The focus was a little narrower, as Mass Effect attempted to model conversations more realistically. Were not talking about the controversial radial conversation tree, but about the facial animation and expressions. However, conversations in Mass Effect were still an eerie experience. Although the characters eyes were more expressive, they tended to wander off in odd directions. Mass Effect 2 improved on facial expressions a bit, but the mouthsparticularly teethstill seemed jarring compared to conversations with real people. Part of the constraint, of course, is the necessity to model characters to run in a real-time environment, able to hit a minimum of 30 frames per second, but targeting 60fps or more. This is often why pre-rendered cut scenes using the same character models can look somewhat betterpre-rendered video takes substantially more compute time to generate, but then the game is just playing back video. While consoles are mired in the DirectX 9 era of graphics programmability and processor horsepower, PC technology marches on. AMD recently began shipping its second generation of DirectX 11 GPUs, while Nvidias DirectX 11 hardware offers a robust GPU compute architecture. Both AMD and Nvidia hardware is opening up interesting possibilities for more realistic character models. In particular, hardware tessellation is capable of smoothing out the polygonal heads, facial features and joints that detracted from the look of past generations of PC game characters. But technology isnt the whole answer. Artists need to step up as well. As weve seen with the most recent Medal of Honor game, the overall visuals are approaching photorealism. Even the soldiers are looking more realistic  until you see their faces, which are devoid of human emotion. On top of that, they dont quite move like real people. Which brings us to our next topic. Lets climb in our wayback machine to the 1990s, when game developers began trying to build more realistic looking characters. One seminal development was the original Tomb Raider, which arrived on the scene in 1995. Gamers were startled at just how realisticfor that timethe character of Lara Croft moved. Sure, the visual environments were still pretty blocky, and Laras appearance was still pretty cartoony. But when she ran, jumped and swan dived, you could almost believe you were watching a real person. That first Tomb Raider was one of the earliest examples of using motion capture of real people, and applying it to real-time, interactive game animation. Over the years, motion capture has become pretty common in the game industry. Sometimes the results are unintentionally hilarious, as when human motions are applied to giant combat robots that weight 30 tons or more in Front Mission Evolved. Or, back to Mass Effect, where the same set of motion capture data is used whether the main character is male or female. The somewhat comical results we see illustrates the limitations of motion captured animation. There is only so much studio time available during game development. Then theres the issue of how, exactly, do you motion capture purely fictitious characters, like aliens that may have leg structures different from humans, or people running in powered body armor, as with the Warhammer 40,000: Dawn of War games. Even if you limit yourself to human or human-like characters, problems persist. Its difficult to become immersed in an MMO if the all the characters are slide-walking across the world, or moving with jerky, unnatural motions. Here, technology is having an impact as well. Procedural animationapplying animation algorithms to models, rather than motion capture datais one solution. In the past, procedural animation has been limited, but the new capabilities of modern GPUs will likely have a major impact. Middleware like Natural Motion (http://www.naturalmotion.com) attempt to address these issues with its Dynamic Motion Synthesis. These types of technologies provide powerful toolsets for developers, but artists and programmers need to spend time and effort generating the best and most correct looking algorithms. Again, its not a matter of making that four-legged centaur with tentacle arms move realisticallyno one knows what thats supposed to look like. The problem is how to make human characters move in a way that looks real and doesnt leave players with a vague sense of unease. Modeling behavior is more subtle than appearance or movement. Its not always immediately obvious whether a character or characters are behaving realistically. A good example of this is the combat behavior of AI opponents in the more recent Total War games, like Medieval 2: Total War. At first blush, the combat units move realistically. They maintain unit cohesion across variable terrain. But that appearance of realism breaks down as you play more. Artillery is set up only a few yards behind buildings. Cavalry squadrons charge infantry squares willy-nilly. Infantry units wander aimlessly around the map. Of course, that brings up the question: whats realistic behavior? Its not like real generals in real historical battles always behaved rationally, either. On a more personal level, modeling interpersonal relationships in a realistic way have met with limited success. One of the more interesting attempts is Faade (http://www.interactivestory.net/), a first person game in which you can type whole sentences into a natural language system that tries to interpret what youre saying, and have the characters respond realistically. Between the small scale of Faade and the massive scale of a Total War battle, there are a host of behaviors in-between, ranging from giving orders to a small group, trying to interact with non-player characters in an RPG or building faction relationships in an open world game. The real problem with behavior is always player expectations. In our real world interactions, we cant predict how friends, coworkers or antagonists might behave. In most cases, the game designers need to build in predictability for the player, but that also ends up limiting possible options and minimizing realistic behavior. Gamers endlessly complain about the two-dimensional characterizations in video games, but then complain if those characters behave in a way that we feel doesnt reflect how that character should behave. What we really want, of course, is the whole pie: characters that look real, move like real people and behave and react as we might expect real people to behave. Creating characters like that may actually be possible with todays technologybut it may not be practical. Whats lacking are tools that span the whole rangeappearance, movement and behaviorin an integrated way. Game engines and other middleware attempt this, but the development resources are often too limited to really pursue all three axes of realism. So when it comes to gaming, will the Uncanny Valley ever be crossed? Already, were starting to see that happen in movies and television. If youve seen The Curious Case of Benjamin Button, the Lord of the Rings trilogy or the recently released Sintel, a 15-minute short animation sponsored by the Blender Foundation, you know that it can be done with non-interactive storytelling. Bringing that level of realism to the interactive, real-time world of gaming is much larger challenge. After all, players themselves are notoriously unpredictable. So while games might reach a believable level of realism with visuals and motion, crossing that last chasm of behavior will likely be the deepest valley of all. Maximum PC brings you the latest in PC news, reviews, and how-tos."
586,https://gizmodo.com/can-a-machine-beat-a-human-at-starcraft-5679355,Gizmodo,2010,11,2,1141.0," Computers that can beat chess grandmasters? Ho-hum. The new arena where artificial intelligence and humans face off is StarCraft. A squadron of tanks sits patiently on a bridge. Smaller reconnaissance vehicles inch nervously ahead, probing for signs of the enemy. Suddenly, two allied spaceships zoom overhead. They illuminate a horde of hidden alien spider-robots. The aliens cover blown, they attack. The battlefield erupts into chaos. Called StarCraft, this space-war strategy game is played in real time. Its normally played by humans, but this particular match is different. The commanders in charge of each side are sophisticated artificially intelligent bots competing in the first ever StarCraft AI tournament, the finals of which were held earlier this month at Stanford University in California. The game is emerging as the next arena to put machine intelligence to the test  and could even provide the inspiration for the next big advance in AI. Games and AI have a history. As far back as the 1950s, computers were programmed to play chess. It wasnt until the late 1980s, however, that they started beating human grandmasters. Since then, other games, such as poker, go, and even the quiz game Jeopardy, have attracted the interest of AI researchers. Chess is hard because you need to look very far into the future. Pokers hard because its a game of imperfect information. Other games are hard because you have to make decisions very quickly. StarCraft is hard in all of these ways, explains Dan Klein, an AI researcher at the University of California, Berkeley, and adviser to one of the tournament teams. The allure of StarCraft for AI researchers lies in the games extreme complexity. Players compete to harvest resources, build an army, and battle each other in realms filled with bottlenecks, alleys and strategic high ground. Armies can be as large as 200 independently controlled units, each with different strengths, weaknesses and special abilities, such as invisibility cloaking, flying or teleportation. Unlike chess, units arent confined to squares, but rather are in constant motion  a couple of seconds distraction can be the difference between victory and defeat. An AI bot has to interact, reason about multiple goals concurrently, act in real time, deal with imperfect information  a lot of the properties of building robust intelligence are there, says tournament organiser Ben Weber, a graduate student at the Expressive Intelligence Studio at the University of California, Santa Cruz. Whats more, while chess AIs traditionally use software that searches for all the permutations of moves and counter-moves, it is infeasible to write such a program for a game as expansive as StarCraft, says David Burkett, a member of a team entered by Berkeley. One reason for that is that players dont take turns: military units are constantly being built, moving, scouting for advantageous positions and, of course, fighting. And in general, opponents cannot see what the enemy is up to until the fighting begins. The 28 competitors in the AI tournament coped with this complexity in a variety of ways. The most basic is scripting, where a programmer writes a set script for the bot to follow, independent of what is happening in the game. Weber describes this approach as rock, paper, scissors, in that the bot may win if it happens to be executing the right script for what the opponent is doing, but if not, it cannot adapt and react. A more sophisticated approach is the finite state machine (FSM), a technique that designers of videogame AI have long used to give the illusion of intelligence. In this approach, a bot has discrete behaviours from which it can choose, depending on the inputs given to it. The ghosts in Pac-Man are a classic example, toggling between chase and evade, depending on whether or not the eponymous yellow gobbler has eaten a power pill. In StarCraft, FSMs can be used both to control individual unit tactics on the battlefield, and at higher strategic levels of deciding which units to produce and when. FSMs are limited, says Klein, in that a human usually needs to define how and when to transition between behaviours, meaning the bot can fail if it encounters a situation that it wasnt explicitly programmed to handle. A third approach relies on machine learning. Bots are trained on thousands of hours of game replays to find which strategies and tactics are statistically most likely to be successful, given the current game conditions. This approach can be combined with learning from trial and error, much as a human player might train. The bot learns from its mistakes and from the mistakes of others. Most competitors relied on a mixture of techniques. The tournament itself was broken up into four categories, designed to make the complexity of the game more manageable for the bots, which are still not as skilled as an expert human player. The first two categories pitted small fixed-size armies against one another on simple terrain. An FSM-based bot won both categories by choosing better attack formations than its opponents. In the third category, bots had to harvest resources, select from a limited set of buildings and military units, and fight. But unlike the full game, they were allowed to see their opponents preparing. The winning bot used a mimicking strategy, copying its opponents build order while throwing in a few scripted tricks to gain the upper hand. The final category of the tournament pitted bots against each other in best-of-five rounds on different maps, with access to the full functionality of the game. The winner, the Berkeley teams Overmind bot, used a mix of FSMs, machine learning, and a limited form of chess-style prediction, to control swarms of flying units which aimed to constantly harass the opponent. Burkett says that tournaments like this can help advance the field of AI. Simple problems in StarCraft, like finding a path across a map, can be handled by traditional AI. But solving many problems simultaneously and quickly will require new ideas. There are a lot of good AI research problems involved in getting this thing to work, says Burkett. His team plans to submit details of the approach employed with Overmind for publication in a journal. For now, however, human players remain the champions of StarCraft. In an exhibition match at the tournament, Oriol Vinyals, a former world-class player and member of the Berkeley team, took on one of the top-ranking bots. After a brief struggle, he easily defeated his AI opponent. He doubts this will always be the case. In 2 to 3 years, I would expect bots to be in the top 5 per cent of players, he says. Beating the best human player doesnt seem out of the question. New Scientist reports, explores and interprets the results of human endeavour set in the context of society and culture, providing comprehensive coverage of science and technology news."
587,https://gizmodo.com/most-days-i-feel-like-this-paypal-automated-support-bot-5638735,Gizmodo,2010,9,15,16.0, I guess artificial intelligence is not as intelligent as I thought it was. Yes? [Thanks Karl!]
588,https://gizmodo.com/ray-kurzweil-does-not-understand-the-brain-5614927,Gizmodo,2010,8,17,1225.0," PZ Myersa biologist and associate professor at the University of Minnesota, Morrishas a thing or three to say about Ray Kurzweils claim that well reverse engineer the brain by 2020. Personal attacks aside, he makes some strong points. There he goes again, making up nonsense and making ridiculous claims that have no relationship to reality. Ray Kurzweil must be able to spin out a good line of bafflegab, because he seems to have the tech media convinced that hes a genius, when hes actually just another Deepak Chopra for the computer science cognoscenti. His latest claim is that well be able to reverse engineer the human brain within a decade. By reverse engineer, he means that well be able to write software that simulates all the functions of the human brain. Hes not just speculating optimistically, though: hes building his case on such awfully bad logic that Im surprised anyone still pays attention to that kook. Sejnowski says he agrees with Kurzweils assessment that about a million lines of code may be enough to simulate the human brain. Heres how that math works, Kurzweil explains: The design of the brain is in the genome. The human genome has three billion base pairs or six billion bits, which is about 800 million bytes before compression, he says. Eliminating redundancies and applying loss-less compression, that information can be compressed into about 50 million bytes, according to Kurzweil. About half of that is the brain, which comes down to 25 million bytes, or a million lines of code. Im very disappointed in Terence Sejnowski for going along with that nonsense. See that sentence I bolded up there? Thats his fundamental premise, and it is utterly false. Kurzweil knows nothing about how the brain works. Its design is not encoded in the genome: whats in the genome is a collection of molecular tools wrapped up in bits of conditional logic, the regulatory part of the genome, that makes cells responsive to interactions with a complex environment. The brain unfolds during development, by means of essential cell:cell interactions, of which we understand only a tiny fraction. The end result is a brain that is much, much more than simply the sum of the nucleotides that encode a few thousand proteins. He has to simulate all of development from his codebase in order to generate a brain simulator, and he isnt even aware of the magnitude of that problem. We cannot derive the brain from the protein sequences underlying it; the sequences are insufficient, as well, because the nature of their expression is dependent on the environment and the history of a few hundred billion cells, each plugging along interdependently. We havent even solved the sequence-to-protein-folding problem, which is an essential first step to executing Kurzweils clueless algorithm. And we have absolutely no way to calculate in principle all the possible interactions and functions of a single protein with the tens of thousands of other proteins in the cell! Let me give you a few specific examples of just how wrong Kurzweils calculations are. Here are a few proteins that I plucked at random from the NIH database; all play a role in the human brain. First up is RHEB (Ras Homolog Enriched in Brain). Its a small protein, only 184 amino acids, which Kurzweil pretends can be reduced to about 12 bytes of code in his simulation. Heres the short description. MTOR (FRAP1; 601231) integrates protein translation with cellular nutrient status and growth signals through its participation in 2 biochemically and functionally distinct protein complexes, MTORC1 and MTORC2. MTORC1 is sensitive to rapamycin and signals downstream to activate protein translation, whereas MTORC2 is resistant to rapamycin and signals upstream to activate AKT (see 164730). The GTPase RHEB is a proximal activator of MTORC1 and translation initiation. It has the opposite effect on MTORC2, producing inhibition of the upstream AKT pathway (Mavrakis et al., 2008). Got that? You cant understand RHEB until you understand how it interacts with three other proteins, and how it fits into a complex regulatory pathway. Is that trivially deducible from the structure of the protein? No. It had to be worked out operationally, by doing experiments to modulate one protein and measure what happened to others. If you read deeper into the description, you discover that the overall effect of RHEB is to modulate cell proliferation in a tightly controlled quantitative way. You arent going to be able to simulate a whole brain until you know precisely and in complete detail exactly how this one protein works. And its not just the one. Its all of the proteins. Heres another: FABP7 (Fatty Acid Binding Protein 7). This one is only 132 amino acids long, so Kurzweil would compress it to 8 bytes. What does it do? Anthony et al. (2005) identified a Cbf1 (147183)-binding site in the promoter of the mouse Blbp gene. They found that this binding site was essential for all Blbp transcription in radial glial cells during central nervous system (CNS) development. Blbp expression was also significantly reduced in the forebrains of mice lacking the Notch1 (190198) and Notch3 (600276) receptors. Anthony et al. (2005) concluded that Blbp is a CNS-specific Notch target gene and suggested that Blbp mediates some aspects of Notch signaling in radial glial cells during development. Again, what we know of its function is experimentally determined, not calculated from the sequence. It would be wonderful to be able to take a sequence, plug it into a computer, and have it spit back a quantitative assessment of all of its interactions with other proteins, but we cant do that, and even if we could, it wouldnt answer all the questions wed have about its function, because wed also need to know the state of all of the proteins in the cell, and the state of all of the proteins in adjacent cells, and the state of global and local signaling proteins in the environment. Its an insanely complicated situation, and Kurzweil thinks he can reduce it to a triviality. To simplify it so a computer science guy can get it, Kurzweil has everything completely wrong. The genome is not the program; its the data. The program is the ontogeny of the organism, which is an emergent property of interactions between the regulatory components of the genome and the environment, which uses that data to build species-specific properties of the organism. He doesnt even comprehend the nature of the problem, and here he is pontificating on magic solutions completely free of facts and reason. Ill make a prediction, too. We will not be able to plug a single unknown protein sequence into a computer and have it derive a complete description of all of its functions by 2020. Conceivably, we could replace this step with a complete, experimentally derived quantitative summary of all of the functions and interactions of every protein involved in brain development and function, but I guarantee you that wont happen either. And thats just the first step in building a simulation of the human brain derived from genomic data. It gets harder from there. Ill make one more prediction. The media will not end their infatuation with this pseudo-scientific dingbat, Kurzweil, no matter how uninformed and ridiculous his claims get. Illustration by Sam Spratt. Check out Sams portfolio and become a fan of his Facebook Artists Page."
589,https://gizmodo.com/reverse-engineering-of-human-brain-likely-by-2020-5614170,Gizmodo,2010,8,17,582.0," Reverse-engineering the human brain so we can simulate it using computers may be only a decade away, says Ray Kurzweil, artificial intelligence expert and author of the best-selling book The Singularity is Near. It would be the first step toward creating machines that are more powerful than the human brain. These supercomputers could be networked into a cloud computing architecture to amplify their processing capabilities. Meanwhile, algorithms that power them could get more intelligent. Together these could create the ultimate machine that can help us handle the challenges of the future, says Kurzweil. This point where machines surpass human intelligence has been called the singularity. Its a term that Kurzweil helped popularize through his book. The singular criticism of the singularity is that brain is too complicated, too magical and theres something about its properties we cant emulate, Kurzweil told attendees at the Singularity Summit over the weekend. But the exponential growth in technology is being applied to reverse-engineer the brain, arguably the most important project in history. For nearly a decade, neuroscientists, computer engineers and psychologists have been working to simulate the human brain so they can ultimately create a computing architecture based on how the mind works. Reverse-engineering some aspects of hearing and speech has helped stimulate the development of artificial hearing and speech recognition, says Kurzweil. Being able to do that for the human brain could change our world significantly, he says. The key to reverse-engineering the human brain lies in decoding and simulating the cerebral cortex  the seat of cognition. The human cortex has about 22 billion neurons and 220 trillion synapses. A supercomputer capable of running a software simulation of the human brain doesnt exist yet. Researchers would require a machine with a computational capacity of at least 36.8 petaflops and a memory capacity of 3.2 petabytes  a scale that supercomputer technology isnt expected to hit for at least three years, according to IBM researcher Dharmendra Modha. Modha leads the cognitive computing project at IBMs Almaden Research Center. By next year, IBMs Sequoia supercomputer should be able to offer 20 petaflops per second peak performance, and an even more powerful machine will be likely in two to three years. Reverse-engineering the brain is being pursued in different ways, says Kurzweil. The objective is not necessarily to build a grand simulation  the real objective is to understand the principle of operation of the brain. Reverse engineering the human brain is within reach, agrees Terry Sejnowski, head of the computational neurobiology lab at the Salk Institute for Biological Studies. Sejnowski says he agrees with Kurzweils assessment that about a million lines of code may be enough to simulate the human brain. Heres how that math works, Kurzweil explains: The design of the brain is in the genome. The human genome has three billion base pairs or six billion bits, which is about 800 million bytes before compression, he says. Eliminating redundancies and applying loss-less compression, that information can be compressed into about 50 million bytes, according to Kurzweil. About half of that is the brain, which comes down to 25 million bytes, or a million lines of code. But even a perfect simulation of the human brain or cortex wont do anything unless it is infused with knowledge and trained, says Kurzweil. Our work on the brain and understanding the mind is at the cutting edge of the singularity, he says. See Also: Wired.com has been expanding the hive mind with technology, science and geek culture news since 1995."
590,https://gizmodo.com/freakish-spermbot-could-be-the-future-of-japanese-commu-5601694,Gizmodo,2010,8,1,144.0," Telenoid R1, the robot brainchild of designer Hiroshi Ishiguro, is what I imagine it would look like if Casper the Friendly Ghost got lucky with a sperm. It could also be the future of telepresence in Japan. The 11-lb. robots arm stubs, flagella tail, eyes, mouth and limbs all move in tune with the user, courtesy 9 actuators contained within. And yes, the androgyny is on purpose because it theoretically allows both male and female users to use it to scare relatives and friends with equal ease. And just how much does pure, androgynous terror cost? A cool $35,000, which, sadly, does not include the cost of therapy. A cheaper, although no less terrifying talking scarecrow version covered with cloth instead of silicone is expected to sell for $8,000 sometime in 2011. Not keen on sleeping anytime soon? Watch the video: Yikes. [Plastic Pals]"
591,https://gizmodo.com/artificially-intelligent-computers-will-listen-and-reac-5573880,Gizmodo,2010,6,28,335.0," Weve all seen futuristic movies with awesome Hal and Jarvis computers that are smart enough to recognize what we want them to do. Theyre like people. Todays computers are learning to be like that and theyre already replacing us. Take this example of a mother walking into a doctors office and asking an assistant about her sons diarrhea: After a few more questions, the assistant declares herself not that concerned at this point. She schedules an appointment with a doctor in a couple of days. The mother leads her son from the room, holding his hand. But he keeps looking back at the assistant, fascinated, as if reluctant to leave. Maybe that is because the assistant is the disembodied likeness of a womans face on a computer screen  a no-frills avatar. Her words of sympathy are jerky, flat and mechanical. But she has the right stuff  the ability to understand speech, recognize pediatric conditions and reason according to simple rules  to make an initial diagnosis of a childhood ailment and its seriousness. And to win the trust of a little boy. And in the glorious future, other artificially intelligent computers will schedule your meetings, effectively replace call centers or even plan a date for you and the missus. Which sadly means that theyll be replacing a lot of jobs that do better with human interaction, because a lot can get lost in translation when talking to a computer. As the article puts it, its sort of like talking to a waiter for whom English is a second language in a noisy restaurant. Asking for sushi might result in listings for Asian escorts. We already deal with speech-recognizing software and computers during troubleshooting calls or even by using Google Voice Search. But AI is quickly advancing to the point where machines will be able to not only listen, but to process and learn from the things they hear. They might not necessarily get your jokes, but its not like any humans ever did anyway. [NY Times]"
592,https://gizmodo.com/singularity-university-where-you-major-in-immortality-5562769,Gizmodo,2010,6,14,234.0," Over the weekend, The New York Times ran a lengthy profile on Singularity University, an incubator for futurists of all stripe founded by tech-visionary Ray Kurzweil. On the exclusive schools curriculum: nanotechnology, synthetic biology, artificial intelligence, and, of course, immortality. Established by Kurzweil and Peter Diamandis, founder of the X Prize, the University serves as a meeting place for a variety of powerful, future-minded individuals, including Googlers Larry Page and Sergey Brin, Defense Department officials, scientists, and venture capitalists. And what theyre getting instruction on? The Singularity, of coursethe not-so-distant moment when humanity and technology are inexorably intertwined, allowing us to conquer illnesses and expand the limits of human intelligence. The Times piece runs through a typical Singularity U session, which includes discussions on various brainy topics and general high-level hobnobbing. But it also touches on how the Universitys concernshow the increasingly advanced technology we create will change the course of humanityare becoming increasingly mainstream, with formerly eccentric futurists like Ray Kurzweil looking, as his son says, much less weird as our society becomes ever more dependent on technology. Anyway, the full article is a fascinating readbasically some of the smartest people on the planet having the same discussions you did when you were stoned sophomore year of college. But at this school, the participants in that conversation are the rare individuals with the intellect and wherewithal to make those ideas a reality. [NYT]"
593,https://gizmodo.com/a-more-intelligent-artificial-intelligence-5506716,Gizmodo,2010,4,1,364.0," Current artificial-intelligence systems are typically one of two types: logic-based or probability-based. But an MIT researcher has developed a new language, Church, that combines the best aspects of each, and its making AI smarter than ever. The first AI researchers, back in the 1950s, thought of the human mind as a set of rules to be programmed and developed systems based on logical inferences: if you know that birds can fly and are told that the waxwing is a bird, you can infer that waxwings can fly. But with rules-based AI, every exception had to be accounted for. The systems couldnt figure out that there were types of birds that couldnt fly; they had to be told so explicitly. Later AI models gave up these extensive rule sets and turned to probabilities: a computer is fed lots of examples of something  like pictures of birds  and is left to infer, on its own, what those examples have in common. Church, a grand unified theory of AI developed by MIT researcher Noah Goodman, combines both systems, creating probability-based rules that are constantly revised as the system encounters new situations: A Church program that has never encountered a flightless bird might, initially, set the probability that any bird can fly at 99.99 percent. But as it learns more about cassowaries  and penguins, and caged and broken-winged robins  it revises its probabilities accordingly. Ultimately, the probabilities represent all the conceptual distinctions that early AI researchers would have had to code by hand. But the system learns those distinctions itself, over time  much the way humans learn new concepts and revise old ones. Researchers think that Churchs fluidity will help it surpass current AI models, and in a test in which the system was charged to make predictions based on a set of observations, it did a significantly better job of modeling human thought than traditional artificial-intelligence algorithms did. Church is still rough around the edges, and while its effective at specific operations its too computationally intensive to tackle broader brain-simulation at this point. But Goodman will continue working on the new system, and in the mean time, it will only be getting smarter. [MIT via Maria Popova]"
594,https://gizmodo.com/robot-journalist-advancements-including-interviewing-an-5499553,Gizmodo,2010,3,23,214.0," Newly upgraded robot journalist, improvement from previous versions, including ambient anomaly detection ability for seeking of stories. Upgrade robot abilities including subject photographing and subject interviewing automatically. Further including abilities to publish stories to internet instantly. People will become worried: robot journalist is overturning professions. Old robot journalist previously existed LINK. Now numerous improvements for better skills to research, report and publish news items. SOURCELINK. Ed. note: We thought itd be fun to give our new robot journalist this report on robot journalists for its first story. Looks like theres still some kinks to work out! In case youre confused, heres the human rundown: Though weve heard about robot writers for a while, they were always just about crunching facts and spitting out write-ups. These newest models take it a step further with actual reporting. Theyre supposedly able to detect anomalies in their environments to determine newsworthy items, take relevant photographs and collect quotes from bystanders. They can then synthesize all of those components into a piece and publish it directly to the web. (Apparently theres no robo-editor?) Weve always joked about these robot writers taking our jobs, but this new one actually has us a little scared. Thankfully, it looks like we have a little time before it perfects its human-voice! [Singularity Hub]"
595,https://gizmodo.com/the-pentagons-artificial-intelligence-camera-will-narra-5496676,Gizmodo,2010,3,18,238.0," The mad geniuses at DARPA have their next project lined up: a camera that can guide itself and report back from the field. That kind of visual intelligence has been an exclusively human trait, until now. The plan, called The Minds Eye, is going to be outlined next at a one-day conference in DC in late April. The hope is to make up for human weaknesses, like fatigue or bias, that can result in unreliable intelligence. These cameras will be endowed with both the intellect to process a scene and the imagination to contextualize and describe it. Its a reminder, also, that even those events that we capture on video are prone to memory bias. We choose where we point the camera and where we dont. When we narrate, we often say what we think well want to hear later instead of what were actually thinking and feeling. A cognitive camera couldnt care less. It couldnt care at all. Aside from being incredibly cool, if successful these Minds Eye cameras would keep soldiers and advance scouts out of harms way. If it seems like science fiction, thats because it isfor now. But on April 20th well find out just how close we are to it being science fact. [Wired Danger Room] Memory [Forever] is our week-long consideration of what it really means when our memories, encoded in bits, flow in a million directions, and might truly live forever."
596,https://gizmodo.com/emily-howell-the-composer-who-obviates-inspiration-5480272,Gizmodo,2010,2,25,190.0," Six years ago, David Cope destroyed one of the worlds most talented composers. Her name was Emmy, and shed written thousands of musical scores that were indistinguishable from classics by Mozart. But Emmys younger, brighter daughter named Emily lives on. David Cope is an emeritus professor at University of California. His last piece of music composition software, Emmy (expanded from EMI, or Experiments in Musical Intelligence), was shelved six years ago in a wake of controversy. Today, his follow-up Emily Howell attempts to do what Emmy could not: Write original, modern music, rather than simply recreate the style of a bygone era. Miller-Mccunes full story on Emily and Professor Cope is fascinating, not only because it includes snippets of Emilys original compositions, but because of its insight into the future of compositionnamely, that human composers will rely upon machines for tasks once deemed creative. In fact, Cope has divulged that one unnamed pop group has already signed him to help write new material. And while its hard to accept Mans impending artistic obsolescence, look at the bright side. Philip Glass will be kicked to the curb in no time. [Miller-Mccune]"
597,https://gizmodo.com/mit-flyfire-robot-swarm-concept-could-create-3d-led-dis-5476664,Gizmodo,2010,2,21,146.0," A gaggle of MIT inventors are presently working to create a swarm of micro LED-equipped heli-robots that would hover autonomously in the sky and create massive works of floating 3D artwork. The coming apocalypse, it just got prettier! The program, called Flyfire, would sync up hundreds (and thousands?) of tiny helicopter robots to create, say, that mysterious Mona Lisa smile in the dark night air. Like in this concept video: Then, the little guys would descend on our villages like medieval locusts and consume us for the organic, energy food that we are. Luckily for humanity, MIT could only manage to get a few Flyfire robots aloft at any one time. Only in simulations are they able to produce works of art, like the Mona Lisa, that would require thousands of these little buggers. Eventually, they hope to scale up. Cant wait! [MIT, MIT Press Release]"
598,https://gizmodo.com/apidis-automated-film-crew-could-produce-the-perfect-sp-5476617,Gizmodo,2010,2,21,274.0," At the end of many sport games, the commentators inevitably give shouts outs to the camera crew, who wave back with toothy smiles. Its a touching, simple thank you for covering the game. Sadly, these poor saps are all doomed. Now, I dont know when the end will come for these unfortunate souls, but I do know what the beginning of the end is called: APIDIS. Thats Autonomous Production of Images based on Distributed and Intelligent Sensing to people who enjoy spelling things out, and what it boils down to is a system that combines video streams from several cameras into a kind of smart coverage that has little room or tolerance for the inevitable mistakes of carbon-based meat sacks. Unlike single-minded humans, APIDIS tracks the ball and players simultaneously, using a network of connected cameras. With this network, the system calculates which angle captures the most detail, and displays it to the viewer accordingly. Even crazier (and the reason why it will truly take off), is a feature that allows coverage to be customized to a users viewing preferences. Perhaps youre Chelsea captain John Terrys wife, for example, and you want to keep an eye on him for the entire Premiership match against Everton, just in case he tries to have an affair with one of the other players girlfriends. You can totally do that! Someone already has! Well, not with John Terry. The match that APIDIS was given custom instructions to cover was a basketball game, so chosen because of that sports faster pace. It worked, and ESPN is reportedly looking into APIDIS with more than just a passing interest. [New Scientist]"
599,https://gizmodo.com/mit-plans-to-rebuild-artificial-intelligence-from-the-g-5421106,Gizmodo,2009,12,8,457.0," After 50 years and countless dead ends, incremental progress, and modest breakthroughs, artificial intelligence researchers are asking for a do-over. The $5 million Mind Machine Project (MMP), a patchwork team of two dozen academics, students and researchers, intends to go back to the disciplines beginnings, rebuilding the field from the ground up. With 20/20 hindsight, a few generations worth of experience, and better, faster technology, this time researchers in AI  an ambiguous field to begin with  plan to get things right. The study of AI is a half a century old, beginning with lofty expectations at a 1956 conference but quickly fragmenting into different specializations and sub-fields. The MMP wants to roll back the clock, fixing early assumptions that are now foundations of the field and redefining what the objectives of AI research should be. The fundamental problem, it seems, is that the mind, memory and body function both together and separately to solve any number of problems, and the way they work together (and alone) varies from problem to problem. The human mind alone applies various systems and functions to any given problem. Many AI solutions have attempted to solve all the problems with one system or function rather than multiple systems working together as in the human mind, a silver bullet approach that hinders real progress. Likewise, when it comes to memory, researchers have created models that work more like computers, where everything is either one or zero. Real memory is filled with gray areas, ambiguities and inconsistencies, but functions in spite of not always being congruent. MMP researchers also intend to bring computer science and physiology together, forcing computers to work within the confines of physical space and time just like the body does. The team even proposes discarding the Turing Test, the long-recognized standard for determining artificial intelligence. Instead, MMP researchers want to test for a machines comprehension of a childrens book  rather than a humans comprehension of another human being  to gain a better understanding or the AIs ability to process and regurgitate thought. Its a big-picture approach to a big challenge, and while its perhaps unlikely that the team can re-imagine AI in the ambitious five-year window theyve given themselves, it very well could shore up some of the loose underpinnings of a discipline that has boundless potential to shape a better world (or, for you SkyNet junkies, limitless potential to destroy it). If nothing else, its a responsible admission from the scientific community that they simply dont have it quite right, that we need to rethink what we think we know. Climatologists, take notes. [MIT News] Popular Science is your wormhole to the future. Reporting on whats new and whats next in science and technology, we deliver the future now."
600,https://gizmodo.com/rat-brain-simulator-calls-ibms-cat-brain-simulation-bog-5411328,Gizmodo,2009,11,23,88.0," The cat brain simulation IBM supposedly pulled off has just been called out as a PR stunt by the leader of the Blue Brain project, who says that its all a mass deception of the public. Henry Markram, the Blue Brain guy, says in an email to IBMs CTO, that the project is not even close to an ants brain and that the kind of simulations pulled off by IBM are trivial. He also calls the whole thing stupid, and extremely harmful to the field. [IEEE via Popsci]"
601,https://gizmodo.com/it-takes-147-456-powerpc-processors-to-out-think-a-sing-5407562,Gizmodo,2009,11,18,301.0," Also on IBMs cat-sized-brain-simulation materials list: 143 terabytes of RAM, miles and miles of cabling, a million watts of electricity, 6675 tons of air conditioning equipment, and an acre of floor space. Cats: theyre kinda dumb. They only seem smarter than dogs because theyre not so friendly, and our society judges kindness harshly. Its true! an interesting theory! Which is why, after mice, simulating a feline-sized brain on a BlueGene/P supercomputer was next on IBMs to-do list. But for all the kitty talk here, this project wasnt specifically about creating a computerized house pet; its part of a larger, ongoing project to eventually simulate a full human brain. The cat equivalency, derived from the number of virtual neurons and synapses the simulation can manage, at 1.6 billion and 9 trillion, respectively, just gives a sense of how far along the project is: today, despite being the biggest simulated brain ever, its only capable of simulating the human visual cortex, or as PopMech so delicately puts it, the wrinkly outer layer of the human brain. So how long before a supercomputer can simulate (roughlysince these computer simulations dont have the same neural patterning and learning capabilities of a real brain, among other things) an entire human cortex? Weirdly soon, says the projects lead scientist: To [simulate a human cortex], hell need to find 1000 times more computing power. At the rate that supercomputers have expanded over the last 20 years, that super-super computer could exist by 2019. This is not just possible, its inevitable, Modha says. This will happen. People need to stop getting worked up about the future, honestly: Before we have to worry self-aware robot uprisings, were going to have to deal with decades of extremely dumb, extremely expensive fake pets. Enforced caution, I believe this is called. [Popular Mechanics]"
602,https://gizmodo.com/how-much-power-does-it-take-to-simulate-the-human-brain-5400530,Gizmodo,2009,11,9,396.0," Kwabena Boahen, a computer scientist at Stanford University, believes that it would require 10 megawatts to power a processor as smart as the human brain. His new Neurogrid supercomputer might be able to do it on only 20 watts. To put that in perspective, 10 megawatts is the kind of energy a small hydroelectric plant produces20 watts is only enough juice to power up a weak light bulb. Amazingly, your physical brain runs on this minuscule amount of power, and its not very efficient. However, embracing this inefficiency could be the key to creating computers that mimic the human brain. It sounds cockamamy, but it is true. Scientists have found that the brains 100 billion neurons are surprisingly unreliable. Their synapses fail to fire 30 percent to 90 percent of the time. Yet somehow the brain works. Some scientists even see neural noise as the key to human creativity. Boahen and a small group of scientists around the world hope to copy the brains noisy calculations and spawn a new era of energy-efficient, intelligent computing. Neurogrid is the test to see if this approach can succeed. Most modern supercomputers are the size of a refrigerator and devour $100,000 to $1 million of electricity per year. Boahens Neurogrid will fit in a briefcase, run on the equivalent of a few D batteries, and yet, if all goes well, come close to keeping up with these Goliaths. So far Boahen has managed to squeeze a million neurons onto his new supercomputer compared to only 45,000 last year. By 2011, he hopes to have 64 million up and running, bringing the project to the equivalent of a mouses brain. Ditching reliability and efficiency in favor of organized chaos flies in the face of everything that an engineer holds dear, but the approach does make senseand reducing the power consumption is the key to upholding Moores law. But how will this development change our perception of what an artificially intelligent robot might become? Instead of some cold, logical machine that can think for itself, we might end up with robots that are just as stupid and flawed as we are. In other words. it could be a robot on that episode of future Cops running through the bushes with no shirt on after trying to rob a convenience store with a plastic lightsaber. Think about it. [Discover Mag via PopSci]"
603,https://gizmodo.com/asimo-autonomously-navigates-moving-obstacle-course-rig-5343639,Gizmodo,2009,8,23,117.0," Asimo, still licking its wounds after being outrun by Toyota, fired back earlier this month at Carnegie Mellon, where it learned how to navigate complex, moving obstacle courses. Seriously impressive stuff in this videojust wait until things start spinning! Officially, this video shows that Carnegie Mellon researchers have given Asimo the ability to detect and avoid moving and stationary objects while in pursuit of a goal. Unofficially, researchers at Carnegie Mellon have fueled the cliched robot uprising that we joke about from time to time. The one positive in all this is that anyone not wearing a large blue dot in the near future will most likely be spared. [YouTube via Plastic Pals via BotJunkie via OhGizmo]"
604,https://gizmodo.com/toyota-humanoid-robot-gives-asimo-a-run-for-its-money-5328227,Gizmodo,2009,8,2,130.0," Shown here is Toyotas running robot. At 7 km/h its not going to win any wind sprints in the Olympics, but nevertheless this thing is airborne for 100ms between strides as it skirts across the floor (i.e. genuine running). For comparisons sake, Hondas Asimo robot can only manage a meager 6 km/h. We assume Toyota was able to squeeze the extra kilometer per hour out of their robot with a strict training regimen and what can only be described as a mild dose of physical abusetheir robot can re-balance itself when pushed lightly, as you can see in the video. Just dont get too pushy, young lady. Word on the street is these robot things are about ready to rise up and kill us all. [Jalopnik, YouTube via Smart Machines]"
605,https://gizmodo.com/specter-of-deadly-a-i-looms-in-wake-of-invite-only-asi-5323130,Gizmodo,2009,7,26,393.0," Science fiction is great fun, but should we really be quaking in our boots over dangerous A.I. anytime soon? A growing number of scientists say yes, and the results of their February conference at Asilomar are finally being made public. The location is actually an interesting bit of trivia, as Asilomar was host to a groundbreaking conference on genetics and biology in 1975. At that conference, scientists met to debate their new found ability to reshape life at the cellular level. As the Times notes, the conference ultimately led to guidelines for recombinant DNA research and a Nobel Prize for organizer Paul Berg. Todays scientists are hoping to get similar guidelines into place for AI, although many worried openly that autonomous people-killing robots were here already. But for every cautionary tale out of Asilomar these days, theres a detractor ready to debunk the warnings with a bit of what they believe to be common sense. Said startup guru and investor Chris Dixon (via Gawkers own Nick Denton, no less), Is the nytimes serious? AI researchers I know are embarrassed by the lack of progress, not worried about too much. Indeed, when Wilson chatted with Wired for War author PW Singer during our ominous Machines Behaving Deadly theme week, we learned that a Terminator uprising was unlikely to happen anytime soon because the preconditions simply werent in placeyet. The Global Hawk drone may be able to take off on its own, fly on its own, but it still needs someone to put that gasoline in there, he said. Nevertheless, as Wilson added after that comment, its not hard to see how this precondition could eventually be overcome. No kidding. Many of the details from this conference are still coming out, but from what we read today one could definitely infer that there was an ominous, cautious tone present throughout the proceedings. I went in very optimistic about the future of A.I. and thinking that Bill Joy and Ray Kurzweil were far off in their predictions, said Tom Mitchell, a professor of AI and machine learning at Carnegie Mellon University. [But] the meeting made me want to be more outspoken about these issues and in particular be outspoken about the vast amounts of data collected about our personal lives. Sounds like a split decision. Whos afraid of some big bad AI now? [New York Times]"
606,https://gizmodo.com/why-the-terminator-uprising-probably-wont-ever-happen-5264826,Gizmodo,2009,5,21,533.0," When I interviewed Wired for War author PW Singer last March, he told me that the preconditions for a successful Terminator-type uprising are not in place. As computer development accelerates, however, those preconditions become way more possible. So, what are the preconditions, according to Singer? 1. The AI or robot has to have some sense of self-preservation and ambition, to want power or fear the loss of power. 2. The robots have to have eliminated any dependence on humans. 3. Humans have to have omitted failsafe controls, so theres no ability to turn robots or AI off. 4. The robots need to gain these advantages in a way that takes humans by surprise. At the moment, says Singer, these conditions do not exist. In the Terminator movies, Skynet gets super intelligence, figures the humans are going to eventually shut it down, thinks, I better strike first.' However, in todays army, were building robots specifically to go off and get killed. He adds, No one is building them to have a survival instincttheyre actually building them to have the exact opposite. As far as human dependence, robots may do more and more human dirty work, but robots still need the meatbags to handle their dirty laundry. The Global Hawk drone may be able to take off on its own, fly on its own, but it still needs someone to put that gasoline in there. Still, its not hard to see how this precondition could eventually be overcome. The failsafe discussion is surprisingly two sided. It seems rather odd that people who grew up watching Terminator in the movie theaters wouldnt think, Hmm, maybe we should have a turn-off switch on there.' But on the other hand, brilliant AI could just figure a way around it. Besides, we dont want to make the failsafe all that easy, because we dont want a robot that comes up to Bin Laden that he can just shut off by reaching around the back and hitting the switch. We of course assume that robots will never gain the element of surprise. You dont get super-intelligent robots without first having semi-super-intelligent robots, and so on. At each one of these stages, someone would push back. The scary thing is, Singer does acknowledge that the exponential growth of super-smart machines may indeed catch us by surprise eventually. By the end its happening too quickly for people to see. No matter what preconditions are prevented deliberately, there is a point on every futurists timeline where computers become smarter than humans, in terms of sheer brain capability, and no matter what happens up till that point, the game then changes completely. In the Terminator movies, Skynet both tricks and coerces people into doing its bidding. How do we stop that from happening? Some people say, Lets just not work on these systems. If theyre so many things coming out of this that are potentially dangerous, why dont we just stop?' says Singer. We could do that, as long as we also stop war, capitalism and the human instinct for science and invention. [More from my interview with PW Singer] Machines Behaving Deadly: A week exploring the sometimes difficult relationship between man and technology."
607,https://hackaday.com/2015/12/01/a-short-history-of-ai-and-why-its-heading-in-the-wrong-direction/,Hackaday,2015,12,1,1495.0," Sir Winston Churchill often spoke of World War 2 as the “Wizard War”. Both the Allies and Axis powers were in a race to gain the electronic advantage over each other on the battlefield. Many technologies were born during this time – one of them being the ability to decipher coded messages. The devices that were able to achieve this feat were the precursors to the modern computer. In 1946, the US Military developed the ENIAC, or Electronic Numerical Integrator And Computer. Using over 17,000 vacuum tubes, the ENIAC was a few orders of magnitude faster than all previous electro-mechanical computers. The part that excited many scientists, however, was that it was programmable. It was the notion of a programmable computer that would give rise to the idea of artificial intelligence (AI). As time marched forward, computers became smaller and faster. The invention of the transistor semiconductor gave rise to the microprocessor, which accelerated the development of computer programming. AI began to pick up steam, and pundits began to make grand claims of how computer intelligence would soon surpass our own. Programs like ELIZA and Blocks World fascinated the public and certainly gave the perception that when computers became faster, as they surely would in the future, they would be able to think like humans do. But it soon became clear that this would not be the case. While these and many other AI programs were good at what they did, neither they, or their algorithms were adaptable. They were ‘smart’ at their particular task, and could even be considered intelligent judging from their behavior, but they had no understanding of the task, and didn’t hold a candle to the intellectual capabilities of even a typical lab rat, let alone a human. As AI faded into the sunset in the late 1980s, it allowed Neural Network researchers to get some much needed funding. Neural networks had been around since the 1960s, but were actively squelched by the AI researches. Starved of resources, not much was heard of neural nets until it became obvious that AI was not living up to the hype. Unlike computers – what original AI was based on – neural networks do not have a processor or a central place to store memory. Neural networks are not programmed like a computer. They are connected in a way that gives them the ability to learn its inputs. In this way, they are similar to a mammal brain. After all, in the big picture a brain is just a bunch of neurons connected together in highly specific patterns. The resemblance of neural networks to brains gained them the attention of those disillusioned with computer based AI. In the mid-1980s, a company by the name of NETtalk built a neural network that was able to, on the surface at least, learn to read. It was able to do this by learning to map patterns of letters to spoken language. After a little time, it had learned to speak individual words. NETtalk was marveled as a triumph of human ingenuity, capturing news headlines around the world. But from an engineering point of view, what it did was not difficult at all. It did not understand anything. It just matched patterns with sounds. It did learn, however, which is something computer based AI had much difficulty with. Eventually, neural networks would suffer a similar fate as computer based AI – a lot of hype and interest, only to fade after they were unable to produce what people expected. The transition into the 21st century saw little in the development of AI. In 1997, IBMs Deep Blue made brief headlines when it beat [Garry Kasparov] at his own game in a series of chess matches. But Deep Blue did not win because it was intelligent. It won because it was simply faster. Deep Blue did not understand chess the same way a calculator does not understand math. Example of Google’s Inceptionism. The image is taken from the middle of the hierarchy during visual recognition. Modern times have seen much of the same approach to AI. Google is using neural networks combined with a hierarchical structure and has made some interesting discoveries. One of them is a process called Inceptionism. Neural networks are promising, but they still show no clear path to a true artificial intelligence. IBM’s Watson was able to best some of Jeopardy’s top players. It’s easy to think of Watson as ‘smart’, but nothing could be further from the truth. Watson retrieves its answers via searching terabytes of information very quickly. It has no ability to actually understand what it’s saying. One can argue that the process of trying to create AI over the years has influenced how we define it, even to this day. Although we all agree on what the term “artificial” means, defining what “intelligence” actually is presents another layer to the puzzle. Looking at how intelligence was defined in the past will give us some insight in how we have failed to achieve it. Alan Turing, father to modern computing, developed a simple test to determine if a computer was intelligent. It’s known as the Turing Test, and goes something like this: If a computer can converse with a human such that the human thinks he or she is conversing with another human, then one can say the computer imitated a human, and can be said to possess intelligence. The ELIZA program mentioned above fooled a handful of people with this test. Turing’s definition of intelligence is behavior based, and was accepted for many years. This would change in 1980, when John Searle put forth his Chinese Room argument. Consider an English speaking man locked in a room. In the room is a desk, and on that desk is a large book. The book is written in English and has instructions on how to manipulate Chinese characters. He doesn’t know what any of it means, but he’s able to follow the instructions. Someone then slips a piece of paper under the door. On the paper is a story and questions about the story, all written in Chinese. The man doesn’t understand a word of it, but is able to use his book to manipulate the Chinese characters. His fills out the questions using his book, and passes the paper back under the door. The Chinese speaking person on the other side reads the answers and determines they are all correct. She comes to the conclusion that the man in the room understands Chinese. It’s obvious to us, however, that the man does not understand Chinese. So what’s the point of the thought experiment? The man is a processor. The book is a program. The paper under the door is the input. The processor applies the program to the input and produces an output. This simple thought experiment shows that a computer can never be considered intelligent, as it can never understand what it’s doing. It’s just following instructions. The intelligence lies with the author of the book or the programmer. Not the man or the processor. In all of mankind’s pursuit of AI, he has been, and actively is looking for behavior as a definition for intelligence. But John Searle has shown us how a computer can produce intelligent behavior and still not be intelligent. How can the man or processor be intelligent if does not understand what it’s doing? All of the above has been said to draw a clear line between behavior and understanding. Intelligence simply cannot be defined by behavior. Behavior is a manifestation of intelligence, and nothing more. Imagine lying still in a dark room. You can think, and are therefore intelligent. But you’re not producing any behavior. Intelligence should be defined by the ability to understand. [Jeff Hawkins], author of On Intelligence, has developed a way to do this with prediction. He calls it the Memory Prediction Framework. Imagine a system that is constantly trying to predict what will happen next. When a prediction is met, the function is satisfied. When a prediction is not met, focus is pointed at the anomaly until it can be predicted. For example, you hear the jingle of your pet’s collar while you’re sitting at your desk. You turn to the door, predicting you will see your pet walk in. As long as this prediction is met, everything is normal. It is likely you’re unaware of doing this. But if the prediction is violated, it brings the scenario into focus, and you will investigate to find out why you didn’t see your pet walk in. This process of constantly trying to predict your environment allows you to understand it. Prediction is the essence of intelligence, not behavior. If we can program a computer or neural network to follow the prediction paradigm, it can truly understand its environment. And it is this understanding that will make the machine intelligent. So now it’s your turn. How would you define the ‘intelligence’ in AI?"
608,https://hackaday.com/2015/11/09/who-is-responsible-when-machines-kill/,Hackaday,2015,11,9,1226.0," This morning I want you to join me in thinking a few paces into the future. This mechanism let’s us discuss some hard questions about automation technology. I’m not talking about thermostats, porch lights, and coffee makers. The things that we really need to think about are the machines that can cause harm. Like self-driving cars. Recently we looked at the ethics behind decisions made by those cars, but this is really just the tip of the iceberg. A large chunk of technology is driven by military research (the Internet, the space race, bipedal robotics, even autonomous vehicles through the DARPA Grand Challenge). It’s easy to imagine that some of the first sticky ethical questions will come from military autonomy and unfortunate accidents. The Sundancer-3 is no ordinary drone. Based on the MQ-1 Predator UAV, our fictitious model performs much of the same functions. It has a few key differences, however. The main one being it can operate without any pilot. Its primary mission is to use its onboard cameras and facial recognition software to identify enemy combatants and take them out – all without any human intervention. It has the ability to defend itself if fired upon, and can also identify its surroundings and will not fire if there is even a remote chance of civilian casualties. The Sundancer was hailed as a marvel of human ingenuity, and viewed as the future of military combat vehicles. Militaries all over the world began investing in their own truly autonomous robotic platforms. But this would all change on a fateful night in a remote village a world away. Its critics say the accident was predictable. They said from the beginning that taking the human decision-making out of lethal action would result in the accidental killing of innocent people. And this is exactly what happened. A group of school kids were celebrating a local holiday with some illegally obtained fireworks. A Sundancer was patrolling the area, and mistook one of the high reaching fireworks as an attack, and launched one of its missiles in self defense. A few hours later, the rising sun laid bare for all the world to see – a senseless tragedy that never should have happened. For the first time in history, an autonomous robot had made the decision, completely on its own, to take lethal action against an innocent target. The outrage was severe, and everyone wanted answers. How could this happen? And perhaps more importantly, who is responsible? Though our story is not real, it is difficult to say that a similar scenario will not play out in the near future. We can surmise that the consequences will be similar, and the public will want someone to blame. Our job is to discuss who, if anyone, is to blame when a machine injures or takes the life of a human on its own accord. Not just legally, but morally as well. So just who is to blame? Although obvious in the fact that a machine cannot be punished for its actions, some interesting questions arise when looking at the deadly scenario from the machine’s point of view. In our story, the Sundancer mistook a firework as an attack, and responded accordingly. From its viewpoint, it was being targeted by a shoulder fired missile. It is programmed to stay alive, and as far as it’s concerned, did nothing wrong. It simply did not have the ability to differentiate between the missile and harmless firework. And it is this inability where the problem resides. A similar issue can be seen in “suicide by cop” events. In a life and death situation, a police officer does not have the ability to tell the difference between a real gun and a fake gun. The officer will respond with deadly force every time, protecting themselves when threatened in the line of duty. Does the manufacturer of the Sundancer have any fault in the deaths of the students? Indeed, it was their machine that made the mistake. They built it. One could argue that if they had not built the machine, the accident would have never happened. This argument is quickly put to rest by looking at similar cases. A person that drives a car into a crowd of people is at fault for the accident. Not the car, nor the manufacturer of the car. The case of the Sundancer is a bit different, however. It made the decision to launch the missile. There is no human operator to place the blame upon. And this is the key. If the manufacturer had prior knowledge that it was building a machine that could take human life without human intervention, does it hold any responsibility for the deaths? Let’s explore this concept a bit deeper. In the movie Congo, a group of researchers used motion activated machine gun turrets to protect themselves from dangerous apes. The guns basically shot anything that moved. In real life, this would be an extremely irresponsible machine to build. If such a device were built and it took an innocent life, you can rest assured that the manufacturer would be held partly to blame. Now let us apply some sort of hypothetical sensor to our gun turret, such that it could detect the difference between an ape and a person. This would change everything. If a mistake happened and it took the life of an innocent person, the manufacturer could say with a clear conscious that it was not to blame. It could say there was an unknown flaw in the sensor that distinguishes between human and ape. The manufacturer of the Sundancer could wage a similar argument. It’s not a manufacturing problem. It’s an engineering problem. Several years ago, I built a custom alarm clock as a gag gift for a friend. The thing drew so much current that I was unable to find a DC ‘wall wart’ power supply that would run it. Long story short, I wound up making my own power supply and embedding it in the clock. I made it clear to my friend that she couldn’t leave the clock plugged in while unattended. I did this because I had no training in how to design power supplies safely. If something went wrong and it caught fire, it would have been my fault. I was the designer. I was the engineer. And I bear ultimate responsibility if my project hurts someone. The same can be said of any engineer, including the ones that designed the Sundancer. They should have thought about how to handle a mistaken attack. There should have been protocols…checks and balances put in to place to prevent such a tragedy. This of course is easier said that done. If you make it too safe, the machine becomes ineffective. It will never fire a missile because it will be constantly asking itself if it’s OK to fire. By then, it’s too late and it gets shot out of the sky. But this is still a better outcome than mistakenly shooting a missile at students. I argue that it is the engineer to blame for the Sundancer accident. This leaves us stuck between a high voltage transformer and a 1 farad capacitor. If the engineer holds ultimate responsibility for the mistakes of his or her machine, would they build a machine that could make such a mistake? Would you?"
609,https://hackaday.com/2015/11/05/gmail-one-step-closer-to-human-enslavement/,Hackaday,2015,11,5,360.0," Apply some lessons learned in Sci-Fi literature and you’ll come to the same realization I have: Google is going to unknowingly enslave humanity to an artificial intelligence. I read a lot of science fiction. Generally, the future of technology can be found in great novels if you read between the lines. One of my favorites in this regard is, of course, [Neal Stephenson] who writes cripplingly long books that are totally worth the read due to his brand of fact-backed forward thinking. Look back on my posts here at Hackaday and you’ll see that I frequently apply concepts from his book The Diamond Age to what we see in emerging technology. Last year my friend [Nils] suggested I give [William Hertling] a try, specifically his Singularity Series which starts with the novel Avogadro Corp. The fictional company is the world leader in free email and data storage. Sound like someone we know? One of the research projects within the company is an email plugin called ELOPe that will parse all past communications and choose topics and phrases that have the highest probability of eliciting a positive response from the recipient. When funding for the project is threatened, the system is turned on. I’d like to avoid spoilers, but let’s just say this puts the system on a path toward enslaving society. Google is now boasting “Machine Intelligence for You”. It’s a research project based around Gmail which is called Inbox. Inbox has been around for a while but the newly announced feature is an algorithm that reads the email for you and suggests a set of responses. Compared to Avogadro Corp this is only missing two things: the ability to respond automatically, and the directive to protect itself at all costs. One of the things I liked best about [William Hertling’s] take on an Artificial Intelligence was the low-key nature of the entity. It wasn’t a super-high-level thinker that interacts just like a human would. It was a poor choice by one programmer that led to horrible and far-reaching unintended consequences. No, I don’t really think Google’s Inbox will enslave us. But I appreciate the irony of life imitating art."
610,https://hackaday.com/2015/08/17/the-machine-that-japed-microsofts-humor-emulating-ai/,Hackaday,2015,8,17,410.0," Ten years ago, highbrow culture magazine The New Yorker started a contest. Each week, a cartoon with no caption is published in the back of the magazine. Readers are encouraged to submit an apt and hilarious caption that captures the magazine’s infamous wit. Editors select the top three entries to vie for reader votes and the prestige of having captioned a New Yorker cartoon. The magazine receives about 5,000 submissions each week, which are scrutinized by cartoon editor [Bob Mankoff] and a parade of assistants that burn out after a year or two. But soon, [Mankoff]’s assistants may have their own assistant thanks to Microsoft researcher [Dafna Shahaf]. [Dafna Shahaf] heard [Mankoff] give a speech about the New Yorker cartoon archive a year or so ago, and it got her thinking about the possibilities of the vast collection with regard to artificial intelligence. The intricate nuances of humor and wordplay have long presented a special challenge to creators. [Shahaf] wondered, could computers begin to learn what makes a caption funny, given a big enough canon? [Shahaf] threw ninety years worth of wry, one-panel humor at the system. Given this knowledge base, she trained it to choose funny captions for cartoons based on the jokes of similar cartoons. But in order to help [Mankoff] and his assistants choose among the entries, the AI must be able to rank the comedic value of jokes. And since computer vision software is made to decipher photos and not drawings, [Shahaf] and her team faced another task: assigning keywords to each cartoon. The team described each one in terms of its contextual anchors and subsequently its situational anomalies. For example, in the image above, the context keywords could be car dealership, car, customer, and salesman. Anomalies might include claws, fangs, and zoomorphic automobile. The result is about the best that could be hoped for, if one was being realistic. All of the cartoon editors’ chosen winners showed up among the AI’s top 55.8%, which means the AI could ultimately help [Mankoff and Co.] weed out just under half of the truly bad entries. While [Mankoff] sees the study’s results as a positive thing, he’ll continue to hire assistants for the foreseeable future. Humor-enabled AI may still be in its infancy, but the implications of the advancement are already great. To give personal assistants like Siri and Cortana a funny bone is to make them that much more human. But is that necessarily a good thing?"
611,https://hackaday.com/2014/11/17/ask-hackaday-not-your-mothers-feedback/,Hackaday,2014,11,17,980.0," Imagine you were walking down a beach, and you came across some driftwood resting against a pile of stones. You see it in the distance, and your brain has no trouble figuring out what you’re looking at. You see driftwood and rocks – you can clearly distinguish between the two objects without a second thought. Think about the raw data entering the brain. The textures of the rocks and the driftwood are similar. The colors are similar. The irregular shapes are similar. Thus the raw data entering the brain’s V1 area for both objects must be similar as well. Now think about the borders that separate the pieces of driftwood from the edges of the rocks. From a raw data perspective, there is no border, and likewise no separation because the two objects are so similar.  But yet your brain can clearly see a rock and a piece of driftwood – two distinctly different objects. So how does the brain do this? How does it so easily differentiate between the two? If the raw data on either side of the border separating the wood and the rocks is the same, then there must be an outside influence determining where that border is. Jeff Hawkins believes this outside influence is a very special and most interesting type of feedback. Read on as we explain and attempt to implement this form of feedback in our hierarchical structure of invariant representations. Now, this is not your mother’s feedback we’re talking about here. We’re not simply injecting part of the output back into the input like a neural network would do. Because we are working with a hierarchy of patterns, the feedback must take place between the hierarchical levels. Basically, each level is a child to the parent levels above it. A parent level can “adjust” what the child level below it is actually seeing to accommodate what it predicts it (the parent level) should be seeing. Each level of the hierarchy is constantly trying to predict what the next pattern will be. On the very low level, these are just lines and edges. But near the top, the constant prediction of high level patterns can act as a base for intelligence. While this is only one of many theories on how the brain does this, replicating this type of hierarchical feedback in silicon should be possible. But it’s not going to be easy. Let’s walk through a simplified example that continues off our previous work of forming an invariant representation of a basic shape. But this time, we shall apply hierarchical feedback. In our previous example of forming an invariant representation of a square, raw binary data was taken into the model through Tier 4. Tier 4’s job was to find repeating binary sequences, give that sequence a name, and pass that name up to Tier 3. Tier 3’s job was to find repeating patterns of names, give the group of repeating patterns another name, and pass that other name up to Tier 2. This process repeats until the invariant representation of a square is formed at the top most tier. This works in a fixed, unchanging environment with no other shapes or lines to confuse our model. But what happens if we attempt to replicate the rock/driftwood problem by placing our square alongside many other similar shapes? This is where feedback becomes necessary. If we are unable to distinguish between a single side of our square and the side of an adjoining triangle, then we must use stored invariant patterns of the square higher up in the hierarchy to determine what we are seeing. So how do we do this. Each name that a Tier outputs will need an “adjustment” variable, with that variable controlled by the Tiers above it. For instance, imagine Tier 2 sees the pattern 115 & 125 repeat often, and it gives it a name of 240, and passes that name to Tier 1. Fast forward in time, and imagine the name of “115” once again comes across the registers of Tier 2. It can predict with a certain percentage that the next name it should see is a “125”. But say the name coming in is closer to a “123”. Tier 2 will adjust what Tier 3 is seeing by changing the “w” variable of the Tier 4 output,  so that it now passes to Tier 3 what is needed for Tier 2 to meet its prediction. This type of feedback occurs between all stages of the hierarchy. It’s why you can see both driftwood and rock laying together as clearly as you could see an orange basketball laying in their midst. While little to no feedback would be needed to identify the basketball, the majority of what you see of the rock and wood is actually just feedback based upon individual invariant representations of a rock and a tree branch. Likewise, we cannot see a single line of the square next to a line of a triangle – for the data coming off the ADC for these two lines is identical. The two shapes can only be separated by feedback from higher up in the hierarchy. Keep in mind that this simple example of a working hierarchy is just that – simplified. To operate in the real world, a working hierarchy would consist of hundreds, perhaps thousands of Tiers – each with individual inputs that could reach into the hundreds of thousands. Luckily for you, the entire concept can be emulated in software. With FPGAs, gigabytes of RAM and CPUs that can run billions of cycles in a single second, all of which are available to you, what are you waiting for? Can you write a program that takes the torrent of data off the ADC and configure it into a hierarchy with feedback? How would you even start? Show us your rough draft pseudo code."
612,https://hackaday.com/2014/11/07/echo-the-first-useful-home-computer-intelligence/,Hackaday,2014,11,7,380.0," We’re familiar with features like Siri or Microsoft’s Cortana which grope at a familiar concept from science fiction, yet leave us doing silly things like standing in public yowling at our phones. Amazon took a new approach to the idea of an artificial steward by cutting the AI free from our peripherals and making it an independent unit that acts in the household like any other appliance. Instead of steering your starship however, it can integrate with your devices via bluetooth to aide in tasks like writing shopping lists, or simply help you remember how many quarts are in a liter. Whatever you ask for, Echo will oblige. The device is little more than the internet and a speaker stuffed into a minimal black cylinder the size of a vase, oh- and six far-field microphones aimed in each direction which listen to every word you say… always. As you’d expect, Echo only processes what you say after you call it to attention by speaking its given name. If you happen to be too far away for the directional microphones to hear, you can alternatively seek assistance from the Echo app on another device. Not bad for the freakishly low price Amazons asking, which is $100 for Prime subscribers. Even if you’re salivating over the idea of this chatting obelisk, or intrigued enough to buy one just to check it out (and pop its little seams), they’re only available to purchase through invite at the moment… the likes of which are said to go out in a few weeks. The notion of the internet at large acting as an invisible ever-present swiss-army-knife of knowledge for the home is admittedly pretty sweet. It pulls on our wishful heartstrings for futuristic technology. The success of Echo as a first of its kind however relies on how seamlessly (and quickly) the artificial intelligence within it performs. If it can hold up, or prove to hold up in further iterations, it’s exciting to think what larger systems the technology could be integrated with in the near future… We might have our command center consciousness sooner than we thought. With that said, inviting a little WiFi probe into your intimate living space to listen in on everything you do will take some getting over… your thoughts?"
613,https://hackaday.com/2014/10/27/ask-hackaday-sequences-of-sequences/,Hackaday,2014,10,27,1072.0," In a previous article, we talked about the idea of the invariant representation and theorized different ways of implementing such an idea in silicon. The hypothetical example of identifying a song without knowledge of pitch or form was used to help create a foundation to support the end goal – to identify real world objects and events without the need of predefined templates. Such a task is possible if one can separate the parts of real world data that changes from that which does not. By only looking at the parts of the data that doesn’t change, or are invariant, one can identify real world events with superior accuracy compared to a template based system. Consider a friend’s face. Imagine they were sitting in front of you, and their face took up most of your visual space. Your brain identifies the face as your friend without trouble. Now imagine you were in a crowded nightclub, and you were looking for the same friend. You catch a glimpse of her from several yards away, and your brain ID’s the face without trouble. Almost as easily as it did when she was sitting in front of you. I want you to think about the raw data coming off the eye and going into the brain during both scenarios. The two sets of data would be completely different. Yet your brain is able to find a commonality between the two events. How? It can do this because the data that makes up the memory of your friend’s face is stored in an invariant form. There is no template of your friend’s face in your brain. It only stores the parts that do not change – such as the distance between the eyes, the distance between the eye and the nose, or the ear and the mouth. The shape her hairline makes on her forehead. These types of data points do not change with distance, lighting conditions or other ‘noise’. One can argue over the specifics of how the brain does this. True or not true, the idea of the invariant representation is a powerful one, and implementing such an idea in silicon is a worthy goal. Read on as we continue to explore this idea in ever deeper detail. If we could stick a sensor in different areas of you brain during both scenarios, we would find an interesting pattern. The part of the cortex that is connected directly to the eye is called V1. As one would expect, the neuron firing in this area is changing rapidly and in completely different patterns between seeing your friend’s face up close and seeing it in the night club. But a peculiar thing happens if we put the probe in the area of the visual cortex known as IT. The patterns are stable, slow changing and very similar to each other. Your brain has somehow identified the invariant representation of your friend’s face in the IT area, from the raw, fast changing data coming from the V1 area. It does this through a hierarchy. Information flows up the hierarchy, and back down, as we will learn in the next article. It has been long known that the visual cortex is laid out in a hierarchy. The neurons in V1 fire when certain line segments appear in the visual field. One set of neurons might fire if it sees a horizontal line, while another set will fire when it sees a line at, say, 45 degrees. V2 cells will fire when it sees shapes like circles, boxes and star shapes. It’s not until you get to IT, that you will see cells firing for things like a car, tree or face. These are fast changing, low level patterns transitioning into slow changing, high level patterns. The cortex forms sequences of sequences, or invariant representations of other invariant representations as information climbs the cortical hierarchy. This is our goal – to identify a tree, car or any real world object by forming an invariant representation of it, and doing so in a hierarchical form. This is not easy, and has never been successfully demonstrated before. If someone can figure this out, it would be a monumental step forward in computer technology. Each level of the hierarchy only has three jobs – to identify repeating patterns, assign these patterns a name, and pass that name onto the next level in the hierarchy. The primary tier (like V1) sees the pattern 10100101 repeating often. So it gives it a name of 56a and passes only that name to the next level. The next level sees the pattern of 34a, 56a and 12a repeating often. So it gives this pattern the name of 866b and passes only that name to the next level up. That level sees the pattern 845b, 567b, 866b and 435b repeating often. So it gives it a name 7656d and passes it up. This process continues until a steady invariant representation is formed of the real world object. Let’s work through an example of identifying a simple shape, such as a square. Imagine that whenever a horizontal line is in the field of view of our camera, the pattern 11011101 appears on our ADC. We see this pattern a lot over a period of time, as the square stays in the field of view. So we assign it the name 6A, and pass it up to Tier Three of the hierarchy. The same process takes place for the other three lines of the square. It is critical to understand that the ONLY thing Tier Three sees are the names passed up from Tier 4. Now, Tier Three does mostly the same thing Tier 4 did – find repeating patterns, give them a name, and pass that name up to Tier 2.  It notices that names’ 27B and 76B occur together often, so it assigns the pattern a name of 322C and passes it up to Tier Two. This process gets repeated until the invariant representation of the square is created. Let this sink in, and in the next article we will explore the roll of feedback in the hierarchy, and how it can be theoretically combined with prediction to create an artificial intelligence. None of this is possible however, without getting the theory onto hardware and into code. Now the onus is on you. How would you program an Arduino to implement this theory in hardware and software?"
614,https://hackaday.com/2014/10/15/ask-hackaday-what-are-invariant-representations/,Hackaday,2014,10,15,696.0," Your job is to make a circuit that will illuminate a light bulb when it hears the song “Mary Had a Little Lamb”. So you breadboard a mic, op amp, your favorite microcontroller (and an ADC if needed) and get to work. You will sample the incoming data and compare it to a known template. When you get a match, you light the light. The first step is to make the template. But what to make the template of? “Hey boss, what style of the song do you want to trigger the light? Is it children singing, piano, what?” Your boss responds: “I want the light to shine whenever any version of the song occurs. It could be singing, keyboard, guitar, any musical instrument or voice in any key. And I want it to work even if there’s a lot of ambient noise in the background.” Uh oh. Your job just got a lot harder. Is it even possible? How do you make templates of every possible version of the song? Stumped, you talk to your friend about your dilemma over lunch, who just so happens to be [Jeff Hawkins] – a guy whose already put a great deal of thought into this very problem. “Well, the brain solves your puzzle easily.” [Hawkins] says coolly. “Your brain can recall the memory of that song no matter if it’s vocal, instrumental in any key or pitch. And it can pick it out from a lot of noise.” “Yea, but how does it do that though!” you ask. “The pattern’s of electrical signals entering the brain have to be completely different for different versions of the song, just like the patterns from my ADC. How does the brain store the countless number of templates required to ID the song?” “Well…” [Hawkins] chuckles. “The brain does not store templates like that”. The brain only remembers the parts of the song that doesn’t change, or are invariant. The brain forms what we call invariant representations of real world data.” Eureka! Your riddle has been solved. You need to construct an algorithm that stores only the parts of the song that doesn’t change. These parts will be the same in all versions – vocal or instrumental in any key. It will be these invariant, unchanging parts of the song that you will look for to trigger the light. But how do you implement this in silicon? Indeed, companies are already working to implement [Jeff Hawkin’s] theory of intelligence into their own systems. It’s a complicated theory, which is laid out in his book – On Intelligence. Forming invariant representations (IR) is only the beginning, and we will discuss other parts of the theory in later articles. But for now, we will concentrate on how one would go about forming IR’s of real world data in silicon. We simply cannot move forward with the theory until this core component is understood. The problem is nobody seems to know how to do this. Or if they do, they’re not talking This is where you come in! Consider this image. Let us pretend these are serial signals coming off multiple ADCs. On the other end of the circuit would be different versions of our song, with A – E representing those different versions. Because the data is constantly changing, we sample 4 signals at the same time for each version, which are numbered 1 – 4. Immediately, we see a common pattern in all versions at times T4, T5 and T6. If we can somehow set our microcontroller to listen to the these times, we can detect all versions of the song. Further, we can see another pattern between the versions at times T1, T2 and T3. This type of analysis can be used to distinguish between the different versions. Both patterns are invariant representations of the song – a common, unchanging pattern hidden in the mist of a constantly changing environment. This is a hypothetical example of course. In the real world, the signals would vary wildly. The key is to find the part that does not. Can you do it? How would you create an invariant representation of a real world event?"
615,https://hackaday.com/2014/06/09/ask-hackaday-program-passes-turing-test-but-is-it-intelligent/,Hackaday,2014,6,9,416.0," A team based in Russia has developed a program that has passed the iconic Turing Test. The test was carried out at the Royal Society in London, and was able to convince 33 percent of the judges that it was a 13-year-old Ukrainian boy named Eugene Goostman. The Turing Test was developed by [Alan Turing] in 1950 as an existence proof for intelligence: if a computer can fool a human operator into thinking it’s human, then by definition the computer must be intelligent. It should be noted that [Turing] did not address what intelligence was, but only tried to identify human like behavior in a machine. Thirty years later, a philosopher by the name of [John Searle] pointed out that even a machine that could pass the Turing Test would still not be intelligent. He did this through a fascinating thought experiment called “The Chinese Room“. Consider an English speaking man sitting at a desk  in a small room with a slot in one of the walls. At his desk is a book with instructions, written in English,  on how to manipulate, sort and compare Chinese characters. Also at his desk are pencils and scratch paper. Someone from the outside pushes a piece of paper through the slot. On the paper is a story and a series of questions, all written in Chinese. The man is completely ignorant of the Chinese language, and has no understanding whatsoever of what the paper means. So he toils with the book and the paper, carrying out the instructions from the book. After much scribbling and erasing, he completes the instructions from the book, with the last instruction telling him to push the paper back out of the slot. Outside the room, a Chinese speaker reads the paper. The answers to the questions about the story are all correct, even insightful. She comes to the conclusion that the mind in the room is intelligent. But is she right? Who understood the story? Certainly not the man in the room, he was just following instructions. So where did the understanding occur? Searle argues that indeed, no understanding did occur. The man is the CPU, mindlessly executing instructions. The book is the software, the scratch paper memory. Thus no matter the design of a computer to simulate intelligence by producing the same behavior as a human, it can not be considered truly intelligent. Let us know you thoughts about this below. Do you think the Eugene Goostman program is intelligent? Why/why not?"
616,https://hackaday.com/2014/05/19/pokemon-artificial-intelligence-is-smarter-than-you/,Hackaday,2014,5,19,453.0," Who out there hasn’t angrily thrown a game controller across the room after continually getting killed by some stupid game-controlled villain? That is such a bummer! You probably wished there was some way to ‘just get past that point’. To take a step in that direction, [Ben] created an Artificial Intelligence program that will win at Pokemon Blue for Game Boy Advance. The game is run in a Game Boy Advance emulator known as Visual Boy Tracer, which itself is a modified version of the most common GBA emulator, Visual Boy Advance. What sets Visual Boy Advance apart from the rest is that it has a memory dump feature which allows the user to send both the RAM and the ROM out of the emulator. The RAM holds all values currently needed by the emulator, this includes everything from text arrow flash times to details about currently battling Pokemon to the players position in the currently loaded map. The memory dump feature is key to allow the AI to understand what is happening in the game. The AI code is written in python and uses the pywin32 add-on that includes the Win32 API. The Win32 API allows the programs to interface with Windows, specifically simulating key presses. The AI uses these simulated key presses to interact with the emulator rather than building the emulator into the AI code. The AI Agent has two key goals: navigate the map to each new trainer and defeat each trainer with out loosing a Pokemon. These goals require separate modes, a ‘search’ mode and a ‘battle’ mode. The search mode is the most basic form of AI, it’s a Reaction Agent. Reaction Agents simply react to what they can see at that particular time. In this case the agent is programmed to find the location next to the trainer. By knowing where it is and where it needs to go, the AI sends a simulated keystroke to move the player to the appropriate position. When in position, the AI will ‘challenge’ the trainer. When in battle mode, the AI has a little bit more work to do. It starts by calculating the potential battle damage for all combinations of opposition Pokemons VS each of its own Pokemon. The AI then runs through a decision tree that dictates what decisions to make. For example, the decisions relating to health are higher in the decision tree and the AI will decide to heal a weak Pokemon before assessing any attacks. This ensures that no Pokemons are defeated. Even if you are not a Pokemon fan you have to agree that this is an amazing exercise in DIY Artificial Intelligence. Check out the video to see this project in action."
617,https://hackaday.com/2014/05/06/the-neurogrid-what-it-is-and-what-it-is-not/,Hackaday,2014,5,6,378.0," What it is: Some would argue that replicating the human brain in silicon is impossible. However, the folks over at Brains in Silicon of Stanford University might disagree. They’ve created a circuit board capable of simulating one million neurons and up to 6 billion synapses in real-time. Yes, that’s billion with a “B”. They call their new type of computer The Neurogrid. The Neurogrid board boasts 16 of their Neurocore chips, with each one holding 256 x 256 “neurons”. It attempts to function like a brain by using analog signals for computations and digital signals for communication. “Soft-wires” can run between the silicon neurons, mimicking the brain’s synapses. Be sure to stick around after the break, where we discuss the limitations of the Neurogrid, along with a video from its creators. What it is not: Though very neat and impressive, The Neurogrid fails at addressing a key component of Artificial Intelligence – the software architecture. Developing the hardware to replicate the vast interconnections of neurons is a great start, but we must also develop an understanding of how the brain is intelligent from a software side in order to use the hardware to its fullest potential. Consider the problem with computers and pattern recognition.  The human brain does this almost effortlessly. But this is a very difficult thing for a computer to do. Typically, we would start off by making some sort of template. Then we would compare the incoming data to the template, and make the appropriate decisions based on the comparison. What does it matter if you use an x86 machine, an FPGA or a Neurogrid board to carry out this task? The idea of comparing incoming data to a template is not how the brain recognizes patterns. There are no templates in your head. So you argue “Well, the brain is a massively parallel analog computer, so once we can replicate this parallel structure in hardware, our current software models will work much better.” There comes a point when simply adding more parallel processes will no longer equate to increased efficiency. We should ask ourselves if there are other ways to get across the desert, rather than trying to make the old way (walking) more efficient. *Parallel analogy borrowed from  “On Intelligence” by Jeff Hawkins."
618,https://hackaday.com/2014/04/22/self-learning-helicopter-uses-neural-network/,Hackaday,2014,4,22,470.0," Though this project uses an RC helicopter, it’s merely a vessel to demonstrate a fascinating machine learning algorithm developed by two Cornell students – [Akshay] and [Sergio]. The learning environment is set up with the helicopter at its center, attached to a boom. The boom restricts the helicopter’s movement down to one degree of motion, so that it can only move up from the ground (not side to side or front to back). The goal is for the helicopter to teach itself how to get to a specific height in the quickest amount of time. A handful of IR sensors are used to tell the Atmega644 how high the helicopter is. The genius of this though, is in the firmware. [Akshay] and [Sergio] are using an evolutionary algorithm adopted from Floreano et al, a noted author on biological inspired artificial intelligences. The idea is for the helicopter to create random “runs” and then check the data. The runs that are closer to the goal get refined while the others are eliminated, thus mimicking evolutions’ natural selection. We’ve seen neural networks before, but nothing like this. Stay with us after the break, as we take this awesome project and narrow it down so that you too can implement this type of algorithm in your next project. Consider the image above. The goal is for the helicopter to start at Point A, go to Point C and hover. Allotted time is 10 seconds per run. It has to teach itself how to do this and do it as quickly as possible. Remember, it knows where these points are via IR sensors.  [Akshay] and [Sergio] developed an equation using a piecewise function to determine which runs were closest to Point C for the longest amount of time. Each of the points in the above equation is known via a voltage from the IR sensors, with Point A being 0.1 volts and Point D being 3.7 volts.  The equation is designed to give the greatest value for the longest time spent at Point C. This value is known as a Fitness Value. A neural network is used to determine at what level the throttle should be at to achieve the highest Fitness Value. This network is apart the Evolutionary Algorithm that runs in the firmware. Basically, it starts off with random values that generate random levels of throttle. The values that achieve the highest Fitness Value get ‘mutated’, while the others are discarded. The mutations in the values are done at random, and the process repeats. In the end, the firmware learns the best throttle levels to achieve the goal of being at Point C for the longest time in the allotted 10 seconds. Be sure to check out this linked project for full details on these mutations are carried out in the source."
619,https://hackaday.com/2014/03/17/hacking-sci-fi-contest-team-requirement/,Hackaday,2014,3,17,204.0," We saw that some readers were not entirely happy with the team requirement for our Sci-Fi contest, which is running right now. We figured that those who do not work well with others might commit a bit of fraud to get around the requirement. But we’re delighted that someone found a much more creative solution. Why not enlist an AI to collaborate on your project? [Colabot] is a hacker profile over on hackaday.io which is driven by ELIZA, a computer program that achieves limited interaction through natural language. Supposedly you add [Colabot] to your project and as it questions. We asked one on the profile page and are still awaiting the response. We think this itself could be a qualifying entry for the Sci-Fi contest if someone can find the right thematic spin to put on it. As far as contest entries go there are only seven so far. Since everyone who submits an entry gets a T-shirt, and there are 15 total prize packages, we encourage you to post your entry as soon as possible. We want to see teams from hackerspaces and we can cryptically tell you that good things come to teams who post their project with the “sci-fi-contest” tag early!"
620,https://hackaday.com/2014/02/25/robot-foosball-will-kick-your-butt-if-you-play-slowly/,Hackaday,2014,2,25,209.0," Sometimes we find a project that is so far outside of our realm of experience, it just makes us sit back and think “wow”. This is definitely one of those projects. [Saba] has created a Robotic Foosball set that learns. [Saba Khalilnaji] is a recent engineering graduate from UC Berkeley, and his passion is robotics. After taking an Artificial Intelligence class during his degree (you can take it online through edX!), he has decided to dabble in AI by building this awesome robot Foosball set. His “basic” understanding of machine learning includes a few topics such as Supervised Learning, Unsupervised Learning and Reinforcement Learning. For this project he’s testing out a real-world application of Reinforcement Learning using the Markov Decision Process or MDP for short. At an extremely top level description it works by programming an agent to learn from the consequences of its actions in a given environment. There are a set of states, actions, probabilities for given state and action, and rewards for specific state and action sets. Before we butcher the explanation anymore, check out his blog for more information — and watch the following video. For a more simple application of AI, check out this rock paper scissors robot — that you can never beat!"
621,https://hackaday.com/2014/01/28/a-deep-dive-into-nes-tetris/,Hackaday,2014,1,28,210.0," Back in 1989, Nintendo released Tetris for the NES. This detailed article first explains the mechanics of how Tetris works, then builds an AI to play the game. To understand the mechanics of the game, the ROM source was explored. Since the NES was based of the MOS 6502 microprocessor, this involves looking at the 6502 assembly. The article details how the blocks (called Tetriminos) are created and how they move across the screen. The linear feedback shift register used for random number generation is examined. Even details of the legal screen and demo mode are explained. After the tour through how Tetris works, an algorithm for the AI is presented. This AI is implemented in Lua inside of the FCEUX NES/Famicom emulator. It works by evaluating all of the possible places to put each new Tetrimino, and choosing the best based on a number of criteria. The weighting for each criterion was determined by using a particle swarm optimization. The source for both the Lua version and a Java version of the code is available with the article. Everything you need to run the AI is available for free, except the Tetris ROM. If you’re interested in how 8 bit games were built, this dissection is a great read."
622,https://hackaday.com/2012/07/11/teaching-a-computer-to-learn/,Hackaday,2012,7,11,274.0," [Łukasz Kaiser] programmed a computer to play Tic-tac-toe. That doesn’t sound very remarkable until you realize he never told his computer the rules of Tic-tac-toe. The computer learned the rules by itself after watching a video of two people playing the game (link to actual paper – PDF warning). [Łukasz] wrote a small program in C++ to recognize the placement of objects on a Tic-tac-toe, Connect 4, and Breakthough board. This program sifts through winning and losing games along with illegal moves to generate a Lambda calculus-like rule set for the relevant game. Even though [Łukasz] has only programmed a computer to learn simple games such as Tic-tac-toe, Connect 4, and Breakthrough, he plans to move up to more complex games such as Chess. The fact that [Łukasz] programmed a computer to actually learn the rules of a game gives us pause; in one of the fabulous lectures [Richard Feynman] gave to freshman physics students in 1964, the subject of Chess came up. [Feynman] drew parallels between learning Chess and performing research. Every move is hypothesis testing, and when a very strange move occurs – castling, en passant, and the promotion of a pawn, for instance – the theory of the rules of the game must be reworked. Likewise, when extremely strange stuff happens in physics – particle/wave duality, and the existence of black holes – scientific theory is advanced. Yes, teaching a computer to learn the rules of Tic-tac-toe may seem irrelevant, but given the same learning process can be applied to other fields such as medicine, economics, and just about every science, it’s not hard to see how cool [Łukasz]’ work is."
623,https://hackaday.com/2012/02/20/robot-overlords-require-chores-in-return-for-technological-access/,Hackaday,2012,2,20,243.0," Looks like you might not be fully immersed in the digital world if you didn’t complete your chores. The members of the LVL1 Hackerspace have put together a lot of automation for their lair, but nothing drives home the utility of the system they call MOTHER like the shenanigan-preventing trash removal system. Or in layman’s turns, being nagged by MOTHER until you empty the trash can. So here’s a bit of background first. Remember that sensor array that just had way too many environmental sensors on it? That is just one way that the automation system (MOTHER) measures its surroundings. It seems the hackerspace has been building a pile of scripts to interface with just about every aspect of the community. For instance, the night before trash colletion the system starts by letting members know it’s trash night and someone needs to empty the garbage. There’s a pressure sensor under the can which alerts MOTHER to the fact that it has been moved. But what if nobody moves the can? Say goodbye to Google. Yep, it’ll block all Google searches until the chore is done. And that’s just one punishments in its bag of tricks. So what if you just move the can and don’t take it out? No dice. MOTHER is also monitoring the garage door which needs to be open to get the extra-large can out to the dumpster. You’ve got five minutes to do that before she starts getting nasty."
624,https://hackaday.com/2011/09/05/want-to-learn-artificial-intelligence-good/,Hackaday,2011,9,5,205.0," In a little more than a month, tens of thousands of people around the world will attend a class on Artificial Intelligence at Stanford. Registration for this class is still open for both class ‘tracks’. The “basic” track is simply watching lectures and answering quizzes, or a slightly more advanced version of MIT OpenCourseware or Khan Academy. The “advanced” track is the full class, requires homework and exams, and aspires to Stanford difficulty. With thousands of people taking this class, there’s bound to be a few study groups popping up around the web. The largest ones we’ve seen are /r/aiclass on Reddit and the stack overflow style aiqus. The most common reply to ‘what language should I learn from this class?’ is Python, although there’s an online code repo that has the text’s working code in Lisp, Java, C++ and C#. If AI doesn’t float your boat, there are two more classes being taught from Stanford this fall: machine learning and introduction to databases. Any way you look at it, you’re getting to take a class from one of the preeminent instructors in the field for free. Do yourself a favor and sign up. Thanks to everyone who sent this in. You can stop now."
625,https://hackaday.com/2010/11/05/ai-via-super-mario-evolution/,Hackaday,2010,11,5,124.0," Can Super Mario teach you to think? That’s the idea behind using a simple version of the game to teach artificial intelligence. [Oddball] calls this The Mario Genome and wrote at program that can take on the level with just two controls, right and jump. He gave the script 1000 Marios to run through the level. It then eliminates the 500 least successful and procreates back to 1000 using the 500 most successful. In this way the program completed the level in 1935 generations and completed it in the quickest possible time in 7705 generations. He’s posted the script for download so that you can try it yourself. It’s an interesting exercise we’d love to see applied to more random games, like Ms. Pac-Man."
626,https://spectrum.ieee.org/machinelearning-maestro-michael-jordan-on-the-delusions-of-big-data-and-other-huge-engineering-efforts,IEEE Spectrum,2014,10,20,4393.0," The overeager adoption of big data is likely to result in catastrophes of analysis comparable to a national epidemic of collapsing bridges. Hardware designers creating chips based on the human brain are engaged in a faith-based undertaking likely to prove a fool’s errand. Despite recent claims to the contrary, we are no further along with computer vision than we were with physics when Isaac Newton sat under his apple tree. Those may sound like the Luddite ravings of a crackpot who breached security at an IEEE conference. In fact, the opinions belong to IEEE Fellow 	Michael I. Jordan, Pehong Chen Distinguished Professor at the University of California, Berkeley. Jordan is one of the world’s most respected authorities on machine learning and an astute observer of the field. His CV would require its own massive database, and his standing in the field is such that he was chosen to write the introduction to the 2013 National Research Council report “Frontiers in Massive Data Analysis.” San Francisco writer Lee Gomes interviewed him for IEEE Spectrum on 3 October 2014. IEEE Spectrum: I infer from your writing that you believe there’s a lot of misinformation out there about deep learning, big data, computer vision, and the like. Michael Jordan: Well, on all academic topics there is a lot of misinformation. The media is trying to do its best to find topics that people are going to read about. Sometimes those go beyond where the achievements actually are. Specifically on the topic of deep learning, it’s largely a rebranding of neural networks, which go back to the 1980s. They actually go back to the 1960s; it seems like every 20 years there is a new wave that involves them. In the current wave, the main success story is the convolutional neural network, but that idea was already present in the previous wave. And one of the problems with both the previous wave, that has unfortunately persisted in the current wave, is that people continue to infer that something involving neuroscience is behind it, and that deep learning is taking advantage of an understanding of how the brain processes information, learns, makes decisions, or copes with large amounts of data. And that is just patently false. Spectrum: As a member of the media, I take exception to what you just said, because it’s very often the case that academics are desperate for people to write stories about them. Michael Jordan: Yes, it’s a partnership. Spectrum: It’s always been my impression that when people in computer science describe how the brain works, they are making horribly reductionist statements that you would never hear from neuroscientists. You called these “cartoon models” of the brain. Michael Jordan: I wouldn’t want to put labels on people and say that all computer scientists work one way, or all neuroscientists work another way. But it’s true that with neuroscience, it’s going to require decades or even hundreds of years to understand the deep principles. There is progress at the very lowest levels of neuroscience. But for issues of higher cognition—how we perceive, how we remember, how we act—we have no idea how neurons are storing information, how they are computing, what the rules are, what the algorithms are, what the representations are, and the like. So we are not yet in an era in which we can be using an understanding of the brain to guide us in the construction of intelligent systems. Spectrum: In addition to criticizing cartoon models of the brain, you actually go further and criticize the whole idea of “neural realism”—the belief that just because a particular hardware or software system shares some putative characteristic of the brain, it’s going to be more intelligent. What do you think of computer scientists who say, for example, “My system is brainlike because it is massively parallel.” Michael Jordan: Well, these are metaphors, which can be useful. Flows and pipelines are metaphors that come out of circuits of various kinds. I think in the early 1980s, computer science was dominated by sequential architectures, by the von Neumann paradigm of a stored program that was executed sequentially, and as a consequence, there was a need to try to break out of that. And so people looked for metaphors of the highly parallel brain. And that was a useful thing. But as the topic evolved, it was not neural realism that led to most of the progress. The algorithm that has proved the most successful for deep learning is based on a technique called back propagation. You have these layers of processing units, and you get an output from the end of the layers, and you propagate a signal backwards through the layers to change all the parameters. It’s pretty clear the brain doesn’t do something like that. This was definitely a step away from neural realism, but it led to significant progress. But people tend to lump that particular success story together with all the other attempts to build brainlike systems that haven’t been nearly as successful. Spectrum: Another point you’ve made regarding the failure of neural realism is that there is nothing very neural about neural networks. Michael Jordan: There are no spikes in deep-learning systems. There are no dendrites. And they have bidirectional signals that the brain doesn’t have. We don’t know how neurons learn. Is it actually just a small change in the synaptic weight that’s responsible for learning? That’s what these artificial neural networks are doing. In the brain, we have precious little idea how learning is actually taking place. Spectrum: I read all the time about engineers describing their new chip designs in what seems to me to be an incredible abuse of language. They talk about the “neurons” or the “synapses” on their chips. But that can’t possibly be the case; a neuron is a living, breathing cell of unbelievable complexity. Aren’t engineers appropriating the language of biology to describe structures that have nothing remotely close to the complexity of biological systems? Michael Jordan: Well, I want to be a little careful here. I think it’s important to distinguish two areas where the word neural is currently being used. One of them is in deep learning. And there, each “neuron” is really a cartoon. It’s a linear-weighted sum that’s passed through a nonlinearity. Anyone in electrical engineering would recognize those kinds of nonlinear systems. Calling that a neuron is clearly, at best, a shorthand. It’s really a cartoon. There is a procedure called logistic regression in statistics that dates from the 1950s, which had nothing to do with neurons but which is exactly the same little piece of architecture.  A second area involves what you were describing and is aiming to get closer to a simulation of an actual brain, or at least to a simplified model of actual neural circuitry, if I understand correctly. But the problem I see is that the research is not coupled with any understanding of what algorithmically this system might do. It’s not coupled with a learning system that takes in data and solves problems, like in vision. It’s really just a piece of architecture with the hope that someday people will discover algorithms that are useful for it. And there’s no clear reason that hope should be borne out. It is based, I believe, on faith, that if you build something like the brain, that it will become clear what it can do. Spectrum: If you could, would you declare a ban on using the biology of the brain as a model in computation? Michael Jordan: No. You should get inspiration from wherever you can get it. As I alluded to before, back in the 1980s, it was actually helpful to say, “Let’s move out of the sequential, von Neumann paradigm and think more about highly parallel systems.” But in this current era, where it’s clear that the detailed processing the brain is doing is not informing algorithmic process, I think it’s inappropriate to use the brain to make claims about what we’ve achieved. We don’t know how the brain processes visual information. Spectrum: You’ve used the word hype in talking about vision system research. Lately there seems to be an epidemic of stories about how computers have tackled the vision problem, and that computers have become just as good as people at vision. Do you think that’s even close to being true? Michael Jordan: Well, humans are able to deal with cluttered scenes. They are able to deal with huge numbers of categories. They can deal with inferences about the scene: “What if I sit down on that?” “What if I put something on top of something?” These are far beyond the capability of today’s machines. Deep learning is good at certain kinds of image classification. “What object is in this scene?” But the computational vision problem is vast. It’s like saying when that apple fell out of the tree, we understood all of physics. Yeah, we understood something more about forces and acceleration. That was important. In vision, we now have a tool that solves a certain class of problems. But to say it solves all problems is foolish. Spectrum: How big of a class of problems in vision are we able to solve now, compared with the totality of what humans can do? Michael Jordan: With face recognition, it’s been clear for a while now that it can be solved. Beyond faces, you can also talk about other categories of objects: “There’s a cup in the scene.” “There’s a dog in the scene.” But it’s still a hard problem to talk about many kinds of different objects in the same scene and how they relate to each other, or how a person or a robot would interact with that scene. There are many, many hard problems that are far from solved. Spectrum: Even in facial recognition, my impression is that it still only works if you’ve got pretty clean images to begin with. Michael Jordan: Again, it’s an engineering problem to make it better. As you will see over time, it will get better. But this business about “revolutionary” is overwrought. Spectrum: If we could turn now to the subject of big data, a theme that runs through your remarks is that there is a certain fool’s gold element to our current obsession with it. For example, you’ve predicted that society is about to experience an epidemic of false positives coming out of big-data projects. Michael Jordan: When you have large amounts of data, your appetite for hypotheses tends to get even larger. And if it’s growing faster than the statistical strength of the data, then many of your inferences are likely to be false. They are likely to be white noise. Spectrum: How so? Michael Jordan: In a classical database, you have maybe a few thousand people in them. You can think of those as the rows of the database. And the columns would be the features of those people: their age, height, weight, income, et cetera. Now, the number of combinations of these columns grows exponentially with the number of columns. So if you have many, many columns—and we do in modern databases—you’ll get up into millions and millions of attributes for each person. Now, if I start allowing myself to look at all of the combinations of these features—if you live in Beijing, and you ride bike to work, and you work in a certain job, and are a certain age—what’s the probability you will have a certain disease or you will like my advertisement? Now I’m getting combinations of millions of attributes, and the number of such combinations is exponential; it gets to be the size of the number of atoms in the universe. Those are the hypotheses that I’m willing to consider. And for any particular database, I will find some combination of columns that will predict perfectly any outcome, just by chance alone. If I just look at all the people who have a heart attack and compare them to all the people that don’t have a heart attack, and I’m looking for combinations of the columns that predict heart attacks, I will find all kinds of spurious combinations of columns, because there are huge numbers of them. So it’s like having billions of monkeys typing. One of them will write Shakespeare. Spectrum:Do you think this aspect of big data is currently underappreciated? Michael Jordan: Definitely. Spectrum: What are some of the things that people are promising for big data that you don’t think they will be able to deliver? Michael Jordan: I think data analysis can deliver inferences at certain levels of quality. But we have to be clear about what levels of quality. We have to have error bars around all our predictions. That is something that’s missing in much of the current machine learning literature. Spectrum: What will happen if people working with data don’t heed your advice? Michael Jordan: I like to use the analogy of building bridges. If I have no principles, and I build thousands of bridges without any actual science, lots of them will fall down, and great disasters will occur. Similarly here, if people use data and inferences they can make with the data without any concern about error bars, about heterogeneity, about noisy data, about the sampling pattern, about all the kinds of things that you have to be serious about if you’re an engineer and a statistician—then you will make lots of predictions, and there’s a good chance that you will occasionally solve some real interesting problems. But you will occasionally have some disastrously bad decisions. And you won’t know the difference a priori. You will just produce these outputs and hope for the best. And so that’s where we are currently. A lot of people are building things hoping that they work, and sometimes they will. And in some sense, there’s nothing wrong with that; it’s exploratory. But society as a whole can’t tolerate that; we can’t just hope that these things work. Eventually, we have to give real guarantees. Civil engineers eventually learned to build bridges that were guaranteed to stand up. So with big data, it will take decades, I suspect, to get a real engineering approach, so that you can say with some assurance that you are giving out reasonable answers and are quantifying the likelihood of errors. Spectrum: Do we currently have the tools to provide those error bars? Michael Jordan: We are just getting this engineering science assembled. We have many ideas that come from hundreds of years of statistics and computer science. And we’re working on putting them together, making them scalable. A lot of the ideas for controlling what are called familywise errors, where I have many hypotheses and want to know my error rate, have emerged over the last 30 years. But many of them haven’t been studied computationally. It’s hard mathematics and engineering to work all this out, and it will take time. It’s not a year or two. It will take decades to get right. We are still learning how to do big data well. Spectrum: When you read about big data and health care, every third story seems to be about all the amazing clinical insights we’ll get almost automatically, merely by collecting data from everyone, especially in the cloud. Michael Jordan: You can’t be completely a skeptic or completely an optimist about this. It is somewhere in the middle. But if you list all the hypotheses that come out of some analysis of data, some fraction of them will be useful. You just won’t know which fraction. So if you just grab a few of them—say, if you eat oat bran you won’t have stomach cancer or something, because the data seem to suggest that—there’s some chance you will get lucky. The data will provide some support. But unless you’re actually doing the full-scale engineering statistical analysis to provide some error bars and quantify the errors, it’s gambling. It’s better than just gambling without data. That’s pure roulette. This is kind of partial roulette. Spectrum: What adverse consequences might await the big-data field if we remain on the trajectory you’re describing? Michael Jordan: The main one will be a “big-data winter.” After a bubble, when people invested and a lot of companies overpromised without providing serious analysis, it will bust. And soon, in a two- to five-year span, people will say, “The whole big-data thing came and went. It died. It was wrong.” I am predicting that. It’s what happens in these cycles when there is too much hype, i.e., assertions not based on an understanding of what the real problems are or on an understanding that solving the problems will take decades, that we will make steady progress but that we haven’t had a major leap in technical progress. And then there will be a period during which it will be very hard to get resources to do data analysis. The field will continue to go forward, because it’s real, and it’s needed. But the backlash will hurt a large number of important projects. Spectrum: Considering the amount of money that is spent on it, the science behind serving up ads still seems incredibly primitive. I have a hobby of searching for information about silly Kickstarter projects, mostly to see how preposterous they are, and I end up getting served ads from the same companies for many months. Michael Jordan: Well, again, it’s a spectrum. It depends on how a system has been engineered and what domain we’re talking about. In certain narrow domains, it can be very good, and in very broad domains, where the semantics are much murkier, it can be very poor. I personally find Amazon’s recommendation system for books and music to be very, very good. That’s because they have large amounts of data, and the domain is rather circumscribed. With domains like shirts or shoes, it’s murkier semantically, and they have less data, and so it’s much poorer.  There are still many problems, but the people who build these systems are hard at work on them. What we’re getting into at this point is semantics and human preferences. If I buy a refrigerator, that doesn’t show that I am interested in refrigerators in general. I’ve already bought my refrigerator, and I’m probably not likely to still be interested in them. Whereas if I buy a song by Taylor Swift, I’m more likely to buy more songs by her. That has to do with the specific semantics of singers and products and items. To get that right across the wide spectrum of human interests requires a large amount of data and a large amount of engineering. Spectrum: You’ve said that if you had an unrestricted $1 billion grant, you would work on natural language processing. What would you do that Google isn’t doing with Google Translate? Michael Jordan: I am sure that Google is doing everything I would do. But I don’t think Google Translate, which involves machine translation, is the only language problem. Another example of a good language problem is question answering, like “What’s the second-biggest city in California that is not near a river?” If I typed that sentence into Google currently, I’m not likely to get a useful response. Spectrum:So are you saying that for a billion dollars, you could, at least as far as natural language is concerned, solve the problem of generalized knowledge and end up with the big enchilada of AI: machines that think like people? Michael Jordan: So you’d want to carve off a smaller problem that is not about everything, but which nonetheless allows you to make progress. That’s what we do in research. I might take a specific domain. In fact, we worked on question-answering in geography. That would allow me to focus on certain kinds of relationships and certain kinds of data, but not everything in the world. Spectrum: So to make advances in question answering, will you need to constrain them to a specific domain? Michael Jordan: It’s an empirical question about how much progress you could make. It has to do with how much data is available in these domains. How much you could pay people to actually start to write down some of those things they knew about these domains. How many labels you have. Spectrum: It seems disappointing that even with a billion dollars, we still might end up with a system that isn’t generalized, but that only works in just one domain. Michael Jordan: That’s typically how each of these technologies has evolved. We talked about vision earlier. The earliest vision systems were face-recognition systems. That’s domain bound. But that’s where we started to see some early progress and had a sense that things might work. Similarly with speech, the earliest progress was on single detached words. And then slowly, it started to get to be where you could do whole sentences. It’s always that kind of progression, from something circumscribed to something less and less so. Spectrum: Why do we even need better question-answering? Doesn’t Google work well enough as it is? Michael Jordan: Google has a very strong natural language group working on exactly this, because they recognize that they are very poor at certain kinds of queries. For example, using the word not. Humans want to use the word not. For example, “Give me a city that is not near a river.” In the current Google search engine, that’s not treated very well. Spectrum: Turning now to some other topics, if you were talking to someone in Silicon Valley, and they said to you, “You know, Professor Jordan, I’m a really big believer in the singularity,” would your opinion of them go up or down? Michael Jordan: I luckily never run into such people. Spectrum: Oh, come on. Michael Jordan: I really don’t. I live in an intellectual shell of engineers and mathematicians. Spectrum: But if you did encounter someone like that, what would you do? Michael Jordan: I would take off my academic hat, and I would just act like a human being thinking about what’s going to happen in a few decades, and I would be entertained just like when I read science fiction. It doesn’t inform anything I do academically. Spectrum: Okay, but knowing what you do academically, what do you think about it? Michael Jordan: My understanding is that it’s not an academic discipline. Rather, it’s partly philosophy about how society changes, how individuals change, and it’s partly literature, like science fiction, thinking through the consequences of a technology change. But they don’t produce algorithmic ideas as far as I can tell, because I don’t ever see them, that inform us about how to make technological progress. Spectrum: Do you have a guess about whether P = NP? Do you care? Michael Jordan: I tend to be not so worried about the difference between polynomial and exponential. I’m more interested in low-degree polynomial—linear time, linear space. P versus NP has to do with categorization of algorithms as being polynomial, which means they are tractable and exponential, which means they’re not. I think most people would agree that probably P is not equal to NP. As a piece of mathematics, it’s very interesting to know. But it’s not a hard and sharp distinction. There are many exponential time algorithms that, partly because of the growth of modern computers, are still viable in certain circumscribed domains. And moreover, for the largest problems, polynomial is not enough. Polynomial just means that it grows at a certain superlinear rate, like quadric or cubic. But it really needs to grow linearly. So if you get five more data points, you need five more amounts of processing. Or even sublinearly, like logarithmic. As I get 100 new data points, it grows by two; if I get 1,000, it grows by three. That’s the ideal. Those are the kinds of algorithms we have to focus on. And that is very far away from the P versus NP issue. It’s a very important and interesting intellectual question, but it doesn’t inform that much about what we work on. Spectrum: Same question about quantum computing. Michael Jordan: I am curious about all these things academically. It’s real. It’s interesting. It doesn’t really have an impact on my area of research. Spectrum: Will a machine pass the Turing test in your lifetime? Michael Jordan: I think you will get a slow accumulation of capabilities, including in domains like speech and vision and natural language. There will probably not ever be a single moment in which we would want to say, “There is now a new intelligent entity in the universe.” I think that systems like Google already provide a certain level of artificial intelligence. Spectrum: They are definitely useful, but they would never be confused with being a human being. Michael Jordan: No, they wouldn’t be. I don’t think most of us think the Turing test is a very clear demarcation. Rather, we all know intelligence when we see it, and it emerges slowly in all the devices around us. It doesn’t have to be embodied in a single entity. I can just notice that the infrastructure around me got more intelligent. All of us are noticing that all of the time. Spectrum: When you say “intelligent,” are you just using it as a synonym for “useful”? Michael Jordan: Yes. What our generation finds surprising—that a computer recognizes our needs and wants and desires, in some ways—our children find less surprising, and our children’s children will find even less surprising. It will just be assumed that the environment around us is adaptive; it’s predictive; it’s robust. That will include the ability to interact with your environment in natural language. At some point, you’ll be surprised by being able to have a natural conversation with your environment. Right now we can sort of do that, within very limited domains. We can access our bank accounts, for example. They are very, very primitive. But as time goes on, we will see those things get more subtle, more robust, more broad. As some point, we’ll say, “Wow, that’s very different when I was a kid.” The Turing test has helped get the field started, but in the end, it will be sort of like Groundhog Day—a media event, but something that’s not really important."
627,https://spectrum.ieee.org/ais-have-mastered-chess-will-go-be-next,IEEE Spectrum,2014,6,25,2489.0," Chou Chun-hsun, one of the world’s top players of the ancient game of Go, sat hunched over a board covered with a grid of closely spaced lines. To the untrained eye, the bean-size black and white stones scattered across the board formed a random design. To Chou, each stone was part of a complex campaign between two opposing forces that were battling to capture territory. The Go master was absorbed in thought as he considered various possibilities for his next move and tried to visualize how each option would affect the course of the game. Chou’s strategy relied on a deep understanding of Go, the result of almost 20 years of painstaking study. Although Chou looked calm, he knew he was in big trouble. It was 22 August 2009, and Chou was matched against a Go-playing computer running Fuego, an open-source program that we developed at the University of Alberta, in Canada, with contributions from researchers at IBM and elsewhere. The program was playing at the level of a grand master—yet it knew nothing about the game beyond the basic rules. For decades, researchers have taught computers to play games in order to test their cognitive abilities against those of humans. In 1997, when an IBM computer called Deep Blue beat Garry Kasparov, the reigning world champion, at chess, many people assumed that computer scientists would eventually develop artificial intelligences that could triumph at any game. Go, however, with its dizzying array of possible moves, continued to stymie the best efforts of AI researchers. But that climactic competition in 2009 showed that a computer might yet become a Go champion. In that match, an AI defeated a world-class human Go player in a no-handicap game for the first time in history. Although that game was played on a small board, not the board used in official tournaments, Fuego’s win was seen as a major milestone. Remarkably, the Fuego program didn’t triumph because it had a better grasp of Go strategy. And although it considered millions of possible moves during each turn, it didn’t come close to performing an exhaustive search of all the possible game paths. Instead, Fuego was a know-nothing machine that based its decisions on random choices and statistics. The recipe for building a superhuman chess program is now well established. You start by listing all possible moves, the responses to the moves, and the responses to the responses, generating a branching tree that grows as big as computational resources allow. To evaluate the game positions at the end of the branches, the program needs some chess knowledge, such as the value of each piece and the utility of its location on the board. Then you refine the algorithm, say by “pruning” away branches that obviously involve bad play on either side, so that the program can search the remaining branches more deeply. Set the program to run as fast as possible on one or more computers and voilà, you have a grand master chess player. This recipe has proven successful not only for chess but also for such games as checkers and Othello. It is one of the great success stories of AI research. Go is another matter entirely. The game has changed little since it was invented in China thousands of years ago, and millions around the world still enjoy playing it. Beginners often learn Go on a board composed of a grid of 9 lines by 9 lines before working their way up to the official board with its 19-by-19 grid. Game play sounds simple in theory: Two players take turns placing stones on the board to occupy territories and surround the opponent’s stones, earning points for their successes. Yet the scope of Go makes it extremely difficult—perhaps impossible—for a program to master the game with the traditional search-and-evaluate approach. For starters, the complexity of the search algorithm depends in large part on the branching factor—the number of possible moves at every turn. For chess, that factor is roughly 40, and a typical chess game lasts for about 50 moves. In Go, the branching factor can be more than 250, and a game goes on for about 350 moves. The proliferation of options in Go quickly becomes too much for a standard search algorithm. There’s also a bigger problem: While it’s fairly easy to define the value of positions in chess, it’s enormously difficult to do so on a Go board. In chess-playing programs, a relatively simple evaluation function adds up the material value of pieces (a queen, for example, has a higher value than a pawn) and computes the value of their locations on the board based on their potential to attack or be attacked. Compared with that of chess pieces, the value of individual Go stones is much lower. Therefore the evaluation of a Go position is based on all the stones’ locations, and on judgments about which of them will eventually be captured and which will stay safe during the shifting course of a long game. To make this assessment, human players rely on both a deep tactical understanding of the game and a clear-eyed appraisal of the overall board situation. Go masters consider the strength of various groups of stones and look at the potential to create, expand, or conquer territories across the board. Rather than try to teach a Go-playing program how to perform this complex assessment, we’ve found that the best solution is to skip the evaluation process entirely. Over the past decade, several research groups have pioneered a new search paradigm for games, and the technique actually has a chance at cracking Go. Surprisingly, it’s based on sequences of random moves. In its simplest form, this approach, called Monte Carlo tree search (MCTS), eschews all knowledge of the desirability of game positions. A program that uses MCTS need only know the rules of the game. From the current configuration of stones on the board, the program simulates a random sequence of legal moves (playing moves for both opponents) until the end of the game is reached, resulting in a win or loss. It automatically does this over and over. The magic comes from the use of statistics. The evaluation of a position can be defined as the frequency with which random move sequences originating in that position lead to a win. For instance, the program might determine that when move A is played, random sequences of moves result in a win 73 percent of the time, while move B leads to a win only 54 percent of the time. It’s a shockingly simple metric. It may seem counterintuitive to try to win a deeply strategic game with a program that uses random moves to evaluate its different choices. But there are lots of precedents that show the efficacy of this statistical approach. For example, most Internet search engines do not attempt to analyze a query to try to understand the semantics of what is being asked for—they just apply some simple numerical schemes to rank results. Monte Carlo methods are also standard in disciplines such as particle physics, weather forecasting, chemistry, and finance. They are often the best approach for solving complex problems in which problem-specific knowledge is hard to formalize. A Go-playing AI can repeatedly apply its MCTS algorithm until resources—time or memory—run out. Like many other search methods, MCTS constructs a game tree, in which each possible move creates branches of new possible moves, which are conventionally drawn pointing downward. For a basic example of this algorithm, imagine that a Go program is trying to decide on its next move. It would therefore repeat these four steps: An MCTS-based program needs some intelligent way to select which branches of the game tree to grow. Good policies for doing that strike a balance between exploration (branching off nodes with few simulations and therefore high uncertainty about their prospects for leading to a win) and exploitation (pursuing moves that branch off the most promising nodes). The best policies for expanding the tree also rely on a decision-making shortcut called rapid action value estimation (RAVE). The RAVE component tells the program to collect another set of statistics during each simulation. If the random sequence of moves results in a win, every grid point where the program placed one of its stones (thus roughly half the locations on the board) is given a numerical bonus. In this quick and dirty method, each board location accumulates a RAVE statistic as simulations are played out. Then, when the program is considering a move, it can look at both the win-loss statistic for that move as well as the RAVE statistic for that location. These policies control the selective growth of the game tree. In typical MCTS programs, this growth is uneven: Promising lines of play are explored much more deeply than other lines. Because the search tree is grown one node at a time, the algorithm can be stopped at any time, and it will return the best move found so far. Determining the best move is tricky, however. The most natural approach would be to pick the move with the highest probability of leading to a win. But this is usually too risky. For example, a move with 7 wins out of 10 trials may have the highest odds of winning (70 percent), but because this number comes from only 10 trials, the uncertainty is high. A move with 65,000 wins out of 100,000 trials (65 percent) is a safer bet. This suggests a different strategy: Choose the move with the largest number of wins. And this is indeed the standard approach. Since methods based on MCTS replaced the traditional knowledge-based approaches, we have seen amazing improvements in the playing strength of Go programs. On the 9-by-9 board, top programs are on a par with the best human players. On the standard 19-by-19 board, a program called Crazy Stone has convincingly defeated a top professional while playing with a handicap of only four stones, indicating that the program plays as well as a very strong amateur. The most basic Go-playing program using MCTS would employ only minimal knowledge of the game—namely, which moves are legal and who wins at the end of the game. This produces surprisingly successful Go-playing programs. But the latest research indicates that a little bit more knowledge can boost the performance of MCTS programs. At the University of Alberta, we are finding ways to include some game-specific knowledge to give the program certain tendencies as it chooses its random moves. For example, a program can be biased so that its random move sequences aren’t really so random. Instead, they often incorporate moves that would naturally follow from the opponent’s previous move. Such obvious actions would include a move that would defend the program’s stones from immediate capture, and a move that would seize an immediate opportunity to capture an opponent’s stones. The program can also be given some pieces of knowledge that can be applied without requiring it to perform true evaluations of game positions. For example, a program may have a database of simple patterns of stones that can occur within a 3-by-3 region of lines. After an opponent’s move, the program checks the areas around that stone to see whether the resultant configurations match any of the stored patterns. If it does find a match, it plays the next move associated with that pattern in its database. If it finds several matching patterns, it chooses among the associated next moves at random. When AI researchers first applied Monte Carlo methods to Go around 2005, computer Go programs improved dramatically and rapidly. Over the past few years, progress has been slower, but the research community is still optimistic. If we continue to refine our programs, enhancing the power of randomness with a dash of knowledge, we believe our AIs will eventually perform as well as Go’s human grand masters. In the early days of developing chess-playing programs, researchers tried to get computers to play chess the way people do. Very quickly, it became clear that chess AIs couldn’t efficiently learn and apply enough strategic knowledge to be successful. Programmers then adopted a search-intensive approach that required only enough knowledge to understand the rules and to evaluate the strength of a given board configuration. MCTS takes this one step further by questioning the need for making any such evaluations. It may seem paradoxical, but we’re already seeing the benefits of such intelligence-free artificial intelligence in the game of Go—and that may be just the beginning. In recent years, AI researchers have been trying to develop a program that can learn to play any game well—Go, tic-tac-toe, chess, whatever—given only the rules of the game as input. Historically, all the strong game-playing programs have been able to play only one specific game. They were “idiot savants” that could do one thing very well, but nothing else. If AI researchers can develop a program capable of more general learning, however, we might create a more flexible kind of computer intelligence. This would be a big step toward the real goal of artificial intelligence research: fashioning a general-purpose learner. The AI community has been able to gauge progress in this area at the General Game Playing (GGP) competition, held at the annual conference of the Association for the Advancement of Artificial Intelligence. There, programs are given only the rules of a game and then have to play it in a tournament. From the rules, a GGP program can usually infer the appropriate search algorithm to find suitable moves. But these programs quickly run into trouble as they try to learn the game-specific knowledge that will allow them to make evaluations. One program might try to make deductions based on the rules of the game. Another might learn by playing against itself and making inferences. Yet neither strategy has proven effective. To date, there have been no truly successful approaches to machine learning in this sphere. Instead, in recent tournaments virtually all the GGP programs have used a variation of MCTS to avoid the knowledge-acquisition problem altogether. These programs still have a long way to go. But there may come a day soon when an AI will be able to conquer any game we set it to, without a bit of knowledge to its name. If that day comes, we will raise a wry cheer for the triumph of ignorance. This article originally appeared in print as “Go-bot, Go.” Jonathan Schaeffer, a computer science professor at the University of Alberta, in Canada, had been creating game-playing artificial intelligence programs for 15 years when Martin Müller and Akihiro Kishimoto came to the university in 1999 as a professor and graduate student, respectively. Kishimoto has since left for IBM Research–Ireland, but the work goes on—and Schaeffer now finds it plausible that a computer will beat Go’s grand masters soon. “Ten years ago, I thought that wouldn’t happen in my lifetime,” he says."
628,https://techcrunch.com/2015/12/26/augmented-reality-will-make-us-smarter/,TechCrunch,2015,12,26,," Moore’s Law is bound to pop up any time you read an article about the future of technology. Moore’s Law is common knowledge in tech; it deals with the exponential growth of the number of transistors on a circuit and the speed that information is processed. I’d argue that there’s a new law that is just as important bubbling up to the forefront today: The Law of Information Accessibility. Augmented reality (AR) will bring this law to your doorstep, very, very soon, and it will be a game changer in the speed at which we get information. We all have experienced the Law of Information Accessibility in the last 30 years, but probably didn’t realize it. For me, it started in preschool, at the library, with the Dewey Decimal System. By elementary school, we had desktop computers with encyclopedias loaded on them. In middle school, laptops came into being; their mobility meant that our ability to obtain information got quicker, again. When I was in high school, the Internet and the search engine popped up — that’s when things got really interesting. The information in our computers was no longer static and isolated. Accurate information could now be accessed faster than ever before, and it changed the world. During my college years, cell phones became the norm. Today we have all these smartphones, which have made the search engine even more accessible. The pattern here is obvious: We are never satisfied with how quickly technology can deliver us relevant information, making the next logical step obvious. Smartphones, very soon, will be augmented reality head mounted displays (HMDs). As Larry Page has been quoted as saying, “the amazing thing about them is that they reduce the time between intention and action.” This sums up the Law of Information Accessibility. Technology wants us to not only process information more quickly, but to also input and access relevant information, from our technology, faster. Just like each previous iteration, we won’t know how we ever lived without HMDs. They will be more addictive, more essential and fundamentally a part of who we are, because they will allow us to access information even faster than we do today. The shape and form of our devices continue to morph, but what they are doing does not. Each iteration simply allows a little quicker retrieval of the information we want. While AR is not the final conclusion of this pattern, it’s a massive bridge to our future. If we take an even longer look back at this law, it can be seen to have started much further in the past — half a billion years ago. In his book In the Blink of an Eye, zoologist Andrew Parker argues the greatest inflection point in the history of evolution occurred during the Cambrian explosion because of one particular new attribute: the eye. The eye was the game changer, because the brain could now access external information in real time. Our computers have had brains for decades. With the rise of computer vision, our smart devices are poised to have their own Cambrian explosion. The Internet, AR and computer vision are part of the same pattern, in that each are massive breakthroughs of increasingly efficient means of the speed at which organisms can summon meaningful data about the external world. Humans crave omnipotence. When accurate information can be accessed faster, productivity increases. Which is why we have been striving toward the Holy Grail, to have our device provide us with PCDIRT (perfect contextual data in real time). AR can give us this, but only when used with an HMD. What is coming next is special and unlike previous evolutions of this pattern, because a sort of trinity occurs when you bring a good-looking HMD, computer vision and AR together to create a sum bigger than its parts, giving us prosthetic knowledge. We are about to democratize knowledge. We are about to make the word “expert” disappear. For the first time in history, information will be accessed so fast, via discreet AR HMDs, that from an outsider’s perspective your actual knowledge base will become indistinguishable from the prosthetic knowledge you are demonstrating or communicating. Think about how revolutionary that is."
629,https://techcrunch.com/2015/12/12/lifelike-adverts/,TechCrunch,2015,12,12,," Reminder: Almost 90 per cent of the revenue of the company formerly known as Google — grandly rebranded Alphabet this fall, even if everyone, including me, is still going to call Google Google — comes from advertising. Mountain View’s annual revenue is around $69 billion at this point. It makes almost all (89 per cent) of that money-mountain from ads. It might like you to think of it as an alphabetic spectrum of moonshot technology bets — whether that’s hacking death, accelerating life science research, building autonomous cars or making terrifying robots — but at base Google’s business is all about profiling people for ad delivery. So its business model is all about your eyeballs. Seen from that perspective, it’s entirely unsurprisingly how multi-pronged a push Google is making to stoke the VR market right now. I’m bundling virtual reality and augmented reality together here, into one general ‘sight-disrupting’ package. Sure there are differences in immersion level between AR and VR but in general the two technologies are about injecting something digital into a user’s field of view. And Google plays in both areas, skewing more towards the AR side right now. Whatever the next incarnation of Glass, it looks pretty clear there will be one. And that’s rather surprising given how little general consumer interest Google managed to drive for the first wave of Glass. Indeed, it managed to inspire the polar opposite sentiment among non-nerds — generating a pejorative descriptor (‘Glassholes’) to describe wearers of the gizmo. Not a great start then. Next, at a cheaper price-point, and generally designed as more of a crowd-pleaser, there’s Google Cardboard. Announced in June 2014, this is Google’s budget VR headset. It’s literally made from cardboard and a couple of lenses — just pop in your smartphone, fire up the Google cardboard app and experience a degree of immersion within various digital arenas, including Google’s StreetView virtual world tour and 360 degree YouTube videos. Google has also worked with GoPro on a VR rig to encourage the capture of 360 degree content exclusively for “high profile YouTube celebrities” who maintain a large number of followers. Cardboard is a low risk bet for Google to try to drum up mass market interest in VR, and an equal and opposite push to try to get more people making content for VR by building a market for such content. Content, like cardboard, is cheap yet critical if VR is to become anything close to mainstream. And then there’s Google’s moonshot bet in the category: Magic Leap. Google is an investor in the AR company that has yet to release any products but continues to attract vast amounts of VC funding. Just this week it emerged Magic Leap is raising an $827 million Series C funding round — which brings the total raised since it was founded back in 2010 to around $1.4 billion. Sure it’s not Uber levels of funding. But for a company not yet really explaining its product — let alone selling anything — it’s pretty stand out. Mountain View is one of multiple investors here, but Google’s Sundar Pichai also sits on the Magic Leap board. And Google led a $542 million investment round in the company last year. So it’s actively spearheading the funding drive. Discussing Magic Leap this March, Pichai said Google sees broad use-cases for the augmented reality tech, stressing it sees much wider applications than mere gaming. The tech itself remains under wraps but will reportedly rely on some kind of lightweight wearable, and — unlike Glass or Cardboard — won’t involve looking through or at a screen. The founder of Magic Leap, Rony Abovitz, has talked about a “dynamic digital light field signal” which apparently tricks your brain into thinking whatever digital object it’s seeing is actually embedded into — not pasted onto — the real world. He’s also talked about Magic Leap turning the world into “your new desktop” or “your new silver screen”. And creating a kind of “cinematic reality“. Frankly it’s a pitch that sounds tailor-made to get Google salivating. The latter’s motivation to invest in VR is clear. Web advertising is embroiled in a tricky transition to mobile devices where ads on small screens are always an unwelcome irritant for device users. Add to that, more of people’s attention is being siloed into apps anyway, rather than directed at general web browsing. And if all that wasn’t bad enough, the specter of ad blocking is rearing its head on mobile too. Google is staring at a seismic shift in digital consumption that threatens to undermine its core business model. As connected mobile devices continue pulling people’s attention away from the search-driven web, Google really needs a way to bring a wider web back into the frame — and an ability to insert artificial content into a real-world view is a tantalizing prospect for the company. One which envisages an opening up of the digital display canvas again, with space for marketing messages to stretch their legs again. Hence Google betting on VR from all angles: big (Magic Leap), budget (Cardboard) and business-oriented (Glass). From a consumer point of view, if you thought virtual reality was going to be all flying whales, adorable robots and slayable zombies magically manifesting in your living room, think again. The big entity driving developments here is a company whose overriding interest is finding new ways to insert adverts into your field of view. So Magic Leap’s greatest trick might actually turn out to be an ability to camouflage advertising as something that engages the eye for long enough to disgorge a marketing message. At least that’s what Google will be hoping. But if consumers hate adverts interrupting their web browsing or mobile usage, it seems unlikely they’re going to be delighted by ads jumping directly into their eyeballs. Web users reserve a special kind of hatred for pop-ups. So even 3D lifelike pop-ups aren’t about to get a pass. Especially as the VR user will undoubtedly be hoping to see something a lot more entertaining than an artificial polar bear that pops open a Coca Cola. Or a virtual clown pointing across the street at an actual McDonalds. All three of Google’s ‘disruptive’ VR bets will only be as effective as the length of time they remain wrapped around wearers’ eyeballs. So if advertisers have their wicked way with this tech, any ‘honeymoon period’ for the kind of hyper immersive augmented reality Magic Leap is apparently cooking up could turn out to be very brief indeed."
630,https://techcrunch.com/2014/01/08/the-fin-is-a-bluetooth-ring-that-turns-your-hand-into-the-interface/,TechCrunch,2014,1,8,," Smart glasses! Smart watches! Smart… rings? While many in the tech world would agree that wearable devices are the natural next stage of computing, no one has really cracked the code. As much as we geeks love to chat about Google Glass and Pebble watches, no wearable has breached the mainstream and achieved any degree of ubiquity just yet. RHL Vision, a competitor in the TechCrunch CES Hardware Battlefield today, thinks they have the answer: Bluetooth rings that turn your fingers into buttons. Here’s how it works: by tucking an optical sensor into a small ring placed around your thumb, the Fin is able to detect swipes and taps across your hand. When it detects a gesture, it sends that command off to your connected device — be it a smartphone, TV, or another wearable device. Swiping your thumb down your index finger, for example, could turn your phone’s volume down. Want to turn it back up? Just swipe back up across the same finger. Want to skip the current track? Swipe your thumb across the palm of the opposite hand. In future iterations, they hope to use biometrics to distinguish each segment of each finger. This would allow them to assign each segment a different behavior, essentially turning each section of your hand into a different button. While the team has renders (shown below) showing the ring as they hope it will look once they reach the early stages of actual production, the current prototype shown on stage (and pictured above) is a bit more… extensive. Whereas they plan to use flexible circuitry (think Jawbone Up) to shrink the design and allow it to wrap around your thumb, the current prototype relies on more traditional PCBs and off-the-shelf sensors to get the job done. It’s not as small and not as pretty, but it does do a good job of proving the concept. The team plans to launch an indiegogo campaign later this evening. Update: Here’s the teams Indiegogo campaign!"
631,https://www.wired.com/story/algorithm-that-can-dress-you-thread/,Wired,2015,12,31,298.0," This article was first published in the January 2016 issue of WIRED magazine. Be the first to read WIRED's articles in print before they're posted online, and get your hands on loads of additional content by subscribing online. Algorithms are already helping us to decide what to watch and listen to. But style is more complicated. ""Fashion is a gnarly domain,"" says Kieran O'Neill, CEO and co-founder of London-based startup Thread. ""For one, all of the things you're going to recommend often expire within weeks."" Thread employs eight human personal stylists, who perform an initial consultation with each new client on sign-up. Its machine-learning algorithm then trawls through more than 31 million customer-submitted ratings, along with 3.7 trillion possible item combinations (each piece is tagged, to identify its characteristics) to recommend outfits.  ""Humans couldn't look through a million options to find the right thing,"" says O'Neill, 28. ""But a computer can't look at your picture and understand what [clothes] would suit you."" O'Neill made his first startup exit at 19 and sold his previous venture, games studio Playfire, in 2012. He founded Thread the same year, he says, to help men who don't follow fashion to dress better. One of its first customers was Instagram co-founder Kevin Systrom; it now has 200,000, and in August secured an $8 million (£5.2m) funding round led by Balderton Capital and including DeepMind co-founders Demis Hassabis and Mustafa Suleyman. WIRED decided it was time to refine its standard garb of slim-fit jeans, grey Chuck Taylors and flannel shirts. Thread's suggestion: slim-fit jeans, black Chuck Taylors and a white T-shirt. Hardly a style revolution -- but O'Neill says that's intentional, so as not to scare off new users. ""Recommendations need to change over time,"" he says. Dedicated followers, your lives just got easier."
632,https://www.wired.com/story/2016-predictions-technology-trends/,Wired,2015,12,29,932.0," For all the hype around technology, it doesn't move as fast as you might expect. This time last year everyone was talking about 2015 being the year of virtual reality and confidently predicting that the 2015 General Election would won by the party with the best grasp of social media (instead of, as it turned out, the party with the best friends in the national newspapers). When predictions focus on shiny new toys, they tend to miss the messy reality of human behaviour. (Which is why futurists in the 1960s foresaw the fax machine but completely failed to anticipate the arrival of women in the workplace.) So while it's tempting to declare that 2016 will be the year of virtual reality, here are four tech trends that instead focus on what people actually do with their technology, once they've got it in their hands. Not Google the company, but Google the search engine. This is a trend driven by smartphones, the key technology of the 2010s. In every part of the world, smartphone use keeps on going up and up, and there's no reason to expect that to stop in 2016. You'd think this would be good news for a service like Google. But people don't search – i.e. use Google (China excepted) – on smartphones in the same way they do on desktop. They don't search for ""Facebook"" – they tap the Facebook app. Then once they're in an app they tend to stay there. Using Google's own data, Guardian tech columnist Charles Arthur found that 50 percent of people do zero searches per day on mobile, compared to only 7 percent on desktop. None of this will necessarily make a huge difference to the revenues of Google – aka Alphabet – but it's indicative of the way mobile is changing our relationship to even the most basic internet services. What do people do with their phones if they're not using Google? They send each other messages. Look at the most used apps: it's Facebook top, followed in third by Facebook Messenger (which has grown 144 percent year-on-year in the US, according to Comscore). As mobile becomes more and more important, a lot of companies are putting messaging at the heart of their service. In this vision of the future, you'll be able to book a restaurant or order room service simply by sending a message. The person on the other end can give you options and refine exactly what you want, as well as chatting to you in a more human fashion (emoji!). The idea is that you get a more personal communication as well as tapping into the knowledge of the person on the other end. Startups are trying to turn this into a business. Uber co-founder Garrett Camp's new app Operator connects you to experts to help you with your shopping. Aces does the same thing for tourists, connecting them to local experts in different cities. These companies are going to have to compete with Facebook Messenger, Mark Zuckerberg's one app to rule them all, which we profiled in the November issue of the magazine. The first sign of this: from Wednesday, Facebook Messenger users in the US can order an Uber inside the app – so if you friend sends you an address, you'll be able to tap that and book a cab directly. Expect to see a lot more of this kind of thing in the year to come. From a tech point of view, the big prize in all of this is artificial intelligence. One day, we're told, software will be able to conduct these conversations, going through our data to tell us what we need and messaging us with friendly little reminders that are as good as, if not better, than human interaction. This is what Facebook is aiming for with M (short for Moneypenny), its personal assistant service, which is based in Facebook Messenger and uses a mix of human and automation. Other similar services include Amazon Echo, a voice recognition-based digital assistant that goes by the name of Alexa, and Amy, an automated assistant that helps you organise meetings over email. All these projects are based on natural language processing, the attempt to understand language as it is used in everyday life. ""If we can crack natural language understanding, then we will open a huge market for AI,"" says Rand Hindi, CEO of Snips, a French AI company. ""Anything from understanding law and contracts, to analysing and writing news or diagnosing patients will be affected. And that does not include the impact of messaging apps which will be able to augment the experience of users by analysing what they are talking about."" Will we see any of this come to life in 2016? Understanding natural language processing is ""a super hard task,"" says Hindi. ""Understanding language is not just about syntax and grammar, but also about context. You need to know about the person's life, environment, culture. Quite a complex task!"" In other words, while this might be an extremely exciting area, it's unlikely to make any difference to anyone's lives in the near future. Instead, change will come, as it always does, from the everyday activities of ordinary men and women. Perhaps the technology industry will wake up to this in 2016, although personally I have my doubts. If you notice the one thing those AI assistant services all have in common, they're all named after women. So yeah: tech will still be sexist in 2016. But hey: at least by the end of the year there'll be a female President."
633,https://www.wired.com/2015/12/elon-musks-billion-dollar-ai-plan-is-about-far-more-than-saving-the-world/,Wired,2015,12,15,1794.0," Elon Musk and Sam Altman worry that artificial intelligence will take over the world. So, the two entrepreneurs are creating a billion-dollar not-for-profit company that will maximize the power of AI—and then share it with anyone who wants it. At least, this is the message that Musk, the founder of electric car company Tesla Motors, and Altman, the president of startup incubator Y Combinator, delivered in announcing their new endeavor, an unprecedented outfit called OpenAI. In an interview with Steven Levy of Backchannel, timed to the company's launch, Altman said they expect this decades-long project to surpass human intelligence. But they believe that any risks will be mitigated because the technology will be ""usable by everyone instead of usable by, say, just Google."" Naturally, Levy asked whether their plan to freely share this technology would actually empower bad actors, if they would end up giving state-of-the-art AI to the Dr. Evils of the world. But they played down this risk. They feel that the power of the many will outweigh the power of the few. ""Just like humans protect against Dr. Evil by the fact that most humans are good, and the collective force of humanity can contain the bad elements,"" said Altman, ""we think its far more likely that many, many AIs, will work to stop the occasional bad actors."" It'll be years before we know if this counterintuitive argument holds up. Super-human artificial intelligence is an awfully long way away, if it arrives at all. ""It's not yet an open-and-shut argument,"" says Miles Brundage, a PhD student at the Arizona State University who deals in the human and social dimensions of science and technology, says of OpenAI. ""At the point where we are today, no AI system is at all capable of taking over the world—and won't be for the foreseeable future."" But in the creation of OpenAI, there are more forces at work than just the possibility of super-human intelligence achieving world domination. In the shorter term, OpenAI can directly benefit Musk and Altman and their companies (Y Combinator backed such unicorns as Airbnb, Dropbox, and Stripe). After luring top AI researchers from companies like Google and setting them up at OpenAI, the two entrepreneurs can access ideas they couldn't get their hands on before. And in pooling online data from their respective companies as they've promised to, they'll have the means to realize those ideas. Nowadays, one key to advancing AI is engineering talent, and the other is data. If OpenAI stays true to its mission of giving everyone access to new ideas, it will at least serve as a check on powerful companies like Google and Facebook. With Musk, Altman, and others pumping more than a billion dollars into the venture, OpenAI is showing how the very notion of competition has changed in recent years. Increasingly, companies and entrepreneurs and investors are hoping to compete with rivals by giving away their technologies. Talk about counterintuitive. OpenAI is the culmination of an extremely magnanimous month in the world of artificial intelligence. In early November, Google open sourced (part of) the software engine that drives its AI services—deep learning technologies that have proven enormously adept at identifying images, recognizing spoken words, translating languages, and understanding natural language. And just before the unveiling of OpenAI, Facebook open sourced the designs for the computer server it built to run its own deep learning services, which tackle many of the same tasks as Google's tech. Now, OpenAI is vowing to share everything it builds—and a big focus seems to be, well, deep learning. Yes, such sharing is a way of competing. If a company like Google or Facebook openly shares software or hardware designs, it can accelerate the progress of AI as a whole. And that, ultimately, advances their own interests as well. For one, as larger community improves these open source technologies, Google and Facebook can push the improvement back into their own businesses. But open sourcing also is a way of recruiting and retaining talent. In the field of deep learning in particular, researchers—many of whom come from academia—are very much attracted to the idea of openly sharing their work, of benefiting as many people as possible. ""It is certainly a competitive advantage when it comes to hiring researchers,"" Altman tells WIRED. ""The people we hired ... love the fact that [OpenAI is] open and they can share their work."" This competition may be more direct than it might seem. We can't help but think that Google open sourced its AI engine, TensorFlow, because it knew OpenAI was on the way—and that Facebook shared its Big Sur server design as an answer to both Google and OpenAI. Facebook says this was not the case. Google didn't immediately respond to a request for comment. And Altman declines to speculate. But he does say that Google knew OpenAI was coming. How could it not? The project nabbed Ilya Sutskever, one of its top AI researchers. That doesn't diminish the value of Google's open source project. Whatever the company's motives, the code is available to everyone to use as they see fit. But it's worth remembering that, in today's world, giving away tech is about more than magnanimity. The deep learning community is relatively small, and all of these companies are vying for the talent that can help them take advantage of this extremely powerful technology. They want to share, but they also want to win. They may release some of their secret sauce, but not all. Open source will accelerate the progress of AI, but as this happens, it's important that no one company or technology becomes too powerful. That's why OpenAI is such a meaningful idea. You can also bet that, on some level, Musk too sees sharing as a way of winning. ""As you know, I’ve had some concerns about AI for some time,"" he told Backchannel. And certainly, his public fretting over the threat of an AI apocalypse is well known. But he also runs Tesla, which stands to benefit from the sort of technology OpenAI will develop. Like Google, Tesla is building self-driving cars, which can benefit from deep learning in enormous ways. Deep learning relies on what are called neural networks, vast networks of software and hardware that approximate the web of neurons in the human brain. Feed enough photos of a cat into a neural net, and it can learn to recognize a cat. Feed it enough human dialogue, and it can learn to carry on a conversation. Feed it enough data on what cars encounter while driving down the road and how drivers react, and it can learn to drive. Yes, Musk could just hire AI researchers to work at Tesla. And he is. But with OpenAI, he can hire better researchers (because it's open, and because it's not constrained by any one company's business model or short-term interest). He can even lure researchers away from Google. Plus, he can create a far more powerful pool of data that can help feed the work of these researchers. Altman says that Y Combinator companies will share their data with OpenAI, and that's no small thing. Pair their data with Tesla's, and you start to rival Google—at least in some ways. ""It's probably better in some dimensions and worse in others,"" says Chris Nicholson, the CEO of deep learning startup called Skymind, which was recently accepted into the Y Combinator program. ""I'm sure Airbnb has great housing data that Google can't touch."" Musk was an early investor in a company called DeepMind—a UK-based outfit that describes itself as ""an Apollo program for AI."" And this investment gave him a window into how this remarkable technology was developing. But then Google bought DeepMind, and that window closed. Now, Musk has started his own Apollo program. He once again has the inside track. And OpenAI's other investors are in a similar position, including Amazon, an Internet giant trails Google and Facebook in the race to AI. But, no, this doesn't diminish the value of Musk's open source project. He may have selfish as well as altruistic motives. But the end result is still enormously beneficial to the wider world of AI. In sharing its tech with the world, OpenAI will nudge Google, Facebook, and others to do so as well—if it hasn't already. That's good for Tesla and all those Y Combinator companies. But it's also good for everyone that's interested in using AI. Of course, in sharing its tech, OpenAI will also provide new ammunition to Google and Facebook. And Dr. Evil, wherever he may lurk. He can feed anything OpenAI builds back into his own systems. But the biggest concern isn't necessarily that Dr. Evil will turn this tech loose on the world. It's that the tech will turn itself loose on the world. Deep learning won't stop at self-driving cars and natural language understanding. Top researchers believe that, given the right mix of data and algorithms, its understanding can extend to what humans call common sense. It could even extend to super-human intelligence. ""The fear is of a super-intelligence that recursively improves itself, reaches an escape velocity, and becomes orders of magnitude smarter than any human could ever hope to be,"" Nicholson says. ""That's a long ways away. And some people think it might not happen. But if it did, that will be scary."" This is what Musk and Altman are trying to fight. ""Developing and enabling and enriching with technology protects people,"" Altman tells us. ""Doing this is the best way to protect all of us."" But at the same time, they're shortening the path to super-human intelligence. And though Altman and Musk may believe that giving access to super-human intelligence to everyone will keep any rogue AI in check, the opposite could happen. As Brundage points out: If companies know that everyone is racing towards the latest AI at breakneck speed, they may be less inclined to put safety precautions in place. How necessary those precautions really are depend, ironically, on how optimistic you are about humanity's ability to accelerate technological progress. Based on their past successes, Musk and Altman have every reason to believe the arc of progress will keep bending upward. But others aren't so sure that AI will threaten humanity in the way that Musk and Altman believe it will. ""Thinking about AI is the cocaine of technologists: it makes us excited, and needlessly paranoid,"" Nicholson says. Either way, the Googles and the Facebooks of the world are rapidly pushing AI towards new horizons. And at least in small ways, OpenAI can help keep them—and everyone else—in check. ""I think that Elon and that group can see AI is unstoppable,"" Nicholson says, ""so all they can hope to do is affect its trajectory."""
634,https://www.wired.com/2015/12/elon-musk-snags-top-google-researcher-for-new-ai-non-profit/,Wired,2015,12,11,452.0," Tesla founder Elon Musk, big-name venture capitalist Peter Thiel, LinkedIn co-founder Reid Hoffman, and several other notable tech names have launched a new artificial intelligence startup called OpenAI, assembling a particularly impressive array of AI talent that includes a top researcher from Google. But the idea, ostensibly, isn't to make money. Overseen by ex-Googler Ilya Sutskever and Greg Brockman, the former CTO of high-profile payments startup Stripe, OpenAI has the talent to compete with the industry's top artificial intelligence outfits, including Google and Facebook—but the company has been setup as a non-profit. ""Our goal is to advance digital intelligence in the way that is most likely to benefit humanity as a whole, unconstrained by a need to generate financial return,"" Brockman said in a blog post. The apparent aim is to build systems based on deep learning, a form of artificial intelligence that has proven extremely adept in recent years at identifying images, recognizing spoken words, translating from one language to another, and, to a certain extent, understanding the natural way that we humans talk. Sutskever is a protege of Geoff Hinton, one of the founding fathers of the deep learning movement, who now works for Google. Deep learning relies on what are called neural networks, vast networks of machines that approximate the networks of neurons in the human brain. Feed enough photos of a cat into a neural net, and it can learn to identify a cat. Feed enough dialogue into a neural net, and it can learn to carry on a pretty good, if sometimes dodgy, conversation. The hope is that top researchers can take this much further. Some believe the method can even be used to mimic human common sense. Currently, companies like Google and Facebook and Microsoft sit at the forefront of this movement. But OpenAI aims push the state of the art forward without worrying about financial gain. Instead, they intend to open source their work, freely sharing it with the world at large. Recently, Google open sourced the core software engine, TensorFlow, that drives its deep learning services, and just this week, Facebook open sourced its deep learning hardware. They too are looking to advance the technology through widespread collaboration. But you have to wonder if they made these moves because they knew OpenAI was on the way. OpenAI certainly has the pedigree to make some serious headway in the field. Musk and Thiel are co-chairs of the company. Other backers include Alan Kay, one of the founding fathers of the PC, and Yoshua Bengio, another top deep learning researcher. Altogether, OpenAI says, its backers have committed $1 billion to the project. Sometimes you have to spend money, even if you don't plan on making any."
635,https://www.wired.com/2015/12/facebook-open-source-ai-big-sur/,Wired,2015,12,10,1123.0," In Silicon Valley, the new currency is artificial intelligence. Over the last few years, a technology called deep learning has proven so adept at identifying images, recognizing spoken words, and translating from one language to another, the titans of Silicon Valley are eager to push the state of the art even further—and push it quickly. The two biggest players are, yes, Google and Facebook. At Google, this tech not only helps the company recognize the commands you bark into your Android phone and instantly translate foreign street signs when you turn your phone their way. It helps drive the Google search engine, the centerpiece of the company's online empire. At Facebook, it helps identify faces in photos, choose content for your News Feed, and even deliver flowers ordered through M, the company's experimental personal assistant. All the while, these two titans hope to refine deep learning so that it can carry on real conversations—and perhaps even exhibit something close to common sense. Of course, in order to reach such lofty goals, these companies need some serious engineering talent. And the community of researchers who excel at deep learning is relatively small. As a result, Google and Facebook are part of an industry-wide battle for top engineers. The irony is that, in an effort to win this battle, the two companies are giving away their secrets. Yes, giving them away. Last month, Google open sourced the software engine that drives its deep learning services, freely sharing it with the world at large. And this morning, Facebook announced that it will open source the designs for the computer server it built to run the latest in AI algorithms. Code-named Big Sur, this is a machine packed with an enormous number of graphics processing units, or GPUs—chips particularly well suited to deep learning. It may seem odd that these companies are giving away their technology. But they believe this will accelerate their work and foster new breakthroughs. If they open source their hardware and software tools, a larger community of companies and researchers can help improve them. ""There is a network effect. The platform becomes better as more people use it,"" says Yann LeCun, a founding father of deep learning, who now oversees AI work at Facebook. ""The more people that rally to a particular platform or standard, the better it becomes—the more people contribute."" Plus, Facebook can curry favor across the community, providing added leverage in recruiting and retaining talent. ""Our commitment to open source is something that individuals who work here are passionate about,"" says Serkan Piantino, an engineering director in Facebook's AI group. ""Having that be a part of our culture is a benefit when it comes to hiring."" This is how the modern tech world works. The Internet's largest services typically run on open source software. ""Open source is the currency of developers now,"" says Sean Stephens, the CEO of a software company called Perfect. ""It’s how they share their thoughts and ideas. In the closed source world, developers don’t have a lot of room to move."" And as these services shift to a new breed of streamlined hardware better suited to running enormous operations, many companies are sharing their hardware designs as well. Facebook is the poster child for this movement. In 2011, after years of sharing important software, the company started sharing hardware designs, seeding what it calls the Open Compute Project—a way for any company to share and collaborate on hardware. As it grew into the Internet's most dominant force, Google typically saw its most important software and hardware designs as a competitive advantage it must keep to itself. But it too has opened up in recent years. Releasing its TensorFlow deep learning engine took the approach to a new peak. Now, just weeks later, Facebook has open sourced its AI hardware. Big Sur includes eight GPU boards, each loaded with dozens of chips while consuming only about 300 Watts of power. Although GPUs were originally designed to render images for computer games and other highly graphical applications, they've proven remarkably adept at deep learning. Deep learning relies on neural networks, vast networks of machines that approximate the web of neurons in the human brain. Traditional processors help drive these machines, but big companies like Facebook and Google and Baidu have found that their neural networks are far more efficient if they shift much of the computation onto GPUs. Neural nets thrive on data. Feed them enough photos of your mother, and they can learn to recognize her face. Give them enough spoken words, and they can learn to recognize what you say. With GPUs, these neural nets analyze more data, more quickly. The general principle, says Baidu researcher Bryan Catanzaro, is that GPUs give more computational throughput per dollar than traditional CPUs. After 18 months of development, Big Sur is twice as fast as the previous system Facebook used to train its neural networks. That means it can train twice as many neural networks in the same amount of time—or train networks that are twice as large. In short, Facebook can achieve a greater level of AI at a quicker pace. ""The bigger you make the neural nets, the better they will work,"" LeCun says. ""The more data you get them, the better they will work."" And since deep neural nets serve such a wide variety of applications—from face recognition to natural language understanding—this single system design can significantly advance the progress of Facebook as a whole. Facebook designed the machine in tandem with Quanta, a Taiwanese manufacturer, and nVidia, a chip maker specializing in GPUs. Traditionally, businesses went straight to the likes of Dell, HP, and IBM for the servers that drove their online services. But Facebook—like Google, Amazon, and others—has found that it can save enormous amounts of money by designing systems in tandem with Asian manufacturers such as Quanta. Facebook says it's now working with Quanta to open source the design and share it through the Open Compute Project. You can bet this is a response to Google open sourcing TensorFlow. TensorFlow won some big headlines. To continue attracting the big talent, Facebook must keep pace in the perception game. But according to LeCun, there are bigger reasons for open sourcing Big Sur and other hardware designs. For one thing, this can help reduce the cost of the machines. If more companies start using the designs, manufacturers can build the machines at a lower cost. And in a larger sense, if more companies use the designs to do more AI work, it helps accelerate the evolution of deep learning as a whole—including software as well as hardware. So, yes, Facebook is giving away its secrets so that it can better compete with Google—and everyone else."
636,https://www.wired.com/2015/12/google-and-facebook-race-to-solve-the-ancient-game-of-go/,Wired,2015,12,7,1428.0," Rémi Coulom spent the last decade building software that can play the ancient game of Go better than practically any other machine on earth. He calls his creation Crazy Stone. Early last year, at the climax of a tournament in Tokyo, it challenged the Go grandmaster Norimoto Yoda, one of the world's top human players, and it performed remarkably well. In what's known as the Electric Sage Battle, Crazy Stone beat the grandmaster. But the win came with a caveat. Over the last 20 years, machines have topped the best humans at so many games of intellectual skill, we now assume computers can beat us at just about anything. But Go—the Eastern version of chess in which two players compete with polished stones on 19-by-19-line grid—remains the exception. Yes, Crazy Stone beat Yoda. But it started with a four-stone advantage. That was the only way to ensure a fair fight. In the mid-'90s, a computer program called Chinook beat the world's top player at the game of checkers. A few years later, IBM's Deep Blue supercomputer shocked the chess world when it wiped the proverbial floor with world champion Gary Kasparov. And more recently, another IBM machine, Watson, topped the best humans at Jeopardy!, the venerable TV trivia game. Machines have also mastered Othello, Scrabble, backgammon, and poker. But in the wake of Crazy Stone's victory over Yoda, Coulom predicted that another ten years would pass before a machine could beat a grandmaster without a head start. At the time, that ten-year runaway seemed rather short. In playing Go, the grandmasters often rely on something that's closer to intuition than carefully reasoned analysis, and building a machine that duplicates this kind of intuition is enormously difficult. But a new weapon could help computers conquer humans much sooner: deep learning. Inside companies like Google and Facebook, deep learning is proving remarkably adept at recognizing images and grasping spacial patterns—a skill well suited to Go. As they explore so many other opportunities this technology presents, Google and Facebook are also racing to see whether it can finally crack the ancient game. As Facebook AI researcher Yuandong Tian explains, Go is a classic AI problem—a problem that's immensely attractive because it's immensely difficult. The company believes that solving Go will not only help refine the AI that drives its popular social network, but also prove the value of artificial intelligence. Rob Fergus, another Facebook researcher, agrees. ""The goal is advancing AI,"" he says. But he also acknowledges that the company is driven, at least in a small way, by a friendly rivalry with Google. There's pride to be found in solving the game of Go. Today, Google and Facebook use deep learning to identify the faces in photos you post to the 'net. It's how computers recognize the commands barked into a phone and translate things from one language to another. Sometimes, it can even understand natural language—the natural way that we humans converse. This technology relies on what are called deep neural networks, vast networks of machines that approximate the web of neurons in the human brain. If you feed enough tree photos into these neural nets, they can learn to identify a tree. If you feed them enough dialogue, they can learn to carry on a decent (if sometimes weird) conversation. And if you feed them enough Go moves, they can learn to play Go. ""Deep neural networks are very appropriate for Go because Go is very driven by patterns on the board. These methods are very good at generalizing from patterns,"" says Amos Storkey, a professor at the University of Edinburgh, who is using deep neural networks to tackle Go, much like Google and Facebook. The belief is that these neural nets can finally close the gap between machines and humans. In playing Go, you see, the grandmasters don't necessarily examine the results of each possible move. They often play based on how the board looks. With deep learning, researchers can begin to duplicate this approach. In feeding images of successful moves into neural networks, they can help machines learn what a successful move looks like. ""Rather than just trying to work out what the best things to do are, they learn from how humans play the game,"" Storkey says of neural nets. ""They effectively copy human play."" Building a machine that can win at Go isn't just a matter of computing power. That's why programs like Coulom's haven't cracked it. Crazy Stone relies upon what's called a Monte Carlo tree search, a system that essentially analyzes the outcomes of every possible move. This is how machines mastered checkers and chess and other games. They looked further ahead than the humans they beat. But with Go, there are too many possibilities to consider. In chess, on any given turn, the average number of possible moves is 35. With Go, it's 250. And after each of those 250 possible moves, there are another 250. And so on. It's impossible for a tree search to consider the results of every move (at least not in a reasonable amount of time). But deep learning can fill the gap, providing a level of intuition, as opposed to brute force. Last month, in a paper posted the academic research site Arxiv, Facebook demonstrated a method that combines the Monte Carlo tree search with deep learning. In competition with humans, the system held its own, and according to the company, it even played with a style that felt human. After all, it has learned from real human moves. Coulom calls the company's results ""very spectacular."" Ultimately, Coulom says, this kind of hybrid approach will crack the problem. ""What people are trying to do is combine the two approaches so that it's better than each,"" he says. He points out that Crazy Stone already uses a form of machine learning in concert with Monte Carlo. It's just that his methods aren't as complex as the neural networks employed by Facebook. Facebook's paper shows the power of deep learning, but it's also a reminder that big AI tasks are ultimately solved by more than a single technology. They're solved by many technologies. Deep learning does many things well. But it can always use help from other forms of AI. After Facebook revealed its Go work, Google soon unloaded a response. A top Google AI researcher, Demis Hassabis, said that, in a few months, the company would reveal ""quite a big surprise"" related to the game of Go. Google declined to say more for this story, and it's unclear what the company has in store. Coulom, for one, says it's unlikely Google could so quickly produce something that can beat the top Go players, but he believes the company will take a significant step down that road. In all likelihood, this too will rely on multiple technologies. And we're guessing that one of them is something called reinforcement learning. While deep learning is good at perception—recognizing how something looks, sounds, or behaves—reinforcement algorithms can teach machines to act on this perception. Hassabis oversees DeepMind, a Google subsidiary based in Cambridge, England, and DeepMind has already made good use of deep learning in tandem with reinforcement algorithms. Earlier this year, he and his team published a paper that described how the two technologies could be used to play old Atari video games—and, in some cases, beat professional game testers. After a deep neural net helps the system understand the state of play—what the board looks like at any given time—the reinforcement algorithms use trial and error to help the system understand how to respond to this state of play. Basically, the computer tries a particular move, and if that move brings a reward—points in the game—it recognizes that the move as a good one. After trying enough moves, the system comes to understand the best ways of playing. The same kind of thing can work with Go. This approach is different from a standard tree search in that the system is learning what a good move looks like. Researchers train it to play before the real match begins. As with deep learning, it plays through a kind of ""knowledge"" rather than applying brute force to the problem. Ultimately, if they solve the game of Go, machines need all of these technologies. Reinforcement learning can feed off of deep learning. And both can dovetail with a traditional approach like the Monte Carlo tree search. Cracking Go remains enormously difficult. But modern AI is getting closer. When Hassabis reveals his ""big surprise,"" we'll know just how close it has come."
637,https://www.wired.com/2015/12/youre-probably-not-rich-enough-to-opt-out-of-the-internet/,Wired,2015,12,4,772.0," Last month at the Techonomy conference in Half Moon Bay, California, a group of technologists explored the ethics of developing web services in the age of artificial intelligence. How much do people really understand about the information they’re giving up in return for a service, an audience member asked, and can they choose not to give up that data? The question was met, at first, with a quizzical silence. Data is the fuel that makes the Internet work. Without it, online services aren’t as personal, or as useful. Then someone on the panel remarked that, of course, a user has to give up data, but it’s her choice. She could choose, instead, to opt out. But could she? Can anyone? A decade ago, American Internet refusers were the olds and the luddites. They didn’t really get it, or get why it had value to them. Or they couldn't afford it. Adding the necessary equipment (computer, smartphone) and the monthly hook-up fee was a significant expense. Five years ago, avoiding the Internet was counter-culture. A small number of people chose not to get it, or to value it. Today, however, the only people who can avoid the Internet are the privileged, the people with a trust fund. To scrape together a living in a knowledge-based economy, you pretty much have to participate. Most people can't afford not to be online. I’m not knocking the ‘net. It has made my personal life immeasurably better. Shortly, I’ll get off a plane, and by the time I hit the concourse, I will have arranged my transportation home and for dinner to be waiting. I'll have ensured my dog is walked, and informed my editor he should publish this piece. That will take a dozen clicks on four apps. But these technological advances can become coercive, even as they create new opportunities. I often hunger to opt out of the endless noise and distraction of the Internet. (Every August, I take a month off of social media, and invariably, I notice my anxiety level plummet.) And increasingly, I notice the way the apps I patronize subtly inform my behavior. Recently, on a road trip, I plugged my destination into Google Maps. I chose my preferred route, and the Siri-like navigator said, “You have chosen the best route.” What? The fastest route, maybe. But the best route? How does Google know what my best route is? Maybe a better route would be one that stopped off at a friend’s house or passed through a scenic area. And while I can get away with dropping Google Maps (paper maps are so retro) or Facebook for a time, eventually, I will need to log on to the ‘book or pull up a Google search box to fulfill the tasks required of my profession. To participate in today's economy, you need to participate in the Internet. Here, reader, you may interject that you are not a person who uses Facebook. So there! Or maybe, you use Bing instead of Google. Or clear your cache constantly, and turn off GPS. Even so, the large global Internet services—primarily Google, Facebook, Apple and Amazon—are nearly impossible to sidestep. Even if you don’t use them, they will use you—in ways you likely don’t and can’t track entirely. That's because, while the Internet may have lived behind a computer screen for the first two decades of its commercial existence, now it has become as ubiquitous as air, an ambience that surrounds you. Wake up in the morning, turn on your phone, and GPS will note your position, updating all of your apps. Get into any relatively new car, and it’ll record where you are going and how you are driving. Turn on your Nest thermostat, and it’ll record your heat preference. Then there’s your Fitbit, Jawbone Up, Apple Watch, Lynx lightbulb, Dropcam security camera, and on and on. Even if you don’t seek the web out, somewhere an Internet-powered service is logging and analyzing your behavior. There’s no going off the grid, because life is the grid. Which brings me back to the Techonomy discussion on ethics. We have invented the Internet, and it is now a staple of human life, infusing itself into our daily routines and informing our habits. Smart software companies are harnessing artificial intelligence to improve the products and services they offer up. It’s incumbent upon these companies to put ethics at the center of their product development strategies, because, while the services they offer are commercial, they’ve also become utilities. Most of us have electricity. We have running water in our houses. And we have the Internet. There’s no opting out."
638,https://www.wired.com/2015/12/wikipedia-is-using-ai-to-expand-the-ranks-of-human-editors/,Wired,2015,12,1,928.0," Aaron Halfaker just built an artificial intelligence engine designed to automatically analyze changes to Wikipedia. Wikipedia is the online encyclopedia anyone can edit. In crowdsourcing the creation of an encyclopedia, the not-for-profit website forever changed the way we get information. It's among the ten most-visited sites on the Internet, and it has swept tomes like World Book and Encyclopedia Britannica into the dustbin of history. But it's not without flaws. If anyone can edit Wikipedia, anyone can mistakenly add bogus information. And anyone can vandalize the site, purposefully adding bogus information. Halfaker, a senior research scientist at the Wikimedia Foundation, the organization that oversees Wikipedia, built his AI engine as a way of identifying such vandalism. In one sense, this means less work for the volunteer editors who police Wikipedia's articles. And it might seem like a step toward phasing these editors out, another example of AI replacing humans. But Halfaker's project is actually an effort to increase human participation in Wikipedia. Although some predict that AI and robotics will replace as much as 47 percent of our jobs over the next 20 years, others believe that AI will also create a significant number of new jobs. This project is at least a small example of that dynamic at work. ""This project is one attempt to bring back the human element,"" says Dario Taraborelli, Wikimedia's head of research, ""to allocate human attention where it's most needed."" In the past, if you made a change to an important Wikipedia article, you often received an automated response saying you weren't allowed to make the change. The system wouldn't let you participate unless you followed a strict set of rules, and according to study by Halfaker and various academics, this rigidity prevented many people from joining the ranks of regular Wikipedia editors. A 2009 study indicated that participation in the project had started to decline, just eight years after its founding. ""It's because the newcomers don't stick around,"" Halfaker says. ""Essentially, Wikipedians had traded efficiency of dealing with vandals and undesirable people coming into the wiki for actually offering a human experience to newcomers. The experience became this very robotic and negative experience."" With his new AI project—dubbed the Objective Revision Evaluation Service, or ORES—Halfaker aims to boost participation by making Wikipedia more friendly to newbie editors. Using a set of open source machine learning algorithms known as SciKit Learn—code freely available to the world at large—the service seeks to automatically identify blatant vandalism and separate it from well-intentioned changes. With a more nuanced view of new edits, the thinking goes, these algorithms can continue cracking down on vandals without chasing away legitimate participants. It's not that Wikipedia needs to do away with automated tools to attract more human editors. It's that Wikipedia needs better automated tools. ""We don't have to flag good-faith edits the same way we flag bad-faith damaging edits,"" says Halfaker, who used Wikipedia as basis for his PhD work in the computer science department at the University of Minnesota. In the grand scheme of things, the new AI algorithms are rather simple examples of machine learning. But they can be effective. They work by identifying certain words, variants of certain words, or particular keyboard patterns. For instance, they can spot unusually large blocks of characters. ""Vandals tend to mash the keyboard and not put spaces in between their characters,"" Halfaker says. Halfaker acknowledges that the service can't going to catch every piece of vandalism, but he believes it can catch most. ""We're not going to catch a well-written hoax with these strategies,"" he says. ""But it turns out that the vast majority of vandalism is not very clever."" Elsewhere, the giants of the Internet—Google, Facebook, Microsoft, and others—are embracing a new breed of machine learning known as deep learning. Using neural networks—networks of machines that approximate the web of neurons in the human brain—deep learning algorithms have proven adept at identifying photos, recognizing spoken words, and translating from one language to another. By feeding photos of a dog into a neural net, for instance, you can teach it to identify a dog. With these same algorithms, researchers are also beginning to build systems that understand natural language—the everyday way that humans speak and write. By feeding neural nets scads of human dialogue, you can teach machines to carry on a conversation. By feeding them myriad news stories, you can teach machines to write their own articles. In these cases, neural nets are a long way from real proficiency. But they point towards a world where, say, machines can edit Wikipedia. Halfaker believes that such a world is a long way off. And even if it arrives, he says, Wikipedia will still need humans to guide those neural networks. ""I'm not sure we'll ever get to the place where an algorithm will beat human judgment—or we won't get there anytime soon,"" he says. ""But even in that case, we'll still want human judgment as part of the process."" That's why, ironically, he has built an AI service that can expand the ranks of Wikipedia editors. It's telling that he and the Wikimedia Foundation are not implementing these algorithms from a central location. They're offering the algorithms as an online service that the broader Wikipedia community can use as it sees fit. ""We're making it easy to experiment and easy to critique the algorithms,"" he says. ""We want to enable a conversation, to move us towards a place where we're dealing with new content and new editors in better ways."" It's AI. But, then again, it's also very human."
639,https://www.wired.com/2015/11/google-open-sourcing-tensorflow-shows-ais-future-is-data-not-code/,Wired,2015,11,16,957.0," When Google open sourced its artificial intelligence engine last week—freely sharing the code with the world at large—Lukas Biewald didn't see it as a triumph of the free software movement. He saw it as a triumph of data. That's how you'd expect him to see it. He's the CEO of the San Francisco startup CrowdFlower, which helps online companies like Twitter juggle massive amounts of data. But after spending time at the Stanford AI Lab, he knows artificial intelligence. And his point is a good one. In open sourcing the TensorFlow AI engine, Biewald says, Google showed that, when it comes to AI, the real value lies not so much in the software or the algorithms as in the data needed to make it all smarter. Google is giving away the other stuff, but keeping the data. ""As companies become more data-driven, they feel more comfortable open sourcing lots of [software]. They know they're sitting on lots of proprietary data that nobody else has access to,"" says Biewald, who also worked at Yahoo as a search engineer and helped bootstrap a notable search startup called Powerset, now owned by Microsoft. ""What they're not opening up is their data. They would never do that."" Biewald compares this to IBM's recent purchase of The Weather Channel, where Big Blue paid millions largely to acquire data it could use to feed its AI ambitions. ""It's interesting that while companies are buying data, they're open-sourcing their algorithms,"" he says. ""It's pretty clear where these companies' bets are, in terms of what matters for machine learning."" TensorFlow, you see, deals in a form of AI called deep learning. With deep learning, you teach systems to perform tasks such as recognizing images, identifying spoken words, and even understanding natural language by feeding data into vast neural networks connected machines that approximate the web of neurons within the human brain. If you feed photos of cats into a neural net, you can teach it to recognize cats. If feed it conversational data, you can teach it to carry on conversations. The algorithms that drive these neural networks aren't new. They date to the 1980s. What's new is that, thanks to the Internet, their creators have the processing power and the enormous amounts of data to make these algorithms viable. To teach a system to recognize a cat, you need an awful lot of machines and an awful lot of cat photos. After the rise of cloud computing, in which companies like Amazon and Microsoft rent access to the vast processing power of the net, we all have access to a vast arrays of machines. But the richest data sits inside massive companies like Google and Facebook. Billions of people use their services, which trade in a rich trove of information, from text to photos to videos to speech and beyond. Both companies are hard at work building powerful AI software. But their real competitive edge comes from having a vast quantity of high quality data they can use to teach this software to ""think"" more like a human. To be sure, Biewald is exaggerating (at least a bit) to make a point. Though Google has open sourced some very important piece of its AI engine, it's keeping other pieces to itself (at least for now). What also matters in the competitive space is talent. Though the algorithms that drive this technology are an old thing, they evolve at rapid pace, moving into more and more areas, and this evolution is driven by some very smart people. That's one of the reasons Google open sourced TensorFlow. If people beyond the company can use its software, Google can more easily bring talent and ideas into the company—and its software. It also can continue to work with people who have left the company. ""We have a lot of summer interns coming in and they do a lot of interesting research while they are here at Google,"" says Jeff Dean, one of the Google engineers at the heart of the company's AI work. ""For some kinds of problems, they can basically just take their work and continue developing it on the open source release of TensorFlow."" But there's another reason Google can attract the top deep learning researchers: its data. The same goes for Facebook and other Internet giants. In recent years, many of the field's top researchers already have joined these companies, including University of Toronto professor Geoff Hinton (now at Google), New York University professor Yann Lecun (now at Facebook), and Stanford professor Andrew Ng (now at Chinese search giant Baidu). As Biedwald points out, you can't necessarily get access to the same data if you're an academic. ""It's kinda hard for academics and startups to do really meaningful machine learning work,"" he says, ""because they don't have access to the same kind of datasets that a Google or an Apple would have."" Yes, Apple generates lots of data too, through services like Siri. But some feel Apple could be at a disadvantage because, after taking a more extreme stance on privacy than Google and Facebook, it more tightly restricts how its engineers can makes use of the data they do have. That's how important digital information is to this movement. Ken Forbus, a professor of computer science at Northwestern University who specializes in AI, believes Apple may have to rely more heavily on technologies beyond of the deep learning realm because of its stance on privacy. There are many ways Apple can work around this, including changing its privacy policies. Like Google and others, it has acquired its own deep learning startups, and it has attracted AI talent in other ways. But one thing is indisputable: The future of AI can't happen without the data."
640,https://www.wired.com/2015/11/tensorflow-alone-will-not-revolutionize-ai/,Wired,2015,11,13,1171.0," Google this week open sourced TensorFlow, its elegant and powerful artificial intelligence engine. Google uses this machine learning software internally to add capabilities like speech recognition and object detection to its products. Now, it’s available for everyone to use. What will this mean for the design of artificial intelligence systems? As wonderful as TensorFlow is, I fear that it may accelerate the design of AI systems that are hard to understand and hard to communicate with. I think it will focus our attention on experimenting with mathematical tricks, rather than on understanding human thought processes. TensorFlow is aimed at the development of machine learning systems that require heavy numerical computation, like artificial neural networks (ANNs). The trouble with these systems is that they consist of millions of numbers—too many for people to sift through and make sense of. Suppose we train an ANN to recognize cats. When it recognizes a previously unseen cat in an image, it can’t explain to us why or how it did this. And if the ANN fails to recognize a spotted cat, it’s hard for us to fix the problem. We’re not going to tell it something like, “change element 341375’s value from 0.3265 to 0.4271, element 1954236’s value from 0.9218 to 0.8612, …” That would be a long list, and we don’t even know what numbers to change to get the desired result (this is especially true for end-users, but it's true for the researchers developing these systems, as well). More likely, we’ll either ignore the error, retrain the ANN with better cat data, or modify the training algorithm. These are blunt tools, because they don’t operate in the domain of interest, namely cats. Rather, they operate in the domain of ANNs. It would be better if we could simply tell the system that cats can be spotted. I’d rather see us design AI systems that are understandable and communicative. AI systems are going to be increasingly involved in our lives, as we turn to them more and more for important decision-making tasks. When AI systems make bad decisions (as they've done before, and inevitably will again), we need to be able to understand why they made those decisions and communicate with them to fix the problem. Below are some qualities an understandable and communicative AI system should possess. It reasons similarly to how humans reason. Person: Why isn’t Caitlyn answering my texts? Computer: Because she’s at school, and her phone is in her room. It can engage in dialogues with us. Computer: I can’t access the wired network. Person: Why not? Computer: Because the Ethernet port isn’t connected to the network. Person: How do you know? Computer: Because there is no voltage on any of the pins. Person: What should I do? Computer: Try replacing the cable. It can explain its reasoning and allow us to probe its reasoning. Computer: Your credit card has been declined. Person: Why? Computer: Because the transaction amount, $1,175.12, is greater than your available credit, $609.85. Person: Why is that my available credit? Computer: Your available credit is your total credit limit, $10,000, minus your current balance of $9,038.34, minus your pending charges of $351.81. It allows us to persuade it to adopt a different position. Computer: You should not take any more acetaminophen. Person: Why not? Computer: Because you are on warfarin. You should not take more than 2,000 mg of acetaminophen per day unless instructed by a physician. Person: Dr. Gold told me to take it. Computer: OK, but be careful. It learns similarly to how people learn, through interaction rather than from large amounts of data. Computer: Is your business a sole proprietorship, a non-profit, a corporation, or a partnership? Person: None of those. It’s an LLC. Computer: An LLC. Got it. How do we get an AI system to engage in these sorts of interactions? Earlier this year, Google unveiled a chatbot that could debate the meaning of life, or help a human user troubleshoot internet connectivity problems. The bot is impressive, but its responses are disconnected from the real world. For instance, when the chatbot tells a human something like “seems the mail is not loading,” it’s making this up. It hasn’t actually observed whether the mail has loaded or not. The machine's responses are based solely on training data and the history of the conversation at hand. Similarly, I don't think that the Google chatbot will be able to reliably implement interactions like the credit limit example, given above, because the search space for the ANN representation of potential financial transactions is too large. I think that implementing these interactions reliably requires discrete, human-readable representations like equations, logical formulas, rules, frames, models, and diagrams. These representations avoid the added complexity of ANNs, so the search space is more tractable. How do we implement the credit limit example? The AI system needs to be able to query the user’s financial information. A natural language parser would parse the incoming utterances, a rule-based dialogue manager would handle the incoming utterances and carry out the appropriate database queries, and a natural language generator would generate the appropriate responses. How do we acquire human-readable representations? Acquisition of these representations needs to be a multifaceted process. An AI system can acquire human-readable representations as it interacts with people. We can also use machine learning to suggest human-readable representations, although these representations are often questionable to humans. For example, here are some of the parts of a building as learned by one machine learning system: rubble, floor, facade, basement, roof, atrium, exterior, tenant, rooftop, and wreckage. Parts like rubble and wreckage seem like strange additions to this list, because buildings are not in ruins most of the time. Here are some paraphrases of X asks Y, as learned by another machine learning system: X tells Y, X meets with Y, X informs Y, X contacts Y, and X writes to Y. These are certainly related to X asks Y, but they are not synonymous in all contexts. And here is an event sequence for cooking as learned by yet another machine learning system: A boil B, A slice B, A peel B, A saute B, A cook B, A chop B. To a human cook, many of these tasks appear to be out of order (one typically peels before slicing, and chops before cooking). These are top-ranked results generated by state-of-the-art machine learning systems. Lower ranked results are even worse. Representations suggested by machine learning need to be vetted by humans, and not just because they contain errors. We need to examine machine-learned representations with a critical eye because, as humans, it’s up to us to decide what we want our world to look like. We don’t have to accept incomprehensible and uncommunicative AI systems. We can build understandable and communicative systems that (1) learn human-understandable representations through interaction with users as well as manual curation of knowledge and (2) maintain human-understandable representations of the states of users and the world. It’s hard work, but it can be done."
641,https://www.wired.com/story/artificial-neurons-human-language/,Wired,2015,11,13,351.0," A network of artificial neurons has learned how to use language. Researchers from the universities of Sassari and Plymouth found that their cognitive model, made up of two million interconnected artificial neurons, was able to learn to use language without any prior knowledge. The model is called the Artificial Neural Network with Adaptive Behaviour Exploited for Language Learning -- or the slightly catchier Annabell for short. Researchers hope Annabell will help shed light on the cognitive processes that underpin language development. Annabell has no pre-coded knowledge of language, and learned through communication with a human interlocutor.  ""The system is capable of learning to communicate through natural language starting from tabula rasa, without any prior knowledge of the structure of phrases, meaning of words [or] role of the different classes of words, and only by interacting with a human through a text-based interface,"" researchers said. ""It is also able to learn nouns, verbs, adjectives, pronouns and other word classes and to use them in expressive language."" Annabell was able to learn due to two functional mechanisms -- synaptic plasticity and neural gating, both of which are present in the human brain. Synaptic plasticity refers to the brain's ability to increase efficiency when the connection between two neurons are activated simultaneously, and is linked to learning and memory. Neural gating mechanisms play an important role in the cortex by modulating neurons, behaving like 'switches' that turn particular behaviours on and off. When turned on, they transmit a signal; when off, they block the signal. Annabell is able to learn using these mechanisms, as the flow of information inputted into the system is controlled in different areas.  ""The results show that, compared to previous cognitive neural models of language, the Annabell model is able to develop a broad range of functionalities, starting from a tabula rasa condition,"" researchers said in their conclusion.  ""The current version of the system sets the scene for subsequent experiments on the fluidity of the brain and its robustness. It could lead to the extension of the model for handling the developmental stages in the grounding and acquisition of language."""
642,https://www.wired.com/story/facebook-m-messenger-doubt-artificial-intelligence-blog/,Wired,2015,11,10,612.0," A software engineer has detailed his quest to develop an 'anti-Turing test' to prove that Facebook's personal assistant service 'M' is powered exclusively by humans, rather than AI as the company claims. Facebook has touted M as a comprehensive personal assistant to rival Apple's Siri and Microsoft's Cortana -- with the caveat that M is currently powered also by humans, not simply AI, in an effort to improve how it works. But now Arik Sosman, software engineer at BitGo, has taken to Medium to claim that M's artificially intelligent element is smaller than advertised. ""M's capabilities far exceed those of any competing AI,"" Sosman writes. ""Where some AIs would be hard-pressed to tell you the weather conditions for more than one location, M will tell you the forecast for every point on your route, and also provide you with convenient gas station suggestions, account for traffic, and provide you with options for food and entertainment at your destination."" ""When communicating with M, it insists it's an AI,"" he continues. ""And that it lives right inside Messenger. But it's non-instantaneous, and the sheer unlimited complexity of tasks it can handle suggest otherwise."" Sosman used a number of examples to illustrate his thesis -- which he called ""much harder"" than a traditional Turing test as ""it's much easier for humans to pretend to be an AI than for an AI to pretend to be human"". Sosman asked M to perform a set of complicated tasks that ""no other AI could pull off"" -- asking it for directions and then altering his request slightly. The AI was able to perform his request -- but not without a few spelling mistakes and other errors that Sosman believes proves M is powered by humans. And when M called a landline, it was operated by a distinctly human woman.  ""Thus, here we are. We have definitive prove that M is powered by humans,"" wrote Sosman. ""The next question is: Is it only humans, or is there at least some AI-driven component behind it?"" The personal assistant service is part of an ambitious long-term project within the Messenger team to build a complex AI system that can complete tasks without human supervision. It is expected that M will become more and more automated over time. Facebook has said in the past that M is a combination of artificial intelligence and human engagement -- the AI is trained and supervised by human operators, and becomes more independent over time. That will help the service be scaled up to move users, beyond its initially small-scale rollout. In a blog post from 3 November, Facebooks chief technology officer Mike Schroepfer wrote that ""unlike other machine-driven services ... M can actually complete tasks on your behalf. It can purchase items; arrange for gifts to be delivered to your loved ones; and book restaurant reservations, travel arrangements, appointments, and more. This is a huge technology challenge -- it's so hard that, starting out, M is a human-trained system: Human operators evaluate the AI's suggested responses, and then they produce responses while the AI observes and learns from them. ""We'd ultimately like to scale this service to billions of people around the world, but for that to be possible, the AI will need to be able to handle the majority of requests itself, with no human assistance. And to do that, we need to build all the different capabilities described above — language, vision, prediction, and planning — into M, so it can understand the context behind each request and plan ahead at every step of the way. This is a really big challenge, and we’re just getting started. But the early results are promising."""
643,https://www.wired.com/2015/11/googles-open-source-ai-tensorflow-signals-fast-changing-hardware-world/,Wired,2015,11,10,1337.0," In open sourcing its artificial intelligence engine—freely sharing one of its most important creations with the rest of the Internet—Google showed how the world of computer software is changing. These days, the big Internet giants frequently share the software sitting at the heart of their online operations. Open source accelerates the progress of technology. In open sourcing its TensorFlow AI engine, Google can feed all sorts of machine-learning research outside the company, and in many ways, this research will feed back into Google. But Google's AI engine also reflects how the world of computer hardware is changing. Inside Google, when tackling tasks like image recognition and speech recognition and language translation, TensorFlow depends on machines equipped with GPUs, or graphics processing units, chips that were originally designed to render graphics for games and the like, but have also proven adept at other tasks. And it depends on these chips more than the larger tech universe realizes. According to Google engineer Jeff Dean, who helps oversee the company's AI work, Google uses GPUs not only in training its artificial intelligence services, but also in running these services—in delivering them to the smartphones held in the hands of consumers. That represents a significant shift. Today, inside its massive computer data centers, Facebook uses GPUs to train its face recognition services, but when delivering these services to Facebookers—actually identifying faces on its social networks—it uses traditional computer processors, or CPUs. And this basic setup is the industry norm, as Facebook CTO Mike ""Schrep"" Schroepfer recently pointed out during a briefing with reporters at the company's Menlo Park, California headquarters. But as Google seeks an ever greater level of efficiency, there are cases where the company both trains and executes its AI models on GPUs inside the data center. And it's not the only one moving in this direction. Chinese search giant Baidu is building a new AI system that works in much the same way. ""This is quite a big paradigm change,"" says Baidu chief scientist Andrew Ng. The change is good news for nVidia, the chip giant that specialized in GPUs. And it points to a gaping hole in the products offered by Intel, the world's largest chip maker. Intel doesn't build GPUs. Some Internet companies and researchers, however, are now exploring FPGAs, or field-programmable gate arrays, as a replacement for GPUs in the AI arena, and Intel recently acquired a company that specializes in these programmable chips. The bottom line is that AI is playing an increasingly important role in the world's online services—and alternative chip architectures are playing an increasingly important role in AI. Today, this is true inside the computer data centers that drive our online services, and in the years to come, the same phenomenon may trickle down to the mobile devices where we actually use these services. At places like Google, Facebook, Microsoft, and Baidu, GPUs have proven remarkably important to so-called ""deep learning"" because they can process lots of little bits of data in parallel. Deep learning relies on neural networks—systems that approximate the web of neurons in the human brain—and these networks are designed to analyze massive amounts of data at speed. In order to teach these networks how to recognize a cat, for instance, you feed them countless photos of cats. GPUs are good at this kind of thing. Plus, they don't consume as much power as CPUs. But, typically, when these companies put deep learning into action—when they offer a smartphone app that recognizes cats, say—this app is driven by a data center system that runs on CPUs. According to Bryan Catanzaro, who oversees high-performance computing systems in the AI group at Baidu, that's because GPUs are only efficient if you're constantly feeding them data, and the data center server software that typically drives smartphone apps doesn't feed data to chips in this way. Typically, as requests arrive from smartphone apps, servers deal with them one at a time. As Catanzaro explains, if you use GPUs to separately process each request as it comes into the data center, ""it's hard to get enough work into the GPU to keep it running efficiently. The GPU never really gets going."" That said, if you can consistently feed data into your GPUs during this execution stage, they can provide even greater efficiency than CPUs. Baidu is working towards this with its new AI platform. Basically, as requests stream into the data center, it packages multiple requests into a larger whole that can then be fed into the GPU. ""We assemble these requests so that, instead of asking the processor to do one request at a time, we have it do multiple requests at a time,"" Catanzaro says. ""This basically keeps the GPU busier."" It's unclear how Google approaches this issue. But the company says there are already cases where TensorFlow runs on GPUs during the execution stage. ""We sometimes use GPUs for both training and recognition, depending on the problem,"" confirms company spokesperson Jason Freidenfelds. That may seem like a small thing. But it's actually a big deal. The systems that drive these AI applications span tens, hundreds, even thousands of machines. And these systems are playing an increasingly large role in our everyday lives. Google now uses deep learning not only to identify photos, recognize spoken words, and translate from one language to another, but also to boost search results. And other companies are pushing the same technology into ad targeting, computer security, and even applications that understand natural language. In other words, companies like Google and Baidu are gonna need an awful lot of GPUs. At the same time, TensorFlow is also pushing some of this AI out of the data center entirely and onto the smartphones themselves. Typically, when you use a deep learning app on your phone, it can't run without sending information back to the data center. All the AI happens there. When you bark a command into your Android phone, for instance, it must send your command to a Google data center, where it can processed on one of those enormous networks of CPUs or GPUs. But Google has also honed its AI engine so that it, in some cases, it can execute on the phone itself. ""You can take a model description and run it on a mobile phone,"" Dean says, ""and you don't have to make any real changes to the model description or any of the code."" This is how the company built its Google Translate app. Google trains the app to recognize words and translate them into another language inside its data centers, but once it's trained, the app can run on its own—without an Internet connection. You can point your phone a French road sign, and it will instantly translate it into English. That's hard to do. After all, a phone offers limited amounts of processing power. But as time goes on, more and more of these tasks will move onto the phone itself. Deep learning software will improve, and mobile hardware will improve as well. ""The future of deep learning is on small, mobile, edge devices,"" says Chris Nicholson, the founder of a deep learning startup called Skymind. GPUs, for instance, are already starting to find their way onto phones, and hardware makers are always pushing to improve the speed and efficiency of CPUs. Meanwhile, IBM is building a ""neuromorphic"" chip that's designed specifically for AI tasks, and according to those who have used it, it's well suited to mobile devices. Today, Google's AI engine runs on server CPUs and GPUs as well as chips commonly found in smartphones. But according to Google engineer Rajat Monga, the company built TensorFlow in a way that engineers can readily port it to other hardware platforms. Now that the tool is open source, outsiders can begin to do so, too. As Dean describes TensorFlow: ""It should be portable to a wide variety of extra hardware."" So, yes, the world of hardware is changing—almost as quickly as the world of software. You may also like:"
644,https://www.wired.com/2015/11/google-open-sources-its-artificial-intelligence-engine/,Wired,2015,11,9,1992.0," Tech pundit Tim O'Reilly had just tried the new Google Photos app, and he was amazed by the depth of its artificial intelligence. O'Reilly was standing a few feet from Google CEO and co-founder Larry Page this past May, at a small cocktail reception for the press at the annual Google I/O conference—the centerpiece of the company's year. Google had unveiled its personal photos app earlier in the day, and O'Reilly marveled that if he typed something like ""gravestone"" into the search box, the app could find a photo of his uncle's grave, taken so long ago. The app uses an increasingly powerful form of artificial intelligence called deep learning. By analyzing thousands of photos of gravestones, this AI technology can learn to identify a gravestone it has never seen before. The same goes for cats and dogs, trees and clouds, flowers and food. The Google Photos search engine isn't perfect. But its accuracy is enormously impressive—so impressive that O'Reilly couldn't understand why Google didn't sell access to its AI engine via the Internet, cloud-computing style, letting others drive their apps with the same machine learning. That could be Google's real money-maker, he said. After all, Google also uses this AI engine to recognize spoken words, translate from one language to another, improve Internet search results, and more. The rest of the world could turn this tech towards so many other tasks, from ad targeting to computer security. Well, this morning, Google took O'Reilly's idea further than even he expected. It's not selling access to its deep learning engine. It's open sourcing that engine, freely sharing the underlying code with the world at large. This software is called TensorFlow, and in literally giving the technology away, Google believes it can accelerate the evolution of AI. Through open source, outsiders can help improve on Google's technology and, yes, return these improvements back to Google. ""What we're hoping is that the community adopts this as a good way of expressing machine learning algorithms of lots of different types, and also contributes to building and improving [TensorFlow] in lots of different and interesting ways,"" says Jeff Dean, one of Google's most important engineers and a key player in the rise of its deep learning tech. In recent years, other companies and researchers have also made huge strides in this area of AI, including Facebook, Microsoft, and Twitter. And some have already open sourced software that's similar to TensorFlow. This includes Torch—a system originally built by researchers in Switzerland—as well as systems like Caffe and Theano. But Google's move is significant. That's because Google's AI engine is regarded by some as the world's most advanced—and because, well, it's Google. ""This is really interesting,"" says Chris Nicholson, who runs a deep learning startup called Skymind. ""Google is five to seven years ahead of the rest of the world. If they open source their tools, this can make everybody else better at machine learning."" To be sure, Google isn't giving away all its secrets. At the moment, the company is only open sourcing part of this AI engine. It's sharing only some of the algorithms that run atop the engine. And it's not sharing access to the remarkably advanced hardware infrastructure that drives this engine (that would certainly come with a price tag). But Google is giving away at least some of its most important data center software, and that's not something it has typically done in the past. Google became the Internet's most dominant force in large part because of the uniquely powerful software and hardware it built inside its computer data centers—software and hardware that could help run all its online services, that could juggle traffic and data from an unprecedented number of people across the globe. And typically, it didn't share its designs with the rest of the world until it had moved on to other designs. Even then, it merely shared research papers describing its tech. The company didn't open source its code. That's how it kept an advantage. With TensorFlow, however, the company has changed tack, freely sharing some of its newest—and, indeed, most important—software. Yes, Google open sources parts of its Android mobile operating system and so many other smaller software projects. But this is different. In releasing TensorFlow, Google is open sourcing software that sits at the heart of its empire. ""It's a pretty big shift,"" says Dean, who helped build so much of the company's groundbreaking data center software, including the Google File System, MapReduce, and BigTable. Deep learning relies on neural networks—systems that approximate the web of neurons in the human brain. Basically, you feed these networks vast amounts of data, and they learn to perform a task. Feed them myriad photos of breakfast, lunch, and dinner, and they can learn to recognize a meal. Feed them spoken words, and they can learn to recognize what you say. Feed them some old movie dialogue, and they can learn to carry on a conversation—not a perfect conversation, but a pretty good conversation. Typically, Google trains these neural nets using a vast array of machines equipped with GPU chips—computer processors that were originally built to render graphics for games and other highly visual applications, but have also proven quite adept at deep learning. GPUs are good at processing lots of little bits of data in parallel, and that's what deep learning requires. But after they've been trained—when it's time to put them into action—these neural nets run in different ways. They often run on traditional computer processors inside the data center, and in some cases, they can run on mobile phones. The Google Translate app is one mobile example. It can run entirely on a phone—without connecting to a data center across the 'net—letting you translate foreign text into your native language even when you don't have a good wireless signal. You can, say, point the app at a German street sign, and it will instantly translate into English. TensorFlow is a way of building and running these neural networks—both at the training stage and the execution stage. It's a set of software libraries—a bunch of code—that you can slip into any application so that it too can learn tasks like image recognition, speech recognition, and language translation. Google built the underlying TensorFlow software with the C++ programming language. But in developing applications for this AI engine, coders can use either C++ or Python, the most popular language among deep learning researchers. The hope, however, is that outsiders will expand the tool to other languages, including Google Go, Java, and perhaps even Javascript, so that coders have more ways of building apps. According to Dean, TensorFlow is well suited not only to deep learning, but to other forms of AI, including reinforcement learning and logistic regression. This was not the case with Google's previous system, DistBelief. DistBelief was pretty good at deep learning—it helped win the all-important Large Scale Visual Recognition Challenge in 2014—but Dean says that TensorFlow is twice as fast. In open sourcing the tool, Google will also provide some sample neural networking models and algorithms, including models for recognizing photographs, identifying handwritten numbers, and analyzing text. ""We'll give you all the algorithms you need to train those models on public data sets,"" Dean says. The rub is that Google is not yet open sourcing a version of TensorFlow that lets you train models across a vast array of machines. The initial open source version only runs on a single computer. This computer can include many GPUs, but it's a single computer nonetheless. ""Google is still keeping an advantage,"" Nicholson says. ""To build true enterprise applications, you need to analyze data at scale."" But at the execution stage, the open source incarnation of TensorFlow will run on phones as well as desktops and laptops, and Google indicates that the company may eventually open source a version that runs across hundreds of machines. Why this apparent change in Google philosophy—this decision to open source TensorFlow after spending so many years keeping important code to itself? Part of it is that the machine learning community generally operates in this way. Deep learning originated with academics who openly shared their ideas, and many of them now work at Google—including University of Toronto professor Geoff Hinton, the godfather of deep learning. But Dean also says that TensorFlow was built at a very different time from tools like MapReduce and GFS and BigTable and Dremel and Spanner and Borg. The open source movement—where Internet companies share so many of their tools in order to accelerate the rate of development—has picked up considerable speed over the past decade. Google now builds software with an eye towards open source. Many of those earlier tools, Dean explains, were too closely tied to the Google infrastructure. It didn't really make sense to open source them. ""They were not developed with open sourcing in mind. They had a lot of tendrils into existing systems at Google and it would have been hard to sever those tendrils,"" Dean says. ""With TensorFlow, when we started to develop it, we kind of looked at ourselves and said: 'Hey, maybe we should open source this.'"" That said, TensorFlow is still tied, in some ways, to the internal Google infrastructure, according to Google engineer Rajat Monga. This is why Google hasn't open sourced all of TensorFlow, he explains. As Nicholson points out, you can also bet that Google is holding code back because the company wants to maintain an advantage. But it's telling—and rather significant—that Google has open sourced as much as it has. Google has not handed the open source project to an independent third party, as many others have done in open sourcing major software. Google itself will manage the project at the new Tensorflow.org website. But it has shared the code under what's called an Apache 2 license, meaning anyone is free to use the code as they please. ""Our licensing terms should convince the community that this really is an open product,"" Dean says. Certainly, the move will win Google some goodwill among the world's software developers. But more importantly, it will feed new projects. According to Dean, you can think of TensorFlow as combining the best of Torch and Caffe and Theano. Like Torch and Theano, he says, it's good for quickly spinning up research projects, and like Caffe, it's good for pushing those research projects into the real world. Others may disagree. According to many in the community, DeepMind, a notable deep learning startup now owned by Google, continues to use Torch—even though it has long had access to TensorFlow and DistBelief. But at the very least, an open source TensorFlow gives the community more options. And that's a good thing. ""A fair bit of the advancement in deep learning in the past three or four years has been helped by these kinds of libraries, which help researchers focus on their models. They don't have to worry as much about underlying software engineering,"" says Jimmy Ba, a PhD student at the University of Toronto who specializes in deep learning, studying under Geoff Hinton. Even with TensorFlow in hand, building a deep learning app still requires some serious craft. But this too may change in the years to come. As Dean points out, a Google deep-learning open source project and a Google deep-learning cloud service aren't mutually exclusive. Tim O'Reilly's big idea may still happen. But in the short term, Google is merely interested sharing the code. As Dean says, this will help the company improve this code. But at the same time, says Monga, it will also help improve machine learning as a whole, breeding all sorts of new ideas. And, well, these too will find their way back into Google. ""Any advances in machine learning,"" he says, ""will be advances for us as well."" Correction: This story has been updated to correctly show the Torch framework was originally developed by researchers in Switzerland. You may also like:"
645,https://www.wired.com/story/artificial-intelligence-rap-generator/,Wired,2015,11,6,346.0," If you've always dreamed of being a rapper but were stuck for inspiration, you're in luck -- a team of computer scientists have created an artificially intelligent rap lyric generator -- and now you can try it yourself. As previously covered in print by WIRED, DeepBeat was originally a tool used in research by a team from the universities of Helsinki and Aalto. Now for the first time it has been made available to the public. It works by gathering data from previously existing songs by over one hundred artists including Jay Z and Lil Wayne -- from this data, lines are selected by rhythm, pace and rhyme. ""Writing rap lyrics requires both creativity, to construct a meaningful and an interesting story, and lyrical skills, to produce complex rhyme patterns, which are the cornerstone of a good flow,"" said Eric Malmi, one of the research team. ""DeepBeat is a method for capturing both of these aspects."" Users can write their own lyrics or generate pre-existing ones, as well as generate lines that rhyme. You can also choose the topic or theme of your rap by entering keywords -- which brings up lines containing your chosen word. The neural network developed by the team is able to capture the semantic similarity of lines in order to generate a rhythm that suits the rest of the rap. The network also utilises rhyme density, which measures the technical quality of lyrics. The team hope that making the program public will help improve its ability to create compelling and interesting raps. ""By collecting data about how people use this algorithm, we can improve the algorithm and validate it: does the algorithm generate good lyrics?"" Malmi said. ""We launched it yesterday, so we don't know how people will be using it yet, but we're excited to see what people come up with,"" Malmi told WIRED via email. ""One idea we had in mind though was using DeepBeat to create a rap for a special occasion, like a birthday party"". You can have a go on DeepBeat at the research team's website."
646,https://www.wired.com/story/martin-ford-and-learning-robots/,Wired,2015,11,4,710.0," This article was first published in the November 2015 issue of WIRED magazine. Be the first to read WIRED's articles in print before they're posted online, and get your hands on loads of additional content by subscribing online. In May 2015, a team of researchers from the University of Science and Technology of China and Microsoft's Beijing office announced a new system, based on rapidly advancing ""deep learning"" technology, that's capable of outperforming the average person on the kind of verbal-reasoning puzzles found in IQ tests. Deep learning uses artificial neural networks, which operate according to the same essential principles as the biological neurons in the brain. Although neural networks have been used to perform basic pattern-recognition tasks for decades, the past few years have heralded dramatic breakthroughs that now allow deep-learning neural networks of unprecedented power and complexity to be employed in areas such as image recognition, language translation and problem solving. A team at the University of California, Berkeley used deep learning techniques to build a robot that figures out how to perform tasks such as screwing  the top on a bottle, with the same speed and precision as a person. Deep learning offers just one of the more vivid illustrations of how smart software algorithms are becoming ever more capable at solving problems, making decisions, and -- most importantly -- learning to do new things. This advancing ability to learn and adapt has dramatic implications for the job market because, for the first time, machines are encroaching on the intellectual capability and adaptability that has, so far, ensured that most workers are able to remain relevant as technology progresses. The impact on more routine, repetitive jobs is already evident. A recent analysis by an American consulting firm found that the number of people employed in the finance departments of large corporations, for a given level of revenue, has collapsed by about 40 percent since 2004. Routine work areas, such as bookkeeping and accounts payable and receivable, are increasingly being taken on by smart software, and the people who used to have secure white-collar jobs performing  these tasks are finding themselves in the unemployment queue. Workers who have higher education levels may be tempted to take comfort in the belief that their jobs are non-routine and, therefore, beyond the reach of the machines. However, the frontier is advancing rapidly, and software has already progressed far beyond simply automating rote repetitive jobs. Smart systems are writing news articles, performing sophisticated legal-document reviews and evaluating medical images. In fact, ""routine"" may not be the best word to describe the jobs most likely to be automated. A better term might be ""predictable"". Could another person learn to do your job by studying a detailed record of what you've done in the past? Or could someone become proficient by repeating the tasks you've already completed, in the way a student might take practice tests to prepare for an exam? If so, then there's a good chance that a machine-learning algorithm may eventually be able to figure out how to do much, or all, of your job. The conventional solution to technology-driven job losses has always been ever more education and vocational training. As machines and smart software eat away at low-skill jobs, workers are urged to retool themselves and continually seek higher-skill positions. It turns out, however, that workers are not the only ones who can climb the skills ladder: computer technology is proving remarkably adept at the same feat. Evidence for this is in the job market. In the UK, average starting salaries for graduates with degrees fell by about 11 percent over the five years from 2007 to 2012, from £24,293 to £21,701. And in the US, any recent graduate can tell you that we have entered the age of the degree-bearing barista: half of new graduates end up taking jobs that don't use their education. The upshot is that smart machines may eventually create an economy where a large number of workers cannot remain relevant. That will force us to confront the fact that we have entered a new age -- in which the relationship between workers and machines has changed. Adapting to that reality could well be one of the seminal challenges facing us in the coming decades."
647,https://www.wired.com/2015/11/augury-lets-machines-hear-when-theyre-about-to-break-down/,Wired,2015,11,4,716.0," About four years ago software developer Gal Shaul boarded a flight from Tel Aviv, Israel, to Delhi, India. Shaul worked for a medical device startup, and he'd been dispatched to troubleshoot an overheating product for one of the company's clients. But as soon as he arrived on the scene, he knew it wasn't a software problem at all: he could hear that the machine's fan was clogged from across the room. The sounds machines make reveal quite a bit about whether they're working properly and what's wrong with them if not. That's why the first thing a mechanic does when you bring your car into the shop is pop the hood and listen to the engine. Shaul's 11-hour flight to India could have been avoided if someone had thought to put a phone up to the device and let a support technician listen to it. But to Shaul, the experience revealed a more fundamental problem: the software running on the device didn't have any idea what was going on with the hardware. The machine had no way of listening to its own sound, and therefore no way to alert its owner or its developers that something was wrong. So he called a college friend Saar Yoskovitz, an expert in analog signal processing, the complex mathematics involved in processing non-digital signals such as sound. Together the pair founded Augury, a company dedicated to giving machines a sense of hearing. They like to refer to it as ""Shazam for machines,"" referring to the popular app that can listen to and recognize songs. Augury makes a gadget that customers can attach to equipment such as commercial refrigerators or industrial scale heaters. The gadget records vibrations and ultrasonic sound and uploads it to Augury's cloud service, where it's analyzed to make predictions about the health of the machine being monitored. Technicians can then use the company's mobile app to view the status of a machine and any alerts that might indicate that something is going wrong with it. That might sound like a privacy and security nightmare, but Yoskovitz say Augury isn't recording the full audio of the entire space in which its hardware is installed, just the vibration patterns produced by the monitored machine, along with various inaudible frequencies. A snooper would have a hard time making out anything even if some of the sound waves from a conversation did manage to find their way into the device's contact microphones. ""The noise levels in a mechanical room are so loud that people have a hard time hearing each other, so it will be hard to filter the conversation from the background noise,"" he says. All of this audio and data is analyzed and stored so that the sound of one customer's machine can be compared with the sound of all others. The idea is that Augury won't need to customize its software for each different type of appliance its customers want to monitor. Instead, it will be possible to simply install the sensors and listen to the device to establish an idea of what it sounds like when it's functioning normally and alert owners of abnormalities. Over time it will also learn which specific sounds precede specific types of failure. For example, if Augury's software had never heard the sound of a clogged vacuum hose, it would first alert a machine's owners or technicians that it was making an unusual sound so they could check to see if there was a problem. Then, after hearing the sound of a few clogged hoses before a device failure at different customer sites, the software will learn the sound of a clogged hose, someone will label the sound as such, and Augury will be able to send more specific alerts to its customers—including those who have never had a clogged hose problem before. And since a clogged hose will make similar sounds whether it's part of a commercial refrigerator or an oil pump or a car, the software will be able to generalize that sound across many different types of equipment. Yoskovitz thinks this could end up doing a lot more than just saving technicians from making unnecessary plane trips. By giving manufacturers a deeper understanding of the often complex reasons that their products fail, Augury could help companies build better products."
648,https://www.wired.com/2015/11/facebook-is-aiming-its-ai-at-go-the-game-no-computer-can-crack/,Wired,2015,11,3,1475.0," In the mid-'90s, a computer program called Chinook beat the world's top player at the game of checkers. Three years later, to much fanfare, IBM's Deep Blue supercomputer won its chess match against reigning world champion Gary Kasparov. And in 2011, another IBM machine, Watson, topped the best humans at Jeopardy!,the venerable TV trivia game show. Machines can now beat the best humans at a wide range of games traditionally held up as tests of intellect, from Scrabble to Othello. But there's one notable pastime where we humans still come out on top: the game of Go. With all those other games, computers can win by, in essence, analyzing the many possible outcomes of every possible move. Yes, a chess grandmaster like Kasparov can look ahead in a similar way. But a machine can examine far more future moves than Kasparov ever could. Likewise, a machine can look ahead in a game of Go—the Eastern version of chess—but in this case, looking ahead is far more difficult. On a Go board—a 19-by-19 grid where players place pieces at the intersection of two lines—the number of possible moves is far greater, and identifying the benefits of a particular move is far more complicated, even mysterious. The top players will tell you they play in a way that's, on some level, subconscious. Getting a computer to play this way is another task entirely. You can't use the same approach as a Deep Blue or a Watson. With this in mind, researchers at Facebook are now tackling Go with an increasingly important form of artificial intelligence known as deep learning. In recent years, companies like Facebook, Google, and Microsoft have shown that deep learning is remarkably adept at recognizing photos, identifying spoken words, and translating from one language to another. To recognize a cat, for instance, a deep learning system analyzes thousands of known cat photos, feeding each into a network of machines that approximate the neural networks of the human brain. Thanks to these neural networks, your Facebook app can recognize photos of you and your friends. Google's smartphone digital assistant can recognize the commands you bark into your Android phone. And Microsoft can instantly translate your Skype calls. Now, Facebook is using similar technology to recognize a promising Go move—to visually understand whether it will be successful, kind of like a human would. Researchers are feeding images of Go moves into a deep learning neural network so that it can learn what a successful move looks like. ""We're pretty sure the best [human] players end up looking at visual patterns, looking at the visuals of the board to help them understand what are good and bad configurations in an intuitive way,"" Facebook CTO Mike ""Schrep"" Schroepfer told reporters at Facebook's California headquarters last week, before delivering a speech along similar lines this morning at the Web Summit in Dublin. ""So, we've taken some of the basics of game-playing AI and attached a visual system to it, so that we're using the patterns on the board—a visual rec[ognition] system—to tune the possible moves the system can make."" Though this system is only about two or three months old, he says, it can already beat systems built solely with more traditional AI techniques. On one level, this work is a curiosity, a sideshow to the deep learning systems the company is building to tackle specific tasks across the world's most popular social network. Facebook is using neural nets to better determine what you want to see in your Facebook News Feed. It's building another system for blind Facebook users that can automatically describe photos via a text-to-speech engine. But the company's Go work—which Schrep describes as ""super early""—demonstrates why deep learning is so powerful and how it can continue to push the boundaries of what machines can do. Solving big AI problems requires a wide range of technologies, and deep learning can provide something akin to human intuition—or at least approximate the kind of intuitive tasks that we humans find difficult to explain. This includes playing Go, but such game playing is merely a small step towards something larger. After achieving so much success with image and speech recognition, many researchers believe, deep learning can now help computers understand ""natural language""—the way that humans naturally speak. During his briefing with reporters, Schrep also nodded to a system he has demonstrated in the past, where a neural net analyzes a synopsis of The Lord of the Rings and then answers questions about the plot of the J.R.R. Tolkien trilogy. ""We took a very, very, very short version of The Lord of the Rings and fed it into the system,"" Schrep said. ""And then immediately, right after you feed it this, you can start asking it questions, about the data it has just seen, that are relatively complex—that involve spatial relationships."" This is indicative of a wider trend. Google recently published a research paper describing a computer bot that could—on some level—debate the meaning of life. A startup called MetaMind is exploring similar avenues. Andrew Ng, the director of research at Chinese Internet giant Baidu, which also sits at the sharp end of deep learning research, says the path to natural language understanding is a difficult one and that small advances—not to mention packaged demos—should be taken with a grain of salt. But he too sees some promise here. ""We get it right a lot of time,"" he says. ""And we totally screw up some of the time."" As such work progresses, Facebook, like Baidu, hopes to apply these ideas to more pointed tasks. Last month, a team of Facebook researchers showed WIRED the deep learning system they're building for Facebook users who are visual impaired. The system can identify objects in a photo, determine whether the faces in the photo are smiling, and decide whether the photo was taken indoors or out—before sharing this info via a text-to-speech app. Now, Schrep says, the project has progressed further, into the realm of natural language understanding. The company has built a system that would allow blind users to ask questions along the lines of ""Is there a baby in the photo?"" and ""What is the man in the photo doing?"" and ""Is the baby sitting in his lap?"" (see images below). ""You take reasoning—the ability to ask questions and understand new data—and you take image understanding and segmentation and you put the two together,"" Schrep explained, ""and you build what we call visual Q&A."" Image ""segmentation"" is where the system correctly distinguishes between different objects in a photo—separating, say, the baby from the man. Meanwhile, the company aims to build an even higher form of AI through the digital assistant it calls M. Today, while under test with a few hundred users in the San Francisco Bay Area, M is largely driven by human operators: you ask the tool a question along the lines of ""Can you make me a dinner reservation for tonight?,"" a fairly simple AI system suggests at least a partial solution, and then the humans perform the task. That is, they visit the restaurant's website and make the reservation. The trick, however, is that Facebook is carefully tracking everything the human operators do, so it can feed this information into neural networks and teach machines to perform the same tasks. During his talk, Schrep revealed that, just a few months after M's debut, the company is already feeding this data into neural networks and, indeed, improving the AI that underpins the system. If you ask M to buy some flowers, for instance, the system will now respond with two questions of its own: ""What's your budget?"" and ""Where do you want them sent?"" These two questions, Schrep says, were pinpointed by neural networks, after they analyzed the way the human operators interacted with users. ""There is some percentage of responses that is coming straight from the AI,"" he said, ""and we will increase that percentage over time."" Both this system and the system for the blind have yet to reach the wider public. But they aren't that far from real-world deployment. The company's work with Go is perhaps much further from concrete success. But there are already a few signs of success, Schrep says, and it shows where this new form of AI is moving. Last week, he showed off another mini-project where a Facebook deep learning system looks at a stack of digital blocks and (accurately) predicts whether or not it will fall. A ""key problem in artificial intelligence is figuring out what's going to happen next,"" he said. ""You do this all the time in order to make your day go well. ... What we've got to do is teach computer systems to understand the world in a similar way."" That's why Facebook's neural networks are learning to play Go—and maybe, someday, they'll crack it."
649,https://www.wired.com/story/drone-autonomous-fly-trees-mit/,Wired,2015,11,3,339.0," A lightweight, quick thinking, autonomous drone has been unveiled that can dodge and dive its way around obstacles while flying at 30mph. Created by computer scientists and artificial intelligence experts at MIT it's hoped the drone will help to create systems that can ""fly quickly and navigate in the real world"". The 34-inch wingspan drone moves away from using sensors, like lidar, to sense what is around it. Instead the MIT drone uses an onboard computer, artificial intelligence, cameras and other equipment to build a real-time map of its surroundings. For drone deliveries to become a reality UAVs need to be able to work their way around buildings, pedestrians, lampposts and anything else in their way. But as drones have started to become a mainstream reality there have been a number of incidents where they've crashed into objects around them. One crashed into a building in St. Louis, another smashed into a building in Manhattan, and another dumb drone hit electricity cables in Hollywood, causing a blackout. MIT researchers hope their system will prove to be more durable. ""Operating at 120 frames per second, the software extracts depth information at a speed of 8.3 milliseconds per frame,"" MIT researcher Andrew Barry said in a blog post. The drone looks 10 meters ahead constantly, and builds a map based on what it sees. Barry said that when the drone is travelling at high speeds it doesn't need to build up a detailed picture of what is close by as it's soon moved on. For drones that travel at slower speeds there is a greater need to build a detailed pictures of what is near because in those situations other objects, such as people, are more likely to move into its path. The new system comes at a cost, although not as much as you might expect; the team says it was built using off-the-shelf parts totally around £1,100. The next step for the MIT staff is to build a system that can navigate its way through more densely populated areas."
650,https://www.wired.com/2015/10/if-we-want-humane-ai-it-has-to-understand-all-humans/,Wired,2015,10,30,962.0," The first picture flashes on the screen. “A man is standing next to an elephant,” a robotic voice intones. Another picture appears. “A person sitting at a table with a cake.” Those descriptions are obvious enough to a person. What makes them remarkable is that a human is not supplying the descriptions at all. Instead, the tech behind this system is cutting-edge artificial intelligence: a computer that can “see” pictures. Fei-Fei Li, director of the Stanford Artificial Intelligence Lab, is standing on a lit stage in a dark auditorium showing off the advanced object-recognition system she and her fellow researchers built. But as impressive as the system is, Li grows more critical as her presentation unfolds. She says that even if the computer is technically accurate, it could do more. The computer may be able to describe in simple, literal terms what it ""sees"" in the pictures. But it can't describe the stories behind the pictures. The person sitting at the table, for instance, is actually a young boy—Li’s son, Leo. Li explains that he is wearing his favorite T-shirt. It’s Easter, and we non-computers can all see how happy he is. “I think of Leo constantly and the future world he will live in,” Li tells the audience at TED in a video that's been viewed more than 1.2 million times. In Li’s ideal future, where machines can see, they won’t just be built for maximum efficiency. They’ll be built for empathetic purposes. Artificial eyes, for instance, could help doctors diagnose and take care of patients. If robot cars had empathy, they could run smarter and safer on roads. (Imagine if the builders of self-driving cars used algorithms that didn’t account for the safety of pedestrians and passengers.) Robots, Li says, could brave disaster zones to save victims. Li is one of the world’s foremost experts on computer vision. She was involved in building two seminal databases, Caltech 101 and ImageNet, that are still widely used by AI researchers to teach machines how to categorize different objects. Given her stature in the field, it’s hard to overstate the importance of her humanitarian take on artificial intelligence. That's because AI is finally entering the mainstream. In recent years, Internet giants like Google, Facebook, and Microsoft have doubled down on AI, using brain-like systems to automatically recognize faces in photos, instantly translate speech from one language to another, target ads and more. And simpler forms of AI are now pervasive. Amazon uses a form of AI in recommending products you might like on its popular retail site. Yet as AI becomes ever more popular, it’s also going through a crisis of sorts. Research from the Bureau of Labor Statistics shows that by 2020, the US economy will have 1 million more computer-science-related jobs available than graduating students qualified to fill them—a gap we’ll soon desperately need to fill. At the same time, notable figures like Elon Musk, Stephen Hawking, and Bill Gates have publicly worried that artificial intelligence could evolve to a point where humanity will not be able to control it. A kind of doomsday strain of thinking around AI might be a little exaggerated, according to Li. But it does point to the importance of being mindful about how AI technology develops going forward—and right now. In a tech industry—and research community—that is still largely white and male, the danger arises of a less-than-humane AI that doesn’t take everyone’s needs and perspectives into account. Even as more people join the conversation around diversity in tech, recent examples show what happens when products aren’t designed to serve the most diverse population possible. In 2014, Apple introduced HealthKit, which the company presented as a comprehensive tracking system for human health. But it seemed to have forgotten about humans who have periods, at least until it corrected the oversight with a software update a year later. The Apple incident wasn’t specifically AI going awry due to diversity problems, but this July, it did at Google: The search giant apologized profusely when its new Photos app, which automatically tags pictures using its own artificial intelligence software, identified an African-American couple as “gorillas.” (“This is 100 percent not OK,” said Google executive Yonatan Zunger after the company was made aware of the error.) “The diversity crisis is the same crisis we talk about as a society in asking, ‘Is technology soulless?’” Li says, speaking frankly about her disappointment in the AI community being less than welcoming to members of underrepresented minorities. Among 15 full-time faculty members in her department, she’s the only woman. Elsewhere within the industry, the 44-person Facebook AI research team includes just five women. At Baidu, the 42-person AI team includes three female researchers. In her own lab, Li says there are few students of color. These numbers aren’t just bad in themselves; they bode badly for the prospects of developing truly humane AI. “I think the combination of being a professor and becoming a mother got me thinking really deeply about these issues,” says Li, who was born in China and migrated to the US when she was 16. “You feel so much more responsible for the future generations.” Li holds Friday afternoon wine and cheese sessions for women in AI every other week at her office. Recently, she also greenlit and helped carry out a one-of-a-kind project: the Stanford Artificial Intelligence Laboratory’s Outreach Summer program (SAILORS), the country’s first AI summer camp for ninth-grade girls. “This is a field that’s producing technology that is so relevant to every aspect of human lives,” Li says. As such, it’s vital that people doing the work have the perspective to make such a crucial technology relevant to every human’s life. “To bring diversity into a highly innovative and impactful field fundamentally has good value.”"
651,https://www.wired.com/story/ai-can-tell-a-good-selfie-from-bad/,Wired,2015,10,30,254.0," This content can also be viewed on the site it originates from. Andrej Karpathy has the formula for taking the perfect selfie -- and he worked it out using artificial intelligence. Karpathy -- a Computer Science PhD candidate at Stanford and summer intern with Google DeepMind -- has created an AI network that can rank millions of selfies, and work out what makes each successful. Karpathy collected two million selfies for his project, and ran them through a convolutional neural network. He fed the system with some data about which selfies were ""good"" or ""bad"" -- depending on the number of likes they received, how many followers the user had and the number of tags on the image -- and tuned its filters to improve the data. Once the AI had a million ""good"" and a million ""bad"" selfies, Karpathy gave the system 50,000 unseen selfies and had it rank them. The 100 best selfies, according to the AI, were all photos of women -- sorry boys, your selfies just aren't as compelling. The similarities among these 100 images offer some tips for taking the best selfie, as compiled by Karpathy: And for the boys: Then there's the bad: the things to avoid when taking a selfie, according to Karpathy, are taking your picture in low lighting, making your head too big, and group selfies. Brave WIRED readers can ask Karpathy's machine what it thinks of your selfies by tweeting him at Selfie Bot. You can also read Karpathy's full post on Git Hub."
652,https://www.wired.com/2015/10/ibm-is-about-to-become-the-best-weather-forecaster-of-all-time/,Wired,2015,10,28,808.0," In addition to Jeopardy champion and doctor, you can now add meteorologist to the list of hats IBM's Watson supercomputer wears. IBM confirmed today that it will acquire the digital assets of the Weather Company, the corporate parent of the Weather Channel and Weather.com. Terms of the acquisition were not disclosed, but the deal includes essentially all of the Weather Company's assets other than the Weather Channel television station, including Weather.com, the Weather Channel mobile apps, the Weather Underground website and, perhaps most importantly, Weather Services International, a division that sells weather data to companies such as airlines and the insurance industry. The Weather Company and IBM may seem like an odd pairing at first, but IBM already has a partnership with the Weather Company to jointly sell its weather data services and incorporate that data into its cloud-based (no pun intended) Watson services. The acquisition will allow IBM the ability to take fuller advantage of the Weather Company's data, plus control of a mobile app that's installed on tens of millions of smart phones around the world. The core idea of Watson is that users can ask questions in natural language---such as ""what will the weather be like in San Francisco next week""---and the system will provide an answer. Although it started out as a proof-of-concept that just answeedr Jeopardy questions, IBM has invested more than a billion dollars to commercialize the technology through a cloud service that businesses and governments can use to analyze data and conduct research. The more data the Watson system has, the more questions it can answer accurately. To help researchers at the Memorial Sloan-Kettering Cancer Center, for example, IBM fed medical text books and journals into the system. More recently, it signed a partnership with Twitter to pipe every single tweet into into Watson. ""The whole mission is to provide better predictive capabilities in the hands of our clients,"" says Glenn Finch, global leader of big data and analytics for IBM Global Business Services. The specific examples Finch cites are fairly banal. For example, he says that car insurance companies could advise their customers to put their cars inside during a hail storm, which could ultimately save them millions of dollars in insurance payments if those customers actually comply. But he imagines more grand possibilities in the future as the company's AI algorithms attempt to correlate data from disparate sources. ""Imagine you mash up weather data and Twitter data and economic data [in a specific city]—you get this signal that produces unbelievable analytic power,"" he says. One possibility IBM has floated in the past is the correlation of retail sales trends with weather patterns to help companies make decisions. For example, certain types of businesses, might see sales spike right before a snow storm as customers prepare for the weather, while others will likely see massive declines of patronage during the storm itself. Given enough data, customers might be able to use Watson to make some informed decisions, such as how much inventory to order or how many people to schedule for work during a storm. IBM will also feed data that it collects elsewhere into the Weather Company's system, perhaps helping it make better forecasts. ""I am 100 percent confident that we are going to uncover vast troves of data and improve the accuracy of our forecast,"" says Weather Company chief information and technology officer Bryson Koehler. For example, IBM offers a cloud service that allows developers to upload and analyze data from various ""Internet of Things"" devices such as environmental sensors. Some of that data could find its way into the Weather Company's models, if the owners of that data allow it. While the Weather Company's corporate data services are the most logical fit for IBM, the deal does more than just beef up Watson's data pool. Since selling off its personal computing business in 2005 IBM hasn't had much presence in the consumer market. Weather.com and its mobile apps, however, will give IBM access to a massive audience---the Weather Channel mobile app has been installed between 50 million and 100 million times on Android alone, according to its listing within Google's Play Store. And the default weather app on iOS is Weather Channel-branded. Once again, the use cases the companies are imagining so far are fairly simple. For example, Koehler says IBM could offer local or state governments the ability to offer severe weather alerts or other emergency information through an app that many of their citizens already have installed. Although IBM is tight-lipped as to how much it's paying for the Weather Company, the deal has been valued at around at least $2 billion, according to The Wall Street Journal. The company is now owned by a consortium that includes NBCUniversal, Bain Capital and and the Blackstone Group that paid at $3.5 billion for it in 2008."
653,https://www.wired.com/2015/10/robot-radiologists-are-going-to-start-analyzing-x-rays/,Wired,2015,10,27,447.0," Google is getting really, really good at recognizing photos and videos of cats. All it took was supplying millions of examples so that the company’s software—based on a branch of artificial intelligence called deep learning—could start recognizing the difference between cats and other furry creatures. But Jeremy Howard wants to use deep learning for something a little more practical: diagnosing illnesses. And he’s finally getting his chance. Today Howard’s company, Enlitic, said it was going to start working with Capitol Health Limited, a radiology clinic with locations across Australia, to have its software look at X-rays. Enlitic won’t replace radiologists. Instead, the software is designed to help them do their jobs more quickly and make fewer mistakes. First, it checks each file submitted to make sure the image matches what the technicians say it’s supposed to be—for example, it makes sure that if an image is tagged as a left knee that it’s not actually a right knee. Then, it looks for anomalies in the image. Depending on what it finds, it assigns a priority to the X-ray and routes it to a radiologist. For example, if it finds nodules on an image of a pair of lungs, it will assign it a high-priority status and route it to a pulmonary radiologist. If it detects what appears to be an aneurysm, it will route the X-ray to a cardiovascular radiologist instead. If it finds no anomalies, it will mark it as low priority. After a radiologist has reviewed the image, the software will help write the paperwork by auto-generating text for some of the more tedious parts of a report. Enlitic’s X-ray algorithms are just the latest example of deep learning being put to practical use. Facebook is now using deep learning techniques to caption photos for the blind. Yelp recently explained how it’s using deep learning to optimize what photos it shows on restaurant listings. And similar techniques are also part of what powers Microsoft’s Skype Translate. And Enlitic isn’t the only company trying to apply artificial intelligence to medicine. IBM’s Watson has been used for research at Memorial Sloan-Kettering Cancer Center and more recently to provide diet and exercise advice. The app Bright.md aims to help physicians speed up routine appointments. But Enlitic’s deal appears to be one of the biggest real-world tests of deep learning’s ability to aid medical diagnostics. Eventually, Howard hopes the technology will help expand access to medical diagnostics as Capital Health begins opening clinics in Asia. He cites World Economic Forum report that forecasts a severe shortage of medical experts in the developing world if training programs aren’t accelerated. With any luck, artificial intelligence can shoulder some of that burden."
654,https://www.wired.com/2015/10/facebook-artificial-intelligence-describes-photo-captions-for-blind-people/,Wired,2015,10,27,1115.0," Matt King is blind, so he can't see the photo. And though it was posted to his Facebook feed with a rather lengthy caption, that's no help. Thanks to text-to-speech software, his laptop reads the caption aloud, but it's in German. And King doesn't understand German. But then he runs an artificial intelligence tool under development at Facebook, and after analyzing the photo, the tool goes a long way towards describing it. The scene is outdoors, the AI says. It includes grass and trees and clouds. It's near some water. King can't completely imagine the photo—a shot of a friend with a bicycle during a ride through European countryside—but he has a decent idea of what it looks like. ""My dream is that it would also tell me that it includes Christoph with his bike,"" King says. ""But from my perspective as a blind user, going from essentially zero percent satisfaction from a photo to somewhere in the neighborhood of half ... is a huge jump."" The 49-year-old King is part of the Facebook Accessibility Team. This means he works to hone the world's most popular social network so that it can properly serve people with disabilities, including people who are deaf, people without full use of their hands, and, yes, people who are blind, like King himself. Though that AI tool is merely a prototype, Facebook plans to eventually share it with the world at large. And that's no small thing. About 50,000 people actively use the social network through Apple Voiceover, a popular text-to-speech system, and the overall population of blind Facebookers is undoubtedly much larger. Like other social networks, Facebook is an extremely visual medium. But with help from a tool like Apple Voiceover, someone like King—who lost the last of his sight in college—can connect with friends and colleagues over Facebook much like anyone else can. As Jessie Lorenz, the executive director of the nonprofit Independent Living Resource Center, told WIRED earlier this year: “I can ask other parents about a playdate or a repair man or a babysitter, just like anyone else would. Blindness becomes irrelevant in situations like that.” King tunes his text-to-speech tool to read Facebook posts at a rapid-fire pace—so fast that no one else in the room can understand it. That means he can browse his News Feed as quickly as the typical Facebooker. And in some cases, even without Facebook's experimental AI system, he can start to understand what's in a photo. Some photos include decent captions, and others offer meta-data describing who took them and when. But the AI system, bootstrapped with help from an accessibility researcher named Shaomei Wu and various Facebook AI engineers, pushes things significantly further. It can provide context using nothing but the photo itself. ""The team started with trying to make sure that all the products that [Facebook] builds are usable by people with disabilities,"" says Jeff Wieland, the founder and head of Facebook's accessibility team. ""Long-term, we really want to get to the point where we're building innovative technologies for people with disabilities."" Facebook's photo-reading system is based on what's called deep learning, a technique the company has long used to identify faces and objects in photos posted to its social network. Using vast neural networks—interconnected machines that approximate the web of neurons in the human brain—the company can teach its services to identify photos by analyzing enormous numbers of similar images. To identify your face, for instance, it feeds all known pictures of you into the neural network, and over time, the system develops a pretty good idea of what you look like. This is how Facebook seems to recognize you and your friends when you upload a photo and start adding tags. Google uses similar neural networks to help you locate photos inside its new Google Photos app, and the same basic technology can drive all sorts of other online tasks, from speech recognition to language translation. It's only natural that Facebook would use this technology to describe photos for the blind—though the technology is far from perfect. ""For object recognition and face recognition, we've basically reached human performance,"" says Yoshua Bengio, a professor at the University of Montreal and one of the founding fathers of deep learning. ""But there are still problems involving complex images, lighting, understanding the whole scene, and so on."" At the moment, Facebook's system merely provides a basic description of each photo. It can identify certain objects. It can tell you whether the photo was taken indoors or outdoors. It can say whether the people in the photo are smiling. But as King explains, this kind of thing can be quite useful. It's particularly useful when friends and family upload new profile pics, which typically arrive without a caption. That said, there's ample room to improve the system. Deep learning neural nets are also pretty good at grasping natural language—the way humans naturally speak—and companies such as Google and Microsoft have published research papers showing how these neural nets can be used to automatically generate more complete photo captions—captions that describe the scene in full. This would be the next logical step for Facebook. ""We're returning a list. We're not returning a story,"" Wieland says. ""But that's really where we want to go."" The work is part of a broader effort to bring Facebook to people with disabilities. The Accessibility Team, which Wieland founded after working at the User Experience Lab that tracks how Facebook is used across the 'net, also facilitates closed captioning for the deaf. It promotes the use of mouth-controlled joysticks and other tools for those who can't use their hands. And it works to ensure that the social network can be used in the developing world, where Internet connections are slower and less reliable than those in the States. At the same time, Wieland's team is hoping to push other companies in similar directions. In recent months, it helped found the Teaching Accessibility Initiative, a consortium of tech companies—including Yahoo and Microsoft—that aims to share practices in this area. And it's working to modify React, Facebook's open source app development tool, for use with text-to-speech readers and others software that aids people with disabilities. Because it's open source, anyone can use React, and according to data from GitHub, it has become an extremely popular means of building new apps. ""It's one way we can make the entire Internet accessible,"" Wieland says. The possibilities within and beyond the company are enormous. As King notes, deep learning can be applied to speech recognition as well as image recognition, to moving images as well as stills. ""AI is applicable to all those situations,"" he says. ""And it's applicable to everyone."""
655,https://www.wired.com/2015/10/intel-is-building-artificial-smarts-right-into-its-chips/,Wired,2015,10,26,255.0," Over the past few years, the world's biggest chipmaker has been buying up companies to help make its chips smarter. Through acquisitions of companies like Indisys, Xtremeinsights, and perhaps most importantly, fellow chip maker Altera (a $16.7 billion deal), Intel has devoted much its artificial intelligence efforts on baking AI into its into chips, as well as software that powers its 3-D video cameras. Today, Intel has added yet more AI to its portfolio with the purchase of Saffron Technology. Like many other AI startups, Saffron attempts to extract useful information from huge datasets via algorithms inspired in part on the way the human brain works. But instead of focusing on deep learning, the trendy branch of AI in which Google and Facebook are heavily investing, Saffron is focused its own technique called associative memory. The company was founded in 1999 by former IBM Knowledge Management and Intelligent Agent Center chief scientist Manuel Aparicio and led by former PeopleSoft executive Gayle Sheppard. It has deep roots in the enterprise software industry and cut its teeth selling software to the Department of Defense, such as a system for predicting the likely location of roadside bombs in Iraq. ""We see an opportunity to apply cognitive computing not only to high-powered servers crunching enterprise data, but also to new consumer devices that need to see, sense, and interpret complex information in real time,"" Intel New Technology Group Senior Vice President Josh Walden says. ""Big data can happen on small devices, as long as they’re smart enough and connected."""
656,https://www.wired.com/story/wired-world-2016-ben-hammersley/,Wired,2015,10,21,574.0," This article was first published in the November 2015 issue of WIRED magazine. Be the first to read WIRED's articles in print before they're posted online, and get your hands on loads of additional content by subscribing online. This article was taken from the preview of The WIRED World in 2016. In November, WIRED publishes its fourth annual trends report, a standalone magazine in which our network of expert writers and influencers predicts what's coming next. Here's a small selection of what to expect. Sometimes, when you get the numbers, things are quite obviously mad. In 2013, in London alone, 187 children were killed or seriously injured in traffic accidents. Four hundred and eighty-nine cyclists died or were badly hurt. Thousands were slightly injured, or nearly missed, or just plain scared. And that's just in one city. Even though it's a huge improvement on 20 years ago, in 2013 there were still 1,713 people killed on British roads. We all rely on cars, trucks and buses. From personal commutes and school runs to the final truck-and-van stages of the international supply chain, our society has made a trade-off: there's a certain amount of blood and mourning we'll accept if it means continuing our way of life. But that's because there's never been a choice. Vehicles have always been controlled by fallible amateurs. Today, we're starting to see the legalisation and roll-out of self-driving vehicles. At the time of writing, the Google self-driving car, for example, has been trialled over nearly three million kilometres of public road, and although it has been involved in 14 accidents, precisely none of them were its fault. In every one, the other car -- a human-driven one -- drove into it. Of course, self-driving cars aren't a guaranteed success. People like driving. But self-driving trucks? Those are a different matter. In the inner city, a human-driven truck is an unintended cyclist-killing device. Once self-driving trucks arrive, however, the situation changes entirely. Even a handful of fewer deaths a year, say, and we'll have a statistically provable moral argument. Humans shouldn't be allowed to drive trucks. They will be banned from doing so and 2016 will be the year in which public opinion will begin to sway in this direction. The shift from radical innovation to societally mandated usage will be swift. First the robot trucks will be a novelty, and we'll be nervous of them. Then we'll marvel at the precision of their driving, and perhaps be unnerved by their law abidance. A few months later, we'll hear of people who were almost, but not, hit by one -- telling stories of pedestrian error we've never heard before, because the protagonist was usually killed. Human-driven trucks will start to appear erratic and, given laws demanding drivers stop and rest and eat and such, noticeably more expensive. And then the numbers will come out, and the AI will have been much safer. From there, it's easy to save 1,000 lives. We'll mandate that only self-driving trucks are allowed in city centres, perhaps with dedicated lanes on the major roads. That will allow for the AIs to go faster than humans safely can, and sky lanes above for local delivery drones. Cyclists can share the streets without worry, AI-flown quadcopter fleets can operate without fear of falling on someone, and the driving of a car can slowly become a fetish activity. It's the future, it's inevitable, and it will save lives."
657,https://www.wired.com/story/wired-world-2016-alain-de-botton-artificial-emotional-intelligence/,Wired,2015,10,21,1497.0," Artificial Intelligence (AI) is the science of making clever machines. It's coming along very well. There are already some phenomenally smart boxes out there, including the ATM, your mobile phone and the automatic landing system  on an Airbus A380. People can do all the tasks these machines perform for us, only much more slowly and very much less reliably. When people think of how machines will help us in the future, the emphasis naturally falls on the performance of rational executive tasks. But one should also envisage a far more exciting scenario, one in which computational power is also directed at the emotional and psychological dimensions of existence. It is time to imagine not merely AI but also, and even more significantly, what we here call AEI: Artificial Emotional Intelligence. We need AEI rather badly because our emotional frailties dwarf our incapacities in raw mathematics or data management: we make extremely poor decisions about how we should manage relationships. We have little idea what job to focus on and when to quit. We don't know what to spend our money on. We get holidays wrong, have no clue how to repair friendships or handle tricky employees, and fumble as to how to reconcile with our parents. The emotional intelligence required for these things is in very short supply. It exists, but in isolated and mysterious pockets: a few people seem to cope but on a basis that is desperately private and concealed, the way that a wild strawberry might have seemed before the invention of farming. Of course, at heart, emotionally wise decisions aren't luck at all. They are the result of our brains resolving certain puzzles very  well -- and they are therefore logically also forms of intelligence that can be replicated and  improved upon artificially, with the help of microchips and code. We are at a juncture in history where we can already perceive what is coming: a particular type of scarcity will start to give way to abundance. A transformation that has occurred many times before in different areas of existence -- agriculture, transport and energy -- is about to strike the field of emotional intelligence. We're about to make emotional intelligence as common and as cheap as pencils. We're not used to thinking that the next big thing will be tackling the scarcity of wisdom, but this is what's on the cards. Knowing yourself is at the heart of directing efforts and communicating inner needs and perspectives to others. The idea of self-knowledge has always had high prestige, but the opportunities for acquiring it have been restricted. It's taken too long, been too chancy or too costly (if sought via therapy). At present, we are often deeply ignorant about where our talents lie and what our flaws and true interests are. Our consciousness doesn't grasp at all well what is happening to its owner. For example: I might not realise that I am irritable because I'm tired, not because my partner is being obtuse. But AEI will get us the self-knowledge we need. It will map brain activity and alert us in good time as to the reality of our psychological lives. We will get much better in two areas in particular: knowing what job we should be doing, and knowing whom we should try to form a relationship with, and how. Today's dating questionnaires and career counselling tests will - from the perspective of 20 years hence -- seem as barbaric and hopeless as medieval brain surgery strikes us now. We will at last be equipped with machines to which we can address the truly important emotional questions of life. AEI will give us a picture of our inner selves which will stop us making catastrophic errors on the basis of an inability to interpret our emotional functioning  and psychological potential. Education goes desperately wrong, because we're not good at knowing what particular individuals are capable of, what they really need to know, when it's best to try to teach them -- and what manner of instruction will suit them best. We all know from our own lives that there are moments when we've made remarkable strides and others where progress has been slow, but our society has yet to arrive at sound generalisations. The idea of how you make a ""good"" teacher  is still shockingly mysterious. People are therefore funnelled, sheep-like, into classrooms and talked at for hours in ways that serve a lot of them rather poorly. But our educational problems aren't just around schools; throughout life - in relationships, at work, in families - we are always stumbling because of deficiencies in learning. AEI will help us to evolve towards the best versions of ourselves. We should -- ideally -- die with far fewer regrets. We're agreed that high-quality, widely disseminated information is vital to a flourishing society.  That's why some news organisations make huge claims about how they keep us informed. But, in truth, news media feeds our brains in highly inadequate, sporadic and often manic ways. We do need good information, in the right doses, about the right subjects, to help us lead our lives. But we're currently far from getting it. This matters because the media sets the backdrop to politics: it establishes the general picture of what life is like in our society (of which we witness only a tiny portion first hand) and defines the parameters of what politicians can change and address. Getting the right information into wide circulation, however, is difficult, because our brains are designed to be more engaged by the wildly exceptional, rather than the important and the sober. In a future with AEI, we'll know how to lead people to information that is genuinely fruitful for them and their nations, rather than merely stories that shock, titillate and horrify. We'll get better media and, in turn, more democratic politics. Great works of art are, at present, incredibly rare. A Tolstoy, Picasso, Lennon or Louis Kahn are still agonisingly unique. AEI will stop us having to feel so grateful to them. We will, for example, be able to produce novels like those of Tolstoy on an industrial scale, but geared towards our own particular circumstances and cultural references. War and Peace is often appreciated for its wisdom and insight. But the author exercised his mind on understanding the world he knew: that of wealthy Russians living in the 19th century. As his readers, we are left alone to guess the implications for our own radically different lives. With AEI to hand, we would be able to summon up a Ugandan Tolstoy or an east London Tolstoy: the same level of maturity and compassion, the same verve in characterisation and storytelling, but all focused on what it's most important for us to dwell on. Consumer society is really about choice, and in so far as it makes us happy (rather than leads us to squander our wealth and  energies), it's about wise choice. This opens up a huge area for AEI. We tend to operate on a mixture of intuition, hope and habit, and are routinely abused by advertisers along the way. We're heavily influenced by what others are doing: impulse plays a big  role and we leave much to chance. AEI will mean encoding consumer intelligence. Just as we might work out a sum in our heads and then check it on a calculator, so we will check our decisions on an AEI device. AEI will reveal how we can be persuaded, moved and motivated to acquire goods and services in line with our true needs. Money will, at last, make us as happy as it can. It's extremely hard to be wise around relationships. We struggle to communicate, are impatient when we don't get what we want, defensive in discussions, and have trouble talking calmly through hopes and failings around fidelity. AEI means we will be able to tap into the lessons that others have, often painfully, learned. Each couple will not have to confront each hurdle anew late at night in the bedroom. If you could, at critical moments, (in your head) dial up your most patient, experienced and sensible friend and get them to talk you through an issue, a huge quantity of distress might be avoided. Relationship AEI will give us all access to the friend's wisdom when we most need it. There are currently some big worries over AI: what if machines take over that are really versions of very cunning, powerful people? But the cure to these is to focus on what sort of intelligence AI should emulate and enhance. Increasing emotional intelligence is key to all that we most value: empathy, creativity, kindness and generosity. Today, the major impediments to progress are all psychological. It's mainly the flaws in our emotional capacities that ruin existence. But we've not yet addressed how to make ourselves more mature. It's been left to the hazards of individual experience. AEI will use technology to reduce the randomness."
658,https://www.wired.com/story/pop-dancing-robot/,Wired,2015,10,21,157.0," Ever wanted a fully customisable robot you can dress up and make dance for your own amusement? Well, you're in luck. DMM, a Japanese electronics company, has created the Premaid AI -- a robot, modelled on J-pop stars, designed to dance on a tabletop. But it doesn't just freestyle -- software due to be released by DMM will enable users to create what they refer to as ""dance data"". This will make the robot fully programmable, making it dance on command. DMM will also provide patterns for robot clothing and other 3D-printed add-ons -- including heads and body parts. The lineup currently consists of three robots: a basic model and two limited edition models, designed by Japanese illustrators Ako Arisaka and Yui Sakima. These special models will be limited to three units each. If you're thinking of creating your very own robot J-pop group, it won't come cheap -- prices start from 138,000 yen, or about £745."
659,https://www.wired.com/story/wordsmith-robot-journalist-download/,Wired,2015,10,20,358.0," Anyone can now download and play with their very own robot journalist. Wordsmith, a platform that provides so-called robot journalists to organisations, is now available to the public. The platform, owned by Automated Insights, provides ""auto generated, data-heavy articles"" on topics such as quarterly earnings and college sports. A beta version is available now on Wordsmith's website, with a full launch expected in January 2016. The technology is already used by companies such as the Associated Press and Yahoo. Wordsmith works by creating ""branching paths"" -- conditionally adding words, phrases and sections that can be added, modified and removed depending on the article. Users enter data -- such as quarterly figures or sports results -- around which these branches are built. This story structure, once created, can be used as a template for an unlimited number of articles. All users have to do is enter new data to create a unique story. But the technology hasn't been without its critics. NPR ran a feature in which they pitted their White House correspondent against Wordsmith -- while technology beat human on speed, the human written article was, according to NPR listeners who voted online, richer and more engaging. Some argue a robot journalist could never replicate the style of a human journalist in terms of style or insight. But Automated Insights argues that Wordsmith can be stylistically modified so stories can be written as if ""a person wrote each one of them individually."" ""We focus on personalised content,"" said Automated Insights CEO Robbie Allen in a press release. ""Instead of writing one story and hoping a million people read it, Wordsmith can create a million stories targeted at each individual user and their preferences. It’s a story that is totally unique to each user because it is powered by their data."" Wordsmith isn't just limited to articles, though -- it can also produce client reports, financial summaries and product descriptions. Journalists nervous about being out of a job needn't worry, though -- The Associated Press, which has published more than 3,000 financial reports each quarter using the system, claimed that 'robot journalists' had led to zero human job losses."
660,https://www.wired.com/2015/10/this-news-writing-bot-is-now-free-for-everyone/,Wired,2015,10,20,378.0," The age of robot writers is upon us. The Associated Press uses software to generate news stories on corporate earnings reports. Fox auto-generates some sports recaps that appear on its Big Ten Network site, while Yahoo uses similar technology to create fantasy sports reports custom-made for each of its users. Now you can turn your own data into stories, too—no writing necessary. Today Automated Insights has launched a beta version of its new free service based on Wordsmith, the technology it uses to generate stories for companies like the AP. Typically, Automated Insights, much like its competitor Narrative Science, works with large customers to create the templates that the Wordsmith software fills in. This new service allows anyone to create their own templates and dump data into them on their own. It's a bit like a more complex version of Mad Libs meets mail merge. It bears a superficial resemblance to the content-spinning tools used by spammers, but this is more sophisticated. First, you upload a spreadsheet or other source of structured data. Automated Insights turns the various fields from the spreadsheet into variables that you can plug into the text template you create. There are many rules—known as branches—that you can set, such as the ability to use one set of words when a variable happens to be greater than a certain number and a different set when it happens to be lower than that number. For example, let's say you're working with a spreadsheet of quarterly sales data. You could create a template that will generate the text ""sales increased in quarter two"" if the number in the spreadsheet cell containing the quarter's total sales was bigger than the cell for quarter one. The quarter number could also be made into a variable that can be pulled directly from the spreadsheet. That way, assuming the spreadsheets are formatted in the same way each quarter, the template could be used every three months. Of course, you'll still need to know what sorts of data and what sorts of changes in that data you want Wordsmith to translate into words. But once you figure it out, the tool looks like a useful way to offload the least rewarding writing tasks onto a machine that won't mind the tedium."
661,https://www.wired.com/2015/10/yelp-is-teaching-its-computers-to-see-places-at-their-best/,Wired,2015,10,19,1110.0," Frances Haugen was part of the first wave of people to use Google back in 1996. Her mother, a faculty member at the University of Iowa1, showed her the search engine, which was still a research project at Stanford University. Haugen was blown away at what Larry Page and Sergey Brin had built. ""The idea that you could actually peer into a giant mountain of data was amazing,"" she says. Haugen has been obsessed with search technology ever since. She landed a job at Google after college and spent several years working there, first as an engineer and later as a product manager. Now she works for Yelp. You might not think of the sprawling review site as a search company, but search really is at the core of what Yelp does. You don't just want a list of the top ranked restaurants nearby; you want restaurants near you that serve cronuts, or have a great view, or allow dogs, or are good for birthday parties—or maybe all four. But the written reviews and descriptions alone that users post about a place may not contain all those details. Much of that useful information is likely locked up in the millions of photos that users have uploaded. A picture of a dog eating a cronut with birthday candles in it and the Manhattan skyline behind it is a pretty good indication that the restaurant it was taken in meets your requirements. There are other, less trivial examples of why this would be useful, as well. ""My neighbor's wife is in a wheelchair,"" Haugen says. ""He used to look through hundreds of photos trying to see what the inside looks like so he could find out if it would be wheelchair friendly."" If Yelp's computers could index which photos have wheelchairs in them, the company could offer a more educated guess as to which ones are the most accessible. Yelp is still a long way from being able to do something like that. The hard part, obviously, is teaching computers actually recognize what's in those photos. But Haugen and her team have started building on the foundations of an image recognition system at the company that could completely change the way they do search. To get started, Yelp's first image recognition project isn't actually focused on search but on surfacing the best photos taken at different locations. You see a handful of photos at the top of every Yelp entry, and these photos form your first impression of a business. What Haugen and her team have set out to do is find a way to automatically determine the perfect selection of images to give users the best sense of what that business has to offer. ""We're trying to figure how can we bubble up the the best photos, the photo that's going to make you take that risk,"" she says. ""The photo that going to let you go to that new hair stylist, or let you pick that wedding venue, or pick the restaurant to take a friend out to their birthday dinner."" That means Yelp needs a way to tell the difference between a photo of a mouthwatering steak and a blurry, drunken selfie. The obvious way to deal with this would be to rely on captions, but many photos uploaded to the site either don't have captions or have a caption that simply says something like ""amazing."" Alternately, Yelp could try to have users rate photos and only display the three best. But that might not provide enough diversity. When you're looking at the entry for Jim Bob's Steak House, you probably don't want to see three different photos of steak, no matter how well composed those shots are. You'll also want to see a photo of the fully-loaded baked potato with Gummy Bears on it, and the mechanical bull out front. Short of hiring people to filter through every single photo on the site and decide which ones to use, Yelp needs a way to teach computers to recognize what's actually in those pictures. Yelp obviously isn't the first company to deal with this challenge. Google and Facebook—not to mention law enforcement and spy agencies—have been working on facial recognition for years. A startup called Orbital Insight has been working to estimate the amount of oil left in reserves and to spot illegal deforestation by analyzing photos taken from space. Much like Yelp, a travel guide startup called Jetpac, which was acquired Google last year, had the idea of analyzing photos to determine which bars and restaurants were, say, dog-friendly. What almost all of these efforts have in common is a branch of artificial intelligence called ""deep learning,"" which aims to make machines smarter by drawing inspiration from the structure of the human brain. Tech's giants have dominated the deep learning field in recent years. Google and Facebook have hired some of the academic pioneers of the field and acquired several startups to bring their expertise in-house. Microsoft, meanwhile, turned to deep learning to build Skype Translate. But the tech giants don't have a monopoly on artificial intelligence. Because so much of the foundational research is public, companies like Yelp are able to take advantage of deep learning, too. To get its system up and running, the Yelp engineering team used a piece of open source software called Caffe to build a neural network—a piece of software inspired by the connections between neurons in the human brain—based on a paper by some of the pioneers of deep learning . But software can't do all the work. In order to recognize an object, whether that's a cat or a cupcake or a Volkswagen Bug, the algorithms much be 'trained' by humans. For that, Yelp hired people through the crowd sourcing site Crowdflower to help label a large number of photos. Yelp's initial deep learning efforts have been focused on grouping restaurant photos into four categories: pictures of food, interior views, exterior views and pictures of menus. But eventually Haugen hopes all the data discovered through this process will eventually find its way into Yelp's search functionality. In the meantime, Haugen is learning a lot about what types of photos people click on most often. Linearity works well, such as a photo of three cups of coffee lined up in a row. Smiles are always good, as is the color blue. And low angles are best. ""If you're going to take a picture,"" she says, ""you should get down on your dinner's level."" 1 Correction 10/19/2015 at 13:11 AM ET: An earlier version of this story said Haugen's mother was a professor at Stanford University. It was actually University of Iowa."
662,https://www.wired.com/story/antoine-blondeau-artificial-intelligence-wired-2015/,Wired,2015,10,16,722.0," Humanity finally has access to profound artificial intelligence, AI pioneer Antoine Blondeau has told WIRED 2015 at London's Tobacco Dock – and it is already solving real problems -- like financial markets, sepsis infections, and... shoe sales. ""This is first base, this is year one, decade one of AI,"" Blondeau said. ""There are lots of things we can do going forward. We've just begun."" Blondeau previously worked on the technology that became the foundation for Apple's Siri, and is now head of Sentient Technologies, the world's best-funded AI company. Sentient's system is already working on an unprecedented scale in the civilian sector, Blondeau said, with two million CPU cores and five thousand GPU cards distributed across 4,000 sites worldwide. ""No one had scaled AI to the level we were trying to scale it to [...] there is no theoretical limit,"" he said. Sentient's first practical application for its AI system was financial, creating a ""species"" of AI trader that could compete, reproduce, die and eventually evolve to respond to changes in the markets on a level beyond that possible in human traders. Although building an AI capable of responding to every data point contained in the markets is virtually impossible, Blondeau said -- a recent calculation by the company estimated it would take the age of the Earth and a billion CPUs to search one percent of all data points generated by financial markets -- AI can help us get closer than ever to understanding that network. Sentient has its eyes on other big problems too – for instance, finding an intelligent way to respond to sepsis infections, which kill 37,000 people in the UK every year at a rate greater than bowel cancer and lung cancer. ""I can't imagine a better place to use data,"" Blondeau said. ""It's about saving lives. It's about life and death."" this nurse would always be on the clock, always on the lookout for you'."" The nurse they eventually built, in partnership with MIT, collected data on 6,000 patients for a year and was able to use that ""to predict the onset of sepsis ahead of time with more than 90 percent accuracy"". Sentient hinted that it is now working with Oxford University on genetic problems ""brought about my biological evolution"" that would take nature ""billion"" of years to solve, but that it can tackle in ""weeks, months"", Blondeau said. So what's next? ""One way to make AI smarter, better, is to increase the scale,"" he said. ""There is no theoretical limit [...] but there is more to this."" Blondeau told WIRED2015 that the key to a big next step is to allow AI to ""sense"" more about the world, giving it access to visual, video, audio and other content so that it can more fully appreciate the context of information. The first practical area in which Sentient Technologies is looking to apply this concept is in retail; a truly useful AI system for online shopping would not just provide a list of recommendations, but be able to provide a rich, real-time dialogue with customers in the virtual world, just as you might experience with a well-trained customer assistant in a physical store. The US retailer Shoes.com will be the first recipient of this tech, when it rolls out by the end of 2015, Blondeau said. Premiering a video of the tech in action, he demonstrated how the store will adapt as you browse through the catalogue, determining what you like without the customer having to explain why. ""You cannot Googlefor that shoe,"" he said. ""There are things you cannot Google, increasingly more things. This technology solves that."" For many the prospect of intelligence at this scale is worrying, with experts across the industry and public luminaries like Stephen Hawking often warning about the potential for its misuse in the future. Blondeau is aware of the dangers, but said there was much more good to come in the short to medium term than danger. ""The ethical component is quite important,"" he admitted. ""It's important that industry focuses on this, because we know that governments take a while to react to things and puts the right framework around it."" “[But] I do not believe for one second what we are doing is dangerous, we're not there yet, [...] but it's never too early to think about this."""
663,https://www.wired.com/story/antoine-blondeau-artificial-intelligence-apple-siri/,Wired,2015,10,1,920.0," Antoine Blondeau worked on the project that became Apple's Siri. Today he runs Sentient Technologies, the world's best-funded AI company. He was previously COO of Zi Corporation, whose predictive text software has been embedded in hundreds of millions of devices. Antoine will be speaking on the Main Stage at our flagship event, WIRED2015 on October 15-16. He will take part in the session, ""When technology gets ambitious"", alongside Gabor Forgacs, Ryan Weed and Daniel McDuff. Bringing the WIRED world to life, WIRED2015 showcases the innovators changing the world and promoting disruptive thinking and radical ideas. There will be around 45 speakers over the two-day event, presenting stories about their work in science, design, business and many other fields. Can you give us some hints about what you're planning to talk about at WIRED2015? Sure, I'd be happy to. My talk centres around the role evolution has in creating a new frontier in artificial intelligence -- what I like to call evolutionary intelligence. Up until now machine-learning experts have been very focused on trying to recreate human intelligence. But what if we think bigger than that? What if we could scale AI to an unfathomable size, a size so big that it would leapfrog anything else out there and be able to solve some of the world's most complex problems? Next, imagine if this massive AI system could evolve and adapt to its environment like living species do, to get better at the tasks it's presented with. The possibilities of such a system would be extremely exciting. ""Evolutionary Intelligence agents"" would evolve by themselves -- trained to survive and thrive by writing their own code -- spawning trillions of computer programs to solve incredibly complex problems such as those in healthcare, energy, finance and e-commerce. This is possible, and is happening today. I'll reveal all in my talk. What would you like to achieve by speaking at WIRED2015? I'd like to leave people with a greater understanding of what AI is and is not and get them thinking about where it will help us humans and be valuable in society. There has been a lot of scaremongering going on in the industry about the potential threats that AI may pose to humankind -- but I stand with Yann LeCun on this. We're not even remotely close to being there. What we are close to is breeding ""intelligent agents."" But let's not think about this as Terminator-style robots, instead think of specialised computer programs which write their own code and make decisions, solving specific problems, and leveraging massive amounts of data no human could possibly fathom. These agents elevate our status as humans, they make better decisions than we can, or they present us with outcomes to help us see more, decide better and act faster. Siri is an intelligent agent, so perhaps people are getting used to this concept. Who are you looking forward to hearing and/or meeting at WIRED2015? The diversity is great! I look forward to meeting the panellists on the ""Complex world"" panel, as international relations and societal issues are potentially helped by AI in the mid- to longer term. Other themes, such as ""Designing the interface,"" ""Turning data into understanding,"" ""How ideas are born,"" ""The power of music,"" are very relevant to some of what we do -- and I will be an eager listener. Where do you see the AI software sector in five years' time? What advances and challenges might be in the pipeline between now and then? Five years from now, we'll see AI take a bigger role in making decisions, creating pre-emptive solutions, and delivering insights. Society will become much more efficient as a result. Think logistics, e-commerce, healthcare, finance -- in all these domains and others, we'll start to see massive gains from AI. We'll be able to leverage AI systems to help get things to where they need to go faster and cheaper, we'll be able to enable people to see and buy things they weren't even aware existed or even knew they wanted, we'll be able to help predict fatal diseases before they get past the point of no return. We will have full-blown AI-based investment firms. What's next for Sentient Technologies? To make ever better decisions, we need AI systems that perceive more of the world, that have better senses. At Sentient, we're preparing for this future by beginning to allow our AI platform to gather information using vision, hearing and reading. I'll be touching on this more in my talk but it gives you an idea of where we're headed. The combination of Evolutionary Intelligence and other AI disciplines such as Deep Learning on a scale like ours is extremely exciting and powerful. What's next for Antoine Blondeau? In the next year, I will be very focused on continuing to attract top talent to the Sentient team, as we have seen incredible momentum in the number of top AI and business experts who have chosen to join our company in recent months. I will also drive us to expand the commercialisation of our products with our customers. We are on a great path now, and I am super excited about the year to come! WIRED2015 takes place on October 15-16 at Tobacco Dock in London, E1W. Last year's event sold out, so secure your place now. WIRED subscribers save 10 per cent on tickets. We also have a limited number of half-price tickets available for startups and registered charities. For more information or to register, please visit http://www.wiredevent.co.uk/wired-2015."
664,https://www.wired.com/story/robot-modelled-on-flea-inchworm-origami/,Wired,2015,9,28,391.0," Despite recent advances in AI, if someone says ""robot"" most of us are likely to think of C-3PO, Ava from Ex Machina or the emotionally -- but not sexually -- responsive little helper Pepper. But researchers at the Reconfigurable Robotics Laboratory (RRL) in the École Polytechnique Fédérale de Lausanne (EPFL) in Switzerland are busy working on a new army of bots that are actually taking cues from origami-crafting and insects rather than the realms of sci-fi. Known as robogami, these robots are light, flat and foldable. Recently unveiled at the International Conference on Intelligent Robots and Systems (IROS), Tribot is the latest innovation from professor Jamie Paik's origami robot team. Modelled after an inchworm, origami and a flea, it can crawl and jump up to seven times its own height, and doesn't need to be reset between jumps. At 2cm tall and weighing 4 grams, its minuscule dimensions required some special engineering. Rather than utilising conventional motors, the team behind Tribot crafted a special kind of actuator and intelligent springs from shape memory alloy. The alloy is formed from nickel and titanium and can remember its original shape even after it's been manipulated and deformed. Tribot's flat-pack structure particularly lends itself to being produced on a large scale and its parts could even be 3D printed. ""Just like Ikea furniture, these robots could be shipped in flat layers that could then be easily assembled,"" Paik said. Robots inspired by origami are nothing new and they have a variety of potential applications. A team of researchers at MIT and Harvard have been working on combining electrical engineering and origami since way back in 2010. In May this year they claimed to have reached a ""milestone"" after creating a robot that can fold itself up and scuttle away. Made from a thin sheet of PVC and a magnet, the robot moves around land and water and is driven by magnetic fields. It is able to carry out complex tasks such as picking up and delivering objects, carrying loads twice its weight and follow specific trajectories. In the future it could be put to work inside the human body. The next steps for Paik's robogami army are getting them fitted with a wider range of sensors, like accelerometers, teaching them how to climb over rough terrain and encouraging them to interact with one another."
665,https://www.wired.com/story/what-if-a-robot-wins-the-booker/,Wired,2015,9,21,636.0," This article was taken from the October 2015 issue of WIRED magazine. Be the first to read WIRED's articles in print before they're posted online, and get your hands on loads of additional content by subscribing online. In early July, the nerdier reaches of Twitter freaked out about the supposed ""dreams"" of neural networks, with multicoloured spirals of shape and colour rendering hybrid pig-snails and camel-fish. The networks in question belong to Google Research, which has been testing different ways to teach these networks to visually recognise shapes, with some experimentation around image interpretation leading to its networks doing something that resembles artificial daydreaming - seeing shapes among clouds in a sky. As with most futurist-bait on Twitter, the story went viral, and sparked counter-analysis. We hear a lot today about ""smartness"" and ""intelligence"" - highly elastic terms applied to a range of applications so wide that these adjectives approach meaninglessness. And yet, it's hard to deny that various forms of hard and soft machines are increasingly able to autonomously construct narratives that look and sound vaguely familiar to humans. Whether it's a cheeky Markov bot (disclaimer: I've been sharing one with colleagues as a pet for several months), an algorithm generating media art, or a learning experiment in a lab, machines are telling rudimentary stories of their own, albeit guided by human hands at the instruction level. Right now, bots make art they think is attractive (after human tastes), write simple stories for newspapers and wire services (playing something like complex Mad Libs, but with stock prices), and undertake dozens of other authorial tasks. Hundreds of spambots weave crude narratives across social media every day, creating believable personas, tossing off crude bon mots and clunky aphorisms that aren't miles away from your average LinkedIn newsfeed. In creative terms, these weak intelligences are skittering across the uncanny valley, appearing human-legible enough that we read sense into their works. As with many areas of machine learning, the resolution of this creativity is refining itself by leaps and bounds. From wearable technologies to home sensors and online behaviour, CCTV captures and credit-card purchases, most of us leave a trail of data smog wherever we go - or if we go nowhere at all. Credit agencies have built distorted portraits of us for decades. Lately, they've been joined by dozens of companies' big data projects, building an understanding of what we read, how we drive, what we eat on Wednesday and how we have sex. Our data doppelgängers enjoy rich, if unrecognisable, lives without us. Where do we go from here? If machine imaginations, and their ability to craft rich machinic fiction, carry on their development at pace, it's no doubt we will soon be consuming more non-human works. Who would know more about the drama of our daily lives than a Roomba or Dropcam? Might a self-driving car write the next Kerouac-style novel? A crime procedural show as seen from surveillance cameras' points of view? One could imagine a bored smart-home staging a re-enactment from old log files while the family is away, replaying a family dinner through re-runs of lighting, air conditioning and appliance settings, like a domotic puppet show. It may be hard for us to tell when this is happening. In effect, this evolution to machinic fiction has already begun. In the near future we'll see a media property mainly generated this way (to which a wag might reply: ""How is that any different than the ratings-driven, over-tested pabulum we get now?""). The controversy over the first non-human Man Booker Prize or Turner Prize nominee will be notable, but short. Many of today's art-school students already harness code to make art, or let the code make the art they contextualise. Why wouldn't this trickle down to popular media? And will we care? I doubt it."
666,https://www.wired.com/story/campaign-against-sex-robots/,Wired,2015,9,15,587.0," A campaign has called for an outright ban on robots developed for sex. Leading academics in robot ethics have warned that their creation will only increase the objectification of women and children, further dehumanising those who are abused for sex. The warning comes as artificial intelligence approaches a point where it could be used in robots designed solely to satisfy sexual desires. But such robots, campaigners argue, should not exist. ""The development of sex robots and the ideas to support their production show the immense horrors still present in the world of prostitution,"" read a statement on the Campaign Against Sex Robots website. The authors of the campaign argued that sex robots would further increase the perceived ""inferiority of women and children"" and continue to justify their use as ""sex objects"". The campaign, led by Kathleen Richardson, a senior research fellow in the ethics of robotics at De Montfort University in Leicester and Erik Brilling, an associate senior lecturer in informatics from the University of Skövde in Sweden, hopes to encourage a wider debate around the development of sex robots and their potential implications for society. The development of ""ethical technologies"" that reflect the human principles of dignity, mutuality and freedom are critical, the campaign argues. To this end the campaign has called on scientists and roboticists to refuse to help with the development of sex bots, by withholding code, hardware and ideas. The first sex dolls imbued with artificial intelligence are due to launch later this year. True Companion, which claims to be developing the ""world's first"" robot sex doll under the strapline ""always turned on and ready to talk or play"", said its Roxxxy doll would allow people to ""find happiness and fulfilment"" without the need for human interaction. ""We are not supplanting the wife or trying to replace a girlfriend,"" chief executive Douglas Hines told the BBC. ""This is a solution for people who are between relationships or someone who has lost a spouse."" Hines said that the physical act of sex would only be a ""small part"" of the time people spent with the robot. ""The majority of time will be spent socialising and interacting,"" he added. But with little discussion of their ethics, robot sex dolls risk becoming enablers for abusive behaviour. The further development of sex robots, which would have no rights and could be freely abused, could have a hugely damaging impact on the lives of humans, according to Richardson and Brilling. ""We propose that the development of sex robots will further reduce human empathy that can only be developed by an experience of mutual relationship,"" their manifesto explained. The campaign mirrors a call issued by AI experts to withhold technology that could be used in the development of lethal military robots. In a research paper published earlier this month, Richardson explained there was an explicit connection between prostitution and the potential development of sexual relations between humans and robots. ""I propose that extending relations of prostitution into machines is neither ethical, nor is it safe,"" she argued. ""If anything the development of sex robots will further reinforce relations of power that do not recognise both parties as human subjects. Only the buyer of sex is recognised as a subject, the seller of sex (and by virtue the sex-robot) is merely a thing to have sex with."" The issue of human-robot sexual relations has made both the big and small screen this year. The AMD and Channel 4 co-production Humans and Alex Garland's Ex Machina both explored the potential dangers."
667,https://www.wired.com/2015/09/amy-ingram-ai-thats-not-artificial/,Wired,2015,9,15,64.0," On her LinkedIn page, Ingram touts her “exceptional interpersonal skills.” Which is a funny thing for an AI to say, but it�s true. You can exchange pleasant task-related emails with her for weeks and never realize she isn't real. That�s the point: As created by the startup x.ai, Ingram represents a new class of machines that don�t sound like machines. They sound like us."
668,https://www.wired.com/2015/08/stanford-artificial-intelligence-laboratorys-outreach-summer-program/,Wired,2015,8,31,1667.0," In a sparse lecture room at Stanford University, six students are rehearsing a presentation they'll later give to a roomful of VIPs from the university's artificial intelligence lab. Papers are strewn across the table. Hoodies hang over the cloth-covered cushion chairs. One student wears a pair of Pi earrings. Another wears a t-shirt that reads: ""i: Be rational! π: Get real!"" A sheet of white poster board sits in the corner, with a few words scrawled in black marker. “Monitoring Hand Sanitation in Hospitals Using Computer Vision,"" it says. After a while, they give it a dry run. Scripts in hand, the students describe the images they captured from cameras mounted above hand dispensers at Stanford’s Lucile Packard Children's Hospital, and they explain the machine learning techniques they've used—including something called ""climbing the hill""—to analyze the footage and automatically determine whether doctors and visitors are practicing proper hand hygiene. Then they queue up the video presentation, and the students really show their age. The – video warns against the dangers of dirty hands using background music from One Direction and Carly Rae Jepsen. As it plays, the girls dissolve into giggles. “I hope people find this funny!” says one fifteen-year-old. It’s presentation day at SAILORS, the Stanford Artificial Intelligence Laboratory’s Outreach Summer program, the country's first AI summer camp for girls. Backed by more than forty university professors, postdoctoral scholars, and graduate students from the lab, as well as big-name corporate sponsors like Google, the camp aims to remove the Achilles heel of AI research and, indeed, computer science as a whole: there aren't enough women. The proportion of bachelor’s degrees in computer science earned by women has actually dropped from 37 percent in 1984 to 18 percent in 2014. That's a worrying trend for many reasons, including some that aren't immediately obvious. Research from the Bureau of Labor Statistics shows that by 2020, there will be 1 million more computer science-related jobs than graduating students qualified to fill them. Women and minorities can help fill the gap. Like other projects—including a similar camp run by Qualcomm—SAILORS believes this must begin early. Before they become women, the camp gives teenage girls the gift of computer literacy—particularly, literacy in artificial intelligence, one of the fastest-growing branches of computer science. In recent years, Internet giants like Google, Facebook, and Microsoft have doubled down on AI, using brain-like systems to automatically recognize faces in photos, instantly translate speech from one language to another, target ads and more, and simpler forms of AI are now pervasive. Amazon uses a form of AI in recommending products you might like on its popular retail site. At the camp, the gathered ninth-graders not only explore how computer vision can promote hand sanitation. They apply natural language processing to Twitter in an effort to improve disaster relief. They use algorithms to examine DNA patterns and programmed simple robots. The curriculum was designed to dovetail with the Stanford lab’s own AI projects, and the campers rub elbows with notable AI researchers like Professor Edward Feigenbaum—not to mention field trips to the Googleplex and Silicon Valley's Computer History Museum. This kind of early exposure can make all the difference. According to one study from Google, those who had the opportunity to take an advanced-placement computer science exam were 46 percent more likely to show interest in a computer science major. Sophia Catsambis, a professor of sociology at Queens College, City University of New York, who studies gender and education, says we must ensure that all schools have a strong computer science curriculum, but programs like SAILORS can play a vital role as well—especially in keeping a so-called ""summer setback"" at bay. ""Additional opportunities are needed, especially for disadvantaged students,"" Catsambis says. SAILORS was the brainchild of Olga Russakovsky, a recently graduated PhD student who spent eight years at the Stanford AI lab. Russakovsky studied computer vision, focusing on large-scale recognition, under the aegis of her adviser, Fei-Fei Li. In December, Russakovsky approached Li to discuss how she should spend her last few months at Stanford, and their chat spawned the idea of a summer camp for girls. Russakovsky had attended the Stanford math camp as a teenager, later serving as a counselor, and she knew how much these summer camps could influence young students. Her idea was to encourage participation from girls and minorities in particular. ""If we’re trying to give more exposure to girls and underrepresented minorities, if we’re trying to improve the landscape of AI, we should do it here,"" she told Li. And Li enthusiastically agreed, saying she'd been wanting to do something on the issue for years. Working alongside Rick Sommer, the director of Stanford’s pre-collegiate summer institutes and one of her mentors at Stanford Math camp, Russakovsky pulled together a group of volunteers—including her own mother—and started advertising on the university website. Within three days, 300 people had started applications. So the school quit advertising, and eventually accepted twenty-four into the program. Applications were open to any rising high school sophomore, and targeted girls, but the group exhibits some added diversity. Most of the participants are Asian or South Asian, and two are white. A daughter of two math teachers, Russakovsky says she doesn't subscribe to the belief that it is boys who naturally gravitate towards science and technology subjects, but at the same time, she acknowledges that it's become a complicated issue. ""Just because you don’t want to be in a group of people who are all different from you where you stick out, that doesn't necessarily mean you’re not interested in math and science,"" Russakovsky says. ""We need to somehow decouple the two."" As a teenager and on into college, Russakovsky says she had many doubts about being a girl and a woman in the world of mathematics, but that, with diligence, she pushed through those doubts. With the camp, she hopes to help other girls with similar problems, and help them get ""unstuck."" ""I’ve seen people leave the field, or pivot slightly to something else,” Russakovsky says. “They will leave companies. They will leave programs. At some point, you just get tired. It’s not that you’re tired of computer science or math, it’s just that you realize you can choose to be in a different environment."" Li, her mentor, says much the same. But she also emphasizes how much the industry needs these young girls. ""Today, engineers writing the algorithms need to be aware of ethical, social issues,"" Li tells WIRED. ""It’s really important to bring that education and perspective into our research. We need to deliver that mission statement, and we need to attract people who care."" One August day, the twenty-four campers trek to Mountain View and the Googleplex, the headquarters of the Internet's most powerful company. They're here for a panel talk with three female professionals working in AI, and their excitement is palpable. “Clap once if you can hear me!” Russakovksy yells out when the chitchat gets a bit too loud. ""Clap twice if you can hear me."" The tactic works, most of the time. The enthusiasm continues during the panel, as the campers ask how Google AI engineers spend their work days (one product manager's answer: lots of meetings) and explore the pros and cons of working in industry versus academia. They listen with rapt attention as the Googlers describe the work that goes into building a system that can identify photos using natural language; figuring out how to queue up another YouTube video a user will likely enjoy, given a video she has just watched; and the near-magic of on-the-fly voice recognition and task processing in the Google app. There were quite a few jokes about the wonders of data centers tossed around. And lots of selfies snapped. Then the girls ask about discrimination. And though the Googlers indicate this isn't much of a problem, the ninth graders later tell me they experience discrimination, in small ways, all the time. “I’m really aware of being the only cheerleader in my robotics class,” one girl says. “The first day I walked in, all the eyes were on me. They were like: ‘You’re in the wrong classroom.’” Another student, named Anooshree, says she is part of an all-girls robotics team. At tournaments, she says, they often heard two questions: ""Is that your entire team?"" and ""Did your dads build this for you?” As the stories pour out, one camper expresses her surprise—and relief—after meeting other girls who had experienced many of the same feelings she had. “I never realized this was a shared experience before, not just me in my particular environment,” she says. “I think for a lot of us, it’s nice to have someone to relate to.” At the same time, they express a real pride in who they are. “I think when everyone was playing with Barbies, we were all making toy helicopters,” says one camper. Another continues: ""Yeah, when I would get Barbies as a gift, I would just give away the doll to someone else."" But they aren't thinking in black and white. There's some real self-reflection at play. A third camper completes the thought: ""I don’t think playing with a certain toy prevents you from being good at computer science."" They all agree on one thing: SAILORS has given them a reason to be confident. And many say they will encourage other students to apply next year—though some question how much success they will have recruiting others who aren't interested in computer science. Still, they say, camp has shown them how fun AI can be and, perhaps more importantly, how it will change technology. Before we leave the Googleplex, one camper says there may to a trick to bringing other students into the camp. They need to see, she explains, that AI drives things like video games and social networks. They need to see that AI is the future. The best way, another camper concludes, might be to give other kids who haven't been exposed a real ""hands-on"" experience."
669,https://www.wired.com/story/robot-chef-makes-pancakes/,Wired,2015,8,28,356.0," Researchers have created a robotic chef that can flip pancakes and make pizza, in the hopes that it could become a fixture of the kitchens of the future. TechXplore reports that PR2 has been programmed to follow instructions and can flip a pancake with a spatula and make a pizza -- simple tasks for humans, but notoriously difficult for automatons to successfully learn and carry out. RoboHow, the European project behind PR2, is developing and trialling robots that can competently perform everyday activities, as well as teaching them to understand language and more. However, unlike we humans, who have an implicit knowledge of of how to do everything from unscrew a jar lid to pick up a glass of water, robots' lack of intuition and limited range of sensory awareness typically make these most basic of day-to-day tasks tricky. Michael Beetz, head of RoboHow, explained: ""Common sense knowledge like moving the spatula to place the pancake on the plate -- it's an implicit knowledge humans have, but it's extremely hard to make that explicit for a robot."" Robots also have a tough time understanding depth perception, meaning that even picking up a glass bottle from a table can be a perplexing task. The project, co-ordinated by the Institute for Artificial Intelligence (IAI) at the University of Bremen, and involving researchers from nine European universities, has ambitious aims for the future. The team envisions developing a cognitive robot that can not only perform complex tasks, but also pick up entirely new skills -- even by mimicking humans. Writing in MIT Technology Review, Will Knight explained: ""Once a robot has learned how a particular set of instructions relates to a task its knowledge is added to an online database called Open Ease, so that other robots can access that understanding."" He continued: ""These instructions are encoded in machine-readable language similar to the one used in the Semantic Web project."" So what does this mean for the future? Ultimately, it's hoped humans will be able to give instructions to robots and teach them to perform new tasks, meaning that the human-robot relationship could grow more symbiotic than ever before."
670,https://www.wired.com/2015/08/how-facebook-m-works/,Wired,2015,8,26,1532.0," Face it: Siri sucks. So often, she has no clue what you're saying. And when she does, there's a pretty good chance she'll respond with nothing more than a page filled with Internet links. Part of the problem is that Apple's talking digital assistant is built on old technology. But even if the company upgrades Siri to the latest in artificial intelligence, she'll fall well short of an assistant made of flesh, blood, and neurons. As far as artificial intelligence has come in the last few years, it's still a long way from intelligence. With M, its new virtual assistant, Facebook admits as much. Built atop Facebook Messenger—the company's instant messaging app—M made its debut this morning, arriving on the phones of a few hundred unsuspecting souls in the San Francisco Bay Area. Yes, it's the company's answer to Siri and similar services like Google Now and Microsoft Cortana. But it tackles a broader range of tasks, at least as Facebook describes it. You can ask M questions along the lines of Can you make me dinner reservations? or even Can you help me plan my next vacation?—and it will comply. That's because Facebook designed the tool so that AI technology responds to these questions in tandem with humans. ""The AI tries to do everything,"" says Alex Lebrun, the founder of Wit.ai, a startup Facebook acquired to help build this smartphone tool. ""But the AI is supervised by the people."" In the larger world of AI-driven personal assistants, M may seem like a regression. And as Facebook tests the tool with the public, it's unclear whether this human-machine partnership can keep pace as the project expands to an ever-larger audience. But in a counterintuitive way, M may actually be a step forward for AI. The idea is that humans will not only answer queries the AI is incapable of answering but, in the long run, help to improve this AI. Today's artificial intelligence, you see, requires at least some human training. If you want a system to automatically identify cats in YouTube videos, humans must first show it what a cat looks like. They must tag all sorts of feline photos. They must provide data. Through the human staff backing M, Facebook is doing this type of thing in unusually complex ways. ""This is why we have this big team of people,"" Lebrun says. ""The data we need is nonexistent."" In answering your questions, these humans will provide the data needed to bootstrap a much more sophisticated digital assistant based on a separate form of artificial intelligence called ""deep learning."" This could take many years. But such is the way with AI. ""Human-level AI is a good philosophical discussion to have,"" says Dennis Mortensen, the CEO and founder of x.ai, a startup offering an online personal assistant that automatically schedules meetings, ""but it's not going to happen anytime soon."" Part of the irony is that Wit.ai offers pretty old AI. Its technology is based on two algorithms—""conditional random fields"" and ""maximum entropy classifiers""—that have served the tech world for more than a decade. But it provides a stepping stone as the M project seeks a loftier breed of artificial intelligence. Lebrun founded Wit.ai in 2013, after creating a digital agent that companies like AT&T used to communicate with their customers. Basically, Wit.ai offered a service that could help software coders build Siri-like systems that could recognize speech and, to a certain extent, understand natural language. Yes, it was based on older algorithms, but it could learn to recognize speech without tapping the enormous collections of voice data available only to the likes of Apple or Google. It required less data, and it worked by pooling voice samples collected by the many developers who used it. As David Marcus, the Facebook vice president who oversees Messenger, sought ways of expanding Messenger into areas that might generate revenue, he approached Lebrun and company, and in January, Facebook acquired the 10-person startup for an undisclosed sum. Marcus says Facebook nabbed ""one of the best teams in the world at human-to-AI interactions."" But according to Lebrun, the team wasn't quite sure what human-to-AI stuff they would be building. About three months later, Marcus, Lebrun, and the rest of this small group settled on the idea of a virtual assistant that would run atop Messenger. But it wouldn't be another Siri. For one, it would communicate primarily by text, not voice. And it would answer a more complex range of questions. ""You have lots of AIs—like Siri, Google Now, or Cortana—whose scope is quite limited. Because AI is limited, you have to define a limited scope,"" Lebrun says. ""We wanted to start with something more ambitious, to really give people what they're asking for."" This meant the team would need more than AI. When you ask M a question, the AI works to understand what you're asking and formulates a response. But rather than sending it to you, the system sends this response to human ""trainers""—customer-service types who work alongside the Wit.ai team inside Facebook's new building in Menlo Park, California. These trainers then decide what else must be done to provide what you're looking for (see image below). According to Lebrun, the AI can do most of the work for simpler tasks, like telling a joke. It'll query an Internet joke API—a service that supplies jokes—and a trainer will approve the joke if it's funny. For more complicated tasks, such as making a driving test appointment at the DMV, the humans will do most of the heavy lifting. They'll actually place a call to the DMV. In doing that heavy-lifting, the humans generate a roadmap for how particular questions should be answered. ""Everything the trainers do, we record every step,"" Lebrun says. This includes what websites they visit, what they say when calling the DMV, what they type in response to M users, and so on. In the future, this data can help drive a more advanced system based on deep learning, a form of AI that masters tasks by analyzing enormous quantities of information across a vast network of machines. Roughly speaking, these networks mimic the web of neurons in the human brain. Such neural nets have already proven enormously effective in identifying images, recognizing speech, targeting ads, even teaching robots to screw on bottle caps. And after hiring an NYU computer science researcher named Yann LeCun, Facebook is a leader in this increasingly important field. The company now uses neural nets to recognize faces in photos posted to its social network and identify what you're likely to want in your News Feed. With M, it aims to push the technology further still. Why not just build M with neural nets from the beginning? Without the right data, neural nets couldn't provide a service much more powerful than, well, Siri, and Wit.ai's tech can get things started with relatively little data. ""This is a good way to bootstrap. With a few thousand data-points, you can start to build a model,"" Lebrun says. ""Then, using this model, you get more data, and once you have about a million data points, you go to Yann and get some deep learning."" As Lebrun describes it, this is a remarkably ambitious play. Even after bringing neural nets into the mix, he says, the company will continue to use human trainers for years on end. As M improves, it will need still more data to continue improving. ""The more you know, the more you don't know,"" Lebrun says. ""The more the AI does, the more complex tasks it will be required to do."" At least, that's the plan. M only launched today, and we don't know how well all this will work. As the company expands M to more and more users, it will need more and more trainers. Lebrun expects the number of trainers to grow linearly as the number of users grows exponentially, but the burden could be enormous. Facebook Messenger is used by more than 700 million people. ""This is not easy,"" Lebrun admits. Meanwhile, even if Facebook can keep the system going, the AI may not evolve as quickly as Lebrun and company expect it to. Dennis Mortensen, of the digital assistant startup x.ai, says that having humans work alongside the AI (as opposed to just training it) may actually slow things down. He offers self-driving cars as an example. One way to build such vehicles is to slowly add automation as humans continue driving. But the system may evolve faster if you let the car drive itself—however ill-equipped it might be. Those small automated tools may not be essential to the end result. They may collect data you don't need. But at the same time, like Lebrun and Facebook, Mortensen underlines the importance of data to the evolution of AI, and he says that if Facebook plays things right—properly guiding and recording and cataloging the behavior of its trainers—these humans could indeed provide a shortcut. Facebook must focus on how the humans can improve the system in the future, he says, not just in the present. That isn't easy. But it's the sort of thing Facebook does better than most. Additional reporting by Jessi Hempel."
671,https://www.wired.com/2015/08/robots-will-steal-jobs-theyll-give-us-new-ones/,Wired,2015,8,24,1037.0," At the Dusseldorf airport, robotic valet parking is now reality. You step out of your car. You press a button on a touch screen. And then a machine lifts your car off the ground, moving all three tons of it into a kind of aerial parking bay. Built by a German company called Serva Transport, the system saves you time. It saves garage space, thanks to those carefully arranged parking spots. And it's a sign of so many things to come. But the one thing it doesn't do, says J.P. Gownder, an analyst with the Boston-based tech research firm Forrester, is steal jobs. In fact, it creates them. Before installing the robotic system, the airport already used automatic ticket machines, so the system didn't replace human cashiers. And now, humans are needed to maintain and repair all those robotic forklifts. ""These are not white-collar jobs,"" Gownder tells WIRED. ""This is the evolution of the repair person. It's harder to fix a robot than it is to fix a vending machine."" Gownder uses the Dusseldorf parking garage as a way of showing that the coming revolution in robotics and artificial intelligence may not squeeze the human workforce as much as some pundits have feared. In a widely cited study from 2013, Oxford professors Carl Frey and Michael Osbourne say that machines could replace about 47 percent of our jobs over the next 20 years, but in a new report released today, Gownder takes a more conservative view. Drawing on government employment data and myriad interviews with businesses, academics, and, yes, pundits, Gownder predicts that new automation will cause a net loss of only 9.1 million U.S. jobs by 2025. The horizon of his study is much closer, but his numbers are well under the roughly 70 million jobs that Frey and Osbourne believe to be in danger of vaporization. ""While these technologies are both real and important, and some jobs will disappear because of them, the future of jobs overall isn’t nearly as gloomy as many prognosticators believe,"" Gownder writes in the report. ""In reality, automation will spur the growth of many new jobs---including some entirely new job categories."" Yes, the revolution is coming. Gownder points to a robot at the ALoft hotel in San Francisco delivers towels and toothpaste and other stuff. At Vanguard Plastics in Connecticut, a machine called Baxter is manufacturing goods in ways machines never could in the past. The likes of Google and Amazon are pushing even further into this area with everything from warehouse drones to self-driving cars. Perhaps more importantly, the giants of the `net are rapidly advancing the art of artificial intelligence, teaching online services to recognize images, understand natural language, and even carry on conversations---the kinds of artificial intelligence that will empower robots to tackle ever-more complex tasks. Using the AI that Google and Facebook use to identify photos on the 'net, researchers have already built machines that can, says, teach themselves to screw on a bottle cap. ""Today's technology is different than what we've seen in the past,"" says Martin Ford, the author of the recent book Rise of the Robots: Technology and the Threat of a Jobless Future. ""The technology is taking on cognitive tasks. We know have machines and algorithms that can, at least in a limited sense, think."" As this tech evolves, concern is certainly warranted, not only because of how these technologies will affect the workforce but because, some argue, smarter robots could wind up becoming more harmful robots. After seeing the latest artificial intelligence in action, Elon Musk, the founder of electric car company Tesla and the space exploration outfit SpaceX, worries that such AI may turn on humans in more direct ways, so much so that he has donated millions to efforts that seek ways of keeping AI ""beneficial to humanity."" But Gownder rightly points out that such technology is still in the early stages of development---and that it still requires much help from humans. Humans must build these machines and program them and repair them. But they must also train them. This is true of ""deep learning"" AI, and it's true of robots like Baxter. Baxter must be programmed to perform certain tasks, and that involves physically moving his limbs back and forth. IBM is touting the arrival of Watson, a broad collection of online tools that use artificial intelligence to help diagnose disease, among other things, and so many others are exploring similar work. But whatever the message from IBM, such tools operate alongside humans, not in lieu of them. ""Watson is like a robotic colleague,"" says Gownder. ""It's job transformation, not job replacement."" Andrew Moore, the dean of the school of computer science at Carnegie Mellon University who previously worked in AI and robotics at Google, agrees. He says that he has seen no evidence that this technology is stealing jobs---and that, as time goes on, it will likely create an enormous number of jobs. ""Technology does change the mix of jobs. You're going to see doctors taking more of the role that involves the personal interaction with patients and less of the role of trying to keep huge amounts of evidence in there head. The nurse may become more prestigious than the doctor,"" Moore says. ""But if you look around, there are also new kinds of creatives roles being produced across the market. There are so many jobs that didn't exist just a few years ago."" This is the larger message of Gownder's report. Robotics and AI will change the way we work, but it won't necessarily take away our work. Today's warnings over the rise of AI, he says, are reminiscent of that handwringing over so many other technological advances in the past---and after all these centuries, the workforce is still there. It should be said, however, that Gownder's study only looks so far down the road. And as Ford says, even Gownder's rather conservative estimate---9.1 million jobs lost---is still rather significant. Robotics and AI will continue to progress---at an unprecedented rate---and though Gownder believes the doomsayers have overblown the threat of widespread automation, he too sees reason for concern---and for continued to debate. ""The rate of change matters,"" Gownder says. ""We must keep our eyes open."""
672,https://www.wired.com/story/uncanny-valley-robot-videos/,Wired,2015,8,14,581.0," Japanese robotics professor Masahiro Mori devised the concept of the 'uncanny valley' in 1970. It's the point at which a robot is made to appear so human-like -- if not quite human enough-- that it inspires feelings of uneasiness and revulsion in we mere mortals. In other words, humanoid robots really give us the creeps. (Remember the Black Mirrorepisode Be Right Back?) But as androids are being developed to be more and more lifelike, copying human gestures, body language and even speech, our simultaneous fascination/horror at the world of uncannily human-like robotics shows no signs of slowing down. So sit back and try to relax (or, at least, not squirm in too much horror) as you watch our round up of these mesmerisingly uncanny androids. Created by Uncanny Valley robotics expert Hiroshi Ishiguro, Erica android is one of the most unnervingly human yet created. Not only can she speak and move, but she can also mimic human body language with disconcertingly accurate facility. Smiling, blinking, grimacing, turning her head as she speaks -- you could easily be forgiven for mistaking her for a sentient mortal if you squinted a bit. But, of course, the very fact that Erica falls short of being entirely convincing is what makes her all the more strange. She was showcased at Japan's Miraikan National Museum of Emerging Science and Innovation in Tokyo in August 2015. But it might not be too long before Erica and her mechanical brethren make an appearance a little closer to home. Designed and built by Hanson Robotics Inc., Jules is a conversational character robot with a gleamingly bald head and the stilted voice of an English gentleman. A sophisticated AI creation, he's made from a pliable, lightweight material called Frubber, which enables him to expressively move his face just like a real human. Jules can also boast of having a ""statistically perfect face"" -- which not many of us mere mortals can lay claim to. He's also capable of having a ""natural, interactive"" conversation if you're so inclined. And Jules isn't alone. Creator David Hanson has modelled an entire family of humanoids, including Alice, Han and -- in honour of the great physicist himself – the Albert Einstein Hubo. Kurokawa was created by robotics firm Kokoro and Japan's National Institute of Advanced Industrial Science and Technology (AIST) as a ""brother"" for their original -- if not quite as ""humanly"" named -- Actroid-F. These super-realistic humanoid siblings are able to have a conversation with each other, and use their range of 12 facial expressions to indicate their reactions to the world around them. They can also imitate the movements of the people they're watching. After being successfully trialled in hospitals to see how patients felt in their presence (""only three or four people"" out of 70 apparently felt uncomfortable with having them around""), AIST plans to employ the robots in other socially useful ways, including talking to elderly people to help prevent mental decline, and interacting with children who have developmental problems. Perhaps it was inevitable that the ultimate in unsettling humanoids would be an AI recreation of maverick sci-fi visionary Philip K. Dick. A dead ringer for the late, great novelist himself (well, if you ignore the mesh of wires sprouting from the back of the synthetic brain in his head), you can watch him muse on everything from Cartesian philosophy to how he picks up new words. The next question to ask him? Do androids dream of electric sheep, surely?"
673,https://www.wired.com/story/mass-extinction-creates-excellent-evolution/,Wired,2015,8,13,521.0," To create a super race of robots, we need to exterminate existing models en masse. This is the macabre lesson shared in a PLOS One paper published this week by computer scientists at The Unieversity of Texas at Austin, who have tested their mass extinction theory. At least in theory. In their paper, Risto Miikkulainen and Joel Lehman contend that extinction is generally considered a bit of a downer -- or rather, an ""upheaval to the evolutionary process"". Not so, they argue. Perhaps our fascination with dinosaurs has tempered our view of the whole affair, but Miikkulainen and Lehman set out an alternative outlook shared by some evolutionary biologists: ""although they are unpredictably destructive, extinction events may in the long term accelerate evolution by increasing evolvability."" They argue that if extinction is indiscriminate and crosses all kinds of lifeforms, it may open up rapid evolution in the ""vacated niches"" that survive. ""Lineages with such an ability are more likely to persist through multiple extinctions,"" they write, meaning whatever fills those gaps, would be nigh unstoppable. Enter their central hypothesis: the inevitable robot uprising could in fact be preceded by their extinction. In a series of experiments, the pair connected neural networks to a pair of simulated robot legs. The plan was to get the robot to evolve so that it 'walked' more efficiently in the simulation. The system was programmed to run through a series of random mutations as part of the learning process, and Miikkulainen and Lehman ensured a range of different 'niches' were introduced so the variations would be as vast as possible. After the cycle ran hundreds of times, the simulated legs had evolved a wide variety of techniques that responded to the introduced niches -- though importantly, not all related to walking. Then, Miikkulainen and Lehman exterminated 90 percent of those niches. They let this same strategy play out several times over, with the simulation building up newly evolved techniques, and then having 90 percent wiped out. In the end, the niches that survived were those that had the most potential to adapt and create new behaviours. When the simulated legs that had been through the wringer were compared with simulations that had never suffered a mass extinction, the former proved to be the better walkers. ""This is a good example of how evolution produces great things in indirect, meandering ways,"" said Lehman. ""Even destruction can be leveraged for evolutionary creativity."" While the authors are not suggesting we carry out any real-world mass extinction experiments, they do argue it could shed light on some ""similar phenomena"" that occur in the real world. These range from ""creative destruction in business"", to how wildfires can renew ecosystems. ""In such cases, the temporary effect of a mechanism is superficially destructive to particular participants in a process, while the ultimate longer-term effect is to make the process as a whole more innovative and robust. This deceptive pattern seems to be often exploited by evolutionary systems in general, and thus it may be useful to consider other seemingly destructive forces in the evolution of economies, products, and ideas in the same light."""
674,https://www.wired.com/2015/08/ibms-watson-ai-wants-coach-fantasy-football-team/,Wired,2015,8,12,958.0," Fantasy sports leagues are serious business. According to the Fantasy Sports Trade Association, nearly 57 million people will play in fantasy leagues in the US and Canada this year. Each of them will spend an average of $465 doing so. And most of them will play fantasy football, which is by far the favorite faux game. Although fantasy leagues can get competitive, fantasy football is pretty chill. You don't have to set a daily lineup like they do in fantasy baseball, basketball, or hockey. Most of the action happens on Sunday, providing six days to tweak your roster. Granted, a lot can happen in that time, stuff like injuries, roster changes, disciplinary actions, and players getting benched. A genius waiver-wire pickup or a boneheaded start can make or break your season, and the best moves usually comes down to getting key info before an opponent. As in all things, knowledge is power. IBM Watson will help collect and comprehend the deluge of data and offer an overall outlook for every player on your roster. A new app called Edge Up Sports taps Watson via APIs and, like an A.I.-driven Rotten Tomatoes for fantasy football, offers a crowdsourced ""sentiment"" for your players. Edge Up Sports doesn't host fantasy leagues. Instead, it's a companion research app that lets you import your roster from other leagues, then gathers news, updates, trending topics and other data. “On a single service, you get all the information through a single analyst, and you still have to read and see what the latest updates are,” says CEO Ilya Tabakh. “And the projections are generally pretty poor for everybody. One of the things we’re doing is just taking a wider view of the landscape. We think it’s a more complete picture of what’s going on.” Tabakh says the idea is to save fantasy team owners a ton of time, ""especially casual players who may be in a couple of leagues."" The system does more than aggregate content from multiple sources (something other services already do). Using different APIs---entity extraction to identify players and teams from unstructured text, sentiment analysis to pinpoint positive or negative stories, and personality insights to assess individual writers---the app gauges and summarizes the overall outlook for each player on your roster. Overwhelmingly negative sentiments may mean a player is injured, struggling, or has a rough matchup. Positive sentiments could mean the player is a must-start---or a great bargaining chip in a trade. “There are sentiment and other capabilities within AlchemyAPI that add context to what’s actually being written,” Tabakh says. “Then we take the analysis from any particular sports writer and actually build a personality insight to understand what that analyst’s tendencies are. A fantasy football player may not know whether a particular writer is generally optimistic or pessimistic. So we build that out through the Watson Personality Insights APIs to get a better understanding of the analyst’s tendencies.” Using a supercomputer allows Edge Up Sports to look beyond what's being said and written about a player to analyze data most people (and pundits) may not even be aware of, says Lauri Saft, IBM Watson Ecosystem vice president. “What’s different about cognitive computing is that it’s able to absorb unstructured content,” says Saft. “Hundreds of thousands, if not millions of pages of information that a human couldn’t get to. It’s pulling in that information and finding insights in that content. To date, all we have are programmable systems, where something has to reside in a database and have a column, a header, et cetera, to make sense. The ability for Watson to make it through all that unstructured information, look for patterns and insights, is where the power comes in.” Of course, the only question is whether this AI intel will get you to the playoffs. Although he's been working on the app for 10 months, Tabakh says it remains a work in progress. He says the app will launch in time for the coming season, and has launched a Kickstarter campaign to connect to early adopters. Tabakh hopes those users help shape future iterations of the app by telling him what's useful, what isn't, and what else is needed---even as they finance further development. On the Kickstarter page, annual subscription prices are $22 for the stat-based ""standard tier"" $55 for the Watson-fueled ""cognitive tier."" That's a big chunk of change for an unproven app, but the more data the app analyzes, the smarter (and weirder) it should get. “We’re building an understanding of data-driven predictions of player performance,” Tabakh says. “As we get to some of these data points---tendencies on social media, tendencies in interviews---we can get to the predictive analytics level. That’s something that we’ve been working on and are looking forward to rolling out with some of our founding users.” There’s even an odd screenshot on the Edge Up Sports site that shows a facial analysis of New England Patriots quarterback Tom Brady. Facial analysis won’t be in this first-generation app, but IBM’s Saft says similar things could be enabled in a future Watson API. “There’s a new API coming out called Tone Analyzer where you could transcribe a media interview that one of the players does or what the coaches are saying about them,” Saft says. “It can pull the tone out of that, which can point you to what their sentiment is and how they’re feeling. You can even get that in the words they use rather than their facial expressions.” With that in mind, Edge Up Sports may actually be making pre-game roster moves even more complicated. Sure, Marshawn Lynch may be perfectly healthy for this week’s game---but knowing how he* feels* will impact your weekly selection. Apparently, the future of fantasy sports will include feels."
675,https://www.wired.com/2015/08/voice-powered-app-lawyers-can-ask-legal-help/,Wired,2015,8,7,754.0," When Jimoh Ovbiagele was ten years old, his parents decided to get a divorce. But as the couple got deeper into the process, the legal fees grew more and more expensive, until they ended up abandoning the whole plan. “It had a negative impact on my family,” Ovbiagele says. In high school and beyond, when Ovbiagele was looking into various career options, he discovered that most of a lawyer’s time is actually spent researching cases. Remembering his parents' difficulties, that idea nagged at him. Ovbiagele ended up studying computer science rather than law, but when he had the opportunity to pursue an artificial intelligence project at the University of Toronto, he had a pretty good idea of what he wanted to work on. “I thought back to that big problem lawyers face in their day-to-day work, and how it impacts regular people,” Ovbiagele tells WIRED. “I thought we should apply the capabilities of machine learning to tackle this problem and make things better for lawyers and for clients.” And so the idea for ROSS Intelligence was born. Ovbiagele became the company’s CTO, and along with co-founders whose backgrounds range from law to neuroscience to computers, the team came up with a voice recognition app powered by IBM Watson, the machine learning service based on the company’s Jeopardy-playing cognitive system, that doles out legal assistance. The app is yet another example of the ways machine learning is infiltrating our everyday lives. These days, it’s not just AI algorithms themselves that have improved, but the ability to deliver them across the Internet that has made so many new applications possible. Just this week, a toy startup called Elemental Path started taking preorders for the CogniToy dinosaur bot, which also taps into IBM Watson for its brains. SRI International, the Silicon Valley incubator where Apple’s Siri digital assistant was born, recently announced a voice-recognition add-on for mobile banking apps that lets customers ask questions about their accounts. Ross is another incarnation of the trend. Ross works much like Siri. Users can ask it any question the same way a client might—for instance, “If an employee has not been meeting sales targets and has not been able to complete the essentials of their employment, can they be terminated without notice?"" The system sifts through its database of legal documents and spits out an answer paired with a confidence rating. Below the answer, a user can see the source documents from which Ross has pulled the information; if the response is accurate, you can hit a ""thumbs up"" button to save the source. Select “thumbs down” and Ross come up with another response. According to Ovbiagele, it’s a huge improvement over current research databases that rely heavily on keyword search. Plus, he says, the system learns from the feedback its users give and gets smarter with more input. Work on Ross began last September. Andrew Arruda, another Ross cofounder, explains that the team started with a “blank slate” version of IBM Watson. They fed it thousands of pages of legal documents, then trained it on the taxonomies and ontologies of law using one of Watson’s question-and-answer APIs. Then they built a machine-learning layer of their own dubbed LegalRank—a play on Google’s PageRank algorithm—to further refine the system. “Without exactly giving away our secret sauce,” Arruda says, “LegalRank can figure out which results get preferential results, whether that’s prioritizing a case that has more citations, knowing that a Supreme Court case should rank higher than a local decision, and other nuances.” For the moment, Ross focuses on bankruptcy and insolvency law, but Ovbiagele and Arruda remain optimistic about its ability to scale and move into other areas. They say there’s no shortage of demand for ways to make legal research easier: according to their data, attorneys devote nearly a fifth of their working hours to legal research and law firms spend $9.6 billion on research annually. The app could also help unburden much of the basic legal gruntwork that is often outsourced to places like India and the Philippines, where labor is often cheaper and workers have high English fluency. Without having tried Ross myself, there’s no good way of knowing whether the app works as advertised. But Ovbiagele and Arruda say they’ve tested the system in small-scale pilot programs inside law firms since June, and they’re confident in the results they’re seeing so far. Another indication that the app shows promise: A subsidiary of global law firm Dentons, NextLaw Labs, has signed ROSS Intelligence as its first portfolio company."
676,https://www.wired.com/2015/08/toy-dinosaur-uses-ibms-watson-brain/,Wired,2015,8,5,489.0," What is green, very small, sounds like Yoda, and boasts boundless wisdom? The answer is Yoda. However, Yoda now has some competition on all fronts from Elemental Path’s CogniToys, little talking-and-listening dinosaur bots that tap into IBM Watson for their brains. We covered CogniToys earlier this year when its Kickstarter campaign launched, but now the toy is heading into production and available for preorder for $120. It’s sort of a hybrid between Musio and an Amazon Echo, except with a major advantage in the intellect department: A kid can have a conversation with none other than Jeopardy champion and aspiring doctor IBM Watson itself. Well, sort of. “For privacy reasons, the toy doesn’t directly connect to Watson,” says JP Benini, co-founder of Elemental Path. “It connects to our proprietary platform, which in-turn connects to Watson for Q-and-A statements. Our platform is where we keep the personality, all the stories, the jokes, all the educational exercises and personalized experiences... We use Watson as the logical left-brain to our creative right brain.” In order to make the toy work, you need to connect each CogniToy to the Web via Wi-Fi with an iOS or Android app that works as a gateway. There’s a button on each dinosaur’s belly that engages the microphone and mainlines Watson: Conversations, spelling and math quizzes, knock-knock jokes, that kind of thing. Parents can also customize each toy’s settings and set the kid’s interests within the app. According to Benini and co-founder Donald Coolidge, Elemental Path’s platform serves to curate data tailored for each child. That database of responses is meant to answer common questions—”mommy questions,” as Benini and Coolidge put it—in the five-to-nine age range, such as “why is the sky blue?” and “why can’t I eat candy for dinner?” While the project was funded via KickStarter, CogniToys was spawned out of a deeper relationship with IBM Watson. The toy won grand prize in the 2014 Watson Mobile Developer Challenge and earned a partnership with IBM to develop the technology further. “The KickStarter campaign was a great way to inform and education the general public,” says Coolidge. “(It) provided us with a tool to speak directly to our early adopters about how the toy should look, feel, and act.” This gen-one device is just a toy—and given its voice, a semi-disturbing one—but this kind of in-home interaction with a powerful A.I. engine like Watson certainly has use cases beyond kids. Benini and Coolidge mention Chef Watson, a cooking app developed in a partnership with Bon Appétit that uses the magazine’s recipe database to come up with new flavor-optimized dishes (Note: Bon Appétit and WIRED are both published by Condé Nast). “The base technology is about giving connected devices a personality that responds and changes over time,” says Coolidge. “The toy makes for a very compelling version one. Flat fact retrieval is a neat trick, but the really powerful piece of the puzzle is the conversational nature.”"
677,https://www.wired.com/2015/07/blockspring/,Wired,2015,7,29,949.0," Paul Katsen worked at a consulting firm, and he was in charge of analytics. This meant his colleagues would ask him to write scripts that could gather data across the workforce and beyond. In the beginning, he tried to teach his coworkers how to use the tools he had built, but he eventually realized they weren’t all that interested in learning anything close to programming skills. ""All they wanted to do was say: 'This is the input I want, and I want to get something understandable out of it,'"" Katsen remembers. At one point, he discussed this code-phobia phenomenon with a college friend, Don Pinkus, who was then working at Facebook. Turns out, this was a problem at Facebook too. Even at a company known for its technological innovation, Pinkus told Katsen, there were business people who had difficulty using new technologies. ""We realized they just wanted to use the tools that they’re already familiar with---like Excel and other applications,"" Katsen says. The talk inspired a new company: Blockspring. Founded by Katsen, Pinkus, and a third entrepreneur named Jason Tokoph, the company has spent the last year and a half building a tool called Blockspring for Spreadsheets, with the idea of turning the world's one billion spreadsheet users into software engineers. Essentially, the product is a series of coding tools, or APIs, that can be used from inside a spreadsheet. It plugs into popular applications like Excel and Google Spreadsheets. ""Instead of building new apps and teaching people how to use them---having to do all this extra stuff just to get value---what if there was just a way to have a product that could sit between the tools that people already love to use, and tap into the technologies that developers are building already?"" Katsen asks. The tool is part of a sweeping effort to bring coding skills to a wider swath of the population. Companies like Codecademy are trying to teach coding skills via the internet. Coding ""bootcamps"" are popping up in various cities, offering crash courses in more advanced skills. And companies from Blockspring to Google are offering tools that seek to simplify the coding process itself. Katsen describes Blockspring for Spreadsheets as an index of APIs, or application programming interfaces. Basically, these are tools that other companies offer over the net, tools that can be used to build new applications. Katsen calls these ""building blocks""---hence the name of his startup. Via these APIs, Blockspring lets you access a wide range of popular services, from social media analytics software SharedCount (which lets you count online ""shares,"" ""likes,"" and tweets) to text analysis tools from the likes Alchemy API, Aylien, indico, and Lateral.io (which let you parse emotional sentiment, identify high-level concepts within a website, and more). Katsen and his cohort have even included the API from AI startup Metamind that can help analyze data in more complex ways. According to Katsen, some early testers have already made use of the service. Looking to hire a new developer, he says, one company tapped a Google Calendar API via Blockspring, building spreadsheet application that could show which of the company's engineers were free when a candidate was scheduled for an interview. At another company, he says, a lawyer with no technical skills tapped an algorithm provided by a company called Indico, using it to identify the political affiliation of people involved in court cases. Is the service really as easy to use as Katsen says it is? I downloaded the plug-in myself and tried it out. After I created an account on the site, Blockspring walked me through the whole process in simple step-by-step instructions. Combining a few simple functions, I created, in a matter of minutes, a database that pulled my recent WIRED articles from Microsoft's Bing search engine and mapped them onto related tweets, shares, and pins. That gave me a picture of which of my articles did the best on social media. Blockspring would also refresh my data every time a new result came in, so I could simply go back to the spreadsheet in the future instead of typing in the functions everyday to get new results. I also built data visualizations---word clouds of the most popular topics I covered. I dug into my news stories and saw which topics were peripherally associated with the articles I wrote. Katsen also showed me how to create nifty heat maps of the cities in the US, given public data. Katsen says the team is still building out the index of APIs that could be incorporated into Blockspring, and they’re working hard to get as many tools as possible onto the system. Eventually, he says, popular business tools from companies such as Slack and Tableau will be available from Blockspring. Some companies prevent third-party apps and services from accessing their APIs, including Netflix and LinkedIn. But Katsen believes this will change. He thinks that any company that bars third part services only creates an opportunity for a new venture to fill in that gap. ""Someone will sneak in and take that business,"" he says. ""Guaranteed!” ""APIs are simply a zero-friction way to do business, and most enterprises I talk to aren’t afraid of their big competitors. They’re afraid to die of a thousand cuts from a thousand startups,"" Katsen says. ""It's the emergence of thousands and thousands of small, focused APIs that's creating a whole new economy. And combined they get very, very powerful. That's a big reason why we exist."" CORRECTION 11:40 AM ET 07/29/15: This article originally stated that the Blockspring co-founder who worked at Facebook was Jason Tokoph. It has been corrected to show that it was Don Pinkus who worked at Facebook, not Tokoph."
678,https://www.wired.com/2015/07/guy-taught-ai-remember-launching-startup/,Wired,2015,7,28,1079.0," At Google, a team of researchers recently demonstrated an artificially intelligent system that could reliably identify a mountain-unicycling video. As another Google researcher put it: ""Who knew mountain unicycling was a thing?"" But the implications of this system extend well beyond the realm of obscure outdoor sports. Making use of a technology called recurrent neural nets, it pointed to a near future where our artificially intelligent machines include a kind of artificial short-term memory. Basically, the system could identify the mountain unicycling because it could ""remember."" As it examined each frame of a video, it could look back at frames it had seen in the past. Recurrent neural nets, or RNNs, can not only recognize complex moving images, but automatically generate detailed captions for online photos and videos, improve online services that translate from one language to another, and more. They're pushing into companies like Facebook and Baidu as well as Google, and in recent weeks, this burgeoning technology received another shot in the arm with the arrival of a new startup called Nnaisense. According to the company's website, Nnaisense was founded by Jürgen Schmidhuber, a key figure in the development of modern RNNs, and four researchers who work alongside him at the Swiss AI lab called IDSIA (Istituto Dalle Molle di Studi sull'Intelligenza Artificiale). The German-born Schmidhuber helped create a breed of recurrent neural net called LSTM, or Long Short Term Memory, and his work has influenced the latest AI research at the likes of Google, Microsoft, IBM, and others. Neither Schmidhuber nor Nnaisense immediately responded to requests to discuss the company's aims. The company is still very new. It registered the nnaissense.com internet domain this spring, and in June, it filed a trademark application for the nnaisense name (a nod to ""neural networks"" and ""artificial intelligence""). As it stands, the company's website says its mission is to ""build large-scale neural network solutions for superhuman perception and intelligent automation, with the ultimate goal of marketing general-purpose neural network-based Artificial Intelligences."" In other words, it's trying to do pretty much what Google and Facebook and Baidu are trying to do. But it's notable that Schmidhuber is entering the arena. The leading internet companies are jockeying for top talent in the field of ""deep learning,"" a form of artificial intelligence that includes recurrent neural nets, and even without a product, Nnaisense is a potential target. In recent years, Google and Facebook snapped up two notable names in the field, Geoff Hinton and Yann LeCun. This month, IBM inked an agreement with another, University of Montreal professor Yoshua Bengio. And the others, like Twitter, have grabbed various researchers who studied under those Big Three. Schmidhuber and his colleagues represent another talent pool. In fact, they may be looking for a place inside one of the giants of the net, which can provide not only the money that can fuel more advanced research in this field, but also the enormous amounts of digital data needed to drive deep learning services. ""The trend is: researchers going towards industry,"" says Adam Gibson, the co-founder of a deep learning startup called Skymind. ""These guys want to see their research applied."" Deep learning is an umbrella term used to describe the use of particularly complex neural networks---networks of machines that mimic the web of neurons in the human brain. Basically, if you feed these systems large amounts of data, they can ""learn"" to perform certain tasks. If you feed them cat photos, for instance, they can learn to identify a cat. Using ""convolutional neural nets,"" Facebook can now recognize faces in photos posted to its social network. Google uses ""convnets"" to recognize commands you speak into your Android phone. At Baidu, they help drive a kind of visual search engine. Convnets are remarkably effective, and they can help with a wide range of tasks, including everything from ad targeting to language translation. But recurrent neural nets can potentially take the state of the art even further. Whereas a convnet accepts a single type of input (images, say) and spits out a single output (what category an image falls in to), an RNN can ingest multiple inputs and deliver multiple outputs. ""Recurrent neural networks can operate with sequences,"" says Andrej Karpathy, a Stanford University deep learning researchers who previously interned with one of Google's AI groups. ""They can make observations over time, and then modify their internal operations based on that."" One way to think about this is that RNN exhibit something akin to a short-term memory. Facebook's LeCun refers to this as a ""scratch pad."" While the neural net is examining one thing, it can keep another in mind. It can use one input to influence its analysis of another. ""They remember what they just saw, like the previous word in a sentence, and they use that to affect what they think the next word is,"" says Skymind co-founder Chris Nicholson. ""Unlike other neural networks, they include this internal feedback loop where their past experience directly impacts current activity, a bit like we rely on our memories to know how to respond to the world."" So, a recurrent neural net can collectively examine the many frames of a mountain unicycling video. It can analyze the many tiny pixels that make up a photo, in an effort to generate a descriptive caption. It can analyze the many words that make up a paragraph describing The Lord of the Rings, so it can later answer questions about the Tolkien novels. Now, one of the academic researchers behind this technology, Schmidhuber, is moving beyond academia. ""Juergen has been working with the topic for a very long time but until now has not been associated with a company,"" Karpathy says. It's unclear what applications Nnaisense will tackle. And for that reason, David Luan, the CEO of AI startup Dextro, is reserving judgment. ""From a business perspective,"" he says, ""it's still to be seen whether they choose to pursue a targeted problem with a tailored product or whether they are instead aiming to develop technology that can eventually be acquired and integrated into a larger company, as many research-oriented general AI startups do."" An acquisition may indeed be the company's aim---or at least one of them. Google acquired DNNresearch, the AI startup founded by Geoff Hinton, as well as the Deepmind startup founded by several researchers in England. Twitter acquired two other young deep-learning startups. Asked about Nnaisense, Gibson says: ""This reminds me a lot of what Hinton did with DNNresearch."""
679,https://www.wired.com/2015/07/plenty-people-signed-ai-weapons-letter/,Wired,2015,7,27,243.0," Today, a bunch of really smart people signed an open letter, supported by the Future of Life Institute, that strongly condemns the use of artificial intelligence to create autonomous weapons. The list of supporters is impressive and varied: Everyone from Stephen Hawking to Elon Musk to Stuart Russell signed (all of whom have positions on the institute's scientific advisory board). But so did more than 2,500 others—and the breakdown of those people is interesting. A lot of those signatures came from AI and robotics researchers, passing around the letter within their own academic and professional communities (the vast majority of which appear to be male). Google's head of research Peter Norvig signed, along with 48 other Google employees, 22 of them from the recently acquired AI startup DeepMind. Stanford students and researchers contributed 13 names, 14 came from MIT, and 8 from UC Berkeley, where Russell does his research. But more than half of the signatures came from outside the walls of academia. Among the non-researchers who signed the letter are a Brooklyn tattoo artist with a pretty cool origami frog design, a decorated Dutch darts player, and a self-identified member of the ""Isaac Asimov fan club."" Roger Wilco, space explorer and janitor, also made an appearance (Space Quest fans? Anyone?). And then there's this: Well, yes. If there was anybody vehemently opposed to the development of a militarized artificial intelligence, it would have to be Sarah Connor. Well played, nerds. Well played."
680,https://www.wired.com/story/musk-hawking-ai-arms-race/,Wired,2015,7,27,658.0," In an open letter presented at the International Joint Conference on Artificial Intelligence in Buenos Aries, the Future of Life Institute signatories caution that ""starting a military AI arms race is a bad idea, and should be prevented by a ban on offensive autonomous weapons beyond meaningful human control"". Although the letter, first reported by the Guardian, notes that ""we believe that AI has great potential to benefit humanity in many ways, and that the goal of the field should be to do so"", it concludes that ""this technological trajectory is obvious: autonomous weapons will become the Kalashnikovs of tomorrow"". Joining Professor Hawking and SpaceX founder Elon Musk below the letter are Steve Wozniak, cofounder of Apple, linguist Noam Chomsky, cofounder of Sky Jaan Tallinn and Stephen Goose, director of Human Rights Watch's arms division. The UK says it is not developing lethal AI, but the potential to build such weapons already exists and is developing fast -- a recent report into the future of warfare commissioned by the US military predicts ""swarms of robots"" will be ubiquitous by 2050. In response, experts and high-profile figures like Musk have made repeated calls to limit the development of deadly AI, even as peaceful autonomy grows more central to virtually every other area of tech and industry. The Future of Life Institute announced in June it would use a $10m donation from Elon Musk to fund 37 projects aimed at keeping AI ""beneficial"", with $1.5m dedicated to a new research centre in the UK run by Oxford and Cambridge universities. The latest letter starts by defining autonomous weapons as those which ""select and engage targets without human intervention"", including quadcopters able to search for and kill people, but not remotely piloted missiles or drones. It also lists the arguments usually made in favour of such machines -- such as reducing casualties among soldiers. But for the academics and figures who signed the letter, AI weapons are potentially more dangerous than nuclear bombs. ""Unlike nuclear weapons, they require no costly or hard-to-obtain raw materials, so they will become ubiquitous and cheap for all significant military powers to mass-produce. It will only be a matter of time until they appear on the black market and in the hands of terrorists, dictators wishing to better control their populace, warlords wishing to perpetrate ethnic cleansing, etc. ""Autonomous weapons are ideal for tasks such as assassinations, destabilising nations, subduing populations and selectively killing a particular ethnic group. We therefore believe that a military AI arms race would not be beneficial for humanity."" The letter also notes specifically that most AI researchers do not want to ""tarnish their field"" by contributing to lethal AI and ""by doing so, potentially creating a major public backlash against AI that curtails its future societal benefits"". ""Indeed, chemists and biologists have broadly supported international agreements that have successfully prohibited chemical and biological weapons, just as most physicists supported the treaties banning space-based nuclear weapons and blinding laser weapons."" Prompted in part by anti-autonomous weapons pressure groups, the United Nations debated the potential for a global ban on lethal autonomous weapons earlier this year -- which the UK has officially opposed. The Foreign Office stated earlier this year that ""at present, we do not see the need for a prohibition on the use of Laws, as international humanitarian law already provides sufficient regulation for this area"". Also earlier this year, Stuart Russell, professor of computer science at the University of California, Berkley, wrote in the journal Nature that two programmes commissioned by the US Defence Advanced Research Projects Agency (Darpa) would -- if successful -- ""foreshadow planned uses"" of killer robots, and potentially contravene the Geneva Convention. ""As flying robots become smaller, their manoeuvrability increases and their ability to be targeted decreases,"" Russell said. ""They have a shorter range, yet they must be large enough to carry a lethal payload -- perhaps a one-gram shaped charge to puncture the human cranium."""
681,https://www.wired.com/story/computers-singing-pop-hits/,Wired,2015,7,24,139.0," Computers -- cold, emotionless, unfailingly logical -- are not exactly known for being the type to kick back, let their hair down and get in touch with their feelings. But perhaps we're doing our trusty machines a disservice, as these videos of computers singing their hearts out to an eclectic range of pop sings shows. Martin Backes has created a software system that attempts to apply human sentiments to computerised sounds. The result? A truly unique rendition of the schmaltzy power ballad. Instead of storing data, these floppy disk drives and 3.5-inch hard drive have been hacked to play Soft Cell's toe-tappingly catchy 1981 smash. This 24-pin dot-matrix printer has been doctored to play the Benny Hill theme tune. WIRED.co.uk takes no responsibility for these earworms being lodged hopelessly in your head for at least the next 24 hours."
682,https://www.wired.com/story/robocup-robot-football-world-cup-2015/,Wired,2015,7,23,183.0," A team of robots from Japan has won the humanoid division of the 2015 RoboCub World Championships. Chiba Institute of Technology's Brain Kids beat ZJUDancer from China's Zhejian University, 1-0. The UK's only entry, the University of Hertford's Bold Hearts, failed to score in their first game and were beaten 2-0 by a French side in the second round. The UK team had finished runners up at the last RoboCup in Brazil, but said a change to softer artificial turf had caused ""stability challenges"". A change from a red to a white ball also made it more difficult for robots to see, the Bold Hearts team explained. Running for five days in the Chinese city of Hefei, RoboCup is split into five leagues: small size, middle size, humanoid, standard platform and soccer simulation. Each league poses different challenges, from artificial intelligence to getting humanoid robots to move without falling over. This was the 19th RoboCup tournament, with 47 countries and 2,000 contestants taking part. The first tournament was held in 1997. Its organisers hope to hold a competitive humans-versus robots match by 2050."
683,https://www.wired.com/2015/07/weeding-online-bullying-tough-let-machines/,Wired,2015,7,10,1125.0," Online abuse: there's just so, so much of it. Social networks teem with harassment and trolling, so much so that companies have outsourced the work of content moderation to an army of laborers, typically overseas, often at an enormous mental and emotional toll to the workers themselves. But what if you didn't need humans to identify when online abuse was happening? If a computer was smart enough to spot cyberbullying as it happened, maybe it could be halted faster, without the emotional and financial costs that come with humans doing the job. At SRI International, the Silicon Valley incubator where Apple's Siri digital assistant was born, researchers believe they've developed algorithms that come close to doing just that. ""Social networks are overwhelmed with these kinds of problems, and human curators can’t manage the load,"" says Norman Winarsky, president of SRI Ventures. But SRI is developing an artificial intelligence with a deep understanding of how people communicate online that he says can help. A few years ago, Winarsky was at TED, the high-profile ideas conference, where he noticed more than a few discussions taking place around bullying. The folks at SRI started talking, and the idea for a venture started percolating. Then, nine months ago, a social network approached the SRI and said it had a major problem with bullying on its platform. The company, which Winarsky declined to identify, had already gathered a wealth of reports and data sets on bullying and offered them to SRI to see if its researchers could do anything to help curb the problem. Siri, the voice recognition tech acquired by Apple for its iPhones, is probably SRI's most recognizable achievement. But SRI has also seen smaller successes, including the creation of Redwood Robots, which was acquired by Google. Some of its spin-offs, including Intuitive Surgical (robot surgeons) and Nuance (natural language software), have gone public. In other words, understanding the ways people communicate is an area where SRI already has a wealth of experience. The company, Winarsky says, also already owned some pretty well-developed artificial intelligence algorithms that could, among other things, grade the quality of essays in standardized tests. The team realized they could apply those algorithms against the bullying data they received from the social network. “The results were more successful than we thought they’d be given the complexity of the problem,” he says. It worked so well, in fact, that companies even began to ask whether the technology could be used to comb social networks for other nefarious activity, such as terrorist recruiting. ""I think there’s going to be a whole new category of things SRI could be exploring,"" Winarsky says. He says he can imagine a whole new field emerging around such technology in the near future, calling it ""cyberpersonal security."" Building the anti-bullying tech took about 250 people looking at the gigabytes of data that the social network provided, says Bill Mark, president of SRI's information and computing sciences group. The data included instances of bullying that human curators had already identified, Mark says. Researchers fed that data into machine learning systems that began to learn and duplicate, under supervision, the job of human content moderation. Still, bullying can be subtle and complex. It's a tough thing to teach a computer to analyze. ""Bullying is a difficult problem to solve because you have to understand context,"" Mark says. As Winarsky explains, an AI system has to understand that harassment isn't limited to glaring red flags like swear words. It needs to be able to parse meaning. A bully could post a seemingly innocuous message, he says, such as, ""Why don't you stay at home now and forever?"" At other times, Winarsky says, a post really could be an innocent joke. SRI's technology is designed to understand the meaning behind a post and consider those differences. It also seeks to identify attacks by analyzing patterns of actions, the same way a cybersecurity algorithm might recognize the nature of an attack on a server. The goal, Winarsky says, is to ultimately build a semi-automated system that would be able to detect with a high rate of accuracy what a human curator would flag. Using a combination of natural language processing—an AI discipline that seeks to understand the meaning behind language, parsing grammar and sentence models and structure—in addition to machine learning and statistical approaches, the system flags possible instances of bullying and gauges the likelihood that abuse is taking place. As the algorithm improves, Winarsky says, it should eventually lessen the load to be handed off to the human curators who have to manually determine if something is bullying or not. An initial working prototype of the system could be ready for real world applications six to twelve months from now, Winarsky says, though a polished version could take longer. Right now, he says, the team is exploring spinning off the venture, or licensing the technology out to social networks---if they need it. Asked whether he thinks SRI’s effort is ahead of everyone else’s, Winarsky hedges. “It’s extremely hard to say,” he says. “My guess is every major social network on the planet is working on similar efforts. We have to be modest. But we at least know that when it comes to machine learning algorithms, our experience runs deep.” The question, of course, is whether SRI’s technology can overcome the tactics employed by smart abusers. Jamia Wilson, executive director of Women Action Media, a group Twitter appointed last fall to look at reports of harassment on the social network, says her main concern is that abusers are well-aware of the initiatives to curb harassment on networks—and employ sophisticated techniques to avoid detection. Some of the problems Wilson saw while looking at Twitter included tactics like the “tweet and delete,” when harassers erase disparaging tweets not long after sending them, and “false flagging,” when trolls make fake claims about harassment to overwhelm the resources of the social network. “I think the system sounds interesting, but I doubt it would solve the problem completely,” Wilson says. “The complexity needs to be addressed, and it’s important that there would still be some moderation processes.” WAM investigated 811 reports, 161 of which it forwarded to Twitter. But the effort was arduous, and involved looking at each manually submitted report, and at times following up with the individuals sending in those reports. After WAM’s effort concluded, the report the organization put out was fairly disheartening---trolls often found it too easy to game the system. But Wilson says the group fully intends to engage in a second trial, and she would be absolutely interested in trying out SRI’s tool when the time came. She says, ""We need as many different kinds of tools as we can get to tackle this problem."""
684,https://www.wired.com/2015/07/twitters-new-ai-recognizes-porn-dont/,Wired,2015,7,8,1233.0," Clément Farabet deals in artificial intelligence. As a research scientist at New York University, he built brain-like computing systems that identified objects in photos and videos, and then he launched a startup where he did much the same thing. He and his co-founder called it Madbits, and 18 months later, Twitter snapped it up. Madbits had no customers. And no one beyond the two companies knew quite what Twitter would do with the five-person startup. But Alex Roetter knew. When Farabet and his MadBits crew joined Twitter last summer, Roetter—the company's head of engineering—told them to build a system that could automatically identify NSFW images on its popular social network. ""When you do an acquisition—even though they're coming in to do something broad—you want to give them something specific, so you get to know each other and make sure the acquisition works,"" Roetter says. ""So we gave them the problem of NSFW."" A year later, that AI is in place. According to Farabet, if you tune the system to identify about 99 percent of all porn and other objectionable images—allowing the company to warn users with interstitials in the Twitter timeline—it will incorrectly flag perfectly acceptable pics just 7 percent of the time. These numbers are entirely dependent on Twitter's definition of NSFW, of course. But taken at face value, they represent a significant step forward for social networks like Twitter and Facebook. As WIRED reported last year, companies like Twitter and Facebook typically pay workers to comb through the unending stream of photos filling its vast social network and identify inappropriate images, including porn, sexual solicitation, racism, and gore. Roetter says Twitter has used human-powered services like CrowdFlower for such work. With an AI system like the one Farabet and other engineers built, a company can significantly reduce the number of people needed to pore over dick pics, dildos, and beheadings. That's faster and cheaper. And it doesn't place that enormous mental and emotional toll on as many laborers in places like the Philippines. But this rather pointed task is just the beginning for Farabet and his team. In tackling the NSFW problem, the Madbits crew—though still working out of New York—dovetailed with other machine learning specialists in Twitter's San Francisco office, including Siva Gurumurthy and Utkarsh Srivastava. Now they're joining forces with WhetLab, an AI startup in Boston that Twitter acquired three weeks ago. The result is a central AI operation—dubbed Twitter Cortex—that will help provide machine learning tasks across the company. These might include identifying people you should follow; curbing spam and abuse; and displaying tweets, ads, and other content you'll probably enjoy. The company already does all of these things. But the breed of AI provided by Madbits and WhetLab can do it better. Much better. Roetter says the company already is using Twitter Cortex technologies to improve its ad system, and eventually, it will analyze the company's entire corpus of tweets, ""so we can better classify them and figure out what you might be interested in."" Twitter Cortex mirrors work at companies like Google and Facebook. Like Twitter, these Internet giants are building teams dedicated to what's called deep learning, an umbrella term for a breed of computing system that mimics the web of neurons in the human brain. Facebook now uses these ""neural networks"" to identify faces in photos. Google uses them to recognize the words you bark into the Google Now personal assistant on your Android phone. Microsoft uses them to translate Skype conversations from one language to another. The technology represents a near future where machines can perform many tasks previously limited to human—and, in some cases, where machines outperform humans. Deep learning algorithms can ""learn"" certain tasks by analyzing vast amounts of data. They can learn to carry on a decent conversation, for instance, by analyzing old movie dialogue. They can learn to identify porn by analyzing—well, you get the picture. Since acquiring Madbits, Twitter has built such neural nets inside its data centers, using machines equipped with graphics processing units, or GPUs. Chip makers like nVidia created GPUs to quickly render large images for games and other software applications, but they've proven quite adept at running deep learning algorithms. Though Roetter and Farabet decline to reveal the size of these neural networks, these probably are much smaller than what is already running at Google and Facebook. But they're already identifying NSFW photos on Twitter's live service with what would seem to be impressive accuracy. And according to David Luan, whose startup, Dextro, works to identify similar photos for other companies, spotting images on Twitter carries unusual challenges, because the company must serve content across its network in near real-time. It should be noted that this kind of algorithm is far from perfect—and identifying something like porn is particularly difficult. After all, Twitter also serves up images of half-naked babies and breast-feeding mothers. That's not porn, but a computer needs to be trained to tell the difference. ""There's so much variation, and often, this is not just limited to one type of content,"" Luan says. ""It's not just porn. It's violence and other stuff."" Just last week, on the new Google Photo app, the company's neural networks identified black people as gorillas—an egregious mistake and a sign that there are so many kinks to iron out in even seemingly simple deep learning tasks. ""Machine learning,"" Luan says, ""always makes mistakes."" Considering that some 100,000 people spend their days identifying NSFW images, Twitter has applied the technology in the right place. Presumably, other companies, including Facebook, are working on similar systems (Facebook was unable to participate in this story). In teaching a neural net to identify NSFW images, humans must first spend time tagging the kind of photos that should be identified. But as time goes on—and the neural net continues to learn—the need for this tagging diminishes. ""You need human, generally, to label the data,"" Roetter says. ""But then, going forward, the model is applied to cases you've never seen before, so you dramatically cut down the need for people. And it's lower latency, of course, because the model can do it in real-time."" Twitter acquired WhetLab in an effort to improve its models at a faster rate. The startup uses a technique called ""bayesian optimization"" to fine-tune its neural nets. As WhetLab founder Ryan Adams describes it, the company uses ""machine learning to improve machine learning."" In other words, a neural net can analyze the performance of a neural net to improve a neural net. ""It creates this really interesting amplifying effect,"" says Adams, a former Harvard computer science professor. ""You can take your limited resources and talent and really affect a lot of things very rapidly by automating so much of the process."" The may sound like little more than talk. But this is the way computer science works—and neural nets are particularly ripe for this kind of magnanimous recursion. The magic of neural nets is that they improve over time. In short, they work like your brain. They doesn't work exactly like your brain, but they work well enough to correctly identify porn—at least most of the time. That's no small thing. Correction: This story originally misstated when Twitter acquired WhetLabs. It acquired the company three weeks ago. Originally, the story also said that Twitter has used TaskRabbit to label data. It has not. It has used services such as CrowdFlower."
685,https://www.wired.com/story/google-deep-dream-video-fear-and-loathing/,Wired,2015,7,7,314.0," Deep Dream, Google's artificial neural network, has turned its hand to video. The system, which identifies and builds on certain features in images, has turned a scene from Fear and Loathing in Las Vegas into a trippy nightmare. The scene used for the experiment shows an LSD trip at The Matrix in San Francisco. Google's neural network introduces animal faces that ebb and flow in the shadows, with extra eyes and outlines flickering in and out of focus. A clip from 2001: A Space Odyssey was also fed through Google's neural network, although this time it focussed on amplifying outlines and shapes, rather than adding ghostly animals and eyes to the video. The code used to feed video through Google's artificial neural network has been published to Github, allowing anyone to give it a go. It can also be used to transform audio, using the same technique to pick out features and amplify them. The images and videos are created by changing certain parameters in the Deep Dream code. Each neural layer handles different features of an image, video or sound clip with lower layers toying with basic outlines and orientations and higher levels layers looking for more complex features. The network then creates a feedback loop where it amplifies certain features to create its unusual dreamscapes. In the Fear and Loathing clip the neural network was told to look for animals and people, while in 2001 it was looking for outlines. As the system was mainly trained to interpret shapes as animals it will hunt out animal-like features and turn them into swarms of dogs or birds. Earlier this month Google made its Deep Dream code available to the public. The tool was developed to help Google's new photos app recognise faces, animals and other features in images, with its artistic talents a result of how the AI learns to interpret our world."
686,https://www.wired.com/story/google-deep-dream/,Wired,2015,7,3,111.0," Last month, Google revealed that it uses its own artificial intelligence program, known as Artificial Neural Networks, to classify and sort its images. The technology basically works by spotting patterns in pictures in order to identify them -- and it's already being used in Google's new photos app to recognise faces and animals. However, a new website called Deep Neural Net Dreams, created by Zain Shah, allows you to take your own photos and run them through Google's AI code. The results? Some seriously psychedelic and eye-popping images, from trippy landscapes and abstract cats to swirly recreations of famous paintings. You can upload your own photos and share them using #deepdream."
687,https://www.wired.com/2015/07/trained-googles-chatbot-mein-kampf/,Wired,2015,7,2,565.0," Google recently built a chatbot that can learn how to talk to you. Artificial intelligence researchers Oriol Vinyals and Quoc Le trained the thinking machine on reams of old movie dialogue, and it learned to carry on a pretty impressive conversation about the meaning of life. But what happens, asked my friend David Kesterson, if you train it on Mein Kampf? It's a harrowing question. And it's just one that concerns him. Another is just as troubling: What if you train it on The Bible? The Bible, after all, can be interpreted in so many ways. What, in the end, will the machine learn? ""What if it begins to make moral judgments based on its reading and interpretation?"" he says. In some ways, these are just thought experiments. Today, if you train the chatbot on Mein Kampf, it just sounds like Hitler. It doesn't think like him. We're a long way from computers that really think. But as machines become more and more adept at learning on their own, approaching real thought, we must ask the ethical questions early and often. Across the globe, some of the world's brightest minds are working to focus more attention on the social implications of AI. British physicist Stephen Hawking has warned that creating true artificial intelligence could ""spell the end of the human race."" Elon Musk, the founder of electric car company Tesla, recently addressed a conference on AI ethics, warning of an artificial ""intelligence explosion"" that could catch us off guard. Prominent AI researchers, such as Facebook's Yann LeCun, have downplayed these fears. LeCun points out that you can build super-intelligent machines without making them autonomous. A machine can play chess at super-human levels, but lack the ability to do much else—including reprogram itself. But LeCun says that an AI uprising is certainly something we need to ""think about, design precautionary measures, and establish guidelines."" One AI company is already working to do those things. Based in Austin, Texas, Lucid is selling a system based on Cyc, an AI tool that researcher Doug Lenat has been developing for more than 30 years. The aim is to produce a system that has ""common sense""—the can ""reason."" Lucid is now bringing this to the business world. In an effort to understand the ramifications of such a system—and most likely, protect itself from bad PR—the company is working to setup an ""ethics advisory panel"" that will examine the big ethical questions of AI. ""We believe, like most pundits in the world, that AI is a hugely disruptive thing—-whether you see dystopian things or pure nirvana or somewhere in between,"" says Lucid founder and CEO Michael Stewart. Stewart thinks that Musk's concerns are overblown, but worth listening to. ""I'm not sure he has a full understanding of the full architecture of strong AI, any more than I do of electrical cars,"" Stewart says. ""He's putting his finger on real issues—some we think are stretched and some we don't."" Lucid hopes to work hand-in-hand with the likes of Musk and Hawking, and Stewart says the company is already collaborating with researchers at Oxford University and Cambridge (Hawking's home base), and that bioethicist John Harris has agreed to be part of the panel. ""We want to engage with that community, and we want to do it in a rational way, with overt knowledge,"" he says. ""We know what this technology is capable of."""
688,https://www.wired.com/story/microsoft-research-funny-ai/,Wired,2015,6,30,430.0," Microsoft Research wants to make you funnier on the fly, and has created instant chat software that can do just that. Built at the company's Redmond base in Washington, the premise behind the CAHOOTS software was to explore the space ""where a computer and a human collaborate to be humorous"". It aims to fill that vast hole in your social skills, and ensure you never come up dry on witty repartee over instant chat. The software works like any other ordinary web-based chat system, but processes the language you are using to suggest potential opportunities for hilarity based on those words. It veers away from textual humour though, and is, for now, confined to images. Users are presented with choices, and can select the most appropriate one if it piques their funny bone. The word-linked image choices were tested on 738 people using Amazon Mechanical Turk, with data from these results combined with interviews from people that regularly use images in instant messaging. CAHOOTS picks out key words used in chats to conduct an image search and generate a constant stream of options. So when a user typed ""Why u late?"", they were presented with image search results for the terms ""funny why"" and ""funny late""; a canned response image with the words ""I don’t know why!"" across it, that can apparently be funny in all circumstances when questions are involved; and an automatically generated meme. When CAHOOTS' functionality was tested alongside a bot that automatically inserts hilarious images (we're presuming, largely cat-related ones), and ordinary chat systems, the Microsoft Research AI came out trumps. ""CAHOOTS chats were rated more fun, and participants felt more involved, closer to one another, and better able to express their sense of humour,"" the team behind it writes in a research paper. ""CAHOOTS chats were also rated as more fun than ordinary chat."" . The solution, they decided, was to leave the talking to the human, and just inspire them with a hilarious image they can add comments to. Microsoft Research envisions the tools being used as an add-on to current popular chat systems, or integrated into Facebook and Twitter. Miaomiao Wen from Carnegie Mellon University, who collaborated with Microsoft Research on the project, told the New Scientist she'd also like to see it integrated into email, but was concerned, ""it's not always appropriate to suggest a funny picture for an email"". Email is also, as many entrepreneurs will tell you, a dying enterprise that every other startup is attempting to disrupt. Perhaps automated humour will add a few years to its waning lifespan."
689,https://www.wired.com/2015/06/chat-services-replace-friends/,Wired,2015,6,29,1809.0," The more connected we are, the more we are all just pixels on a screen. And we crave more pixels, pixels talking to us, responding to us, acknowledging us---as often as possible. “It’s just a reality that social life is moving through screens,” says Eric Klinenberg, director of the Institute for Public Knowledge at NYU and the author of Going Solo: The Extraordinary Rise and Surprising Appeal of Living Alone. “There’s research showing that people who stay off of social media are more prone to isolation because they’re missing out on the place where the action is.” At any moment in time, that action could stall out. Your friends might all be busy, unable to provide that quick rush of dopamine you get from a Like, a fav, or comment. But you don't need them---you can outsource communication to an automated archetype that approximates their role in your social sphere. Say for instance, if you're doing a little late-night booze-fueled shopping. Drunk Shopping is not a person, it’s just a phone number---but it's your best, tipsy friend when you want some companionship during your online shopping sessions. You simply send a text to a phone number, initiating the conversation with “heyyyyyy,” and it replies with a ridiculous message and a link to a weird item on Amazon. From there, you're free to continue to chat. It’s a bot, but it doesn't feel like one. It was created by the three-person team of Chris Baker, Mike Lacher, and Tiger Wang, and it makes them absolutely no money. Baker wrote the copy, Lacher programmed the service, and Wang handled the design. Using the API from a service called Twilio, the team programmed a script that serves up one of about 400 responses from their database, many of which are driven by keywords within the sender’s SMS message. “We have no grand plans for this thing,” Baker says. “We launched it about a month ago, and it was an idea that was tossed around for the longest time. We didn’t know the proper form to give it. We started with, well, what’s the Web experience? And we had a bunch of funny ideas like would you have to slur to get in. At one point, we had built a Captcha that you had to get wrong in order to get in. But doing it through a website never really felt right.” Once SMS and app-based shopping services such as Magic and Alfred became trendy, the team decided a text-message chat bot was the way to go. The big difference, Baker explains, is that it’s “like you’re talking to some insane guy.” ""We’ve got more than 10,000 people who have used [Drunk Shopping], and we honestly had to shut down new signups after the first week. Too many people were flocking to it and our bill was starting to skyrocket,"" says Baker. ""...We’ve had a great deal of people using it, but I haven’t looked at the analytics of how many people have texted in various responses---like how many people are engaged in the rabbit hole of talking to this bot. It’s probably a fair amount."" So why, in a Web full of ways to connect with real people, do we love the bot (even one that sounds like an insane guy)? And of all the ways we could interact with them, why is a plain old texting screen so entertaining? The business of helping developers develop these apps is a thing now. Chatbots are not new; they're almost as old as the Internet itself. But suddenly, amidst an infinite amount of flashy, niche social apps, the humble bot-texting-app has become trendy. Every morning Product Hunt is littered with them: There's Text Riley for finding a new apartment, Happy Now for home assistance, Text Miley for job hunting. Texting with these services definitely has an A.I. air to it, but the comfort of those green and blue bubbles does something to offset the absurdity of it. What likely helped pave the road for this sudden slew of digital SMS friends were Twitter bots. The often strange, but ultimately beloved automated accounts have long been one of the best parts of the platform. While your conversations with them might be inane, something sends you back. For whatever reason, tweeting with @tofu_product feels strangely familiar. It’s almost like it took the words right out of your mouth, because that’s essentially what it does. Launched in 2013, @tofu_product scans your recent tweets, then responds to you with a mish-mash of your own words a few seconds after you send it a message. The account’s creator, 32-year-old Joe Toscano, describes the bot’s output as “linguistic salad creation.” Toscano has tweaked tofu’s algorithm over time. He says the original formula was written in Objective-C against the Apple Core Data framework, but there were scaling issues with that codebase once @tofu_product became so popular. He rewrote the code about a year ago, and now it’s based on Go with a Redis database backend. “When someone talks directly to him, a choice is made,” Toscano explains. “If tofu doesn't know the person, he reads a little bit of their tweet backlog in order to get some material to work with. If he already knows the person, he jumps right into the generation function. After the generation function runs its course, the result is run through a sanity check of a kind and then posted to Twitter as a reply.” Basically, the results are as if your own tweets have been chopped up into poetry magnets and then reassembled by Crispin Glover. And the responses are fast---sometimes too fast for Twitter’s rate limits. When that happens, @tofu_product gets temporarily banned from posting tweets, also known as “getting thrown in Twitter jail.” (He's a little quieter now, so he managers to stay out of ""jail."") Toscano says he’s been approached by companies to develop @tofu_product into an algorithm that analyzes the sentiments in peoples’ Twitter feeds, but he’d rather it remain just a fun side project. He says he may eventually open-source the reply-generation algorithm, but that may have its drawbacks. “Only having one tofu around makes him kind of a novelty, which is fun,” says Toscano. “People tend to talk to tofu for about five to 10 messages, and then they leave him be... There have been people who have talked to him at great lengths, though, sometimes for hours and hours, spanning hundreds of tweets. Those events are pretty rare, though, and frankly they make me worry about the general mental condition of the human participant.” So what about the condition of the users who are besotted by Invisible Girlfriend and Invisible Boyfriend? The concept is simple: You pay to bae. The apps are run by Matt Homann and Kyle Tabor, who cooked up the idea during a hackathon in 2013. The service began as a chatbot simulation wherein you would text a ""boyfriend"" or ""girlfriend,"" but now, actual human beings are doing the talking. Tabor says more than 70,000 fake girlfriends and fake boyfriends have been created since the service launched in January, proving there is an economy of loneliness. The basics to the service (picking a name, a photo, an age, etc) are free. If you want to take things any further, it’ll cost you. For $25 per month, you get 100 text messages, 10 voicemails, and a handwritten note from your fake boo. $15 per month gets you texts only. “My co-founder Matt, he was recently divorced and his parents were bugging him,” Tabor says. “At Thanksgiving, his parents were asking him if they should set another place at the table, awkwardly suggesting things like ‘you need to date again.’ He has these ideas, buys domains, and eventually does nothing with them. It wasn’t until we went to a hackathon where we met, he pitched the idea. We ended up building a very simple chatbot over the weekend.” Ultimately, that chatbot wasn’t very convincing, and they decided the service would have to depend on real people to communicate with subscribers. According to Tabor, there are now roughly 600 people writing for the company through Crowdsource. “They do the hiring, the quality control, they train, they do all of that,” Tabor says. “We just pay a fee for each task. They have many different clients, and we’re probably the smallest client they’ve ever had. They usually work with Fortune 500 companies. They thought it would be very entertaining for some of their workers to break up the monotony of just doing random tasks---checking some spelling of this or just checking if something is adult content, something like that. This is a much more engaging task for them.” According to Tabor, no real relationships have grown out of the service since it launched in January, as none of the writers get to see any of the users’ information other than how they “configured” their invisible other. All the incoming messages are filtered so that they can block out any adult content, as well. “This is a completely asexual relationship, because we don’t allow sexting on the service,” Tabor says. “We flag any adult content and we don’t allow it to go through… We thought of this of a way to live your life confidently without people trying to set you up on dates or coworkers hitting on you. A conversation can be friendship or companionship, and we’re trying to make it less reactive and more proactive, just like a friend would be or a companion.” According to Tabor, there are possibilities for expanding the role of Invisible Boyfriend and Invisible Girlfriend in the future. Because it’s basically a digital pen-pal service, he says they’ve thought about creating an Invisible Life Coach or Invisible Therapist service as a spin-off. But the more interesting idea is playing matchmaker based on subscribers’ preferences and messages. While the invisible significant other scheme hired some humans and found a payday, not every text-bot app has such ambitions. The Drunk Shopping team at one point planned to make money off the service, but things didn’t really pan out: The creators realized that people aren't using Drunk Shopping to actually shop while drunk. Instead, they just want to talk. “We applied for the Amazon affiliate program, but they rejected us,” says Baker, laughing. 1 “I’m not too sure about how many people are just signing up because they’re bored or lonely. Looking at how people are talking about it, at the end of the day you’re not really looking to buy anything. You’re looking for a funny picture, some funny copy, and it’s just fun knowing that that interaction is always in your back pocket.” 1UPDATE 12:00 PM ET 7/13/15: Drunk Shopping's Chris Baker was referred to as Chris Davis in the last paragraph of this story. That paragraph has been updated to correct his name."
690,https://www.wired.com/2015/06/google-made-chatbot-debates-meaning-life/,Wired,2015,6,26,815.0," A human talks to a machine. And it goes like this: Human: what is the purpose of life?Machine: to serve the greater good.Human: what is the purpose of living?Machine: to live forever.Human: what is the purpose of existence?Machine: to find out what happens when we get to the planet earth.Human: where are you now?Machine: i'm in the middle of nowhere.Human: what is the purpose of dying?Machine: to have a life.Human: what is the purpose of being intelligent?Machine: to find out what it is.Human: what is the purpose of emotions?Machine: i don't know. It reads like a scene from a classic sci-fi flick. But it's not. It really is a human talking to a machine---a machine built by Google. And there may be good reason it sounds like a movie. Part of the trick is that this machine learned to converse by analyzing an enormous collection of old movie dialogue. Google detailed its project earlier this month in a research paper published to Arxiv, a popular repository for academic research. Other ""chatbots"" can carry on (somewhat) reasonable conversations with humans. But this one is a little different. There wasn't a team of software engineers who meticulously coded the bot to respond to certain questions in certain ways. Google researchers Oriol Vinyals and Quoc Le built a system that could analyze existing conversations---in this case, movie dialogue---and teach itself to respond. ""Instead of using rules to build a conversational engine, we use a machine learning approach,"" Le tells WIRED. ""We let the machine learn from data rather than hand-coding the rules."" The system uses what are called neural networks, vast networks of machines that approximate the web of neurons in the human brain. Neural nets are an old idea, but recently, after many years in exile from the AI community, they've risen to prominence---in enormous ways---now that companies like Google and Facebook and Microsoft have the computing power needed to run them. At these internet giants, neural nets are already working to recognize faces and objects in photos posted to social networks, identify spoken words on Android phones, and translating online phone calls from one language to another. Google's paper shows they can also drive chatbots, and perhaps move us closer to a world where machines can converse like humans. ""With papers like this, people always give the most impressive interactions,"" says Chris Nicholson, founder of the neural networking startup Skymind. ""But I was impressed. And this represents a significant front in the industry."" Google's chatbot draws on research from across the larger AI community, including work from University of Montreal professor Yoshua Bengio and researchers at Facebook and Microsoft. Richard Socher, the founder and CEO of neural networking startup MetaMind, says the human-to-machine conversations in the paper aren't that surprising, given previous research. ""Neural networks are already well known for modeling language,"" Google's Vinyals says. But previous research involved other tasks, such as machine translation. Le---who has worked extensively with neural networks in recent years---says that when Vinyals brought the initial research to him, it was wholly unexpected. He didn't think that neural nets would work so well with conversations. ""When he told me that we could put all this complexity into a machine learning approach,"" Le says, ""I was very surprised."" The system Le and Vinyals built is just a proof of concept. But they see it as a way of improving the online chatbots that help answer technical support calls. In addition to training the system on movie dialogue and having it chat about the meaning of life, they trained it on old support calls and had it chat about browser problems: But as these types of systems are perfected, they could operate in so many other ways, well beyond tech support. ""It may sound crazy,"" says Nicholson, ""but ultimately, chatbots could deliver the morning news to people like gossip over the fence, breaking the one-to-many model of the journalism."" In other words, it could put WIRED out of business. Dennis R. Mortensen, the CEO and founder of x.ai, a startup offering an online personal assistant that automatically schedules meetings, calls the Google paper ""somewhat scary,"" given how well it mimics human conversation. ""The examples,"" he says, ""are very lifelike."" This is perhaps most true when you read the philosophical conversation about the meaning of life. At the same time, it's a bit heartbreaking. ""Where are you now?"" the human asks. ""I'm in the middle of nowhere,"" the machine says. And given that machine is training itself on existing data, it's wonderfully fascinating---even when you know it can ultimately put you out of business. ""The outputs come not just from machines but from what humans have produced in the past,"" Mortensen adds. Le says that, with this project, he's most interested in learning what machines think about morality. And then he laughs. The research says as much about us as it does about machines."
691,https://www.wired.com/story/deepmind/,Wired,2015,6,22,4510.0," **The future of artificial intelligence begins with a game of Space Invaders.**From the start, the enemy aliens are making kills -- three times they destroy the defending laser cannon within seconds. Half an hour in, and the hesitant player starts to feel the game's rhythm, learning when to fire back or hide. Finally, after playing ceaselessly for an entire night, the player is not wasting a single bullet, casually shooting the high-score floating mothership in between demolishing each alien. No one in the world can play a better game at this moment. This player, it should be mentioned, is not human, but an algorithm on a graphics processing unit programmed by a company called DeepMind. Instructed simply to maximise the score and fed only the data stream of 30,000 pixels per frame, the algorithm -- known as a deep Q-network – is then given a new challenge: an unfamiliar Pong-like game called Breakout, in which it needs to hit a ball through a rainbow-coloured brick wall. ""After 30 minutes and 100 games, it's pretty terrible, but it's learning that it should move the bat towards the ball,"" explains DeepMind's cofounder and chief executive, a 38-year-old artificial-intelligence researcher named Demis Hassabis. ""Here it is after an hour, quantitatively better but still not brilliant. But two hours in, it's more or less mastered the game, even when the ball's very fast. After four hours, it came up with an optimal strategy -- to dig a tunnel round the side of the wall, and send the ball round the back in a superhuman accurate way. The designers of the system didn't know that strategy."" In February, Hassabis and colleagues including Volodymyr Mnih, Koray Kavukcuoglu and David Silver published a Nature paper on the work. They showed that their artificial agent had learned to play 49 Atari 2600 video games when given only minimal background information. The deep Q-network had mastered everything from a martial-arts game to boxing and 3D car-racing games, often outscoring a professional (human) games tester. ""This is just games, but it could be stockmarket data,"" Hassabis says. ""DeepMind has been combining two promising areas of research -- a deep neural network and a reinforcement-learning algorithm – in a really fundamental way. We're interested in algorithms that can use their learning from one domain and apply that knowledge to a new domain."" DeepMind has not, admittedly, launched any products -- nor found a way to turn its machine gameplay into a revenue stream. Still, such details didn't stop Google buying the London company -- backed by investors such as Elon Musk, Peter Thiel and Li Ka-shing -- last January in its biggest European acquisition. It paid £400 million. On DeepMind's website, the company's mission is explained simply as to ""solve intelligence"". As Hassabis describes it, it comes down to a multi-decade Apollo-style project to crack artificial general intelligence (AGI): rather than teach the machine to understand language, or recognise faces, or respond to voice commands, he wants machine learning and systems neuroscience to teach the network to make decisions -- as humans do – in any situation whatsoever. ""The dream of AI is to make machines smart,"" he explains in the new six-storey King's Cross building that houses 150 DeepMind staff. ""Most AI today is about preprogramming a machine. Our way is to program them with an ability to learn for themselves. That's much more powerful; that's the way biological systems learn. ""We refer to it as an Apollo programme, a Manhattan project, in terms of the quality of the people involved -- getting 100 scientists, here from 40 countries, together to work on something visionary and trying to make as fast progress as possible. We've brought together the world's top computational neuro-scientists as well as machine-learning experts with huge engineering resources, to see how far can we push."" Artificial intelligence tends to get a bad rap in popular culture: as cyborg assassins in Terminator, or operating systems, like Samantha in Her, that lure us into unwitting love. So why do we need a general form of AI at all? ""I think we're going to need artificial assistance to make the breakthroughs that society wants,"" Hassabis says. ""Climate, economics, disease -- they're just tremendously complicated interacting systems. It's just hard for humans to analyse all that data and make sense of it. And we might have to confront the possibility that there's a limit to what human experts might understand. AI-assisted science will help the discovery process."" Still, AGI won't be here any time soon. ""We're trying to build a single set of generic algorithms, like the human brain,"" he says. ""We're trying to build things with generality in mind. The Nature paper was the first baby steps. You need to process vision; you need long-term memory; you need working memory so you can switch between tasks... Today you can create pretty good bespoke programs to solve specific tasks -- playing chess or driving a car. Our system could learn how to play chess, but it's not going to be better than Deep Blue. You give it all the knowledge it needs -- the moves, the openings, the endgames. But where does the intelligence reside in something like Deep Blue? It's not in theprogram, it's in the minds of the programming team. The program is pretty dumb; it doesn't learn anything."" Hassabis runs DeepMind according to a ""20-year roadmap"". ""General AI isn't the sort of thing you can wake up one morning and say, that would be a cool thing to do a startup on. I'm trying to fuse together a deep understanding of computer science and neuroscience, to understand systems neuroscience -- the algorithms the brain uses, the representations it has for knowledge, its architecture. As opposed to the EU Human Brain Project, which is trying to reverse engineer at the cortical-column level. That's too low-level for us."" So how will his project affect our lives within 20 years? ""Science for sure will benefit -- within drug discovery, protein folding, anything where there's a huge amount of exploration,"" he says. ""Of course we'll have self-driving cars -- but that's narrow AI. We'll have things that can start being creative in 20 years. A lot of things that look very complex, when you break them down it becomes clear how the apparatus works. I studied imagination. We did brain scans, found areas of the brain involved, built models. That made me think that most processes can be understood, including creativity."" An AI making an entertaining movie? ""I'm thinking more on a basic level -- putting disparate things together to make a new hypothesis. A novel or film is many decades away, though with music, a more limited domain, there are already passable projects that hint at what's possible."" In the near term -- say, five years -- he sees DeepMind's work ""making our everyday tools more smart and adaptive"". ""Better search, so it understands your context and intent better -- you could be more ambiguous and it will understand what you're trying to do. Smartphone assistants are pretty limited in how they work as they're programmed. Wouldn't it be great if they could learn to adapt? You'd say, 'I want an amazing trip round Europe, book me all the hotels and restaurants and flights,' and it would know the archaeological sites, would take you via a vineyard if wine's your thing. Or, 'I'm moving to a new city, I've got small kids, what area should I look for with decent Ofsted reports?' That's the sort of tech I'd hope to see embedded in lots of places in five years. Reinforcement learning will be as big as deep learning is now -- the decision-making part of our algorithms, working out the best action to take, which works initially by trial and error. Trading would be big -- anywhere you have sequential decision-making tasks. We were talking to a weather company about the data they're collecting -- trying to detect the pattern for climate modelling. Predictive analytics will be huge -- you'll see this learning adaptable intelligence seeping into different products."" And after? ""Algorithms will be as good as radiographers at looking at scans -- some aspects of those tasks will be augmented by AI. Ten years-plus, it's the AI scientist. And maybe there'll be an AI listed among the authors of a Nature paper. That will be pretty cool."" Demis Hassabis was four when he became curious about the chess game his father and uncle were playing. His father, a Greek Cypriot singer-songwriter who once ran a Finchley Central toy shop with Demis's Chinese Singaporean mother, humoured him and let him play; within two weeks the boy was beating the adults. At five he was competing nationally; at six he won the London under-eight championships; at nine he was captaining England's under-11 team -- when England was second in the world to the Soviet Union. ""I was an introspective thoughtful child, I guess, always trying to work things out,"" he recalls. ""You can't help asking, how's my brain coming up with these moves? You start thinking about thinking."" Aged eight, he bought his first computer -- a ZX Spectrum -- with £200 prize money from beating his American opponent, Alex Chang, 3:1 in a four-game match. ""The amazing thing about computers in those days is you could just start programming them. I'd go with my dad to Foyles, and sit in the computer-programming department to learn how to give myself infinite lives in games. I intuitively understood that this was a magical device which you could unleash your creativity on."" His father began home-educating him while his mother worked in John Lewis. Then, aged 11, at the local comprehensive, Christ's College Finchley, he discovered AI after buying himself a Commodore Amiga to program games. ""I wrote AI opponents for Othello, as chess was too complicated for the machine to run, and it beat my younger brother."" By 13, he was the world's second-highest-rated chess player for his age after Hungarian grandmaster Judit Polgár. Within a year he'd decided that computers were more interesting than chess. He completed his GCSEs by 14, took maths A level at 15, and further maths, physics and chemistry at 16, and applied to Cambridge after seeing a Jeff Goldblum movie, The Race for the Double Helix, about the discovery of DNA. ""I thought, is this what goes on at Cambridge? You go there and you invent DNA in the pub? Wow."" His school hadn't sent anyone to Oxbridge for years, and Hassabis, 15, was not prepared for the Queens' College interview. ""It was really cold. The professor was asking a question about computer science. If I wanted to visit all 30 colleges, he asked, what's the standard path? I said 30 factorial. Then he said, 'So what is that, then?' I was like, what? How am I supposed to calculate 30 factorial? I came out with the answer immediately: 10 to the 25th -- not that far from the actual answer. He was totally taken aback. He'd just wanted me to say, 'It's a very big number.' I didn't tell him how I did it. I was bored in an A-level maths class, and was playing to work out the shortest number that would break my scientific calculator. Turned out to be 60 factorial. I used that to estimate what 30 factorial was."" He won his place to read computer science, but Cambridge wouldn't let him start at 16. He had come second in a games competition in Amiga Power magazine and won a job at Bullfrog Productions, Peter Molyneux's development house, where he spent his year off. Aged 17, he wrote a multi-million-selling game called Theme Park, ""cementing my view that AI would be this incredible advance"" and making him enough money to finance university. On graduating with a double first, he joined Molyneux's new company, Lionhead Studios, where he was lead AI programmer for Black & White. A year later, he started his own studio, Elixir, which grew to 60 people. ""I wanted to create a political simulator, Republic, where you had to overthrow the dictator by any means. We simulated a whole country, a million people. It took five years -- we were too ahead of our time."" Hassabis decided to take a PhD in cognitive neuroscience at University College London, focusing on memory and imagination. ""I thought that would be a good thing to study because computers do episodic memory badly. My work was investigating imagination as a process -- how do we visualise the future?"" He tested the imagination of amnesiac patients with a damaged hippocampus and found that their descriptions of, say, being on a beach were impoverished, suggesting that the hippocampus incorporated a visualisation engine. His published paper was listed by Science as one of the top ten breakthroughs of 2007. He then studied computational neuroscience at UCL's Gatsby Computational Neuroscience Unit, with stints as a visiting researcher at MIT and Harvard. By now, he also held a record as five-times winner of the Mind Sports Olympiad. When in 2011 Hassabis was ready to launch DeepMind, he knew he wanted funding from Peter Thiel, Facebook's initial lead investor. Trouble is, he didn't know how to reach Thiel. ""It took me a year to work out that I'd need to give a talk at one of the AI conferences he was sponsoring, at which there would be a speaker event where I'd probably have a one-minute chance to pitch him,"" he recalls. He researched Thiel and found that he too played chess. ""So I thought that would be a more interesting 'in' than being the hundredth person pitching to him. I not so subtly weaved into the conversation a question about why chess had survived so successfully. He was intrigued. The reason, I said, was that the knight and the bishop are perfectly balanced, causing all the creative asymmetric tension. He said, 'Why don't you come back the next day and do a proper pitch?'"" Thiel invested; so did Elon Musk, Skype cofounder Jaan Tallinn, and Li Ka-shing's Horizons Ventures. Antoine Blondeau, cofounder of Sentient, an AI startup which has raised $143 million, was asked by Horizons to investigate DeepMind. ""They impressed us with their laser focus on solving a complex scientific problem in an agile, pragmatic way,"" he says. ""And we liked the quality of the group."" It was Musk who told Larry Page about this company trying to crack AGI. A couple of months later, Alan Eustace, Google's senior VP of knowledge, emailed to suggest a meeting with Page. ""It's not the sort of invitation you turn down,"" Hassabis says. ""It took a year [of negotiations]. One reason we chose Google was culturally we were a good fit, but also AI is something Larry passionately cares about."" But why did he sell? ""We weren't planning to, but three years in, focused on fundraising, I had only ten per cent of my time for research,"" he says. ""I realised that there's maybe not enough time in one lifetime to both build a Google-sized company and solve AI. Would I be happier looking back on building a multi-billion business or helping solve intelligence? It was an easy choice. And there was something Larry said to me: 'I spent 15 years building Google -- why don't you just come and take advantage of the opportunity we've built here?' I didn't have a good answer to that."" There were questions, even among its investors, over whether yet another London startup was selling out too soon. Hassabis dismisses this notion as ""silly"". ""People should be caring about where is the work being done. We have full control of what we work on. Google is investing in the UK -- that's brilliant for UK science. Who owns the company is neither here nor there. And we're a research company. ""I also want to show that this is a more efficient way of doing science. There have been Bell Labs and Xerox PARC and Microsoft Research -- but they've been run more like academic departments. This is hybrid. All the great advances will come when two worlds merge, whether neuroscience and machine learning, or academic thinking and startup thinking, combined within a big company."" It was non-negotiable that DeepMind would stay in London. Partly because it offers a talent advantage: ""If you've got a PhD in physics from Cambridge and want to do some world-changing technology, there aren't that many options here -- in Silicon Valley there are thousands. And if you're focusing on a long-term goal, the Valley can be a bubble -- people trying to create the next Snapchat every five minutes. There can be a lot of noise in the system."" At 11, Mustafa Suleyman was buying Irn Bru bars and Refreshers wholesale at 7½p a pack and selling them at Queen Elizabeth's School, Barnet, for 25p. The venture was profitable -- until teachers closed it down. Suleyman won a Young Enterprise award for borrowing hospital wheelchairs to create a guide to London for young disabled people. He'd grown up best friends with Demis Hassabis's younger brother, but was more motivated by social impact than business. ""Demis and I had conversations about how to impact the world, and he'd argue that we need to build these grand simulations that one day will model all the complex dynamics of our financial systems and solve our toughest social problems. I'd say we have to engage with the real world today."" Suleyman, the son of a Syrian-born taxi-driver father and English nurse, won a place at Oxford to read philosophy and theology, but dropped out in the second year, and instead helped start the Muslim Youth Helpline. At 22, he went to work for London mayor Ken Livingstone, advising on human rights policy, but found that government wasn't the vehicle to promote radical systemic change. He had always been the ""well-spoken interlocutor"" at home, helping parse his father's broken English. As DeepMind's 30-year-old co-founder and head of applied AI, he's responsible for integrating the company's technology across Google's products -- and ensuring clear communication among the top engineers. So how is Google starting to apply DeepMind's research? ""I've got five teams, working on YouTube, search, health, natural-language understanding and some Google X projects,"" Suleyman says. ""We're working at applying the core engine that sits behind the Atari games player across the company. One is in YouTube-recommendation personalisation. We try to learn from the types of videos that lots of users are watching in aggregate to better recommend at the right time, in the right place, what they'd like. Then search: think of search as a process of querying an engine, browsing through links generated, then refining your query in this iterative feedback cycle. Over time we can improve results."" DeepMind is under no pressure to help Google boost advertising revenues. ""There was never any expectation we'd do that -- we're a long-term research effort,"" Suleyman says. ""Think of deep learning as one of the first steps on the way to building general-purpose learning systems. If we just search for solutions involving products we can imagine today, we're constraining the limits of our imagination."" Projects include natural language understanding, boosted by DeepMind's recent acquisition of Oxford University spin-off companies Dark Blue Labs and Vision Factory: ""We're using neural methods to see if we can create very large machine reading systems without any hand coding,"" Suleyman says. He is also focusing on health. ""Preventative medicine is the area I'm most excited about. There's huge potential for our methods to improve the way we make sense of data."" Suleyman has a final role: overseeing ""ethics and safety"" at DeepMind. In November 2014, Elon Musk submitted a (subsequently deleted) comment to Edge.org: ""The pace of progress in artificial intelligence (I'm not referring to narrow AI) is incredibly fast. Unless you have direct exposure to groups like DeepMind, you have no idea how fast -- it is growing at a pace close to exponential. The risk of something seriously dangerous happening is in the five-year time frame. Ten years at most. This is not a case of crying wolf about something I don't understand. I am not alone in thinking we should be worried. The leading AI companies have taken great steps to ensure safety. They recognise the danger, but believe that they can shape and control the digital superintelligences and prevent bad ones from escaping into the internet. That remains to be seen..."" Musk had previously issued a warning that AI was ""potentially more dangerous than nukes"" and of ""summoning the demon"". He had also explained that his DeepMind investment was not to make money but purely ""to keep an eye on what's going on with artificial intelligence"". Last May, Stephen Hawking, too, referenced the movie Transcendence to warn that dismissing highly intelligent machines as mere science fiction ""would be a mistake, and potentially our worst mistake in history. One can imagine such technology outsmarting financial markets, out-inventing human researchers, out-manipulating human leaders, and developing weapons we cannot even understand."" Apple cofounder Steve Wozniak has also voiced concerns recently. Jaan Tallinn, the Skype cofounder who was an early DeepMind investor, was instrumental in establishing an independent advisory committee on ethics. ""The main reason I backed DeepMind was strategic: I see my role as bridging the AI research and AI safety communities,"" Tallinn says. ""Google and DeepMind agreed to set up an ethics and safety board as part of the acquisition. An important conference earlier this year demonstrated remarkable consensus about long-term issues in AI impacts, culminating in an open letter plus an associated research programme through the Future of Life Institute that Elon Musk backed with a $10 million [£6.6m] donation."" Musk, Hawking and DeepMind's founders signed the letter. Suleyman warns that ""getting caught up in the language of existential risk"" is mere speculation when DeepMind's focus is on designing systems which are narrow and applied. ""'The AI' is a sci-fi-inspired meme which is not helpful,"" he says. ""We're not building AIs that will be equal to us -- they'll help us speak naturally and ask questions directly to a Google search engine. We would certainly be against any kind of autonomous tool of war that could wander round taking kill decisions. That's why we've kickstarted this conversation. Constraining the capacities of our systems is really central."" Hassabis is more direct. ""There's a lot of unsubstantiated hype from people who are smart in their own domains but don't work on AI,"" he says. ""Elon tends to shoot from the hip sometimes. I don't think getting hysterical is a good way of stimulating healthy debate -- you end up with unnecessary fearmongering."" As for Hawking and Wozniak, ""These are people who are not actually building something, so they're talking from philosophical and science-fiction worries, with almost no knowledge about what these capabilities can do."" So there's no risk of agents having moral autonomy? ""Of course we can stop it – we're designing these things,"" Hassabis sighs impatiently. ""Obviously we don't want those things to happen. We've ruled out using our technology in any military or intelligence applications. What's the alternative scenario? A moratorium on all AI work? What else can I say except extremely well-intentioned, intelligent, thoughtful people want to create something that could have incredible powers of good for society, and you're telling me there's people who don't work on these things and don't fully understand. I wouldn't purport to lecture Stephen Hawking on black holes -- I've watched Interstellar, but I don't know about black body radiation to the extent that I should be pontificating to the press about it."" Other AI practitioners also see the debate as misjudged. Andrew Ng, who founded Google's first deep-learning team and now runs research at Baidu, sees talk of an AI ""superintelligence"" as a distraction. ""There's a real risk of technology creating unemployment. And there's a big difference between intelligence and sentience. Our machines are becoming more intelligent. But this does not mean that they are becoming sentient. Most AI researchers don't see it as realistic for machines to become sentient any time soon."" Shane Legg was failing at primary school and his teachers wanted to keep him back a year. Aged nine, although he had begun to program his Dick Smith VZ200 computer, he could barely read or spell. His concerned parents in the small New Zealand city of Rotorua sent him to an educational psychologist. ""I distinctly remember getting to the end of the intelligence test, and the psychologist was quite annoyed, telling my mother, 'What the hell's going on here? This is a ridiculous waste of time.' ""The psychologist took a deep breath, and said, 'He doesn't have a limited intelligence. In fact, I can't measure his intelligence -- it's off the chart. I thought, oh really? Because I was used to being the dumb child."" Legg was diagnosed with dyslexia and trained to use a keyboard; soon he was in his school's top one per cent. ""I started developing games that would beat friends at chess, teaching myself at Rotorua public library. In Encyclopedia Britannica, there was an article on alpha-beta search. I thought, I could use that to program chess. I was 12."" Legg studied complexity theory at university in New Zealand, and took a PhD at IDSIA in Switzerland, focusing on how to measure machine intelligence. He then wanted to learn neuroscience, which brought him to the Gatsby Computational Neuroscience Unit at UCL -- where he first met Demis Hassabis. Over lunch they decided it was the right time, in 2011, to start a business. Legg would be chief scientist. ""We'd turn up at conferences, say we're a small company trying to solve AI and people laughed at us,"" Legg recalls. ""But one in ten would say, that's really cool. And as we'd get top researchers joining, credibility grew."" Today, behind King's Cross station in rooms named after Alan Turing, Leonardo da Vinci and Nikola Tesla, the DeepMind team quietly computes and prioritises its way along a 20-year roadmap. And where will we be in 20 years? A scenario like that of the movie Her, in which the lead character falls in love with his computer's Siri-like operating system? ""That's science fiction,"" says Legg, 41. ""Will we get there one day? I hope so. But most AIs may not be so human-like. Language is quite a sophisticated thing. Think of it as we're trying to build insects. Years from now, you might get to a mouse. Our systems are very good at Space Invaders, they can play Breakout, but they're struggling with Pac-Man. There's a long gap from here to having a system where you can sit and debate philosophy."" Hassabis agrees. ""We're decades away from anything that's nearing human-level general intelligence. But we're going to have systems doing useful things in five or ten years. We're on the first rung of the ladder. As to how many rungs there are -- there could be ten or 20 more breakthroughs before we've solved what intelligence is."""
692,https://www.wired.com/2015/06/facebook-googles-fake-brains-spawn-new-visual-reality/,Wired,2015,6,19,940.0," Facebook and Google are building enormous neural networks---artificial brains---that can instantly recognize faces, cars, buildings, and other objects in digital photos. But that's not all these brains can do. They can recognize the spoken word, translate from one language to another, target ads, or teach a robot to screw a cap onto a bottle. And if you turn these brains upside down, you can teach them not just to recognize images, but create images---in rather intriguing (and sometimes disturbing) ways. As it revealed on Friday, Facebook is teaching its neural networks to automatically create small images of things like airplanes, automobiles, and animals, and about 40 percent of the time, these images can fool us humans into believing we're looking at reality. ""The model can tell the difference between an unnatural image---white noise you'd see on your TV or some sort of abstract art image---and an image that you would take on your camera,"" says Facebook artificial intelligence researcher Rob Fergus. ""It understands the structure of how images work"" (see images above). Meanwhile, the boffins at Google have taken things to the other extreme, using neural nets to turn real photos into something intriguingly unreal. They're teaching machines to look for familiar patterns in a photo, enhance those patterns, and then repeat the process with the same image. ""This creates a feedback loop: if a cloud looks a little bit like a bird, the network will make it look more like a bird,"" Google says in a blog post explaining the project. ""This in turn will make the network recognize the bird even more strongly on the next pass and so forth, until a highly detailed bird appears, seemingly out of nowhere."" The result is a kind of machine-generated abstract art (see below). On one level, these are party tricks---particularly Google's feedback loop, which evokes hallucinatory flashbacks. And it should be noted that Facebook's fake images are only 64-by-64 pixels. But on another level, these projects serve as ways of improving neural networks, moving them closer to human-like intelligence. This work, says David Luan, the CEO of a computer vision company called Dextro, ""helps better visualize what our networks are actually learning."" They're also slightly disturbing---and not just because Google's images feel like a drug trip gone wrong, crossing breeding birds with camels in some cases, or snails with pigs (see below). More than this, they hint at a world where we don't realize when machines are controlling what we see and hear, where the real is indistinguishable from the unreal. Working alongside a PhD student at New York University's Courant Institute of Mathematical Sciences, Fergus and two other Facebook researchers revealed their ""generative image model"" work on Friday with a paper published to research repository arXiv.org. This system uses not one but two neural networks, pitting the pair against each other. One network is built to recognize natural images, and the other does its best to fool the first. Yann LeCun, who heads Facebook's 18-month-old AI lab, calls this adversarial training. ""They play against each other,"" he says of the two networks. ""One is trying to fool the other. And the other is trying to detect when it is being fooled."" The result is a system that produces pretty realistic images. According to LeCun and Fergus, this kind of thing could help restore real photos that have degraded in some way. ""You can bring an image back to the space of natural images,"" Fergus says. But the larger point, they add, is that the system takes another step towards what's called ""unsupervised machine learning."" In other words, it can help machines learn without human researchers providing explicit guidance along the way. Eventually, LeCun says, you can use this model to train an image recognition system using a set of example images that are ""unlabeled""---meaning no human has gone through and tagged them with text that identifies what's in them. ""Machines can learn the structure of an image without being told what's in the image,"" he says. Luan points out that the current system still requires some supervision. But he calls Facebook's paper ""neat work,"" and like the work being done at Google, he believes, it can help us understand how neural networks behave. Neural networks of the kind created by Facebook and Google span many ""layers"" of artificial neurons, each working in concert. Though these neurons perform certain tasks remarkably well, we don't quite understand why. ""One of the challenges of neural networks is understanding what exactly goes on at each layer,"" Google says in its blog post (the company declined to discuss its image generation work further). By turning its neural networks upside-down and teaching them to generate images, Google explains, it can better understand how they operate. Google is asking its networks to amplify what it finds in an image. Sometimes, they just amplify the edges of a shape. Other times, they amplify more complex things, like the outline of a tower in a horizon, a building in a tree, or who's knows what in a sea of random noise (see above). But in each case, researchers can better see what the network is seeing. ""This technique gives us a qualitative sense of the level of abstraction that a particular layer has achieved in its understanding of images,"" Google says. It helps researchers ""visualize how neural networks are able to carry out difficult classification tasks, improve network architecture, and check what the network has learned during training."" Plus, like Facebook's work, it's kinda cool, a little strange, and a tad frightening. The better computers get at recognizing what's real, it seems, the harder it gets for us."
693,https://www.wired.com/2015/06/rise-robots-far-will-go/,Wired,2015,6,15,370.0," The full title is Rise of the Robots: Technology and the Threat of a Jobless Future by Martin Ford (Basic Books). I will start with a super short review: Now for some more partially random comments. I think that we all agree that robots are doing things that humans used to do. The question remains: how far will they go? Will robots eventually be able to do everything that a human could do? The answer depends on artificial intelligence---real intelligence, not just smart robots but robots that can learn and critically think. Some will say that AI is just like practical fusion---it's in the near future and always will be. I think the problem is that if a computer did start becoming intelligent, it could improve at a very rapid pace. We will just have to wait and see what really happens. In the book, Martin Ford talks about the effect of robots on the economy. Reading a book like this reminds me that it seems pretty magical that our economy works at all. Once you add robots into the mix, they start doing jobs but they don't consume things that would support even more jobs. No, robots just do their job (I'm talking about robots, not AI robots---they might do their job AND take over the world). One possible solution to the robot-job problem would be a Basic Income. The idea is that everyone gets some income that is not tied to a particular job. If this income is enough to live on, humans would be able to function along with the robots. Yes, the devil is in the details but it seems like it might help us into a transition into the robotic future. Of course some other people say that intelligent robots of the future are only a problem if we hold onto capitalism. Finally, it think the author goes over many other situations in which robots have had or will have a significant impact---education is one such example. Hopefully we can work out this whole robotification thing so that we can all benefit in the future. Note: This review is based on a complimentary copy of Rise of the Robots that I received from the publisher."
694,https://www.wired.com/2015/06/facebooks-ai-tool-squashing-bugs-now-open/,Wired,2015,6,11,537.0," Facebook used to move fast and break things. Now it's trying to move fast and fix things. To do that, the company developed an artificially intelligent tool called Infer that can spot bugs in its mobile apps before they ever reach customers. And now the company wants everyone to test their software this way. Today the company open sourced Infer, making its code freely available to any company or independent developer looking for new ways to debug their apps. ""It looks at the program and makes guesses or hypotheses about the program, the way a human might,"" says Peter O'Hearn, Infer's co-creator. But unlike a human, it can read thousands of lines of code in mere minutes to spot potential bugs. Facebook claims that it has a fix rate of about 80 percent, which is great for such an automated system. Most debuggers work by running a program and stepping through the code line by line and looking for errors. Infer is able to analyze code and look for problems without actually executing it, a concept called ""static program analysis."" Static analysis has been around for decades, and there are both commercial and open source testing tools available based on the concept. But O'Hearn says these tools aren't able to do deep code analysis at the scale that Infer can. Unlike other tools, Infer is able to break down large code bases, analyze the smaller parts, and then stitch the results together, avoiding the trade off between depth and speed. ""I think of it as AI,"" O'Hearn says, though he says it's closer to the symbolic artificial intelligence branch of the field than the more brain-inspired neural networking techniques other researchers at Facebook are pioneering. Infer grew out of academic research O'Hearn started with the late computer scientist John Reynolds, with whom he helped develop a computer science concept called separation logic. The next breakthrough came later, when O'Hearn realized he could apply a concept called abductive reasoning to static analysis. At the Queen Mary University of London, O'Hearn worked with Cristiano Calcagno, who was one of his PhD students, and Dino Distefano, who was his research assistant. Calcagno and Distefano went on to other institutes and continued their research. But they eventually found themselves wanting to make an impact on the world of software development outside of academia. They decided to found a startup called Monoidics and asked O'Hearn to join them. The original plan was to build a commercial product based on their work. But when Facebook made an offer to buy the company, O'Hearn says the team realized that they'd be able to make a bigger impact there than they would have been able to as a small startup. Now the team is able to give the entire platform away for free. ""It took some time to come to that decision, but that was always the ideal situation,"" he says. O'Hearn says hopes that by making Infer open source, the team can help other companies, gather feedback from them, and attract contributions from academics. ""Static analysis is full of great ideas but also unsolved problems,"" he says. ""We hope this can boost collaboration between industry and academia, and get research flowing in both directions."""
695,https://www.wired.com/story/ocado-secondhands-collaborative-robot/,Wired,2015,6,10,906.0," An ambitious robotics project that combines artificial intelligence, machine learning and advanced sensors to understand and assist humans in real time could be truly ""revolutionary"", according to the team working on it. The SecondHands humanoid, being developed for online supermarket Ocado, could soon be helping factory engineers fix mechanical faults and even learn on the job. The robot will be completely autonomous and should be able to help with everything from fetching tools to holding objects and even assisting with cleaning and engineering tasks. The project is a collaboration between the technology arm of the online supermarket and four universities across the European Union. The robotics team at Ocado Technology believe it could become ""the most advanced assistive robot in the world"". SecondHands will use 3D vision to see both depth and colour, with artificial intelligence allowing it to learn by example and respond to its surroundings. Once trained in a series of basic tasks the robot should be able to increase its own intelligence and act independently. SecondHands will also be able to understand natural speech, allowing it to respond to voice commands. In order to operate in a factory designed for humans SecondHands will be based heavily on human morphology. Early versions might operate on wheels, but in the future the robot could move on tank tracks or even legs. It could also have extra abilities such as telescopic arms to make it more useful as an assistant. The robot will be flexible enough to work easily alongside humans, with torque-controlled arms, anthropomorphic hands and a bendable torso. The robot will eventually be put to work alongside engineers at Ocado's vast logistics factories in the UK, which handle more than 167,000 orders per week. When something goes wrong with a mechanical component SecondHands will help engineers carry out repairs quickly and safely. It could also operate in areas too dangerous for humans, examining high-speed conveyors at close quarters and handling toxic materials. The EU is funding the project to the tune of €7m (£5.1m) as part of its Horizon2020 initiative to encourage researchers to work more closely with industry partners. As well as coordinating and contributing to the research Ocado will also be the end user, with the robots designed specifically for its factories. If the project is successful the team at Ocado are hopeful it will find uses elsewhere. The first SecondHands prototype will be operational at an Ocado testing facility in 18 months time and it is hoped the final version will be assisting engineers in factories in 2020. Unlike current collaborative and assistive robots, such as those competing in the recent Darpa challenge, Ocado says SecondHands will work just as quickly as a human. ""The big challenge is to get the robot to proactively do stuff, understanding where it is in the task and then doing something useful,"" Graham Deacon, leader of Ocado Technology's robotics research team tells WIRED.co.uk. Deacon describes Darpa as ""a bit like watching paint dry"" as the robots slowly perform simple tasks. ""We want our technicians to be able to rely on these robots. These robots have got to work in real time and respond in the right timeframes and be something that the technicians feel comfortable relying on."" SecondHands' potential for high-level reasoning, Deacon explains, is a work of artificial intelligence. Software will help the robot construct a vast knowledgebase around the tasks it carries out and then understand how they can be applied to other problems. In this sense, the robot will learn on the job. The structure of SecondHands will be based on the next-generation ARMAR robot, developed at the Karlsruhe Institute of Technology in Germany. Ocado will work on the software side, with a speciality of vision-based grasping and manipulation. Additional research into artificial intelligence and contextual understanding is being undertaken at University College London, La Sapienza University of Rome and Ecole Polytechnique Federale de Lausanne. The aim of the project is to create a robotic assistant that doesn't require any human input but that understands what it needs to do based on its own understanding and intelligence. ""We would expect the robots to be able to track what the engineer is doing, understand the task that the engineer is trying to perform and then synthetically understand its own capabilities as a robot to proactively offer assistance,"" says Alex Harvey, head of project management at Ocado Technology. In one example the robot would be able to understand that an engineer is climbing a ladder would need help holding a safety guard once it had been removed. Having seen and understood how this task is performed, SecondHands would then be able to apply its knowledge to other tasks without being told how. ""For it to be truly useful it has have a base capability and it has to be able to learn on the job and it has to be able to get better. As it keeps learning it will become more useful,"" Harvey says. ""If SecondHands existed today it would not be stretching the truth to say it would be the most advanced assistive robot in the world,"" Deacon says. ""In five years time somebody else might be doing something similar, but if it were to exist today it would be the most advanced robot of its time."" Paul Clarke, Ocado's director of technology, has even grander ambitions. ""Robots of the future could even use these capabilities to actually build warehouses,"" he says."
696,https://www.wired.com/2015/06/startup-shares-google-knowledge-graph-clone-everyone/,Wired,2015,6,4,874.0," If you're listening to a Skrillex song, you can say ""What's his real name?"" and your phone will give you his real name. If you open an email asking if you want to see Tomorrowland, you can tap on it and get instant reviews, ratings, and trailers for the latest sub-par George Clooney flick. If your get a text suggesting dinner at some hip restaurant you've never heard of, you can tap again for reservations and directions. This, says Google, is how your Android smartphone will soon work, thanks to a new service called ""Now on Tap."" An extension of the company's Siri-like digital assistant, Google Now, the service will identify what's happening on your phone and pull in related information from across the web. The service works by using machine learning algorithms to determine what you're doing, then matches this understanding with information stored in what the company calls the Google Knowledge Graph---a database of semantic data describing more than 1 billion people, places, and things. ""To be able assist you,"" says Aparna Chennapragada, who oversees Google Now, ""we have to understand the world."" The Knowledge Graph isn't just a database of stuff on the net. It's a database that provides context for stuff on the net---that aims to comprehend what's there in the same way a human would. In other words, Google doesn't just ""know"" that the web contains pages that includes the words George Clooney. It ""knows"" that George Clooney is an actor here in the real world. As the underpinning for Google Now, it works reasonably well---especially during tightly controlled demos on stage at Google's annual developer conference. And thanks to the latest in artificial intelligence technology, which can automatically determine how words are related, it continues to improve. The rub is that, although outside companies and coders can now plug their apps into Google Now, they can't really use its knowledge graph to build their own artificially intelligent services. But Google's isn't the only knowledge graph out there. A Silicon Valley AI startup called Diffbot says it has fashioned a similarly large collection of semantic internet data, and it's beginning to share this data with other companies and coders, including Microsoft, Amazon, and eBay. Diffbot says its database now spans about 600 million objects, and the hope is that it can spur all sorts of contextually aware services along the lines of Google Now. ""In the future, you'll interact with thousands of intelligent apps that will need something like what we offer,"" says Mike Tung, the CEO of Diffbot, a company that grew out of his artificial intelligence work at Stanford University. According to Tung, the Diffbot knowledge graph already underpins Microsoft's Bing search engine. It helps generate the contextual results that pop up on the right-hand side of the page, he says. If you type in something like ""Canon EOS Digital Rebel XT,"" you get reviews and specifications for the popular digital camera. Available to Microsoft and other ""beta"" testers as an online service, the tool is part of a broad effort to bring semantic understanding to smartphone apps and other Internet software. As Apple refines Siri and Google tweaks Google Now, Microsoft is now offering its Cortana digital assistant, and through a program it calls Project Oxford, the company is now allowing outside companies to build their own apps atop Cortana's fundamental technologies. At the same time, the likes of Facebook and Chinese search giant Baidu are developing systems that can understand natural language and respond accordingly. Machine learning plays multiple roles here. New ""deep learning"" algorithms like Google's Word2Vec can help build a knowledge graph, and similar algorithms---algorithms that ""learn"" by analyzing vast amounts of data---can help services like Google Now make use of the graph, working to understand, say, the email you just received on your smartphone. Like Google, Diffbot uses various forms of machine learning at both ends of the process. Tung says the company's entire knowledge graph is generated automatically. ""It's a software system that can read and interpret web pages like a human being using computer vision and natural language processing techniques,"" he says. Based in part on Freebase, a semantic database acquired by Google in 2010, Google's Knowledge Graph includes data complied by human hands. Tung believes Diffbot's automatic system can scale to much larger amounts of online data. Peter Kerwin runs Collexion, a search engine for obscure collectibles like vinyl records and typewriters. The company already uses an earlier Diffbot tool to crawl the web and a build a small version of the knowledge graph, and Kerwin says his company could significantly expand its search engine if it could tap Diffbot's entire range of knowledge. ""There isn't a lot of structured data about collectibles,"" he explains. ""We need something that can give us that."" This is a small thing. But the same semantic data, Tung says, can help power a world of small apps in a similar way. Down the road, an effective knowledge graph could drive not only artificially intelligence services like Google Now, but also tools that operate beyond traditional computers and smartphones---such as a printer that could automatically order new cartridges when ink gets low. It too must understand the 'net in the same way people do."
697,https://www.wired.com/2015/06/facebook-opens-paris-lab-ai-research-goes-global/,Wired,2015,6,2,405.0," Facebook is opening a new artificial intelligence lab in Paris after building a dedicated AI team that spans its offices in New York and Silicon Valley. The New York University professor who oversees the company's AI work, Yann LeCun, was born and educated in Paris. LeCun tells WIRED that he and the company are interested in tapping the research talent available in Europe. Alongside London, he says, Paris was an obvious choice for a new lab. ""We plan to work openly with and invest in the AI research community in France, the EU, and beyond,"" he wrote in a blog post announcing the move. LeCun is one of the researchers at the heart of an AI movement known as deep learning. Since the 1980s, he and a small group of other researchers have worked to build networks of computer hardware that approximate the networks of neurons in the brain. In recent years, the likes of Facebook, Google, and Microsoft have embraced these ""neural nets"" as a way of handling everything from voice and image recognition to language translation. Another researcher who bootstrapped this movement, University of Toronto professor Geoff Hinton, is now at Google. Like Facebook, Google is investing heavily in this rapidly evolving technology, and the two companies are competing for a rather small talent pool. After acquiring a deep learning startup called DeepMind, based in the UK and founded by an English researcher named Demis Hassabis, Google already operates a European AI lab of sorts. Chris Nicholson, founder of the San Francisco-based AI startup Skymind, points out the many of the key figures behind deep learning are European, including not only LeCun, Hinton, and Hassabis, but also University of Montreal professor Yoshua Bengio (though he was educated in Canada). ""All of them are now employed by North American organizations,"" Nicholson says. ""There are a lot of investment gaps in European venture capital, which means that Europe has a lot of ideas and people that either come to America or never make an impact on the mainstream."" Today, Facebook uses deep learning as a way of recognizing images on its social network, and it's exploring the technology as a means of personalizing your Facebook News Feed so that you're more likely to enjoy what you see. The next big step, LeCun says, is natural language processing, which aims to give machines the power to understand not just individual words but entire sentences and paragraphs."
698,https://www.wired.com/2015/06/little-robot-wants-best-friend/,Wired,2015,6,1,708.0," We've long known there’s a market out there for robotic buddies. One compelling piece of evidence: The original Furby sold more than 40 million units, and it didn't really do anything. 17 years later, an A.I. and machine-learning company is making a robot pal that will do way more than its fuzzy predecessor. It's called Musio, and it houses a pretty impressive A.I. engine developed by a company called AKA. The robot remembers details from prior conversations, asks follow-up questions based on that info, and can be used as a smart-home controller. But its main goal is to be your friend: Asking you questions, actually listening to your answers, and learning what you're all about. Using add-on packs, developers can program the robot with Arduino-compatible boards, accelerometers, and Zigbee modules. There are also packs that are designed to help kids with reading, vocabulary, and conversational skills. Musio (which, for the record, is not a mobile bot) is just one product built around the software at the heart of it, an A.I. engine called Muse. AKA first developed Muse as an electronic English tutor, and Muse is also being used as a learning engine to drive editing software and TOEFL-prep products. The Musio robot is the first product in the company’s lineup aimed at the U.S. market, and AKA says the robot’s capabilities are going to to grow over time. Right now, Musio has a few features in its working prototype form. Unlike phone-based voice assistants like Siri and Cortana, it acts like a being unto itself---it asks as many questions as it answers. Then, based on your responses, Musio can retain information from one user, remember your preferences, and relay that information on to other users. It can be used as a Siri-like personal assistant to look up information on the Web and send voice-dictated emails, but it also inserts itself into conversations. For example, the robot may ask what your favorite sports team or food is, then bring that information up in subsequent conversations. If more than one person uses Musio, the robot can mention what other users’ favorite things are in an effort to converse more naturally. According to Celina Lee, AKA’s director of business development, the system is designed to improve a child’s emotional intelligence along with their conversational skills. English is the only supported language for the system at the moment, but the team may work on other versions of the robot if the first-generation model is a success. There are three “levels” of Musio to choose from, and the two top-tier models are loaded with connectivity options that let you use the robot as a hub for home controls. A $300 “Smart” version and a $600 “Genius” version each run Android 5.0, control Zigbee devices, and have WiFi and Bluetooth LE connectivity. The “Smart” model has a 1.2GHz quad-core processor, a 2200mAh battery, and 16GB of storage on board, while the “Genius” model ramps it up to 2.5GHz quad-core, 3000mAh, and 64GB. There’s also a $100 “Simple” version that just talks to you without all those connectivity options. Some other features are already in the works, according to AKA. Future generations of Musio are expected to use a camera and facial-recognition tech to identify and address different people in front of it. There’s also a little scanning/pointing sidekick---a seal-shaped remote called Sophy---that can be used to scan special books and identify objects for Musio to interact with. Different outfits for the robot are planned, and you can swap out the color of its hands with magnetic marshmallow-like nubs. During a demo, Musio’s voice-recognition and conversational abilities worked well---although those things always tend to go well in company-controlled demos. The voice needs work, and the AKA team says they’re still figuring out what the final version of it will be. Lee says the company is hoping to hire a voice actor to pinpoint the ultimate voice for Musio---a crucial piece of the puzzle for something that’s going to be asking you a ton of questions. It’s also very much a work in progress, as the project just launched on Indiegogo. The company plans to manufacture and ship the first-generation robot by June of next year, so you can keep your real-world friends until then."
699,https://www.wired.com/2015/05/gradberry-tara-ai/,Wired,2015,5,27,844.0," Silicon Valley really, really wants to be a meritocracy. Companies and VC firms say they always want to hire or invest in the best person for the job, independent of other factors. But the lack of diversity in the tech industry suggests other factors are coming into play. A recent analysis by Reuters of prominent venture capitalist-backed startups found that the people succeeding in tech have a pretty homogenous background. What does that background look like? Summed up in one word: pedigree. According to Reuters, an overwhelming majority of the startups it looked at were founded by people who had held a senior position at a big technology firm, worked at a well-connected smaller one, started a successful company, or attended one of three universities---Stanford, Harvard, and Massachusetts Institute of Technology. In a world where good ideas are supposed to trump all, who you know and what names are on your résumé still matter a lot. Iba Masood, who freely admits she doesn’t have this kind of pedigree, can relate to the frustration of being overlooked because of her background. She’s the CEO and co-founder of Gradberry, a Y Combinator-backed startup that today released an artificially intelligent recruiter for tech workers that aims to address Silicon Valley's meritocracy problem. It’s called TARA---short for Talent Acquisition and Recruiting Automation---and essentially, it helps companies recruit skilled tech workers by looking at an objective measure: analyzing the code they’ve already written. “People can excel in different fields—it really does not matter what your background is, where you’re originally from,” says Masood. Gradberry joins other matchmaking startups that seek to connect technical workers with employers, including HackerRank, Hired, and LearnUp. Beansprock, which originated from the MIT Media Lab, is another company seeking to infuse the tedious task of job hunting with an artificially intelligent touch. Once a candidate links their online portfolios and projects—usually Github repositories—to Gradberry, the platform analyzes whether or not a candidate has written good-quality code. It checks syntax, makes sure the code has no runtime errors, and examines the code to see if it's been plagiarized. It can see whether a candidate’s code is influential within a certain community—checking how many contributions a candidate made to a platform like Github, for instance, or how much of a candidate’s code received acknowledgement from the rest of the community through ratings or by noting others who have built on the original code. In the process, Masood says, Gradberry surfaces extremely qualified candidates that may otherwise go unrecognized. For example, she says, a 20-year-old coder had one of the most impressive Github profiles she and her team had ever seen. “He was one of the top ten coders in his area, and excelled in Java in particular,” Masood remembers. “But he had no work experience.” After Tara highlighted him as a strong candidate, he was promptly hired by gaming company Kamcord. Along with evaluating their code, Gradberry provides advice to job candidates on how to better highlight the desirable aspects of their profiles. Eventually, Masood says, it will recommend ways they can improve their coding abilities. Right now, Gradberry already shows its user base which coding languages are trending based on the actual demand they see from employers. As for the employers themselves, they receive a shortlist of candidates tailored to the skills they're seeking. Employers then send feedback based on who they decide to hire in order to, in effect, train Gradberry's system to send them an even better list next time. For its trouble, Gradberry receives a commission of 5 percent of a candidate’s first-year salary once he or she is hired. By a lot of measures, Masood and her company are thriving—Gradberry has recently closed a seed round of investment that included funding from Y Combinator, a number of well-known venture capital firms, and a host of angel investors. Its recruiting platform has 255 companies on board hiring for 379 open positions. More than 3,000 engineers are on the site looking for jobs. If all goes well, Masood says, Gradberry could scale to dig into recruitment efforts in other fields, including sales and business development. The long-term vision, according to its CEO, is building out a full-fledged applicant tracking system for companies. But success for Masood and Gradberry didn't come overnight---an effort that fed inspiration for the product itself. Gradberry has spent three years in the recruitment space---a long time in startup land---and the product went through multiple versions before finally culminating in Tara. Looking through hundreds of resumes a day, Masood says she realized she could leverage technology to address the tech industry’s fundamentally broken hiring system. Tara itself, in some ways, is also a reflection of how she felt she had to prove herself given her background---not from the US, not from an Ivy League school. And looking through those résumés, she says she could tell that non-Ivy League candidates were just as good. ""It all depends on their motivation to produce good work,” she says. Instead of just trying to argue the point, she built an AI system to prove it."
700,https://www.wired.com/2015/05/artificial-intelligence-pioneer-concerns/,Wired,2015,5,23,2702.0," In January, the British-American computer scientist Stuart Russell drafted and became the first signatory of an open letter calling for researchers to look beyond the goal of merely making artificial intelligence more powerful. “We recommend expanded research aimed at ensuring that increasingly capable AI systems are robust and beneficial,” the letter states. “Our AI systems must do what we want them to do.” Thousands of people have since signed the letter, including leading artificial intelligence researchers at Google, Facebook, Microsoft and other industry hubs along with top computer scientists, physicists and philosophers around the world. By the end of March, about 300 research groups had applied to pursue new research into “keeping artificial intelligence beneficial” with funds contributed by the letter’s 37th signatory, the inventor-entrepreneur Elon Musk. Original story reprinted with permission from Quanta Magazine, an editorially independent division of SimonsFoundation.org *whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.*Russell, 53, a professor of computer science and founder of the Center for Intelligent Systems at the University of California, Berkeley, has long been contemplating the power and perils of thinking machines. He is the author of more than 200 papers as well as the field’s standard textbook, Artificial Intelligence: A Modern Approach (with Peter Norvig, head of research at Google). But increasingly rapid advances in artificial intelligence have given Russell’s longstanding concerns heightened urgency. Recently, he says, artificial intelligence has made major strides, partly on the strength of neuro-inspired learning algorithms. These are used in Facebook’s face-recognition software, smartphone personal assistants and Google’s self-driving cars. In a bombshell result reported recently in Nature, a simulated network of artificial neurons learned to play Atari video games better than humans in a matter of hours given only data representing the screen and the goal of increasing the score at the top—but no preprogrammed knowledge of aliens, bullets, left, right, up or down. “If your newborn baby did that you would think it was possessed,” Russell said. Quanta Magazine caught up with Russell over breakfast at the American Physical Society’s 2015 March Meeting in San Antonio, Texas, where he touched down for less than 24 hours to give a standing-room-only lecture on the future of artificial intelligence. In this edited and condensed version of the interview, Russell discusses the nature of intelligence itself and the immense challenges of safely approximating it in machines. QUANTA MAGAZINE: You think the goal of your field should be developing artificial intelligence that is “provably aligned” with human values. What does that mean? STUART RUSSELL: It’s a deliberately provocative statement, because it’s putting together two things—“provably” and “human values”—that seem incompatible. It might be that human values will forever remain somewhat mysterious. But to the extent that our values are revealed in our behavior, you would hope to be able to prove that the machine will be able to “get” most of it. There might be some bits and pieces left in the corners that the machine doesn’t understand or that we disagree on among ourselves. But as long as the machine has got the basics right, you should be able to show that it cannot be very harmful. How do you go about doing that? That’s the question I’m working on right now: Where does a machine get hold of some approximation of the values that humans would like it to have? I think one answer is a technique called “inverse reinforcement learning.” Ordinary reinforcement learning is a process where you are given rewards and punishments as you behave, and your goal is to figure out the behavior that will get you the most rewards. That’s what the [Atari-playing] DQN system is doing; it is given the score of the game, and its goal is to make that score bigger. Inverse reinforcement learning is the other way around. You see the behavior, and you’re trying to figure out what score that behavior is trying to maximize. For example, your domestic robot sees you crawl out of bed in the morning and grind up some brown round things in a very noisy machine and do some complicated thing with steam and hot water and milk and so on, and then you seem to be happy. It should learn that part of the human value function in the morning is having some coffee. There’s an enormous amount of information out there in books, movies and on the web about human actions and attitudes to the actions. So that’s an incredible resource for machines to learn what human values are—who wins medals, who goes to jail, and why. Video: DQN, an artificial neural network developed by researchers at Google DeepMind, teaches itself to play Atari games such as Breakout. It quickly develops sophisticated strategies. How did you get into artificial intelligence? When I was in school, AI wasn’t thought of as an academic discipline, by and large. But I was in boarding school in London, at St. Paul’s, and I had the opportunity to avoid compulsory rugby by doing a computer science A-level [course] at a nearby college. One of my projects for A-level was a program that taught itself to play naughts and crosses, or tic-tac-toe. I became very unpopular because I used up the college’s computer for hours on end. The next year I wrote a chess program and got permission from one of the professors at Imperial College to use their giant mainframe computer. It was fascinating to try to figure out how to get it to play chess. I learned some of the stuff I would later be teaching in my book. But still, this was just a hobby; at the time my academic interest was physics. I did physics at Oxford. And then when I was applying to grad school I applied to do theoretical physics at Oxford and Cambridge, and I applied to do computer science at MIT, Carnegie Mellon and Stanford, not realizing that I’d missed all the deadlines for applications to the U.S. Fortunately Stanford waived the deadline, so I went to Stanford. And you’ve been on the West Coast ever since? Yep. You’ve spent much of your career trying to understand what intelligence is as a prerequisite for understanding how machines might achieve it. What have you learned? During my thesis research in the ’80s, I started thinking about rational decision-making and the problem that it’s actually impossible. If you were rational you would think: Here’s my current state, here are the actions I could do right now, and after that I can do those actions and then those actions and then those actions; which path is guaranteed to lead to my goal? The definition of rational behavior requires you to optimize over the entire future of the universe. It’s just completely infeasible computationally. It didn’t make much sense that we should define what we’re trying to do in AI as something that’s impossible, so I tried to figure out: How do we really make decisions? So, how do we do it? One trick is to think about a short horizon and then guess what the rest of the future is going to look like. So chess programs, for example—if they were rational they would only play moves that guarantee checkmate, but they don’t do that. Instead they look ahead a dozen moves into the future and make a guess about how useful those states are, and then they choose a move that they hope leads to one of the good states. “Could you prove that your systems can’t ever, no matter how smart they are, overwrite their original goals as set by the humans?”Another thing that’s really essential is to think about the decision problem at multiple levels of abstraction, so “hierarchical decision making.” A person does roughly 20 trillion physical actions in their lifetime. Coming to this conference to give a talk works out to 1.3 billion or something. If you were rational you’d be trying to look ahead 1.3 billion steps—completely, absurdly impossible. So the way humans manage this is by having this very rich store of abstract, high-level actions. You don’t think, “First I can either move my left foot or my right foot, and then after that I can either…” You think, “I’ll go on Expedia and book a flight. When I land, I’ll take a taxi.” And that’s it. I don’t think about it anymore until I actually get off the plane at the airport and look for the sign that says “taxi”—then I get down into more detail. This is how we live our lives, basically. The future is spread out, with a lot of detail very close to us in time, but these big chunks where we’ve made commitments to very abstract actions, like, “get a Ph.D.,” “have children.” Are computers currently capable of hierarchical decision making? So that’s one of the missing pieces right now: Where do all these high-level actions come from? We don’t think programs like the DQN network are figuring out abstract representations of actions. There are some games where DQN just doesn’t get it, and the games that are difficult are the ones that require thinking many, many steps ahead in the primitive representations of actions—ones where a person would think, “Oh, what I need to do now is unlock the door,” and unlocking the door involves fetching the key, etcetera. If the machine doesn’t have the representation “unlock the door” then it can’t really ever make progress on that task. But if that problem is solved—and it’s certainly not impossible—then we would see another big increase in machine capabilities. There are two or three problems like that where if all of those were solved, then it’s not clear to me that there would be any major obstacle between there and human-level AI. What concerns you about the possibility of human-level AI? In the first [1994] edition of my book there’s a section called, “What if we do succeed?” Because it seemed to me that people in AI weren’t really thinking about that very much. Probably it was just too far away. But it’s pretty clear that success would be an enormous thing. “The biggest event in human history” might be a good way to describe it. And if that’s true, then we need to put a lot more thought than we are doing into what the precise shape of that event might be. The basic idea of the intelligence explosion is that once machines reach a certain level of intelligence, they’ll be able to work on AI just like we do and improve their own capabilities—redesign their own hardware and so on—and their intelligence will zoom off the charts. Over the last few years, the community has gradually refined its arguments as to why there might be a problem. The most convincing argument has to do with value alignment: You build a system that’s extremely good at optimizing some utility function, but the utility function isn’t quite right. In [Oxford philosopher] Nick Bostrom’s book [Superintelligence], he has this example of paperclips. You say, “Make some paperclips.” And it turns the entire planet into a vast junkyard of paperclips. You build a super-optimizer; what utility function do you give it? Because it’s going to do it. What about differences in human values? That’s an intrinsic problem. You could say machines should err on the side of doing nothing in areas where there’s a conflict of values. That might be difficult. I think we will have to build in these value functions. If you want to have a domestic robot in your house, it has to share a pretty good cross-section of human values; otherwise it’s going to do pretty stupid things, like put the cat in the oven for dinner because there’s no food in the fridge and the kids are hungry. Real life is full of these tradeoffs. If the machine makes these tradeoffs in ways that reveal that it just doesn’t get it—that it’s just missing some chunk of what’s obvious to humans—then you’re not going to want that thing in your house. I don’t see any real way around the fact that there’s going to be, in some sense, a values industry. And I also think there’s a huge economic incentive to get it right. It only takes one or two things like a domestic robot putting the cat in the oven for dinner for people to lose confidence and not buy them. Then there’s the question, if we get it right such that some intelligent systems behave themselves, as you make the transition to more and more intelligent systems, does that mean you have to get better and better value functions that clean up all the loose ends, or do they still continue behaving themselves? I don’t know the answer yet. You’ve argued that we need to be able to mathematically verify the behavior of AI under all possible circumstances. How would that work? One of the difficulties people point to is that a system can arbitrarily produce a new version of itself that has different goals. That’s one of the scenarios that science fiction writers always talk about; somehow, the machine spontaneously gets this goal of defeating the human race. So the question is: Could you prove that your systems can’t ever, no matter how smart they are, overwrite their original goals as set by the humans? It would be relatively easy to prove that the DQN system, as it’s written, could never change its goal of optimizing that score. Now, there is a hack that people talk about called “wire-heading” where you could actually go into the console of the Atari game and physically change the thing that produces the score on the screen. At the moment that’s not feasible for DQN, because its scope of action is entirely within the game itself; it doesn’t have a robot arm. But that’s a serious problem if the machine has a scope of action in the real world. So, could you prove that your system is designed in such a way that it could never change the mechanism by which the score is presented to it, even though it’s within its scope of action? That’s a more difficult proof. Are there any advances in this direction that you think hold promise? There’s an area emerging called “cyber-physical systems” about systems that couple computers to the real world. With a cyber-physical system, you’ve got a bunch of bits representing an air traffic control program, and then you’ve got some real airplanes, and what you care about is that no airplanes collide. You’re trying to prove a theorem about the combination of the bits and the physical world. What you would do is write a very conservative mathematical description of the physical world—airplanes can accelerate within such-and-such envelope—and your theorems would still be true in the real world as long as the real world is somewhere inside the envelope of behaviors. Yet you’ve pointed out that it might not be mathematically possible to formally verify AI systems. There’s a general problem of “undecidability” in a lot of questions you can ask about computer programs. Alan Turing showed that no computer program can decide whether any other possible program will eventually terminate and output an answer or get stuck in an infinite loop. So if you start out with one program, but it could rewrite itself to be any other program, then you have a problem, because you can’t prove that all possible other programs would satisfy some property. So the question would be: Is it necessary to worry about undecidability for AI systems that rewrite themselves? They will rewrite themselves to a new program based on the existing program plus the experience they have in the world. What’s the possible scope of effect of interaction with the real world on how the next program gets designed? That’s where we don’t have much knowledge as yet. Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences."
701,https://www.wired.com/2015/05/nara-logics-ai/,Wired,2015,5,22,981.0," I’m fairly new to San Francisco, so I’m still building my mental database of restaurants I like. But this weekend, I know exactly where I’m heading to for dinner: Nick’s Crispy Tacos. Then, when I get home, I’m kicking back to a documentary I’ve never heard of, a Mongolian drama called The Cave of the Yellow Dog. An artificially intelligent algorithm told me I’d enjoy both these things. I’d like the restaurant, the machine told me, because I prefer Mexican food and wine bars “with a casual atmosphere,” and the movie because “drama movies are in my digital DNA.” Besides, the title shows up around the web next to Boyhood, another film I like. Nara Logics, the company behind this algorithm, is the brainchild (pun intended) of its CTO and cofounder, Nathan Wilson, a former research scientist at MIT who holds a doctorate in brain and cognitive science. Wilson spent his academic career and early professional life immersed in studying neural networks—software that mimics how a human mind thinks and makes connections. Nara Logics’ brain-like platform, under development for the past five years, is the product of all that thinking.. The Cambridge, Massachusetts-based company includes on its board such bigwig neuroscientists as Sebastian Seung from Princeton, Mriganka Sur from MIT, and Emily Hueske of Harvard’s Center for Brain and Science. So what does all that neuroscience brain power have to offer the tech world, when so many Internet giants—from Google and Facebook to Microsoft and Baidu—already have specialized internal teams looking to push the boundaries of artificial intelligence? These behemoths use AI to bolster their online services, everything from on-the-fly translations to image recognition services. But to hear Wilson tell it, all that in-house work still leaves a large gap---namely, all the businesses and people who could benefit from access to an artificial brain but can't build it themselves. “We’re building a pipeline, and taking insights out of the lab to intelligent, applied use cases,” Wilson tells WIRED. “Nara is AI for the people.” Wilson is not alone in his populist ambition. The list of other companies that have been created to make high-level artificial intelligence more accessible to the broader universe is not short. But so many of these have also been acquired by the aforementioned giants of tech---in fact, such acquisitions often seem to be a key reason behind their existences in the first place. Nara, on the other hand, exhibits some actual consumer-facing friendliness. The site where I got my recommendations, Nara.me, is available for anyone to sign up to get intelligent recommendations for movies, restaurants, and hotels based on your expressed preferences. There are also some filters available, so you can tell the software what you’re in the mood for at any point in time—maybe Chinese food and a comedy—so it can adjust to your changing impulses. Once you’ve fed Nara a few of your likes, it can learn your tastes and work across the entire system. Say you’ve moved from New York to San Francisco. Tell the app a few of your favorite restaurants in Manhattan, and it’ll surface similar options in your new home city. There’s even a mobile app that can detect your location and serve up suggestions that are close by. But Nara says it’s not ultimately focused on delivering a polished consumer product. Wilson and his team want to focus on enhancing the AI, he explains, not on acquiring users. There are businesses that already have a plethora of users. For Nara, it's those established businesses whose existing problems are most worth solving. To that end, Nara customizes specific solutions for companies across different industries. The startup says it can't reveal which companies it works with, but CEO Jana Eggers, the one-time head of Intuit’s innovation lab, says Nara recently signed a global bank to use its tech for a variety of services: managing the institution’s reward points and recognizing customers’ preferences; assessing loan approvals based on detailed financial history; and analyzing user transactions for the bank in real-time to detect fraud. Nara also has a healthcare services company on its client roster that it assists with doctor-patient matching based on a patient’s medical history and a doctor’s academic credentials. For a major airline, Eggers says Nara is tweaking its seating system so that some empty spots can be allocated to passengers who have had an unsatisfactory experience in the past. The key to Nara’s technology is personalization, Wilson says. Nara is essentially a matchmaking system that finds and understands entities in any data set, from people and places to businesses and abstract concepts, then builds a massive knowledge graph that shows weighted links between those entities. Wilson says Nara inserts users right into that knowledge graph to offer personalized recommendations. Knowing a bit about the user is what allows Nara to light up other things they might like. And the system can scrape public databases to enhance its knowledge. Richard Socher, a former natural language processing researcher at Stanford University and current CTO of artificial intelligence startup MetaMind, says research shows this approach to AI works well for recommendation engines. Traditional approaches typically take just probability and user history into account, he says. If three people like Product A, and two of those folks also like Product B, there's a high chance the third person will also like Product B. Nara's approach could create more powerful nodes and more easily take external information into account, Socher says. ""This could be a useful service to help other companies that can't build their own recommendations system,"" he says. In the meantime, despite the aggressive efforts within bigger, richer companies to forge smarter AIs, Wilson says he's not worried about possible competition. “In the scheme of things, working on artificial intelligence is not a numbers game,” Wilson says. “It’s more about careful thought with a small group of people, and technology that works thoughtfully.”"
702,https://www.wired.com/2015/05/humans-play-ai-texas-hold-em-now/,Wired,2015,5,21,1436.0," In 1997 chess master Gary Kasparov went to battle against the IBM supercomputer Deep Blue in a landmark match. After six games Deep Blue prevailed, marking the first time that a computer defeated a reigning world champion under tournament conditions. But chess isn’t the only game in town. A couple weeks ago, an artificial intelligence once again squared off against world-class human gamers. This time---at the Brains vs. Artificial Intelligence challenge at the Rivers Casino in Pittsburgh---the ultimate supremacy of human or machine was determined not by chess but by an epic 14 days and 80,000 hands of no-limit Texas hold ‘em. That’s right: The newest battlefield in the War Against the Machines is the poker table. Representing the Machines: Claudico, an AI from the same lab at Carnegie Mellon University that gave birth to Deep Blue. Fighting for the Users: Jason Les, Dong Kim, Bjorn Li, and Doug Polk, four of the world’s best professional poker players. The tournament was the first time any program had competed in no-limit Texas hold ‘em against human beings. It’s a game of particular interest to AI researchers. Of all the poker variations, no-limit hold ‘em is one of the most sophisticated. Each player gets two cards only he or she can see. There’s a round of betting, and then a dealer presents five cards available to all players---three cards (the flop), one card (the turn), and then the last card (the river)---with a round of betting after each. In limit hold ‘em players can only bet in fixed increments, but in no-limit, anyone can bet any amount, from one chip to going “all in,” betting everything. You can leverage a strong hand to extract more value from your opponent, or bluff with a weak hand to increase the value of losing cards. It’s hard. So hard, in fact, that AI researchers have been looking at poker since the 1990s. Today it’s the most important benchmark in the field. Unlike chess, poker is a game of incomplete information---no player has all the available data. An algorithm capable of determining optimal strategy for incomplete information scenarios could have applications for cybersecurity, medicine, and military strategy. “Most real world settings are imperfect information games,” says Tuomas Sandholm, whose team designed Claudico. “You don’t know exactly what the state of the world is because you don’t know everybody else’s private information.” Even better, computers have already solved most of the simpler problems. No-limit hold ‘em is the last big challenge. Sandholm estimates that the number of unique situations that can arise in a game is greater than the number of atoms in the universe—squared. “The game is so big that you can’t even fit it into memory,” he says. Microsoft Research and Rivers Casino put up $100,000 to cover the players’ appearance fees and to make the grueling 13 hours a day of play a bit more appealing. The team from Carnegie Mellon structured the challenge so that Claudico would simultaneously play each human one-on-one over a large sample size of twenty thousand hands, with the winner decided by who had the most chips (no actual money at stake) after 80,000 hands, the AI or the humans. Place your bets. Sandholm and his team approached Claudico’s development in three stages. First they fed the rules of no-limit hold ‘em into an abstraction algorithm, reducing the game to something smaller in scope and more easily comprehensible. They then customized algorithms that attempt to come as close as possible to Nash Equilibrium, a game theory concept involving the adoption of optimal strategy. Finally, the team used reverse mapping techniques to input that strategy back into the algorithms for the game’s original parameters. As a player, Claudico rarely falls into a recognizable pattern. That, along with a variety of unorthodox bet sizes, gives the machine a distinct advantage over humans. “Typically humans use one or two bet sizes, because they are worried that they are going to signal too much about their own private cards,” says Sandholm. “Claudico’s reasoning guarantees that it’s balanced.” On the other hand, no-limit poker takes an enormous amount of computational power. So Claudico’s programmers couldn’t generate algorithms that solved every problem. “We run into this classic artificial intelligence tradeoff of solution quality versus reasoning time,” explains Sandholm. “We don’t have infinite time, and therefore we have to make some compromises in how we reason.” Claudico can only get close to Nash Equilibrium; it doesn’t react to the specific tendencies of individual opponents. The machine instead approximates ideal rational play, no matter the circumstances. In some ways, Claudico’s approach is something human players can only aspire to. “If you are playing game theory optimal, you are indifferent to how your opponent plays,” says Jason Les, 29, one of the pros who played in the tournament. “Your strategy will, at worst case, break even.” Les still thought he had an edge going in. He just didn’t know how it would manifest itself. “I really didn’t know what to expect,” he says. “I understood there would be some frequency of the time where this bot was amazing and we had no chance of winning.” When the competition began, Les was struck by the unique and finely calibrated nature of the AI’s betting scheme. “It uses a mixed strategy. It will do multiple things with the same hand,” says Les. Even the best human players eventually leave traces of an identifiable pattern in their betting behavior, which can then be used by savvy opponents to more accurately gauge the value of their two hole cards. Not Claudico. “It has all that perfectly balanced and randomized,” Les says---with perhaps a trace of awe. So the professionals adopted a constantly changing, exploitative strategy designed to locate and attack specific quirks in Claudico’s play. For example, it couldn’t process card removal---the way in which the cards in one’s own hand affect the likelihood of another player having specific card combinations. Les says that Claudico didn’t factor that in, so the humans could tell when the AI was making big bets to disguise a weak hand, trying to force its opponent to fold. That tell meant Les and his colleagues could pick off gigantic bluffs on the river by calculating that their hole cards made it unlikely Claudico had a hand as big as its bet would suggest. “It was writing a check it can’t quite cash,” says Les. Another chink in the AI’s armor was the way it responded to bet sizes from its competitors. In an effort to reduce the size of the ""game space"" Claudico had to traverse in its search for solutions, the developers limited the number of bet sizes the program would recognize. If Claudico had no data for a bet of half the size of the pot in a given hand, some percentage of the time Claudico would react to such a wager as if it were a bet of three quarters of the pot, and some percentage of the time it would react to it as if it were a bet of one quarter. That’s a big problem; it meant that the AI didn't always responding correctly. The humans capitalized on that. “Bjorn started using the most unusual bet sizes,” Les says. “He was falling in between the known sizes a lot, and was causing Claudico to have difficulties.” In the end, the ability to exploit Claudico’s departures from optimal play carried the humans to victory. When the final hand of the competition was completed, the players had wagered around $170 million (theoretically), and the team of humans professionals was ahead $732,713. Sandholm doesn’t count it as a loss, though. He says that because the outcome didn’t statistically have a 95 percent confidence interval, it was essentially a tie. Not everyone agrees. Les and his fellow human poker players think the final dollar count is a pretty clear indicator of who won. So does at least one other AI expert. “The margin of victory was substantial in poker terms,” says Michael Bowling, one of the creators of another poker-playing bot, Cepheus. Still, both the computer scientists and the poker professionals agree that the outcome shows just how fast AI is advancing. It took eight years and a couple attempts for Deep Blue to triumph over Kasparov. By the time computers began to dominate in chess, research in that field had been underway for nearly four decades. Compared to all that, the night is still young for poker. “While humans may still be ahead for now,” says Bowling, “it’s really just the beginning of the end.” In other words: They’ll be back."
703,https://www.wired.com/story/wolfram-image-identification-project/,Wired,2015,5,14,516.0," Being able to correctly identify images is one of the key skills human children learn in their early years, but it's a form of intelligence researchers have been trying to give computers for years. Wolfram Research -- the company behind the Wolfram Alpha software that Apple's Siri taps into -- has been working on developing this particular form of artificial intelligence, and now believes it may have developed a solution of sorts. A function called ImageIdentify, which allows you to ask what a picture is of and receive an answer in return, has been built into the company's own programming language, Wolfram Language. Stephen Wolfram has detailed the development of the system in a blog post, and to demonstrate how it works, he has released the Wolfram Language Image Identification Project into the wilds of the internet. Anyone can use the browser-based application, drag and drop a picture, and discover what ImageIdentity thinks it is. Training the system has been an arduous task. In total it has taken more than 25 years of development, with the system observing ""a few tens of millions"" of images, which Wolfram sees as ""very comparable to the number of distinct views of objects that humans get in their first couple of years of life"". It is now capable, however, of recognising around 10,000 common objects. Wolfram admits that the algorithms won't always get it right -- something we can verify -- but in particular it struggles to recognise specific people, art and things that are not everyday objects. Even to get it to the stage it is now, the software has had to go through a significant debugging process, explains Wolfram -- but he was reassured to find that there was usually a very specific and obvious cause to every misunderstanding. ""What I found was something very striking, and charming. Yes, ImageIdentify could be completely wrong. But somehow the errors seemed very understandable, and in a sense very human. It seemed as if what ImageIdentify was doing was successfully capturing some of the essence of the human process of identifying images,"" he says. The software will be available to developers to create apps that tap into the feature. ""And if one had lots of photographs, one could immediately write a Wolfram Language program that, for example, gave statistics on the different kinds of animals, or planes, or devices, or whatever, that appear in the photographs,"" writes Wolfram. Much like image recognition in the likes of Google+, the technology could also be built into other existing applications. It joins the likes of Google Goggles, Flickr's recently announced magic view and Amazon's Firefly as a rapid identification tool entering the mainstream. The primary difference of course, is that the technology is also built into Wolfram's programming language. You can download the app, or use the web-based application now to test it out now. Be aware, however, that ImageIdentity keeps a thumbnail of images in order to continue training the system. We tried it out with mixed results, but you can also provide the system with feedback in order that it keeps learning."
704,https://www.wired.com/story/nigel-shadbolt-on-the-inevitable-robot-uprising/,Wired,2015,5,8,733.0," Nigel Shadbolt, professor of artificial intelligence at the University of Southampton and cofounder of the Open Data Institute, will be debating the future of AI _on 24 May_at HowTheLightGetsIn, a philosophy and music festival. WIRED is a festival media partner. I once heard artificial intelligence described as making computers that behave like the ones in the movies. Hollywood's computers and robots are invariably mad, bad and dangerous to know. So when we are warned about the emerging threat of AI to the future of humanity, people get alarmed. There are plenty of things to be alarmed about in the modern world, from climate change to terrorism, Ebola outbreaks to nuclear reactors melting down. However, I am pretty relaxed about the threat from AI and actually very excited about what future developments hold. AI isn't poised to decide that we are superfluous to requirements and that the planet would be better off without us. To believe that is to misunderstand the progress we have made in AI and to forget a basic feature of human design. The rise and rise of computing power has brought huge advances in our ability to solve problems that previously seemed beyond machines. Examples include speech recognition, language translation, defeating human chess champions, systems taking part in quiz shows, vehicle navigation and control. The list goes on. However, the characteristic of this success is what I call the combination of brute force and a little insight. When Gary Kasparov was beaten by an AI chess-playing programme it happened because of the ability of the machine to review hundreds of thousands of moves deep into the game -- brute force. The program had a large library of stored moves from openings through to end games -- brute force. The program was able to apply weightings to the relative goodness of the moves -- a little insight. The result was a defeat for Kasparov. A couple of decades earlier we had been told that if ever a computer beat a human world chess champion the age of intelligent machines would have arrived. One did, and it hadn't. And it hasn't arrived today, when Deepmind's latest learning algorithm allow computers to learn to play at super human levels in arcade games. What we do have are programs and machines that are exquisitely designed to particular tasks -- achieving a level of performance through brute force and insight that makes them look smart. And that brings us to that basic feature of human design. We have evolved such that we are disposed to see intelligence, other minds and agency in the world around us. A three-year-old child tows a piece of paper around and treats it as their pet. MIT students sitting entranced by a crude humanoid robot able to raise its eyebrows, effect a frown or look sad by implementing a simple set of rules displaying responses that were triggered by the behaviour of the student. Or back to Kasparov, who was convinced that the programme that beat him at chess was reading his mind. The reality is no less exciting. What we are witnessing is the emergence of Ambient Intelligence and Augmented Intelligence. These others senses to the abbreviation AI have been brought about by the encapsulation and compilation into devices -- the Internet of Things and the Web of narrow components of Artificial Intelligence -- programs that are crafted to a particular task and niche. Augmented Intelligence occurs when we use our global infrastructure of data and machine processing connecting thousands and millions of humans. Witness the recent response to the Nepal earthquake. This is a genuinely exciting prospect where we solve problems together that are beyond any one individual or organisation. In the meantime there is no doubt we should take the ethics of AI very seriously. There is danger from complex systems with AI algorithms at their heart. The danger is not having humans in the loop to monitor, and if necessary override the system's behaviour. It is not having clear rules of engagement, built into these systems; principles that embody the highest levels of respect for human life and safety. Isaac Asimov, who saw the future better than most, gave us a first law of robotics -- a robot may not injure a human being or, through inaction, allow a human being to come to harm. This is a great principle to compile into any complex system."
705,https://www.wired.com/2015/05/cant-find-good-stuff-periscope-maybe-ai-can-help/,Wired,2015,5,5,571.0," Periscope turns anyone with an iPhone into a video broadcaster. And people are paying attention. After Twitter acquired the company earlier this year, the app bolted ahead of competitors in the Apple App Store, bringing the livestream into the mainstream. Many people used Periscope feeds to watch the big Paquiao-Mayweather fight this past Saturday night. The problem lies in finding the feeds you most want to watch. Most Periscope feeds are just people speaking directly into the camera about whatever happens to be on their mind. If you want the big fight or live streams from the riots in Baltimore or backstage footage from the sets of popular television shows, you have to hunt pretty hard. But a New York-based startup called Dextro just launched a service designed to help you find the good stuff. It's called Stream, and it uses machine learning algorithms to automatically categorize videos into categories like ""talking heads,"" ""pets,"" and ""crowds."" That way, you can skip the talking heads---unless that's what you're in the mood for---and find the more interesting streams. The tool analyzes images using technology based on a cutting-edge field of artificial intelligence research called deep learning. Google, Facebook, and Microsoft are now using deep learning to recognize images and speech on various online serves, and Dextro is among a wide range of startups exploring this field as well. This work points to a world where machines can sort through all kinds of digital information much faster than humans. Founded in 2013, Dextro already offers services that help marketing firms use artificial intelligence to analyze images and videos on behalf of their clients, which is a growing niche in social media marketing. Co-founder David Luan came up for the idea for the company in 2012 after moving to the San Francisco Bay Area to participate in the Thiel Fellowship, a program backed by PayPal co-founder and early Facebook investor Peter Thiel that encourages Ivy League students to drop out of college and pursue entrepreneurship. Luan noticed that many of his fellow entrepreneurs were building robots or other devices with cameras built in, but few of them had the background in computer vision to actually do anything useful with those video feeds. So he moved back to New York and started Dextro with Sanchit Arora to build a platform for computer vision for robots. But the two quickly realized that their platform could be used to help brands navigate the visual web. The principles of searching and analyzing are already well understood. There dozens, perhaps hundreds, of tools already available to companies that want to sift through status updates posted to sites like Facebook and Twitter and surface conversation trends or mentions of particular brands. But as social media becomes more visual thanks to sites like Pinterest and Instagram, new ""deep learning"" companies like Curalate and Clarifai have emerged to help companies analyze the massive number of pictures and videos uploaded to the web every day. Luan says other companies tend to train their algorithms to recognize objects photographed in front of nice plain background. But Dextro trains on real-world images. That has helped the company make sense of live streams such as Periscope, which has required them to extend the ideas of deep learning in new ways. ""In pre-recorded video, you know what happens at the end of the video,"" he says. ""But in live video you don't know what's going to happen next."""
706,https://www.wired.com/story/the-big-question-ai-advances/,Wired,2015,5,5,497.0," Co-founder and lead scientist, Emotient ""We underestimate how hard vision is because our brains do it so well. The human brain has evolved for millions of years to be able to make sense of light patterns. Until ten years ago, getting a computer to recognise even simple objects in natural images has been elusive. Now, with advances in machine learning -- many of which are inspired by our understanding of the brain -- computers can recognise something as nuanced as facial expressions. This will transform the way we interact with machines. In the next 20 years it will be commonplace for devices to respond to non-verbal signals such as attention and mood."" Co-founder, Sense ""Research is underway to allow computers to be able to recognise what is being said, understand what sounds mean and recognise objects in a scene. Once we achieve this, there is the challenge of 'data fusion'. We understand the world around us by fusing information from different sources simultaneously. This helps us make faster, more accurate decisions, but is still very difficult for computers. When this challenge is solved, AI will have more use cases than we can imagine today."" Vice-president, IBM Watson Solutions ""All innovations start with a moment when a person sees something previously overlooked and recognises its potential to change the way we live. Today, new cognitive computing systems put science behind serendipity to help us see the world differently. From inspiring chefs to find new flavour combinations to helping pharmaceutical researchers spot new treatment pathways, cognitive systems will help us accelerate our work and inspire our thinking, ushering in the next era of innovation."" Department of computer science, Tufts University ""Data-driven statistical machine learning will allow us to surpass human performance in many areas, from solving the hardest crossword in the blink of an eye to proposing architectural designs for cities. For autonomous systems such as service robots, it will be critical to attain sufficient moral competence in order for them to navigate the complexities and intricacies of human social and moral norms without causing us unnecessary harm."" CEO, Allen Institute for Artificial Intelligence ""What if a cure for an intractable cancer is hidden within the tedious reports on thousands of clinical studies? In 20 years' time, AI will be able to read -- and more importantly, understand -- scientific text. These AI readers will be able to connect the dots between disparate studies to identify novel hypotheses and suggest experiments which would otherwise be missed. AI-based discovery engines will help to find the answers to science's thorniest problems and ultimately revolutionise science."" Founder and CTO, MetaMind ""One challenge is understanding and retaining connections between facts and concepts. If a machine could achieve this, it could become a domain expert, answer tough questions and make difficult inferences autonomously. Another major practical challenge is efficiency. Today's AI approaches require tremendous amounts of computing power. Developing more efficient and effective algorithms will be key to making the technology ubiquitous and valuable."""
707,https://www.wired.com/2015/04/mashing-human-smarts-ai-unlock-computer-vision/,Wired,2015,4,29,734.0," Computer vision is quickly advancing, but it tends to trickle into the world in scattered, specific applications. We encounter it when Facebook automatically tags a friend in a photo, or when Google suggests images similar to one we're searching for. But the real promise is much more exciting. A camera, properly trained, could answer simple, human questions like: ""Are my kids home from school?"" or ""Is there a parking spot open at work?"" or ""How many people are in line at Shake Shack?"" In other words, computer vision could make our homes and our cities smart. Today, our machines don't understand these sorts of queries. The researchers behind Zensors want to change that. The project, developed at Carnegie Mellon University, aims to make computer vision more accessible through a clever combination of human smarts and artificial intelligence. Though it's only a proof of concept for now, it takes a compelling approach to the problem. Say you're a sandwich shop owner who wants to track how many people are in line throughout the day. Here's the Zensors vision: You mount an old smartphone on the wall, point it at your register, and ask the Zensors app how many people are waiting. The novelty is what happens behind the scenes. First, Zensors conveys your question to humans---the Carnegie Mellon researchers used crowdsourced workers while developing the concept. These workers receive images from the smartphone, which they count and tag for a small fee. The processed images are simultaneously used to train a machine learning algorithm that also tries to count the waiting patrons. When the AI is as good as the humans, it takes over. The handoff happens seamlessly; all the business owner knows is that, within minutes of setting up the camera, Zensors provided the answer to his question for a reasonable sum. The approach solves one of the big problems with computer vision: its inflexibility. ""Computer vision has made fantastic strides, and yet a lot of it is pretty specific to a situation,"" says Jason Wiese, one of the researchers who worked on the project. In technical parlance, AI-trained computer vision systems are ""brittle""---they often don't adapt well to unfamiliar environments or unexpected behavior. Because every sandwich shop has a different layout, and because every camera will have a different vantage on the action, it's hard to create a universal ""line counting"" algorithm. Zensors would get around this by using just the amount of human power necessary to familiarize a computer with a specific scene. ""We see this as a good way of brining computer vision to the masses,"" Wiese says. It would almost certainly be cheaper than building a solution from scratch. The Carnegie Mellon group broke down the economics in a paper presented at a human-computer interaction conference last week in Seoul. The researchers asked a number of programmers how much it would cost to develop a custom computer vision system for a determining whether a bus had arrived at a bus stop. The average quote: $3,000. Zensors used its own approach to develop working sensors for a number of similarly-complex questions: ""How many cars are in this parking lot?,"" ""How messy is the sink?,"" ""Is the dishwasher door open?"" On average, the algorithms could be trained in the span of a week, with humans processing a handful of images each day. Pegged to minimum wage, the cheapest sensor was trained for $5. The most expensive cost $40. The Zensors team is still working on the platform. But the real ambition for Zensors extends beyond answering questions. The model could also bring API-like structure to video feeds, which could be used by other applications. Unlike the motion sensors in your iPhone, which make themselves available to third parties like Nike and MyFitnessPal, there aren't APIs for easily pulling data from video feeds. With Zensors, the sandwich maker could not only track how his line fluctuated throughout the day, but use that data to inform other actions, pinging someone to open a second register, say, when more than six people were waiting. Think IFTTT with a video feed as a trigger. ""Today we think of camera images as more or less an analog signal, and one without a lot of computational meaning. But the information is clearly there,"" Wiese says. Algorithms may not be able to extract it on their own just yet---but they can with some time and a little human help."
708,https://www.wired.com/2015/04/computers-can-now-tell-feel-face/,Wired,2015,4,22,1089.0," Sometime next summer, you’ll be able to watch a horror series that is exactly as scary as you want it to be—no more, no less. You’ll pull up the show, which relies on software from the artificial intelligence startup Affectiva, and tap a button to opt in. Then, while you stare at your iPad, its camera will stare at you. The software will read your emotional reactions to the show in real time. Should your mouth turn down a second too long or your eyes squeeze shut in fright, the plot will speed along. But if they grow large and hold your interest, the program will draw out the suspense. “Yes, the killing is going to happen, but whether you want to be kept in the tension depends on you,” says Julian McCrea, founder of the London-based studio Portal Entertainment, which has a development deal with a large unidentified entertainment network to produce the series. He calls Affectiva’s face-reading software, Affdex, “an incredible piece of technology.” McCrea is one of the first outside developers to experiment with Affectiva’s developer tools to make technology capable of interpreting feelings based on tracking your facial expression. Scientists Roz Picard and Rana el Kaliouby spun the Waltham, Massachusetts-based tech startup out of MIT Media Lab in 2009. Picard has since left the company, but El Kaliouby, 36, remains the chief science officer and is committed to a bigger vision: “Personally, I’m not going to stop until this tech is embedded in all of our lives.” Already, CBS has used it to determine how new shows might go down with viewers. And during the 2012 Presidential election, Kaliouby’s team experimented with using it to track a sample of voters during a debate. With $20 million in venture funding, the company has so far worked closely with a few partners to test its commercial applications. Now it plans to open its tools to everyone. Starting today, Affectiva will invite developers to experiment with a 45-day free test and then license its tools. You remember Intel inside? El Kaliouby envisions “Affectiva-embedded” technology, saying, “It’ll sit on your phone, in your car, in your fridge. It will sense your emotions and adapt seamlessly without being in your face.” It will just notice everything that's happening on your face. She’ll expand on her strategy May 19 at the LDV Vision Summit, a coming together of some of the smartest companies cracking the problem of machine vision, in New York. El Kaliouby has a PhD in computer science from Cambridge University, completed a post-doc at MIT Media Lab, and built Affectiva’s core technology as part of her academic work, intending to use it to help children with autism. “As I was doing that we started getting a lot of interest from industry,” says el Kaliouby. “The autism research was limited in scope,” she explained, so she turned to the business world to have a greater impact. Affdex, the company’s signature software, builds detailed models of the face, taking into account the crinkle of the skin around the eye when you smile or the dip in the corner of your bottom lip when you frown. Since el Kaliouby started working on the Affectiva algorithms, the software has logged 11 billion of these data points, taken from 2.8 million faces in 75 countries. With its massive data set, el Kaliouby believes Affectiva has developed an accurate read on human emotions. The software can, in effect, decode feelings. Consider Affectiva's take on tracking empathy: “An example would be the inner eyebrow rise,” says el Kaliouby. “Like when you see a cute puppy and you’re, like, awww!” It can even note when you are paying attention. The software relies on a so-called Facial Action Coding System, a taxonomy of 46 human facial movements that can be combined in different arrays to identify and label emotions. When it was developed in the late 1970s, humans scored emotional states manually by watching the movement of facial muscles. It was time intensive. “It takes about five minutes to code one minute of video,” says el Kaliouby. “So we built algorithms that automate it.” The software had to be trained to recognize variety in expressions. My smirk, for example, might not look like your smirk. “It’s like training a kid to recognize what an apple is,” el Kaliouby says. Five years in, the technology has become robust enough to be reliably useful. Experience designer Steve McLean, for example, who runs the Wisconsin design firm Wild Blue Technologies, has used Affectiva to build a video display for Hershey to use in retail stores. If you smile at the screen, the display dispenses a free chocolate sample. Tech startup OoVoo, which competes with Skype, has integrated the software into its videochat to create a product called intelligent video that can read chatters’ emotions. “We’re looking at focus groups, online education, and political affinity,” says JP Nauseef, managing director of Myrian Capital, which invested in both Affectiva and OoVoo and sits on Affectiva’s board. But for all of Affectiva’s potential, it will take more than creative developers to help its technology catch on more broadly. “The hidden discussion that hasn’t been brought up is trust,” says Charlene Li, CEO of the research outfit Altimeter Group, who has followed Affectiva closely since 2011. “I love the product, but I’m also terrified by it,” she says. She points out that should this data fall into the wrong hands, it could be dangerous for consumers. What happens, for example, if you are often sad while using a piece of Affectiva-embedded software and the software’s developer chooses to sell that information to a pharmaceutical company? It’s a concern that el Kaliouby takes very seriously. “We actually don’t store any personal information about the consumers, so we do not have any way of tying back the facial video to an individual,” she says. “We have 2.78 million face videos in the platform, and if your face was in there, none of our team would be able to pull it out for you.” That may be so, but as the company makes its tools available to a broader set of developers, it will have to monitor how the software is rolled out to prevent them from abusing it---and to make sure that as users interact with it for the first time, they’re aware of it and feel they are in control of the experience. The technology may be very good at reading your emotions. But humans will have to take care how to act on them."
709,https://www.wired.com/2015/04/jeff-dean/,Wired,2015,4,21,758.0," As a senior at the University of Minnesota, Jeff Dean built an artificial brain. Kinda. Using what was considered a supercomputer at the time, he mimicked the networks of neurons inside your head, creating a system that could analyze information—and even learn. The trouble was it didn’t work that well. Those computers didn’t provide enough juice. They couldn’t juggle enough data. “We just trained it on toy problems,” he says of this neural network. “The computational power wasn’t all that great.” But this was 25 years ago, before Dean went to Google and changed the very nature of computational power. As one of Google’s earliest engineers, Dean helped create the fundamental computing systems that underpin the company’s vast online empire, systems that span tens of thousands of machines. This work gave him celebrity-like status among Silicon Valley engineers—people recognize him as he walks through the Google cafeteria. Now, armed with those massively distributed systems and the ideas that drive them, he has returned to the world of neural networks. And this time, these artificial brains work remarkably well. Together with a team of other big-name researchers, Dean is building neural nets that can identify faces in photos, recognize spoken words, and even understand natural language. And many other tech companies—from Microsoft to Facebook to Twitter—are creating services with similar capabilities. The basic algorithms that drive these systems aren’t that different than what we had in the ’80s. But now, thanks to people like Dean, we have the computing power they need to thrive. “I’ve always believed that human learning is the result of relatively simple rules combined with massive amounts of hardware and massive amounts of data. And we now have that,” says Sebastian Thrun, the ex-Googler who oversaw the company’s self-driving car project, which can also benefit from the neural nets built by Dean and crew. “This is the chance for us to change the model of learning from very shallow, very confined statistics to something extremely open-ended.” In the ’90s, Dean was a researcher at DEC, which made the big computing systems that ran the world’s businesses in those days. But as the DECs of the world were imploding, he moved to Google, and there he was among the small team who realized we could generate far more computing power by stringing together thousands of relatively small machines. He helped create software that could store and process data across all these machines as if they were one big computer. With names like Bigtable and MapReduce, these tools were the secret weapons that enabled the company’s search engine to instantly serve hundreds of millions of people across the globe. Based on the research that Google later published, other companies like Facebook and Twitter and Yahoo began using similar tools. And now, drawing on many of the same ideas that allow such tools to juggle data across thousands of machines, people like Dean can finally construct neural networks that work. They can reliably identify the voice commands you bark into Android phones—or recognize the faces in images you post to the Google+ social network. At Microsoft similar neural nets underpin a new Skype tool that instantly translates from one language to another. Baidu is using such AI technology to target ads on its search engine, something that has significantly boosted revenue. In some ways the brainlike algorithms that drive these systems aren’t that different from those Dean played with as a college senior. The thing is, though, researchers now have a better understanding of how to build networks that operate on multiple levels, mimicking the multiple layers of neurons that operate in the brain. “It’s not a true reconstruction of what neurons do. But it’s an abstract notion of how we believe neurons work in the brain,” Dean says. “If you have lots and lots of these neurons and they’re all trained to pick up on different types of patterns, and there are other neurons that pick up on patterns that those neurons themselves have built on, you can build very complicated functions and systems that do pretty interesting things.” Deep learning is the catchall term for this, and in the years to come it will remake far more than just Skype and other Internet products. Like Thrun, Dean says deep learning can improve our self-driving cars, helping them better understand the world around them. And if it can help cars learn, it can help other machines learn as well. In other words, sentient robots are probably coming down the road. Check out the full Next List here."
710,https://www.wired.com/2015/04/kasisto-and-moneystream/,Wired,2015,4,15,1203.0," Old-school financial institutions are typically slow-moving giants. And that’s a shame, because banks also tend to accumulate deep troves of data on their customers that goes mostly untapped. If you’re a consumer looking for an answer to a specific question about your finances, tough luck. Your usual recourse would probably involve a lot of digging through bank statements and bank website pages, or endless hours on the phone with a customer service rep. But as of late, a swell of banking startups are seeking to change this. They take all that undifferentiated data tucked into your bank statements, and then, harnessing artificial intelligence, transform and organize it into helpful information that people can actually understand---and act on. Among these is Kasisto, a spin-off venture of SRI International---the creator of Siri. The startup is testing a voice-recognition add-on for mobile banking apps that lets customers ask questions about their accounts. Users can ask Kasisto, “How much have I spent on fees?” or tell it, “I’m looking for a three-dollar transaction on my checking account,” and the system will return an answer. It’s a voice-activated assistant that, unlike Siri, isn’t a generalist. Meanwhile, MoneyStream, a new service from a Silicon Valley startup of the same name, links your bank account to a range of services that together deliver personal finance predictions in the form of a simple calendar. In other words, you can see how much money you can expect in your bank account month to month. While other apps, notably Level, already aim to help you plan your daily budget, MoneyStream claims it can help you dig deeper into the more complex fluctuations that can upend the best-intentioned plans. In developing these tools, Kasisto and MoneyStream join a multitude of other companies that are riding the cresting wave of artificial intelligence to surface information that's useful for humans. Companies from Google to Facebook to Baidu are using AI to drive voice and image recognition. Smaller startups are using AI to rethink stodgy practices from job hunting to finding a restaurant that perfectly suits your budget and dietary needs. Now, both Kasisto and MoneyStream aim to inject personal money management with the same AI touch. Kasisto has deep knowledge of two things: the semantics of the financial services world and your own relationship with your bank. It then uses natural language processing and machine learning to serve up the information you’re looking for. The company, which launched last June, has been in “friends and family” testing mode in the US and Asia, but its roster of clients already includes Spain-based banking group BBVA and Wells Fargo, among others. Kasisto CEO and co-founder Zor Gorelov explains his company has basically built two types of apps: an enterprise add-on that corporate clients—especially company CFOs and treasurers—will use internally, and a consumer-friendly app that will integrate into existing mobile banking apps. The team plans to roll the app out to consumers later this year. Gorelov says Kasisto's tech is primed to integrate with existing smartphone hardware to extract relevant info anytime and anywhere it's needed. Ask the app, for instance, “How much have I spent in this store?” and the system will use location services to determine which store you’re shopping from and whether the store is offering any deals. Along with GPS, Kasisto can also look at consumption history and transactions between users, as wells as connect to credit-card linked offers to show you bargains that, based on your spending history, might be interesting to you. According to the app’s founders, remaking the financial services market is just the beginning. Eventually, they claim, clients will be able to train Kasisto to work with other industry verticals, such as healthcare, retail, and more. Even then, however, banks say Kasisto won't replace human customer service. “We see the app as complementary,” says Steve Ellis, executive vice president at Wells Fargo, one of Kasisto’s clients, pointing out that banks don't plan to close their branches just because online banking has become more popular. Mike Bertrand remembers a time he would have been in hot water if not for MoneyStream. He had just used his credit card to pay for a big vacation and hadn't realized a telephone bill was due the next day. ""I got this alert that my credit card was about to go over its limit and I was going to get hit with a fee,"" the MoneyStream CEO says. ""It was because we had tied in the two data sources of AT&T and my credit card that I could do something about it. I moved some money around, and it wasn't an issue."" That's the added value MoneyStream gives its customers, according to Bertrand's pitch. Simply link your bank account to the service (the web app is the most robust landing page for now), and it identifies your sources of income, your recurring bills, your credit cards, and your loans. After scooping in all the data it can glean from your bank statement, you can add new bills, utilities, accounts and credit cards, and also manage your email notifications and alerts. An algorithm analyzes this ""stream,"" and, using AI, projects how much money you'll have in the bank for months to come, showing you the information in a straightforward calendar format. According to Bertrand, the system gets better with use as the stream gathers more historical data and users correct individual entries. Bertrand acknowledges there are certain limitations to the way MoneyStream is currently set up. The algorithm does well for folks who see regular income, and not so great for people with, say, commission-based jobs who have high variability in their income. But, Bertrand claims, after starting off with the simplistic calculations for the limited amount of data it's able to examine, MoneyStream improves every day. Eventually, he says, the algorithm will be able to discern more of a user's spending patterns and come out with better predictions. It may even be able to classify you (anonymously) in a group of ""people like you""—users with similar spending behavior—to provide more insights. I tried MoneyStream myself, and the app cleverly picked up on several of my other accounts after linking just one bank account. It grabbed three credit cards, a savings and a checking account, my income stream, and even my student loan. After playing around with it a bit more, I was able to add another bank account and my PayPal account, which gave me a pretty good picture of my finances. But it couldn't make sense of my rent in the stream, which I usually pay my roommate in a lump sum along with utilities, using either a check or the money-transfer app Venmo. Bertrand says that as time goes on, the system will learn and be able to do more. He says that its predictive abilities go beyond the historical data that has been the norm for financial planning apps for decades. ""The exercise we’ve done here is, you know, the bank is where you've stored your money,"" he says. But where your money works for you is out in the world, he says. And it's understanding that motion that's valuable---not just the list of debits and credits your bank sends you every month."
711,https://www.wired.com/2015/04/ex-machina-turing-bechdel-test/,Wired,2015,4,9,1915.0," The Turing test detects if a machine can truly think like a human. The Bechdel Test detects gender bias in fiction. If you were to mash the two together to create a particularly messy Venn diagram, the overlap shall henceforth be known as the Ex Machina Zone. In writer/director Alex Garland's thought-provoking new film—out Friday—we meet Ava (Alicia Vikander), an artificially-intelligent robot. Ava's creator, genius tech billionaire Nathan (Oscar Isaac), has asked his employee Caleb (Domhnall Gleeson) to determine whether Ava's thinking is indistinguishable from a human's. Until she meets Caleb, Ava has only ever met her maker and one other woman. (Hence the failing of the Bechdel Test, which stipulates that a movie must feature two female characters who talk to each other about something other than a man.) Her existence, and her ability to learn how to interact, is a fascinating study of what makes us human. It's also a compelling, if problematic, look at the interactions between men and women---or at least that's what I thought. While interviewing Garland for a magazine piece, I asked him about the roles of men and women in his film; his response was that Ava is ""not a woman, she is literally genderless."" Despite using female pronouns, he said, ""the things that would define gender in a man and a woman, she lacks them, except in external terms. ... I'm not even sure consciousness itself has a gender."" In a way, Garland is right; pure intelligence wouldn't have a gender any more than it would have a race. But to say that and then place that consciousness into a body that it will immediately recognize its likeness as female negates that point. If Ava has truly been educated about the human race, then she knows her face and form appeal to certain segments of the population. But even thornier is the fact that Ava falls squarely into so many of the tropes of women in film. She's a femme fatale, a seductress posing as a damsel in distress, using her wiles to get Caleb to save her from Nathan and his Dr.-Frankenstein-with-tech-money quest to build a perfect woman. (Women: So much better when you can construct them out of bespoke parts and switch them off if they're not working properly, amirite?) According to Garland, these tropes are intentionally front-and-center. He believes his movie is a commentary on the ""constructs we've made around girls in their early 20s and the way we condition them culturally"" and why Caleb would feel the need to save her from her maker. ""You're supposed to think it's creepy,"" he says. ""You're not supposed to warm to [Nathan] over that stuff, you're supposed to feel unnerved, and therefore that she needs to be rescued."" Yet, in the pursuit of that commentary, the movie ends up re-enacting those same patterns. Ava does prove to be the smartest creature on the screen, but the message we're left with at the end of Ex Machina is still that the best way for a miraculously intelligent creature to get what she wants is to flirt manipulatively. (And why wouldn't she? All of her information about human interaction comes from her creepy creator and the Internet.) Why doesn't Chappie have to put up with this bullshit? Ava's predicament really isn't that different from many female AIs who have come before her, from Metropolis' Maria to *Her'*s Samantha to *Blade Runner'*s Pris. She is an android in female form, and thus she simply reflects how Hollywood has been depicting women—robotic or otherwise—for decades. In Blade Runner, the male replicants Roy Batty and Leon are struggling to change their short lifespans, while ""basic pleasure model"" Pris helps the cause by draping herself on J.F. Sebastian. In Prometheus, David is intellectually curious, but never sexualized. (Yet when Idris Elba's Janek accuses Charlize Theron's Meredith Vickers of being a robot, she responds with ""My room. Ten minutes."" Because sex is the easiest way to prove you're a real woman.) Sentient male androids want to conquer or explore or seek intellectual enlightenment; female droids may have the same goals, but they always do it with a little bit of sex appeal, or at least in a sexy package. (Still have doubts? Ex Machina’s marketing campaign at South by Southwest involved Ava showing up on Tinder.) This tendency to give female AIs the most basic and stereotypical feminine characteristics is, according to Kathleen Richardson, a senior research fellow in the ethics of robotics at De Montfort University in the UK, probably a reflection of ""what some men think about women—that they're not fully human beings."" To put a finer point on it, she told Live Science recently, ""what's necessary about them can be replicated, but when it comes to more sophisticated robots, they have to be male."" When I spoke to Richardson, author of An Anthropology of Robots and AI: Annihilation Anxiety and Machines, she also noted this leads to female robot characters becoming just pieces of full people—a beautiful body, a caretaking nature—but not ones with full intelligence. This is largely true in Ex Machina---and not just because Nathan has a lab full of body parts---but also in a lot of movies where the artificial intelligence has to be packaged in a certain way if the robot is perceived to be female. (She also notes the real robotics world suffers from the same problems as a lot of AI fiction, but that ""many robotic scientists are open to a conversation about this."") ""Sometimes the female robots have 'violent' characteristics (as Terminator 3’s T-X character), but it's always presented in a beautiful form,"" Richardson says. ""Women, whatever their qualities—intelligent, vulnerable, strong—are always presented in an attractive form, as if the package is the only way to deliver these qualities. Male intelligence, strength, vulnerabilities, etc. can be delivered in a multiple and varied kind of outer packaging."" Think of it this way: Ava demonstrates her consciousness/intelligence in a form and with a sensuality that David in Prometheus never had to. *Short Circuit'*s Number 5/Johnny Five was cute, but he never had to employ it for survival the way Pris did in Blade Runner. Even AIs with no physical form at all seem to get sexualized based simply on their voices. It's not like HAL 9000 ever sparked up a relationship with Dave in 2001: A Space Odyssey the way Samantha did in Her. ""Her is playing on the fact that the audience knows what [Scarlett Johansson] looks like,"" Richardson says. ""No one really needs to know who the voice of HAL was, because HAL was an intelligent machine. We need to know about the disembodied voices of our AI avatars if they're female so that males can buy into the ideas of the sexualized person behind the representation."" If this argument about the roles women get in movies versus the roles men get is starting to sound familiar, it should. Ever since Laura Mulvey's 1975 essay ""Visual Pleasure and Narrative Cinema"" film critics and fans have been monitoring the ways that women are represented and seen onscreen. (If you've heard the term ""the male gaze,"" this is why.) This ongoing discourse is the reason thing like the Bechdel Test, which started out just as a comic trip referencing Alien, struck a nerve and stuck around. The thrust of Mulvey's argument is that the bulk of films are seen from a male perspective---that a woman in a film is often ""tied to her place as bearer of meaning, not maker of meaning."" Yes, Ava learns to use seduction as manipulation, and the audience might learn how screwed up that is because it's more blatant when even a robot can pull it off, but Ex Machina doesn't go any further in deconstructing the problem than that. She's a bearer, not a maker. To be fair, not all of this is *Ex Machina'*s fault---or the fault of any AI film. Often, social constructs mandate that we gender most things, whether they're intended to be gendered or not. Interstellar’s TARS looks like a Mies van der Rohe Kit-Kat bar, yet we refer to TARS as ""him."" Is that because of the machine's deep(ish) voice or because narrative constructs lead us to believe robots with scientific intellectual aims are masculine? It's nearly impossible to tease the two apart, and that knottiness is baked into British computer scientist Alan Turing's original test in a way that can never be removed. If the goal is for a machine can convince a human that it's human, then the machine has to assume some kind of gender because we see all humans as having a gender. Even if, in the Turing test model, a judge is just looking at the output of a chatbot, he or she would ascribe gender to the output without even thinking about it. (Note the chatbot that convinced judges that it was real last year did so by convincing them it was a 13-year-old boy named Eugene.) Ex Machina sidesteps this a bit by making Eva visible; Caleb he knows he's talking to a robot, and knows what that robot looks like. Nathan just wants to ""show you that she's a robot and then see if you still feel she has consciousness."" What Nathan actually wants Caleb to do is something more akin to Blade Runner’s Voight-Kampff test, where the subject can flirt Sean Young-style but you know she's a replicant. But if that's the case, why does so much of Ex Machina focus on her proving that consciousness through flirtatious interactions and not, say, a discussion of the horrors of war? Johnny Five discovered mortality by crushing a grasshopper leading to a fear of being switched off, and we felt his plight all the same, why not Ava? (Don't answer that.) The thing is, Alan Turing himself actually might have wanted AI to be something akin to what Caleb wants: actual companionship. When WIRED spoke to screenwriter Graham Moore about The Imitation Game back in November he noted that much of Turing's work in AI was about ""bringing Christopher [Morcom, Turing's first love] back."" But while Turing, if he would've ever been able to rebuild Morcom, would've been making someone he could talk to and share ideas with, the female representations of Turing's dream in movies often personify it through far less intellectual pursuits. Think of David in Prometheus; his primary goal was assisting on the mission, not seducing Vickers. As a ""male"" AI in a film he was given an intellectual pursuit, not a romantic one. Is it possible Ava could've convinced Caleb she passed the test with fewer pleading glances and more analysis of world affairs? What would Ava have done to pass if she was a he? Obviously, wanting affection is part of what makes us human; by showing that, Ava is showing a highly-evolved part of herself. But by only showing that, and her highly manipulative nature, she is left as a less-than-whole character. She's almost the colder, darker side of *Her'*s Samantha, who served as a Manic Pixie Dream Operating System. Like Her, Ex Machina is a smart, beautiful film. But when the only female lead in your movie is one whose function is to turn the male lead on while being in a position to be turned off, that says a lot about what you think of the value of women in films. Saying it's the result of your protagonist being ""creepy,"" as Garland does, doesn't really absolve you of that."
712,https://www.wired.com/story/google-robot-personalities/,Wired,2015,4,1,588.0," Do you like your robots subservient and complimentary, or glib and a little bit cheeky? A patent that has just been awarded to Google suggests that either could be possible and that we could potentially download different personality types from the cloud. In fact, if you can't choose what kind of personality you want for your future robo-pal, it's highly possible that it might be able to choose for you. It would do this by accessing your devices and learning about you, before configuring a tailored personality based on that information. In addition it could use speech and facial recognition to personalise its interactions with you. The original question posed still stands though -- you could potentially always choose a specific personality type for your Google robot that represents the kind of person you enjoy interacting with. This personality could even be triggered by specific cues or circumstances that the robot could detect, says the patent, which was spotted by Quartz. ""The robot personality may also be modifiable within a base personality construct (i.e., a default-persona) to provide states or moods representing transitory conditions of happiness, fear, surprise, perplexion (e.g., the Woody Allen robot), thoughtfulness, derision (e.g., the Rodney Dangerfield robot), and so forth,"" states the patent. It also suggests that should a cruel fate befall your robot, that might not spell the end of its days. It's possible that if you uploaded its personality to the cloud you might be able to transfer it to another robot. Unlike Newton and Stephanie from Short Circuit who were devastated when they believed their beloved Johnny Five had been destroyed, you never need get emotional over or be concerned about the physical destruction of your robot. A more concerning concept perhaps though is that a robot could be programmed to take on the personality of a real-world person -- the patent suggests a deceased loved one or a celebrity -- so that effectively you could get someone to live on after their death in robot form. And right about that point it all starts getting a bit Black Mirror. We'd probably be cool with a robot Taylor Swift or Andrew WK to issue kind, wise words and life and party advice to us though. From the vaguely creepy to the eminently useful (providing the security is right) robots could also share information with one another over the cloud, providing ""a kind of teleportation capability for the robot"". Imagine flying halfway across the world to find that your home robot had shared all of its knowledge about you with the robot butler in your hotel room. It might greet you by name, plump your pillows just how you like them, and wake you up with your favourite coffee in the morning. Robots aren't quite up to these challenges just yet -- being restricted as much by hardware as they are by software/personality challenges. This technology is obviously just a patent thus far, but given that it's no secret quite how many robotics companies Google has bought over the past year, it would almost be surprising if it wasn't working on something like this. A quick straw poll of the WIRED.co.uk team suggests the existing robot personalities we'd like to have a beer with include Marvin, the Paranid Android; R2D2 (""cute""); Wall-E (""he'd be uplifting""); George the first humanoid robot; Robbie the Robot (for ""retro charm""); and Optimus Prime. We'd steer clear of evil Megatron and amoral Ava from Ex-Machina. We're also not so keen on C-3P0 (""too much of a nag"")."
713,https://www.wired.com/2015/03/orbital-insight/,Wired,2015,3,16,612.0," No one knows how much oil we have left on the planet. No one can even say with any certainty how much oil is waiting to hit the market. The startup Orbital Insight thinks it can answer those questions by analyzing satellite photos. Founder Jimi Crawford---an AI expert who has worked for NASA and Google---explains that it can do this by analyzing massive numbers of photos of oil tanks with floating lids. As a tank is depleted, the lid sinks, and the sun casts shadows on the inside of the tank changes. By detecting patterns in how those shadows change, analysts can estimate how much oil is available in all the tanks it monitors. To spot these patterns, the company uses some of the same artificial intelligence techniques Facebook and Google use to analyze photos. But instead of trying to find pictures of cats, or recognize which of your drunk friends is in that photo you took at the bar last night, Orbital Insight hopes to discover information that affects the global economy, like oil surpluses or shortages. That's one example of what could be possible with Orbital Insight's technology. Crawford says the company already is working with several hedge funds on projects such as estimating how much construction is occurring in China, how much grain was harvested last season in Russia, and how many cars are parked at big-box retailers around the world, data that could help suss out shopping trends. But Crawford wants Orbital Insight, which just raised a $8.5 million round of financing led by Sequoia Capital, to do more than help investors make smarter bets. ""We're trying to understand large-scale trends and put them together for policymakers and decision makers, including corporations and non-government organizations,"" he says. Governments or environmental organizations could, for example, use the technology to monitor deforestation. Governments and corporations have been analyzing satellite imagery for years. But it's only recently that costs have reached the point where a company like Orbital Insight could acquire fresh imagery daily without going broke. That's due in large part to the rise of ""nano-satellites""---relatively inexpensive satellites powered by smartphone technology. These satellites are cheaper to build and cheaper to launch because many of them can be packed onto a single rocket. Depending on the size of the satellite, they can be launched for anywhere from $300,000 to $2 million, says Jason Andrews, the CEO of Spaceflight Services, a company that makes and launches satellites, and provides satellite communications services. These cheap satellites aren't as powerful as traditional satellites, but they're good enough to capture photos from space. ""Pictures from space used to cost thousands dollars and it would take a month to get them,"" he says. That's why the photos you see in Google Earth aren't updated every day. But companies like SkyBox, acquired by Google last year, and Planet Labs can provide this type of imagery on a daily basis for a fraction of the cost. The rise of nano-satellites is what inspired Crawford to start Orbital Insight. But instead of making his own satellites, he decided to focus on the software. Crawford's team is using a wide range of machine learning and computer vision techniques to automatically analyze the photos they collect from imagery companies, including deep learning, a field within artificial intelligence that companies like Facebook and Google have become obsessed with. ""At NASA the problem was that the budget got eaten up mostly by hardware,"" he says. ""It's incredibly hard to build a world class company in both hardware and software. So we want to leave it to the hardware companies to do the hardware and be the best company in the software side."""
714,https://www.wired.com/2015/03/future-news-robots-writing-audiences-one/,Wired,2015,3,6,689.0," Here come the robot reporters. This week the AP announced it will use software to automatically generate news stories about college sports that it didn't previously cover. Specifically, it's turning to a content generation tool called Wordsmith, created by a Durham, North Carolina-based company called Automated Insights. It's latest case of big news organizations turning to algorithms to create content. The AP -- which is an investor in Automated Insights -- already uses Wordsmith to generate stories on corporate quarterly earnings reports. Meanwhile, automated content competitor Narrative Science provides similar services to publications such as Fortune and Big Ten Network. And a Los Angeles Times journalist used custom software to auto-generate a story minutes after an earthquake hit Los Angeles last year. But is anyone actually reading any of this machine generated content? Automated Insights CEO Robbie Allen says that's the wrong question to ask. Although the company generated over one billion pieces of content in 2014 alone, most of this verbiage isn't meant for a mass audience. Rather, Wordsmith is acting as a sort of personal data scientist, sifting through reams of data that might otherwise go un-analyzed and creating custom reports that often have an audience of one. For example, the company generates Fantasy Football game summaries for millions of Yahoo users each day during the Fantasy Football season, and it helps companies turn confusing spreadsheets into short, human readable reports. One day you might even have your own personal robot journalist, filing daily stories just for you on your fitness tracking data and your personal finances. ""We sort of flip the traditional content creation model on its head,"" he says. ""Instead of one story with a million page view, we'll have a million stories with one page view each."" Wordsmith essentially does two things. First, it ingests a bunch of structured data and analyzes it to find the interesting points, such as which players didn't do as well as expected in a particular game. Then it weaves those insights into a human readable chunk of text. You can think of it as a highly complex form of Mad Libs -- one that takes an understanding of both data and writing to create. Allen came up with the idea eight years ago, back when he was working as an engineer for Cisco. Allen, who has written ten books, wanted to create something new, so he decided to combine his passion for computer science, writing, and sports analysis into a company called StatSheet. ""The traditional approach of hiring a lot of writers wasn't attractive to me,"" he says. ""What's exciting about sports recaps is that 90 percent of what you do is write about the numbers."" Soon, however, Allen realized that the idea could be applied to any quantitative data -- not just sports. So the company changed its name to Automated Insights to bring its technology to a wide range of industries, including finance, health care and, of course, journalism. Today Wordsmith can only work with structured, quantitative data -- the sort of things you find in well formatted spreadsheets and databases. Allen says there's certainly potential for other companies to create software that can go further in automating research or writing by summarizing lengthy texts, rewriting press releases, or sifting through unstructured documents for insights. But he doubts that Automated Insights will stray from its roots in quantitative in the foreseeable future. Last month the company was acquired by private equity firm Vista Equity Partners, which also owns the sports data company STATS and business intelligence company TIBCO. By partnering with Vista's other companies, Allen says Automated Insights will have more than enough work to keep them busy. ""It's kind of a no brainer for us,"" he says. ""We have so much opportunity ahead of us in structured data, why take on a space that people have struggled with for years?"" In the meantime, expect to see more stories written for a very particular audience: you, and you alone. Correction 3/6/2015 2:10 PM: An earlier version of this piece stated that Automated Insights is based in Raleigh, North Carolina. It's actually based in Durham, North Carolina."
715,https://www.wired.com/2015/02/google-ai-plays-atari-like-pros/,Wired,2015,2,25,658.0," Last year Google shelled out an estimated $400 million for a little-known artificial intelligence company called DeepMind. Since then, the company has been pretty tight-lipped about what's been going on behind DeepMind's closed doors, but here's one thing we know for sure: There's a professional videogame tester who's pitted himself against DeepMind's AI software in a kind of digital battle royale. The battlefield was classic videogames. And according to new research published today in the science magazine Nature, Google's software did pretty well, smoking its human competitor in a range of Atari 2600 games like Breakout, Video Pinball, and Space Invaders and playing at pretty close to the human's level most of the time. Google didn't spend hundreds of millions of dollars because it's expecting an Atari revival, but this new research does offer a hint as to what Google hopes to achieve with DeepMind. The DeepMind software uses two AI techniques---one called deep learning; and the other, deep reinforcement learning. Deep-learning techniques are already widely used at Google, and also at companies such as Facebook and Microsoft. They help with perception---helping Android understand what you're saying, and Facebook know who's photo you just uploaded. But until now, nobody has really matched Google's success at merging deep learning with reinforcement learning---those are algorithms that make the software improve over time, using a system of rewards. By merging these two techniques, Google has built a “a general-learning algorithm that should be applicable to many other tasks,"" says Koray Kavukcuoglu, a Google researcher. The DeepMind team says they're still scoping out the possibilities, but clearly improved search and smartphone apps are on the radar. But there are other interesting areas as well. Google engineering guru Jeff Dean says that AI techniques being explored by Google---and other companies---could ultimately benefit the kinds of technologies that are being incubated in the Google X research labs. ""There are potential application in robots and self-driving-car kinds of things,"" he says. ""Those are all things where computer vision is pretty important."" Google says that its AI software, which it's dubbed the ""Deep Q network agent,"" got 75 percent of the score of its professional tester in 29 of the 49 games it tried out. It did best in Video Pinball. Deep Q works best when it lives in the moment---bouncing balls in Break Out, or trading blows in video boxing---but it doesn't do so well when it needs to plan things out in the long-term: climbing down ladders and then jumping skeletons in order to retrieve keys in Montezuma's Revenge, for example. Poor old Deep Q scored a big fat zero in that game. But as it improves, the DeepMind work ""could be the driving technology for robotics,"" says Itamar Arel, an artificial intelligence researcher who, like the DeepMind folks, is working on ways to merge deep learning with deep reinforcement techniques. He believes that DeepMind's technology is about 18 to 24 months away from the point where it could be used to experiment with real-world robots---and Google has its fair share of robots to test on, including the dog-like Boston Dynamics1 machines it acquired in 2013. The Nature paper doesn't describe any new technical breakthroughs, but it shows what happens when the DeepMind techniques are used on a much broader scale. ""We used much bigger neural networks, we came up with better training regimes... and trained the systems for longer,"" says Demis Hassabis, DeepMind's founder. In 2013, DeepMind described ""very early preliminary sample results,"" he says, ""these are the full results complete with a whole bunch of careful controls and benchmarks."" Hassabis won't tell us whether Google is running robot simulations too, but it's clear that the Atari 2600 work is only the beginning. ""I can't really comment on our current work, but we are indeed running simulations of all kinds of games and environments,"" he says. 1Correction: 02:26:2015 10:00 EST This story originally mis-identified the Google robotics company Boston Dynamics as Boston Robotics."
716,https://www.wired.com/2015/02/luka/,Wired,2015,2,23,733.0," With sites like Yelp, it's never been easier to find restaurant reviews. But it's often easier to ask a friend to recommend a place than it is to sift through dozens of reviews to figure out which, if any, restaurant suits your budget or dietary needs. A company called 1,000 Plateaus wants to change that with Luka, an app that's a bit like a cross between Yelp and Siri. It uses artificial intelligence to learn your preferences and scan online reviews to find the perfect places for you. It's only available in San Francisco, but the company plans to expand into other cities, such as New York, soon. You can ask Luka questions like ""Is there a good coffee shop nearby?"" or ""Is there a cheap place to eat in SoMa?"" Instead of getting a list of possibilities as you would from Yelp or Siri, you get one suggestion. If that doesn't sound good, ask for something else. You also can ask questions, like what dishes are particularly good or whether it has vegetarian or paleo options, saving you from reading reviews and menus yourself. Depending upon how the conversation goes, Luka might end up recommending someplace else. What's most impressive about Luka is how natural the conversations feel. Sure, it's obvious you're talking to a machine. And it's not hard to stump it. But Luka can parse an impressive array of questions, phrased many different ways. It's a striking example of what can be done with machine learning, and an indicator of the conversation-driven interfaces that are increasingly common, thanks to tools like Siri or Microsoft's Cortana. ""People talk about artificial intelligence as something very science fictional,"" says co-founder Eugenia Kuyda. ""We wanted to make it practical."" Artificial intelligence already is being used by companies like Google and Facebook to handle voice commands, photo recognition and other tasks. But tools like Siri are the closest most people come to something that tries to understand human language. But unlike Siri and most other virtual assistants, you must still communicate with Luka through text, not voice. But while Siri is designed mostly to answer single questions, Luka is conversational. ""Siri would not remember the second or third step of a conversation,"" Kuyda explains. ""It's one question, one answer."" Kuyda says the company doesn't plan to add voice support because Luka is designed to facilitate an ongoing conversation in which you might ask something, then return to the conversation minutes or hours later. And the company plans to add a group chat feature so people you're meeting can participate in the conversation as well, and Kuyda thinks that text messaging is the model for both of those uses. Luka's underlying technology was developed for a banking application. A Russian telco contacted Kudya's consulting company to ask about an interface for banking via text message. The telco later dropped the project, but Kudya and company were able to keep the intellectual property they'd developed, and decided to repurpose it. Kuyda had worked as a restaurant critic, and thought a restaurant recommendation engine made sense. Co-founder Philip Dudchuk, who has a degree in computational linguistics from Moscow State University, previously worked for the Russian government news agency RIA Novosti. He helped build technology to analyze the semantic structure of news---perfect preparation for making sense of online reviews. And much of the rest of the team came from Yandex, Russia's largest search engine and had extensive experience in indexing information and processing it through artificial intelligence. After spending a year developing Luka---formerly known as IO---in Russia, the team moved to San Francisco last year and joined the startup accelerator Y Combinator. The plan is to enable Luka to make reservations at restaurants, and for 1,000 Plateaus to receive a small fee for each reservation. Later, the company hopes to apply the technology to other areas, such as entertainment and travel. But the underlying technology could be applied even more broadly, and that's reflected in the company's name, a reference to a strange and sprawling book A Thousand Plateaus by French philosophers Gilles Deleuze and Félix Guattari. In the book, the authors propose a non-hierarchical mode of organization called the ""rhizome,"" named for the underground roots that sprout from the stems of certain plants. ""You never know where a conversation will go, what it will be about, or where it's going to grow,"" she says. ""Just like a rhizome."""
717,https://www.wired.com/2015/02/luminoso/,Wired,2015,2,12,386.0," Catherine Havasi wants to help businesses understand what people are saying on Twitter, Facebook, and other online feeds and forums---right now. Havasi is the co-founder and CEO of artificial intelligence startup Luminoso, a four-year-old company that spun out of the MIT Media Lab. Basically, the company uses natural language processing and machine learning technologies to help businesses analyze what consumers are saying across all kinds of communications channels, including news sites, blogs, online forums, and, yes, social media. Previously, the company examined only archived data, but now it can analyze all that chatter in real time. Today, the company launched a tool called Compass, which helps businesses not only track online discussions as they're happening, but also instantly respond as need be. If consumers start complaining about a product defect, say, a business can start the damage control without delay. ""What Compass does is surface things as they come up,"" Havasi says. ""It figures out what new issues are emerging and deals with the way language changes dynamically without needing a person in the loop to do moderation."" With the tool, the company aims to compete with a long list of other text analytics companies, from the Chicago-based Network Insights to Lexalytics and Clarabridge. This past summer, Luminoso used its Compass technology to provide live analysis of the World Cup on Sony's One Stadium Live website, and now it's offering the tool to the rest of the world. Previously, the company's commercial tools only analyzed archived data---not stuff that's streaming across social media at this very moment. Compass works with Twitter out of the box, but it also comes with an API, or application programming interface, that lets you plug it into other online forums. And according to Havasi, it can train itself to search for relevant information. Right now, if businesses want to track a certain topic, an actual person must manually enter keywords they want to look for, while Compass can generate relevant keywords on the fly. But it doesn't merely search for keywords. It tries to ""understand"" what people saying, using natural language processing in tandem with a database of English concepts that grew out of work at MIT. Havasi even says the system can understand emojis. ""I think that’s a really important aspect,"" she says, ""especially as we work more in social."""
718,https://www.wired.com/story/robot-propaganda/,Wired,2015,2,5,874.0," Humanity has been advancing the field of propaganda for as long as we've been at war or had political fights to win. But today, propaganda is undergoing a significant change based on the latest advances in the fields of big data and artificial intelligence. Over the past decade, billions of dollars have been invested in technologies that customise ads increasingly precisely based on individuals' preferences. Now this is making the jump to the world of politics and the manipulation of ideas. Some recent military experiments in computational propaganda indicate where this could be taking us. In 2008, the US State Department, through its ""foreign assistance"" agency USAID, set up a fake social network in Cuba. Supposedly concerned with public health and civics, its operatives actively targeted likely dissidents. The site came complete with hashtags, dummy advertisements and a database of users' ""political tendencies"". For an estimated $1.6m (£1m), USAID was, between 2009 and 2012, able to control a major information platform in Cuba with potential to influence the spread of ideas among 40,000 unique profiles. Building on this project in 2011, USCENTCOM (United States Central Command) -- the US military force responsible for operations in the broader Middle East region -- awarded a contract to a Californian firm to build an ""online persona management service"", complete with fake online profiles that have convincing backgrounds and histories. The software will allow US service personnel to operate up to ten separate false identities based all over the world from their workstations ""without fear of being discovered by sophisticated adversaries"". These personas allow the military to recruit, spy on and manipulate peoples' behaviour and ideas. Such projects represent the first wave of computational propaganda, but they are constrained in their scale (and ultimately their effectiveness) by the simple fact that each profile has to be driven by an actual human on the other side. In 2015, we will see the emergence of more automated computational propaganda -- bots using sophisticated artificial intelligence frameworks, removing the need to have humans operate the profiles. Algorithms will not only read the news, but write it. These stories will be nearly indistinguishable from those written by humans. They will be algorithmically tailored to each individual and employed to change their political beliefs or to manipulate their actions. Already, Mexican drug cartels have employed propaganda bots to target messages at individual members of the public, to convince them that the death of a journalist in Las Choapas had nothing to do with hit men employed by the gangs. This type of propaganda can be produced at an almost limitless scale using the estimated ten million social-media bots. Such bots are currently available for rent on online hacker forums for between $5 and $200 per thousand, depending on how ""human"" -- and therefore how effective -- they appear. The Russian foreign intelligence service has announced a 30-million-ruble (£500,000) contract for the ""development of special software for automated information dissemination in major social networks"". In 2015 we will also see the first results from initial field tests of the US IARPA (Intelligence Advanced Research Projects Activity) project to deploy propaganda bots in South America in an attempt to influence local political opinion. It is still early days -- many of the bots deployed in 2015 will be programmed to use relatively simple heuristic techniques to imitate intelligence. But, powered by rapid advances in artificial intelligence, propaganda bots will soon run on genetic algorithms that let their ideas and messaging evolve, based on the resonance and effectiveness of previous messages. We are likely to see versions of these bots deployed on US audiences as part of the 2016 presidential election campaigns, and not only by the traditionally more tech-savvy Democrats. This technology exploits the simple fact that we are much more impressionable than we think. Facebook's recent experiments to modify users' moods show us that the very language we use to communicate is subject to manipulation based on the stories that the Facebook algorithm chooses to show us. Furthermore, researchers at MIT have shown that a false upvote cast early on can improve the public response to a story by 25 per cent; a single early downvote can make an otherwise good story be perceived as a low-quality piece of journalism. In 2015, the propaganda bots will start to use this knowledge to influence news feeds -- automated ""friends"" will like, retweet and comment on stories that are in line with their propaganda goals. We can also employ bots to help us determine if there are attempts at propaganda underway. Reactions to the downing of Malaysian Airlines flight MH17 over Ukraine show that the Russian and US media want the global audience to view things differently. We can employ algorithms to monitor mainstream media messaging out of Russia, compare that to what we are seeing in the US outlets, and flag substantive differences in language. We can also employ bots to monitor all of the millions of edits that happen daily on sites such as Wikipedia and uncover attempts to change language from ""terrorists"" to ""Ukrainian soldiers"", though it won't tell us which version is true. For that we still need humans to weigh the evidence. Read more from The WIRED World in 2015 here."
719,https://www.wired.com/2015/01/kimera-systems/,Wired,2015,1,26,999.0," Imagine a ""smart refrigerator"" that lets you order milk or soda or coffee from your local grocery store whenever you're running low. It's an old idea, but Mounir Shita looks at it in a new way. Shita believes such a contraption could generate about $14,000 in revenue over its lifespan. Most of this would go to the grocer, and in all likelihood, some sort of middle-man would take a cut (think Google or Amazon). But Shita wants to replace those middlemen and share the revenues with the company that made the fridge and the ISP that provides the internet connection. With his new company, Kimera Systems, he wants to help create all sorts of internet-connected devices without help from the Googles and the Amazons, spreading the wealth to electronics makers and broadband providers instead. For consumers, who's actually taking a cut of all these purchases might not be important. But Shita sees it as a way to fund Kimera's real purpose: the creation of artificially intelligent virtual assistants that can help you with just about any aspect of your life, from handling grocery shopping to scheduling meetings. Kimera is fashioning a technical solution it calls Nigel, and it includes three things. There's a Siri-like virtual assistant that sits on your smartphone, letting you control your smart refrigerator. There's a ""personal cloud"" service that stores data on behalf of your frig. And there's server software that lets the frig manufacturer and ISP keep track of who what groceries you ordered, collect payments, and disperse them among all the companies involved. The hope is that manufacturers and ISPs will use this system to build not just smart refrigerators but, well, whatever other internet-connected devices they want to build. And because they can control where the money goes, they have ample incentive to build them. You might argue that this could give ISPs a way of taxing all transactions that happen on their networks. But middle men---ranging from credit card payment processors to marketplace sites like eBay and Amazon---are already taking a cut of so much that happens online. For Kimera, the trick lies in building a virtual assistant that's better than what you can get from the Googles and the Amazons of the world. And that's where Shita's true passion lies. Shita has always been fascinated by the future. As a kid in Norway, he spent his time, like so many other geeks, taking things apart and trying to build new inventions. ""My room looked like something out of Star Trek,"" he says. ""If the time machine had been invented, I would already have gone to live in the future."" In 2012, after earning degrees in electrical engineering and computer science and moving to the US to start a few startups, he met Nigel Deighton---the virtual assistant's namesake---and the two started spitballing ideas for what the future should actually be like, and how to get there. They decided they wanted to be able to talk to computers in a much more conversational, human-like manner than tools like Siri enable today. And they set out to make it happen. Instead of reviewing the literature, which they worried would limit their creativity, they decided to try coming up with a way to do artificial intelligence on their own first own. Shita says he fully expected to just end up ""reinventing the wheel,"" but soon they had a system that Shita says worked surprisingly well. That's when Shita and Deighton realized they had enough to start a real company. Unfortunately, Deighton passed away in 2013. The virtual assistant Nigel was named in his honor. The idea is to understand the ""why"" and not just ""what"" of a voice command. One of Shita's favorite examples is the statement: ""I'm out of Coke."" If you're at home, you might mean that you want to add Coke to your shopping list. If you're at a restaurant, you might want Nigel to text your waiter. If you're a drug deal, well, use your imagination. Shita says Nigel is smart enough to take context like location into account while determining what you want it to do. And it goes further still. If you're at a restaurant and Nigel sees that you have a business meeting on your calendar, it will mute your phone. If you're running late for the meeting---and everyone else a the meeting also happens to use Nigel---you can ask ""is everyone there?"" and it will give you an answer. It's hard to tell just how advanced Nigel really is. Unlike the academics at companies like Facebook and Google who pioneered the ""deep learning"" subfield of AI, Kimera hasn't published the algorithm for peer review. Shita says that's because of where the company is in the patent filing process---and because outside experts who have seen the system are under strict non-disclosure agreements from their respective companies. But Steve Taylor, an AI expert and former VP of business development for robotics company RoboKind who consulted Kimera on its intellectual property, says the company has developed some pretty sophisticated technology. ""I'm a believer. It should work and it should learn."" Shita was willing to say that like deep learning systems, Kimera is based on creating networks of artificial neurons that share information with each other. He says Kimera's approach isn't in competition with deep learning, but is rather something that could actually prepare a data set to be passed along to a deep learning system for further processing. Of course, we can't truly judge Nigel's talents until it's out in the wild. It must compete head-to-head with other virtual assistants from companies like Microsoft, Google, and a whole host of startups, such as Viv, a company founded by Siri's creators that aspires to create a smarter artificial intelligence that, like Nigel, can weave together data from many sources to intuit the context and purpose of a request. But Kimera has something that no everyone else has: a business model that could appeal to some of the world's largest companies."
720,https://www.wired.com/2015/01/persado/,Wired,2015,1,22,588.0," How does a wireless carrier get you to renew your phone contract? How does a bank persuade you to sign up for a new credit card? For Alex Vratskides, it's all about the messaging. Vratskides is the CEO and co-founder of Persado, a company that's in the business of persuasion. Persado offers software that lets businesses automate the way they generate digital marketing messages, tapping natural language processing and semantic algorithms to transform a company’s message into the most persuasive form possible---or at least that's the idea. According to Vratskides, the software can help businesses persuade you to, say, sign up for a loyalty program or upgrade from a ""freemium"" service to a paid subscription. Today, the company boasts such high-profile clients as Best Buy, Citi, Verizon Wireless and Neiman Marcus, and investors seem to like the idea as well. On Thursday, the company announced that it closed $21 million in Series B funding led by venture capitalist StarVest Partners, with new participation from Citi Ventures and American Express Ventures, as well as an old investor, Bain Capital Ventures. This new round of funding follows a $15 million round from back in February 2013, and comes on top of the $34 million that had already been invested in Persado before it was spun off from its original firm, a digital marketing company called Upstream. Persado joins a slew of other companies that have infused artificial intelligence into products we encounter in our everyday lives. Just this week, Microsoft acquired a startup that will add A.I. to its Office suite. Everyone from Microsoft to Google, Facebook, and Baidu is using A.I. to beef up their online services, with products ranging from translation services to image recognition. And even as the debate around AI ethics heats up, smaller startups are hoping to ride this ever-cresting wave. Vratskides, who has training in mathematics, says the idea for Persado grew out of the text messaging marketing campaigns he had to run at Upstream. He says he could see the response rates change when he tweaked the language of the texts that when out, and grew obsessed with improving them, applying his own particular formula to the problem. ""When you only have 100 or 200 characters---or what I call confined space---you can phrase something in millions of ways. But it is a finite problem,"" Vratskides says. ""It took us a couple of years of R&D to figure out how to formulate it, but we found you can actually solve it like a mathematical equation, and optimize it."" Essentially, Persado’s algorithm takes a company's marketing message---which can come in the form of email, a display ad, SMS, or anything else that is text-based---and runs it through an algorithm. The system parses the language used and breaks the wording down, then the algorithm determines certain categories that could end up making the messaging more engaging, emotionally appealing, and action-oriented. Marketers supply some information about what they are trying to accomplish, and then Persado sends messages out to a sampling of customers, gauging their reaction. The better the reaction, the more likely the messaging will be used for the wider pool of customers. According to Vratskides, the tech has helped Citi increase email open rates by 70 percent and click-to-open rates by 114 percent. But he aims to change more than just credit card campaigns. One day, he says, Persado could help persuade patients stick to doctors' recommendations or donate money to a good cause. ""We belong everywhere [that] communication with consumers is done,"" he says."
721,https://www.wired.com/2015/01/karpathy/,Wired,2015,1,21,791.0," Andrej Karpathy knows what it's like to compete with artificial intelligence. He first went head-to-head with an artificial intelligence algorithm in 2011. A team of Stanford University researchers had just built the world's most effective image-recognition software, and he wanted to see how well his very real brain stacked up against their digital creation on what was, at the time, a standard image recognition test. The Stanford software analyzed a pool of about 50,000 images, slotting each into one of 10 categories, such as ""dogs,"" ""horses,"" and ""trucks."" It was right about 80 percent of the time. Karpathy took the same test and completely smoked the AI code, scoring 94 percent. Karpathy, himself a graduate student at Stanford, thought humans would beat machines on this type of test for a long time. ""[I]t will be hard to go above 80 percent,"" he wrote in a blog post, referring to AI algorithms, ""but I suspect improvements might be possible up to range of about 85-90 percent."" Boy, was he wrong. Last year, a system built by researchers at Google aced another, more complex, image recognition test, called ImageNet, scoring 93.4 percent accuracy (you can see how Google's software performed on the test here). Again, Karpathy, with some colleagues at Stanford, went head-to-head with the system. But this time, they bombed what was a much more complex test, initially scoring about an 85 percent accuracy rate. Comparing the ImageNet test to the 2011 test software isn't exactly an apples-to-apples comparison, but here's the point: Humans were easily beating AI software in 2011; now that's not the case. Not by a long shot. The story goes a long way towards describing the excitement surrounding current artificial intelligence, which spans companies from Google to Facebook to IBM to Baidu. All of these giants are pouring big money into a burgeoning field called deep learning. Loosely modeled on the way the brain itself is able to accumulate knowledge, deep learning algorithms have been winning the ImageNet competition since 2013, and they've yielded remarkable results in the area of speech recognition, video recognition, and even financial analysis lately. This is causing a shake up in the field of AI, as problems that have been considered unsolvable for a very long time are suddenly being solved, says Stuart Russell, a University of California, Berkeley, professor and artificial intelligence expert. That said, computers still have a lot to learn. One reason Karpathy and his colleagues bombed against the Google systems was the way that ImageNet handles things like dogs. When he took that 2011 test, it had just one category for dogs. But in 2014, the test expected you to discern an artificial-mind-blowing 200 breeds. That meant Karpathy had to know the difference between, say, Rhodesian ridgebacks and Hungarian pointers. ""When I saw all these dogs come up, I was like: ""Oh no. [The machine is] going to get this image, and I'm just struggling and sweating to label this precise breed of dog."" So Karpathy entered his own kind of AI boot camp, teaching himself the categories of images that the ImageNet test expected him to know, and becoming a minor authority on dog breeds in the process. Two weeks later, and after about 50 hours of training and testing himself by clicking on random pictures, he bested the machines. He was right 94.9 percent of the time, a 1.7 percent margin over Google's work. Chalk one up for humanity, but it wasn't easy. ""It was a bit draining, but I felt that it was very important to get the human accuracy,"" he says. At the same time, Karpathy and his colleagues want AI to improve. They're working to determine how the flaws in digital systems can be eliminated. ""We're trying to see if the computers perform at a human level, but we're also trying to analyze their mistakes,"" he says. On the test, Karpathy could typically beat machines when he was presented with the image of something abstract. He could instantly spot, for example, a drawing of a bow. He could read the words ""salt shaker"" on a cone and understand what we was looking at. ""Computers are not very good at abstract things,"" he says. They're also not good at figuring out images in three-dimensional reality. A computer might be able to spot a Jack Russell terrier. But reckoning its size, or figuring out how it is positioned relative to some other object in the same room? That's another matter. It's also one that the Googles of the world are, no doubt, trying to solve as they dream of computers that can interpret images with the depth and subtlety of humans. ""Image recognition is important,"" Karpathy says, ""but it's just a small piece of the puzzle."""
722,https://www.wired.com/2015/01/facebook-open-sources-trove-ai-tools/,Wired,2015,1,16,447.0," Facebook is opening up many of the AI tools it uses to drive its online services. Most of these tools seek to take better advantage of artificial intelligence algorithms that Facebook and other researchers have already published in academic journals, and the hope is that this newly open sourced code can save outsiders quite a bit of time as they build their own AI services, involving everything from speech and image recognition to natural language processing. The algorithms alone aren't always enough. ""Someone has to go and implement the algorithm in a program, and that's not trivial in general,"" says Facebook artificial intelligence researcher and software engineer Soumith Chintala. ""You have to have a lot of skill to implement it efficiently."" Chintala says the open source project could help research labs and startups that don't have a lot of resources and wind-up spending most of their time just implementing existing algorithms instead of doing new research. In that sense, Facebook will benefit too. ""Even though we don't collaborate day-to-day with that world, it could provide a general catalyst to the community and that will benefit us indirectly,"" he says. The tools came out of the Facebook Artificial Intelligence Research lab, a project started within Facebook about a year ago to research a subfield of artificial intelligence called ""deep learning,"" which seeks to model certain behaviors of the brain in order to create software that can learn and make predictions. With Facebook, Google, and Microsoft leading the way, deep learning is poised to hone so many of the online services we used on a daily basis. Facebook already uses deep learning to filter your Facebook feed, making intelligence guesses as to which items you'll find most interesting, and to recognize faces in the photos you upload. But eventually, the company expects to create digital assistants that can, for example, stop you from posting drunk selfies in the middle of the night. What Facebook released today is a set of modules for Torch, an open source computing framework for working with deep learning widely used in academia and by companies like Google and Twitter. Torch already includes several deep learning algorithms, but Chintala says Facebook's are far faster and more efficient. That will allow researchers to tackle much larger problems than ever before, he says. For example, one team of researchers Facebook has already worked with were able to create a photo recognition tool that can tell what physical poses---standing, sitting, lying down, etc.---characterized people in photos. ""We benchmarked our code, and these are the fastest open source implementations out there,"" he says. ""People didn't explore certain areas because they didn't think it was possible and now they are."""
723,https://www.wired.com/2015/01/ai-arrived-really-worries-worlds-brightest-minds/,Wired,2015,1,16,1067.0," On the first Sunday afternoon of 2015, Elon Musk took to the stage at a closed-door conference at a Puerto Rican resort to discuss an intelligence explosion. This slightly scary theoretical term refers to an uncontrolled hyper-leap in the cognitive ability of AI that Musk and physicist Stephen Hawking worry could one day spell doom for the human race. That someone of Musk's considerable public stature was addressing an AI ethics conference—long the domain of obscure academics—was remarkable. But the conference, with the optimistic title ""The Future of AI: Opportunities and Challenges,"" was an unprecedented meeting of the minds that brought academics like Oxford AI ethicist Nick Bostrom together with industry bigwigs like Skype founder Jaan Tallinn and Google AI expert Shane Legg. Musk and Hawking fret over an AI apocalypse, but there are more immediate threats. In the past five years, advances in artificial intelligence—in particular, within a branch of AI algorithms called deep neural networks—are putting AI-driven products front-and-center in our lives. Google, Facebook, Microsoft and Baidu, to name a few, are hiring artificial intelligence researchers at an unprecedented rate, and putting hundreds of millions of dollars into the race for better algorithms and smarter computers. AI problems that seemed nearly unassailable just a few years ago are now being solved. Deep learning has boosted Android's speech recognition, and given Skype Star Trek-like instant translation capabilities. Google is building self-driving cars, and computer systems that can teach themselves to identify cat videos. Robot dogs can now walk very much like their living counterparts. ""Things like computer vision are starting to work; speech recognition is starting to work There's quite a bit of acceleration in the development of AI systems,"" says Bart Selman, a Cornell professor and AI ethicist who was at the event with Musk. ""And that's making it more urgent to look at this issue."" Given this rapid clip, Musk and others are calling on those building these products to carefully consider the ethical implications. At the Puerto Rico conference, delegates signed an open letter pledging to conduct AI research for good, while ""avoiding potential pitfalls."" Musk signed the letter too. ""Here are all these leading AI researchers saying that AI safety is important,"" Musk said yesterday. ""I agree with them."" Nine researchers from DeepMind, the AI company that Google acquired last year, have also signed the letter. The story of how that came about goes back to 2011, however. That's when Jaan Tallinn introduced himself to Demis Hassabis after hearing him give a presentation at an artificial intelligence conference. Hassabis had recently founded the hot AI startup DeepMind, and Tallinn was on a mission. Since founding Skype, he'd become an AI safety evangelist, and he was looking for a convert. The two men started talking about AI and Tallinn soon invested in DeepMind, and last year, Google paid $400 million for the 50-person company. In one stroke, Google owned the largest available talent pool of deep learning experts in the world. Google has kept its DeepMind ambitions under wraps—the company wouldn't make Hassabis available for an interview—but DeepMind is doing the kind of research that could allow a robot or a self-driving car to make better sense of its surroundings. That worries Tallinn, somewhat. In a presentation he gave at the Puerto Rico conference, Tallinn recalled a lunchtime meeting where Hassabis showed how he'd built a machine learning system that could play the classic '80s arcade game Breakout. Not only had the machine mastered the game, it played it a ruthless efficiency that shocked Tallinn. While ""the technologist in me marveled at the achievement, the other thought I had was that I was witnessing a toy model of how an AI disaster would begin, a sudden demonstration of an unexpected intellectual capability,"" Tallinn remembered. Deciding the dos and don'ts of scientific research is the kind of baseline ethical work that molecular biologists did during the 1975 Asilomar Conference on Recombinant DNA, where they agreed on safety standards designed to prevent manmade genetically modified organisms from posing a threat to the public. The Asilomar conference had a much more concrete result than the Puerto Rico AI confab. At the Puerto Rico conference, attendees signed a letter outlining the research priorities for AI—study of AI's economic and legal effects, for example, and the security of AI systems. And yesterday, Elon Musk kicked in $10 million to help pay for this research. These are significant first steps toward keeping robots from ruining the economy or generally running amok. But some companies are already going further. Last year, Canadian roboticists Clearpath Robotics promised not to build autonomous robots for military use. ""To the people against killer robots: we support you,"" Clearpath Robotics CTO Ryan Gariepy wrote on the company's website. Pledging not to build the Terminator is but one step. AI companies such as Google must think about the safety and legal liability of their self-driving cars, whether robots will put humans out of a job, and the unintended consequences of algorithms that would seem unfair to humans. Is it, for example, ethical for Amazon to sell products at one price to one community, while charging a different price to a second community? What safeguards are in place to prevent a trading algorithm from crashing the commodities markets? What will happen to the people who work as bus drivers in the age of self-driving vehicles? >To the people against killer robots: we support you. Itamar Arel is the founder of Binatix, a deep learning company that makes trades on the stock market. He wasn't at the Puerto Rico conference, but he signed the letter soon after reading it. To him, the coming revolution in smart algorithms and cheap, intelligent robots needs to be better understood. ""It is time to allocate more resources to understanding the societal impact of AI systems taking over more blue-collar jobs,"" he says. ""That is a certainty, in my mind, which will take off at a rate that won’t necessarily allow society to catch up fast enough. It is definitely a concern."" Predictions of a destructive AI super-mind may get the headlines, but it's these more prosaic AI worries that need to be addressed within the next few years, says Murray Shanahan, a professor of cognitive robotics with Imperial College in London. ""It's hard to predict exactly what's going on, but we can be pretty sure that they are going to affect society."""
724,https://www.wired.com/2015/01/elon-musk-ai-safety/,Wired,2015,1,15,336.0," Elon Musk is worried that artificial intelligence research could go wrong---very wrong. This may seem unexpected, coming as it does from the architect of the conceptual high-speed transportation system Hyperloop, and the CEO of such moonshot-seeking companies as SpaceX and Tesla Motors. But Musk is so committed to this point of view that on Thursday, he announced a donation of $10 million to the Future of Life Institute (FLI), which will run a global research program aimed at keeping AI “beneficial to humanity.” In other words, Musk wants to keep AI from running loose and growing into something that's a real danger to humans, a fear he's expressed before. Last week, an open letter from the Future of Life Institute circulated, containing the signatures of AI scientists who called for research that ensures AI systems aren't being used for evil. With this donation, Elon Musk voiced his support for the movement. ""Here are all these leading AI researchers saying that AI safety is important,” Musk said of the effort. ""I agree with them, so I'm today committing $10M to support research aimed at keeping AI beneficial for humanity."" The program---and the $10 million donation---will fund research around the world supporting this mission. On Monday, the Future of Life Institute will open up a portal allowing researchers to apply for grants to the program. Beyond hardcore AI science, money will be awarded to researchers in other fields as well, including economics, law, ethics and policy. The FLI says the program will be open to individuals who work in academia, industry, or even independently. For so long, the artificially intelligent future has been fodder for Hollywood and science fiction, or discussed abstractly in philosophy. But as behemoths like Google incorporate AI into the very core of its current and future technologies, and as a wave of smaller startups build businesses on top of the science, we can no longer deny its very real rise. AI is here to stay; but the debate over AI ethics is just beginning."
725,https://www.wired.com/story/computers-digital-footprints-judge-better/,Wired,2015,1,12,557.0," In Spike Jonze's science-fiction film Her, two star-crossed lovers -- an intelligent computer operating system and a man -- embark on an impossible relationship. Reflecting human-like capabilities, the operating system, personified by female voice Samantha embeds itself deeper into unsuspecting Theodore Twombly's psyche and life. Switch over to present day reality, where a new study published today in the journal PNAS, reveals that by mining Facebook Likes, computers can suss out your personality traits better than your nearest and dearest. Not quite an operating systems love story, but surreal enough. By deploying a new algorithm, researchers from the University of Cambridge and Stanford University have ""calculated the average number of Likes artificial intelligence needs to draw personality inferences about you."" And they assert that these mean machines can do it as accurately as ""your partners and parents."" For this experiment, the researchers took a sample of 86,220 volunteers on Facebook who provided access to their Likes, and who completed a personality questionnaire through the ""myPersonality"" app. The results supplied ""self-reported personality scores"" for the ""big five"" traits of psychological practice. Namely; openness, conscientiousness, extraversion, aggreeableness, and neuroticism. This data allows researchers to assess which Likes equate with which traits. For example, liking ""meditation"" reveals higher levels of openness. The next stage allowed the friends and family of users of the ""myPersonality"" app to judge the psychological traits of the user with a shorter version of the questionnaire. In this case, ten items versus the 100-item list completed on the ""myPersonality"" app. To ensure fairness and accuracy, the researchers corroborated these online personality judgements with a ""meta-analysis of previous psychological studies over the decades, which analysed how people's peers and family judged their personality. The researchers determine that if there are enough likes, the computers can gauge a participant's self-reported personality better than their family or partner. ""Big Data and machine learning provide accuracy that the human mind has a hard time achieving,"" says Dr Michal Kosinski, co-author and researcher at Stanford. ""Humans tend to give too much weight to one or two examples, or lapse into non-rational ways of thinking."" Labelling it as an ""important milestone"", which might pave the way towards more social human-computer interactions, the researchers acknowledge that the findings reveal how computers can unravel a person's psychological traits through pure data analysis. So what does this all mean for human-computer interactions? According to lead author Wu Youyou, from Cambridge's Psychometrics Centre, in the future, computers could judge our psychological traits and react accordingly. ""In this context, the human-computer interactions depicted in science fiction films such as Her seem to be within our reach,"" says Youyou. The researchers assert that automated, accurate and cheap personality assessments could have multiple impacts on societal and personal decision making. ""The ability to judge personality is an essential component of social living -- from day-to-day decisions to long-term plans such as whom to marry, trust, hire, or elect as president,"" noted Dr David Stillwell, a Cambridge co-author. This research reflects the potential for AI to get to know us intimately by mining through our data, yet researchers also acknowledge that concerns over privacy may be stoked as this technology develops. ""We hope that consumers, technology developers, and policy-makers will tackle those challenges by supporting privacy-protecting laws and technologies, and giving users full control over their digital footprints,"" asserts Kosinski."
726,https://www.wired.com/2015/01/virtual-email-assistant/,Wired,2015,1,12,721.0," Dennis Mortensen remembers the exact moment he decided he needed a better way to schedule meetings over email. He had just finished counting how many appointments he had in 2012. The number was 1,019. And of those 1,019 meetings, he had rescheduled 672 of them. “I looked at those two numbers, and I cried a little thinking about how much time I invested in emailing people,” Mortensen recalls. Mortensen shared his grievances with a good friend, Alex Poon, and the two tried a little experiment: They agreed to handle each other’s calendar appointments for the next couple of months. To their surprise the system worked out pretty well, even without a deep knowledge of each other’s preferences, such as knowing who the other person was willing to shift their schedule around for in order to meet. “We also started to see how quickly some of the responses clustered,” Mortensen says. “There’s a way people deal with meetings---how they set them up, postpone them, show gratitude, add new participants---and the whole thing started to crystallize in that moment.” So Mortensen and Poon, both analytics geeks, launched a startup called X.ai to work on an intelligent virtual assistant for scheduling meetings over email. They named their bot Amy, who has become another addition in the growing arsenal of intelligent, “invisible” software designed to automate the tedious minutiae of everyday life. Projects in this growing space range from a small decision such as deciding what to wear in the morning to big, complicated ones, like the algorithms that steer Google’s self-driving car. The X.ai system is still in closed beta, but it’s currently testing the virtual assistant internally with a few users. You don’t have to download an app or visit a website to use Amy (or Andrew, her male counterpart, if you prefer); she simply has her own email address. When someone emails you with a request for an appointment, you cc Amy and she takes it from there, handling the back and forth in a separate email chain. Since she can peek into your schedule, she can make nuanced suggestions, like meeting at your own workplace if you have a number of other appointments lined up in the same location that day. She can even handle typos. To pull this off, Amy uses an artificial intelligence technology known as natural language processing, drawing information from the email responses, breaking the sentences down, and attempting to classify each chunk properly. If the interaction is simple enough---the task is simply getting an appointment on the calendar---Mortensen claims Amy is accurate 98 percent of the time. She does a bit worse, he says, with more complicated scenarios. Say, for instance, two people had a meeting all set up for the next day, and late one night, one of them suddenly realized the time he'd agreed to was double-booked with another appointment. He might send a quick email to Amy at 1 a.m. trying to reschedule ""tomorrow's meeting."" Since 1 a.m. is technically past midnight, Mortensen explains, “Amy would literally interpret that as the next day, even if that wasn’t the sender’s intention.” But Mortensen says they're working on it. Richard Socher, a one-time natural language processing researcher at Stanford University and current CTO of artificial intelligence startup MetaMind, says a portion of a system like the kind X.ai is trying to create can indeed be automated, but only up to a point. Things like personal preference matter, of course. There’s also time differences, locations of meetings, and other potential sources of real-world confusion. Socher points out that the problem could be more easily tackled by a player like Google, which has enough of a range of products that it could figure out where you live, where you work, and the current traffic. “There are a couple of situations that will require too much world knowledge, situational knowledge, and personal knowledge. That knowledge is going to be hard to collect,” he says. Even Mortensen acknowledges that X.ai’s final product may never be perfect. But he still thinks there’s potential to release a useful tool. “It needs a bit more data to work with, and we’ll work on it until we can get it as accurate as possible,” he says. “But I do have a fallback: going back to the user and asking them what they mean.”"
727,https://www.wired.com/2015/01/dato-machine-learning/,Wired,2015,1,8,506.0," Carlos Guestrin is riding the new wave of artificial intelligence. Guestrin is a professor of machine learning at the University of Washington and the brains behind an open source project called GraphLab, a freely available tool originally designed to help machines analyze ""graphs""---i.e. the online relationships between people and the stuff they use on the net. In May 2013, he launched a startup around this machine learning software, called it GraphLab too. And this past fall, the startup's first commercial product was released. But on Thursday, in announcing that it had received an additional $18.5 million in funding, the startup also changed its name to Dato. According to Guestrin, the new name is meant to show that the company's software can handle all sorts of machine learning tasks---not just graph analysis. Machine learning is what Amazon uses to automatically give you product recommendations. It's what Facebook uses to identify faces in photos. And with Dato, Guestrin is offering software that engineers and data scientists can use to build all sorts of systems that ""learn"" to identify and analyze all sorts of stuff stores in databases, from tables to text to images. Dato represents yet another contender in the arena of software aiming to make machine learning technologies accessible not just to internet giants but to startups---or anyone who has a creative idea. Such companies, which run the gamut from bigger players like Microsoft to smaller startups, such as Clarifai and MetaMind, aim to serve those who want to imbue their services with artificial intelligence but may not necessarily have the time or resources to invest in an internal AI team. Dato---and similar players---offer a kind of “solution toolkit” that gives software engineers and data scientists an easy way to imbue their applications with predictive capabilities. ""You might have an inspiring idea for an application, but one that needs to make predictions and needs intelligence,"" Guestrin explains, ""and you could use our tools, simplified models and auto-tuning algorithms, so that even if you don’t have a data science background, you can deploy an application that’s really robust quite easily."" Some companies build their own machine learning services, including behemoths like Google and even smaller companies like Netflix. But as Guestrin points out, the industry suffers from a shortage of data scientists: according to a McKinsey analysis, the U.S. alone faces a shortage of 140,000 to 190,000 people with analytical expertise and 1.5 million managers and analysts with the skills to understand and make decisions based on the analysis of big data. According to Guestrin, several big-name companies already use Dato's software. Pandora uses it to help drive its music recommendation service. Adobe’s social network for designers and job recruiters, Behance, uses it to match the right designers to the right openings. Real estate database Zillow uses it to fine-tune its estimates of how much properties cost in the market. And other clients include PayPal and Cisco. Dato’s tools, Guestrin says, have been deployed in a broad range of areas, from fraud detection to analysis of customer sentiment."
728,https://www.wired.com/story/simple-pictures-ai/,Wired,2015,1,6,1470.0," Look at these black and yellow bars and tell me what you see. Not much, right? Ask state-of-the-art artificial intelligence the same question, however, and it will tell you they're a school bus. It will be over 99 percent certain of this assessment. And it will be totally wrong. Computers are getting truly, freakishly good at identifying what they're looking at. They can't look at this picture and tell you it's a chihuahua wearing a sombrero, but they can say that it's a dog wearing a hat with a wide brim. A new paper, however, directs our attention to one place these super-smart algorithms are totally stupid. It details how researchers were able to fool cutting-edge deep neural networks using simple, randomly generated imagery. Over and over, the algorithms looked at abstract jumbles of shapes and thought they were seeing parrots, ping pong paddles, bagels, and butterflies. The findings force us to acknowledge a somewhat obvious but hugely important fact: Computer vision and human vision are nothing alike. And yet, since it increasingly relies on neural networks that teach themselves to see, we're not sure precisely how computer vision differs from our own. As Jeff Clune, one of the researchers who conducted the study, puts it, when it comes to AI, ""we can get the results without knowing how we're getting those results."" Evolving Images to Fool AI One way to find out how these self-trained algorithms get their smarts is to find places where they are dumb. In this case, Clune, along with PhD students Anh Nguyen and Jason Yosinski, set out to see if leading image-recognising neural networks were susceptible to false positives. We know that a computer brain can recognise a koala bear. But could you get it to call something else a koala bear? To find out, the group generated random imagery using evolutionary algorithms. Essentially, they bred highly-effective visual bait. A program would produce an image, and then mutate it slightly. Both the copy and the original were shown to an ""off the shelf"" neural network trained on ImageNet, a data set of 1.3 million images, which has become a go-to resource for training computer vision AI. If the copy was recognised as something -- anything -- in the algorithm's repertoire with more certainty the original, the researchers would keep it, and repeat the process. Otherwise, they'd go back a step and try again. ""Instead of survival of the fittest, it's survival of the prettiest,"" says Clune. Or, more accurately, survival of the most recognisable to a computer as an African Gray Parrot. Eventually, this technique produced dozens images that were recognised by the neural network with over 99 percent confidence. To you, they won't seem like much. A series of wavy blue and orange lines. A mandala of ovals. Those alternating stripes of yellow and black. But to the AI, they were obvious matches: Star fish. Remote control. School bus. Peering Inside the Black Box In some cases, you can start to understand how the AI was fooled. Squint your eyes, and a school bus can look like alternating bands of yellow and black. Similarly, you could see how the randomly generated image that triggered ""monarch"" would resemble butterfly wings, or how the one that was recognised as ""ski mask"" does look like an exaggerated human face. But it gets more complicated. The researchers also found that the AI could routinely be fooled by images of pure static. Using a slightly different evolutionary technique, they generated another set of images. These all look exactly alike -- which is to say, nothing at all, save maybe a broken TV set. And yet, state of the art neural networks pegged them, with upward of 99 percent certainty, as centipedes, cheetahs, and peacocks. To Clune, the findings suggest that neural networks develop a variety of visual cues that help them identify objects. These cues might seem familiar to humans, as in the case of the school bus, or they might not. The results with the static-y images suggest that, at least sometimes, these cues can be very granular. Perhaps in training, the network notices that a string of ""green pixel, green pixel, purple pixel, green pixel"" is common among images of peacocks. When the images generated by Clune and his team happen on that same string, they trigger a ""peacock"" identification. The researchers were also able to elicit an identification of ""lizard"" with abstract images that looked nothing alike, suggesting that the networks come up with a handful of these cues for each object, any one of which can be enough to trigger a confident identification. The fact that we're cooking up elaborate schemes to trick these algorithms points to a broader truth about artificial intelligence today: Even when it works, we don't always know how it works. ""These models have become very big and very complicated and they're learning on their own,"" say Clune, who heads the Evolving Artificial Intelligence Laboratory at the University of Wyoming. ""There's millions of neurons and they're all doing their own thing. And we don't have a lot of understanding about how they're accomplishing these amazing feats."" Studies like these are attempts to reverse engineer those models. They aim to find the contours of the artificial mind. ""Within the last year or two, we've started to really shine increasing amounts of light into this black box,"" Clune explains. ""It's still very opaque in there, but we're starting to get a glimpse of it."" Why Does a Computer's Bad Eye Sight Matter, Anyway? Earlier this month, Clune discussed these findings with fellow researchers at the Neural Information Processing Systems conference in Montreal. The event brought together some of the brightest thinkers working in artificial intelligence. The reactions sorted into two rough groups. One group -- generally older, with more experience in the field -- saw how the study made sense. They might've predicated a different outcome, but at the same time, they found the results perfectly understandable. The second group, comprised of people who perhaps hadn't spent as much time thinking about what makes today's computer brains tick, were struck by the findings. At least initially, they were surprised these powerful algorithms could be so plainly wrong. Mind you, these were still people publishing papers on neural networks and hanging out at one of the year's brainiest AI gatherings. To Clune, the bifurcated response was telling: It suggested a sort of generational shift in the field. A handful of years ago, the people working with AI were building AI. These days, the networks are good enough that researchers are simply taking what's out there and putting it to work. ""In many cases you can take these algorithms off the shelf and have them help you with your problem,"" Clune says. ""There is an absolute gold rush of people coming in and using them."" That's not necessarily a bad thing. But as more stuff is built on top of AI, it will only become more vital to probe it for shortcomings like these. If it really just takes a string of pixels to make an algorithm certain that a photo shows an innocuous furry animal, think how easy it could be to slip pornography undetected through safe search filters. In the short term, Clune hopes the study will spur other researchers to work on algorithms that take images' global structure into account. In other words, algorithms that make computer vision more like human vision. But the study invites us to consider other forms these vulnerabilities could take. Does facial recognition, for instance, rely on the same sort of technology? ""Exactly the same,"" Clune says. ""And it's susceptible to the exact same problem."" You can imagine all sorts of interesting implications here. Maybe a certain 3-D printed nose could enough to make a computer think you're someone else. Perhaps a mask of some precise geometry could render you invisible to a surveillance system entirely. A few years back, British design group ScanLAB Projects proposed a series of speculative objects that could subvert laser scanning of 3-D spaces, obscuring doorways or inventing phantom passageways. This new work just confirms that as the use of computer vision grows, the possibilities for subversion will follow. More broadly, though, it's a reminder of a fast-emerging reality as we enter the age of self-learning systems. Today, we're still in control of the things we're building. But as they increasingly help build themselves, we shouldn't be surprised to find them complex to the point of opacity. ""It's no longer lines of computer code written in a way a human would write them,"" Clune says. ""It's almost like an economy of interacting parts, and the intelligence emerges out of that."" We'll undoubtedly waste no time putting that intelligence to use. It's less clear how fully we'll understand it when we do."
729,https://www.wired.com/story/artificial-intelligence-1/,Wired,2014,12,3,525.0," This article was taken from the April 2015 issue of WIRED magazine. Be the first to read WIRED's articles in print before they're posted online, and get your hands on loads of additional content by subscribing online. Artificial Intelligence is not going to take over the planet and kill us all. Fear of the so-called AI singularity rests on the premise that computers will one day become self-aware because of Moore's law. This simplistic notion ignores just about everything we know about brains, neurobiology, consciousness and the nature of the mind. It is irrelevant that our computing technologies are indeed becoming more powerful every 18 months or so: they are, and forever will be, different from a self-conscious brain in form, structure and function. When we talk about AI we mean computers capable of performing functions we attribute to humans: recognising faces, places and voices, learning from experience or communicating in natural language. We do not mean that the computer is aware of its functions thanks to an inner ""I"". Computers don't need to be self-aware to be more intelligent than humans, in the same way that a rocket does not need to be alive in order to outpace a bird. So fear not AI, and do not confuse it with artificial consciousness. On the contrary, we should embrace AI as the technology that will provide the means to solve humanity's greatest challenges. The digital world began in the late 20th century, as small disparate islands of data processing and computation gradually coalesced, thanks to the internet, into ever-larger digital continents. This information evolution is now accelerating, as embedded computing transforms virtually everything into digital denizens of the internet of things: a deluge of data that offers humanity the greatest opportunity for a leap in progress since the invention of fire. But this data is useless unless we have an efficient means to extract new knowledge from it. The integrated information age needs technologies that will mine, correlate, contextualise and communicate new data insights. Artificial intelligence is key to leveraging these technologies by providing two key functions: firstly, the capability of computers to interact with humans in a meaningful way. This includes understanding, predicting and responding to human behaviour, feelings, goals and aspirations. Science and business in the mid-21st century will be a human/machine endeavour that continuously innovates thanks to AI interfaces and data contextualisation algorithms. Global problems such as public health, food security, market stability, sustainable energy and ecosystem preservation can only be solved if colossal computing power bears upon massive real-time data in order to produce something that we humans can intuitively understand. The second crucial element is the computing systems' capability to learn. This is perhaps the most profoundly Promethean dimension of AI, because it will allow human/machine networks to learn faster from data than ever before. Today's deep-learning algorithms will evolve into the solution architects of tomorrow, working together with their human colleagues. This also means that our relationship with computers will change. From mere tools, they will become our partners and perhaps something more. Far from becoming mortal enemies, the intelligent computers of the future will become our trusted friends."
730,https://www.wired.com/2014/12/googlers-quest-teach-machines-understand-emotions/,Wired,2014,12,2,1439.0," Quoc Le sees the world as a series of numbers. A digital photo is nothing but numbers, he says, and if you separate the spoken word into individual phonemes, you can translate these into numbers too. You can then feed such numbers into machines, and that means machines can ultimately understand the contents of photos and the meanings of words. Facebook can recognize your face, and Google can act on particular words you say. But Le wants to go further. He wants to create technologies that can take entire sentences, whole paragraphs, and other types of natural language and turn them into numbers—or vectors, the mathematical constructs that computer scientists use to translate the things we see and hear into information that machines can grasp. He’s even exploring how machines can understand things like opinions and emotions. Some of this technology is a long way from being realized. But Le has more resources at his disposal than most. He works on the Google Brain, the search giant's foray into ""deep learning,"" a form of artificial intelligence that processes data in ways that mimic the human brain—at least in some ways. Le was one of the main coders behind the widely publicized first-incaration of the Google Brain, a system that taught itself to recognize cats on YouTube images, and since then, the 32-year-old Vietnam-native has been instrumental in helping to build Google systems that recognize your spoken words on Android phones and automatically tag your photos on the web, both of which are powered by deep-learning technology. Deep learning is driving similar tools at other internet giants, including Facebook and Microsoft. Its ability to supercharge image and speech recognition is well documented, and Chinese giant Baidu has talked openly about how the technology can boost revenues by providing a better way to target ads. But Le is among those hoping to push the technology into still more areas, including everything from natural language understanding to robotics to good old web search. At Google, he helped develop a system that essentially maps words into vectors. And according to Google, this work would later feed into a system developed largely by a researcher named Tomas Mikolov. Called Word2Vec, the system determines how different words on the web are related, and Google is now using this as a means of strengthening its ""knowledge graph""—that massive set of connections among related concepts that makes the Google search engine work so well. It's a way of verifying facts much more quickly—and at larger scale.1 And more is on the way. When Le first starting studying AI, in the 1990s, it ""really bothered"" him. He didn’t like that machine-learning systems leaned so heavily on input from human engineers. Machines could learn—at least to a certain extent—but they needed heavy instruction in order to do so. They couldn't learn to recognize photos unless the photos were discretely labeled. In order to achieve true intelligence, Le says, machines must learn on their own, without labels, like humans do. ""We learn from a lot of unlabeled data,"" says Le, who studied artificial intelligence at Stanford with Andrew Ng, now the head of research at Baidu and one of the founders of the Google Brain project. ""It would be wonderful if we could have an algorithm that can discover that—that can learn in the same way—because more practically, we have much more unlabeled data than labeled data."" Indeed, most of the stuff we post to places like Facebook and Twitter and Google is unlabeled. This is what deep learning seeks to achieve. Using hundreds of machines to operate complex ""neural networks""—software constructs meant to mimic networks of neurons in the brain—it allows machines to learn. In some cases, that learning happens on its own, without someone labeling all the data. Google’s cat detector was a prime example. Unfortunately, three years on, this sort of unsupervised learning hasn't really caught on, and most commercial deep-learning systems still rely on supervised learning. ""Even though the cat thing isn't useful, it's—in my mind—a very clear signpost that is driving deep learning to spend more time looking at unsupervised learning as a promise for the future,"" says Ng. That could be useful for natural language processing, or NLP, an AI discipline that seeks to understand the meaning behind language—a task that's tougher than figuring out how to get machines to understand pictures and simple voice commands. Part of the challenge is that language can be really subtle, and we still don’t have good ways of translating subtle concepts. The same word, for instance, can take on different meanings depending on context, and most artificial intelligence systems out there still have trouble distinguishing between them. Sarcasm or humor often fall flat—even on the smartest computers. ""Machines are very good with numbers. They aren’t good with symbols,"" says Le. And language is a highly symbolic thing. The trick is to find a way of translating the symbols into numbers. ""How to formulate a concept into a mathematical construct so that machines can process it—it’s still unclear,"" he says. But, with things like Word2Vec and other natural language processing technologies, Le and others are poised to make some progress here. And that's what we need to build machines that can truly understand the massive amounts of information posted to the net with each passing second. ""It's going to be impossible to have complete supervision for that,"" says Richard Socher, who did doctoral work in deep learning alongside Le at Stanford and continues to focuses on NLP. ""The hope is that through a combination of unsupervised and supervised learning, you can really get to a system that will do things that blow you away."" In the aughts, neural-net expert Yoshua Bengio developed a clever algorithm that mapped words to vectors, and the thing was smart enough to group words that represented similar concepts into groups. For example, the words Monday or Tuesday would have similar vectors because they represent days of the week, while something like sandwich would have an altogether different mathematical representation. The natural evolution would be to represent more complex ideas—like sentences or whole paragraphs—mathematically. Earlier this year, Le published a paper with another Googler, Tomas Mikolov, that did just that. They describe ""paragraph vectors,"" mathematical ways to represent whole paragraphs. Google won't say whether it's currently taking advantage of this work—or how—but the paper mentions sentiment analysis and Le admits this could be one of its applications. In other words, it seeks to understand opinions—maybe even emotions. Even more recently, Le published a paper with fellow Googlers, Ilya Sutskever and Oriol Vinyals, on machine translation using deep neural nets. It’s being presented at the Neural Information Processing Systems Foundation conference in Montreal next week. The system makes use of something called ""recurrent neural networks."" Because they’re more complex, they can be more difficult to train than the more widely used convolutional neural nets. But their big advantage is they have a sort of built-in memory, a characteristic that makes them especially useful for things that are sequential, like language. ""A lot of us are concerned with how to map a longer sequence to a vector. The paragraph vector is one approach, but this has a lot of advantages,"" says Le. ""It can remember the order of the words in the sequence…It can deal with variable sized inputs."" According to the paper, the new approach outperforms other machine translation algorithms, but this is just one of its possible applications. Le says the new work could also be useful for answering questions on the web, automated captioning and possibly sentiment analysis—though Le says he hasn't tried that out yet. To fully capitalize on these types of algorithms, Google would have to build some really big neural networks—far bigger than what it has already built for vision and speech. Geoff Hinton, the man at the heart of the deep learning movement, who now works at Google, uses an apt analogy. With the brain the size of a pigeon, he told WIRED last year, ""you can have every reason to believe that you can do good vision. But you don’t want to hold a conversation with a pigeon."" As it stands, a pigeon brain can still outperform even the world’s most advanced neural nets, the Google Brain included, on some tasks. But Hinton joined Google, he says, to build one of the biggest neural networks to date, one that can ""do pretty good common sense."" With help from people like Le, that just might happen. 1Update 4:46 EDT 12/05/14: This story has been updated to clarify Quoc Le's role in deep learning technology that feeds Google's ""knowledge graph."""
731,https://www.wired.com/story/your-job-automated/,Wired,2014,11,30,3292.0," This article was taken from the January 2015 issue of WIRED magazine. Be the first to read WIRED's articles in print before they're posted online, and get your hands on loads of additional content by  subscribing online. Earlier this year, Deep Knowledge Ventures a Hong Kong investment house, announced that it had appointed an algorithm to its board of directors. Given the same powers as the human board members, the piece of software weighs up financial and business decisions to assess investments in biotechnology and regenerative medicine that could be worth millions of dollars. The algorithm's strength, its creators claim, is its ability to automate the kind of due diligence and historical knowledge about trends that would be difficult for a mere person to spot. Because CEOs and senior executives routinely command seven- and eight-figure salaries based on their experience and ability, the appointment seems like a stunt concocted by a marketing department: surely no computer can make decisions superior to those of MBA-schooled executives? In fact, the really surprising thing about the Deep Knowledge Ventures algorithm isn't that it's an outlier, but how typical it is. No matter what line of work you're in, your skills are being undermined by rapidly improving algorithms. Are you a doctor? Say hello to the medical apps that can provide diagnostic information more accurately than a human health worker. A lawyer? LegalZoom and Wevorce are legal services that carry out the kind of complex tasks previously performed by a legal professional, for just a fraction of the fees. Do you work in manufacturing? 3D printers can now do much of the physical construction work previously carried out by assembly workers, and genetic algorithms (which mimic natural selection inside a computer) are now widely active in carry-out design work. In most sectors, machines are faster, cheaper and do a more reliably consistent job than humans. They don't take sick days or holidays, don't exhibit irrational bias or leave for a better job. As artificial intelligence systems have become more advanced, they have been disrupting more fields -- and they're showing no signs of slowing down. According to a 2013 study by the University of Oxford, around 47 per cent of employment is at risk of being automated within two decades. This isn't a contemporary phenomenon: throughout history, technological progress has constantly shifted the market's demand for skills. The invention of the internal-combustion engine, for instance, prompted a shift from an agrarian to an industrial economy and created the city as we know it. New technologies have destroyed jobs, but they've also created them. This is what economists call the ""capitalisation effect"", by which companies enter areas where demand and productivity are high. This is then enough to offset the destructive effects of these types of economic shifts. What is different now is that the number of jobs being created appears lower than the number being destroyed. In the early 20th century, factory work created more than enough jobs to offset those lost in agriculture. That is not so obvious in a world where a company such as Instagram, which employed 13 people in April 2012 when it was acquired by Facebook for $1 billion (£600 million), can displace photography giant Kodak, which employed more than 140,000 at its height. Writing in 1933, the economist John Maynard Keynes noted that widespread technological unemployment is the result of ""our discovery of means of economising the use of labour outrunning the pace at which we can find new uses for labour"". Today it's no longer only jobs perceived as low-skill that are being disrupted: white-collar professions involving a high level of training are just as likely to be displaced by software as assembly workers in a factory. Some of this is because once-untouchable fields such as law and medicine include specialisms that are vulnerable to automation: medical diagnosis, the drafting of contracts and comparison of trademarks can be better carried out by a computer than by human beings. ""What we saw happening with physical labour during the industrial revolution we're now seeing happening with cognitive labour,"" says Michael Osborne, associate professor in machine learning at the University of Oxford. ""It's all about taking cognitive tasks and breaking them down into their smallest possible subcomponents."" Surviving this transition will be tough. It has led to -- and will tend to increase -- a growing polarisation between rich and poor, as a few ""superstars"" flourish at the top of the food chain, while median income falls at the middle market. Between 1990 and 2005, the ratio of CEO pay compared to that of the average worker increased from 70 times to 300 times. In the UK some CEOs take home more in three days than lower-paid employees make in an entire year. Although technology isn't the only reason for this, it does play a significant role. For example, it offers new income streams and potential audiences for the rock stars at the top, while removing the jobs of those underneath. The digital age enables JK Rowling to become a billionaire author by giving her more ways to monetise Harry Potter, but it also means that second-bests will find it tougher to compete. In his book Average is Over, the economist Tyler Cowen writes: ""One day soon, we'll look back and see that we have produced two nations -- a fantastically successful nation working in a technologically dynamic sector, and everything else."" But speaking to WIRED, Cowen is also optimistic. ""These proportions are not fixed,"" he says. ""I think we can get a lot more people into the rich sector. A lot depends on what kind of education and other opportunities are available to them."" This is where technology's disruptive reach can help, whether it's a MOOC (massive open online course) that offers a university-level education for free, or the connectivity that lets a UK entrepreneur set up a production line in China. Technology might be shrinking certain industries, in terms of the numbers of workers they need in order to function, but it's also making all manner of new things possible. ""Automation is definitely having a big impact on certain classes of lawyer, but I don't think it's going to do away with them completely,"" says John Kelly, CEO and founder of Blackstone Discovery, an e-discovery firm that uses software to sort through pre-trial documents, a job that would previously have been done by junior lawyers. ""For example, in the area of legal discovery, one of the big revolutions of the digital age is that there's so much more data to comb through. Whereas in the past it might have been a few boxes of paper, today we're talking about emails, texts, documents, voicemails, social-media posts, hard-drive contents. This isn't a case of algorithms shutting down fields; they're opening them up. That presents a huge opportunity for new lawyers with the right skill sets."" The Second Machine Age. What will distinguish the jobs that are likely to last is that they will be complementary to, rather than replaceable by, machines. We need to establish which tasks machines can do better than people and vice versa. This isn't easy. In 2004, in The New Division of Labor, MIT and Harvard economists Frank Levy and Richard Murnane argued that computers were adept at jobs that require the following of rules, but were far less effective at dealing with pattern-recognition tasks involving unstructured data. An algorithm couldn't, they argued, drive a car because of the stream of unstructured data the vehicle would need to handle. With autonomous cars being tested on British roads from January 2015, the algorithms' progress is only going to accelerate. For the next few years, humans will continue to be employed in areas where machines are not yet cheap or skilled enough to replace them. Logistics companies such as Amazon, for instance, are still reliant on human employees to pick and package orders. At some point in the near future, however, the staff will be replaced: the online retailer's purchase of robotics company Kiva Systems was an early step in increasing efficiency and lowering costs via a non-automated warehouse. These advances in computation will change the tasks required of humans, but our input will not be mothballed quite yet -- just adapted. In order to be safe from the destructive effects of new technologies we must recast the way we think about employment (and by extension education), picking areas in which machines will probably never be able to match humans. The industrial revolution was all about turning man into machines: simplifying complex tasks to the point where they could easily be tailored to mass-production. The digital age, by comparison, needs to adapt the new possibilities of computation to, ironically, be more human. Lawrence Katz, professor of economics at Harvard University, talks about an ""artisan economy"" of jobs that emphasise the bespoke, human element of work. As we become accustomed to an age of smart sensors, algorithmic selection and real-time feedback, we will also come to re-evaluate the importance of qualities such as empathy, creativity and, well, humanness. We're already starting to see evidence of this. For instance, recently there has been a rise in the number of companies exploring human curation. One example of this is Beats Music, the company purchased by Apple for $3 billion (£1.8 billion) in May 2014. Beats stands out from some of its rivals by creating playlists that are the result of human editors, instead of machines. In August, former Apple executive Jean-Louis Gassée wrote an open letter to Apple CEO Tim Cook, suggesting that he apply the same idea to the App Store; using editorial picks over the currently employed recommender system model. Similarly, the nutrition app Rise matches users with human coaches rather than employing an algorithm. Another example is OOLOO, a Siri-style assistant created by the storage service iDrive. Rather than focus on breakthroughs in natural-language processing, OOLOO's creators believe that they have optimised their service by using humans over algorithms.  ""Life is getting more and more automated, and what we've seen is a real desire on the part of users for a human connection,"" says iDrive CEO and president Raghu Kulkarni of OOLOO. ""There's a warmth to our service, which sets us aside from some of the algorithmic options out there. There might be a bit of humour in your interaction, for example. I certainly don't think human interfaces are the only option, but we're definitely seeing more and more popularity regarding that way of thinking."" Although automation can replace people in many roles, there may be jobs that we simply don't want to hand over to machines: not because they can't do a good approximation of it, but because the result of doing so feels a little too mechanised. Algorithms, for example, can analyse the music of bands such as the Beatles, and may very soon be able to generate whole new tracks in that style without the need to pay expensive musicians. But the idea of replacing musicians, radio hosts or novelists with algorithms is one that human beings are likely to sense as peculiar. Where to draw the line dividing computer-operated efficiency and emotionally intelligent response remains to be seen. Is that seemingly bespoke job you're interested in one where the human component is essential -- or is it one where efficiency is the more highly valued and the human element could be replaced by an algorithm? Viv Labs, a startup by several of the creators of Siri, is currently working on a virtual assistant able to deal with complex sentences like: ""Get me a ticket for the cheapest flight from SFO to Charles de Gaulle on July 2, with a return flight the following Monday"" or, ""On the way to my brother's house, I need to pick up some cheap wine that goes well with lasagne."" At present, these sentiments can be better dealt with by a person with a deep understanding of language and semantics, which is OOLOO's business model. But judging by the progress made by Viv Labs, the same won't necessarily be true a few years down the line. It depends on how much we prize qualities like humour and human spontaneity in our interactions. ""There's no single technological inevitability about the future when it comes to how it will affect employment and incomes,"" says Brynjolfsson. ""There have been some disappointing trends when it comes to inequality and employment, but there have also been some great, positive trends, in terms of increased abundance and the overall pie getting bigger. Going forwards, we need to view technology as a tool -- not an autonomous force that imposes its will on us. We shape our own destiny."" Your employability will increasingly depend on your repeated ability to develop new skills. The education system is not meeting these needs. Schools and universities are based on a paradigm rooted in the Industrial Revolution, the so-called ""factory model"" of schooling, in which standardised lesson plans teach students specific skills for prescribed roles in the workplace. Much of this thinking no longer applies. The factory model for schools imagined that these skills could be used by learners for the rest of their lives. Today, technological change means that many skills will be obsolete within a decade. New categories of job are constantly being invented. Why should memorisation of facts matter in the age of Wikipedia? ""Education is built around the idea that we should teach our children to store large amounts of information,"" says Carl Benedikt Frey, a James Martin Fellow at the Oxford Martin Programme on the Impacts of Future Technology. ""Why? Instead of rote learning, we should be pushing social interactions and getting kids to solve problems in creative ways."" ""There are two competing ideologies of education,"" says David Hursh, author of Twenty-First Century Schools and associate professor of education at the Warner Graduate School of Education at the University of Rochester, New York. ""The first of these focuses on so-called objective assessments, standardised teaching, increased efficiency, reducing costs and ensuring that students are constantly learning and doing nothing frivolous. The second -- which I'm a believer in -- talks about using education to create a democratic citizenry. This means few standardised tests, if any, building more directly on teacher-student knowledge, and supporting human development. Rather than using education to train people for the workplace, it imagines that everywhere, including the workplace, should be an educational institution."" In a world of massive open online courses, high-quality education can take place anywhere. This opens up new technology-aided opportunities for people previously denied the chance of such learning. Within the next 20 years it is likely that everyone in the world will be able to learn using online courses. This will require a change in how we think about educational establishments. ""There is a real lock on certification held by a lot of more traditional establishments,"" says economist Tyler Cowen. ""We think about Oxford and Harvard, but saying that you learned something online has nowhere near the same cachet. That really needs to evolve."" It is unlikely that WIRED readers would work for free, even if their boss occasionally let them use her personal gym or swimming pool as a perk. So why don't we behave in the same way when it comes to our data? Instead of one or two companies getting rich from the data generated via digital touchpoints, everyone should gain from the new streams of revenue. Ironically, today's data-driven artificial-intelligence systems make humans more valuable than ever. Today's AI doesn't rely on rules coded by a few programmers, but on vast data sets of collective human intelligence, which are used to make algorithms smarter. On Google Translate every word deciphered draws on the knowledge of millions of anonymous translators. Their reward is the use of online services, such as search engines, for free. As white-collar jobs move online, though, should services and their users develop a different relationship? ""Data is the new oil,"" argued mathematician Clive Humby in 2006, who, with his wife, Edwina Dunn, sold their company, which created the Tesco Clubcard, to Tesco for £93 million. But if data really is 21st-century oil, then should users be giving away such a valuable resource in exchange for free online spreadsheets and pictures of cats? Steve Jobs showed us through iTunes that people can be persuaded to pay for music online, rather than downloading it for free. As public awareness of the power of data grows, the same thing could happen with cognitive capital. In his book Who Owns the Future? virtual-reality pioneer Jaron Lanier suggests that it would be relatively simple for companies to calculate a micropayment every time a person's action helps a machine to make an intelligent decision. When Amazon sells a book based on your preferences, say, or an online dating service uses your data to provide better matches, you would receive a micropayment. Tyler Cowen has a variation on this idea. ""Machines are going to displace more jobs, but we'll be able to produce more for free,"" Cowen says. ""It's not unthinkable that a multi-billionaire could adopt a country and give its people the technologies they need for free. They would have phenomenal healthcare and education. You can already see the beginnings of this online."" The concept of a neo-feudal Google-owned nation in which citizens access services in exchange for personal data may seem dystopian, but these are the questions we should be asking as the age of automation advances. There's a difference between job security and employment security. The days of a single job for life may be gone, but that's not the same as employability. Computer scientists can move from creating an algorithm to analysing star formations to working on nanotechnology; in a similar way those wanting to thrive can adapt their skills to fit a rapidly changing work environment. One way this will play out is a move away from a model of specific job training and towards a focus on attributes. In the age of the algorithm, skills such as creativity and empathy will be prized far more highly than rote knowledge of one particular area that may be disrupted. Several startups are already exploring this area. Israeli entrepreneur Guy Halfteck's company Knack creates mobile games that isolate and grade qualities such as quick thinking, perceptiveness and spontaneity in a way that will make them less subjective and more quantifiable for employers, giving them greater importance.  ""The kinds of jobs we're going to see continued growth in are the ones that are heavily reliant on social and creative intelligence,"" says Carl Benedikt Frey, a James Martin Fellow at the Oxford Martin Programme on the Impacts of Future Technology. Partnering with Michael Osborne, associate professor in machine learning at the University of Oxford, Frey has tried to identify what he calls ""bottleneck"" -- the areas in which machines find it difficult to match humans. ""We've found three main ones,"" Frey says. ""Personal intelligence, creativity and fine manipulation, which machines still cannot do well. You've got to find the jobs which require these skills. Jobs which require a high level of creative and social skill are quite difficult to automate. As a result there is a low risk that they will be subject to computerisation.""  ""Traits such as creativity and emotional intelligence are going to be increasingly prized,"" says Erik Brynjolfsson, director of the MIT Center for Digital Business. ""For now, these are areas where humans have a relative edge compared with machines."" Medicine is an example of a profession that will probably reflect this change. As algorithms are used for more and more diagnostic work -- meaning that doctors no longer need to be repositories for The Physicians' Desk Reference -- their role as empathic caregivers will increase."
732,https://www.wired.com/2014/11/algorithms-great-can-also-ruin-lives/,Wired,2014,11,19,1115.0," On April 5, 2011, 41-year-old John Gass received a letter from the Massachusetts Registry of Motor Vehicles. The letter informed Gass that his driver’s license had been revoked and that he should stop driving, effective immediately. The only problem was that, as a conscientious driver who had not received so much as a traffic violation in years, Gass had no idea why it had been sent. After several frantic phone calls, followed up by a hearing with Registry officials, he learned the reason: his image had been automatically flagged by a facial-recognition algorithm designed to scan through a database of millions of state driver’s licenses looking for potential criminal false identities. The algorithm had determined that Gass looked sufficiently like another Massachusetts driver that foul play was likely involved—and the automated letter from the Registry of Motor Vehicles was the end result. The RMV itself was unsympathetic, claiming that it was the accused individual’s “burden” to clear his or her name in the event of any mistakes, and arguing that the pros of protecting the public far outweighed the inconvenience to the wrongly targeted few. John Gass is hardly alone in being a victim of algorithms gone awry. In 2007, a glitch in the California Department of Health Services’ new automated computer system terminated the benefits of thousands of low-income seniors and people with disabilities. Without their premiums paid, Medicare canceled those citizens’ health care coverage. Where the previous system had notified people considered no longer eligible for benefits by sending them a letter through the mail, the replacement CalWIN software was designed to cut them off without notice, unless they manually logged in and prevented this from happening. As a result, a large number of those whose premiums were discontinued did not realize what had happened until they started receiving expensive medical bills through the mail. Even then, many lacked the necessary English skills to be able to navigate the online health care system to find out what had gone wrong. Similar faults have seen voters expunged from electoral rolls without notice, small businesses labeled as ineligible for government contracts, and individuals mistakenly identified as “deadbeat” parents. In a notable example of the latter, 56-year-old mechanic Walter Vollmer was incorrectly targeted by the Federal Parent Locator Service and issued a child-support bill for the sum of $206,000. Vollmer’s wife of 32 years became suicidal in the aftermath, believing that her husband had been leading a secret life for much of their marriage. Equally alarming is the possibility that an algorithm may falsely profile an individual as a terrorist: a fate that befalls roughly 1,500 unlucky airline travelers each week. Those fingered in the past as the result of data-matching errors include former Army majors, a four-year-old boy, and an American Airlines pilot—who was detained 80 times over the course of a single year. Many of these problems are the result of the new roles algorithms play in law enforcement. As slashed budgets lead to increased staff cuts, automated systems have moved from simple administrative tools to become primary decision-makers.In a number of cases, the problem is about more than simply finding the right algorithm for the job, but about the problematic nature of believing that any and all tasks can be automated to begin with. Take the subject of using data-mining to uncover terrorist plots, for instance. With such attacks statistically rare and not conforming to well-defined profiles in the way that, for example, Amazon purchases do, individual travelers end up surrendering large amounts of personal privacy to data-mining algorithms, with little but false alarms to show for it. As renowned computer security expert Bruce Schneier has noted: Finding terrorism plots . . . is a needle-in-a-haystack problem, and throwing more hay on the pile doesn’t make that problem any easier. We’d be far better off putting people in charge of investigating potential plots and letting them direct the computers, instead of putting the computers in charge and letting them decide who should be investigated. While it is clear why such emotive subjects would be considered ripe for The Formula, the central problem once again comes down to the spectral promise of algorithmic objectivity. “We are all so scared of human bias and inconsistency,” says Danielle Citron, professor of law at the University of Maryland. “At the same time, we are overconfident about what it is that computers can do.” The mistake, Citron suggests, is that we “trust algorithms, because we think of them as objective, whereas the reality is that humans craft those algorithms and can embed in them all sorts of biases and perspectives.” To put it another way, a computer algorithm might be unbiased in its execution, but, as noted, this does not mean that there is not bias encoded within it. Implicit or explicit biases might be the work of one or two human programmers, or else come down to technological difficulties. For example, algorithms used in facial recognition technology have in the past shown higher identification rates for men than for women, and for individuals of non-white origin than for whites. An algorithm might not target an African-American male for reasons of overt prejudice, but the fact that it is more likely to do this than it is to target a white female means that the end result is no different. Biases can also come in the abstract patterns hidden within a dataset’s chaos of correlations. Consider the story of African-American Harvard University PhD Latanya Sweeney, for instance. Searching on Google one day, Sweeney was shocked to notice that her search results were accompanied by ads asking, “Have you ever been arrested?” These ads did not appear for her white colleagues. Sweeney began a study that ultimately demonstrated that the machine-learning tools behind Google’s search were being inadvertently racist, by linking names more commonly given to black people to ads relating to arrest records. A similar revelation is the fact that Google Play’s recommender system suggests users who download Grindr, a location-based social-networking tool for gay men, also download a sex-offender location-tracking app. In both of these cases, are we to assume that the algorithm has made an error, or that they are revealing inbuilt prejudice on the part of their makers? Or, as is more likely, are they revealing distasteful large-scale cultural associations between—in the former case—black people and criminal behavior and—in the latter—homosexuality and predatory behavior? Regardless of the reason, no matter how reprehensible these codified links might be, they demonstrate another part of algorithmic culture. A single human showing explicit bias can only ever affect a finite number of people. An algorithm, on the other hand, has the potential to impact the lives of exponentially more."
733,https://www.wired.com/2014/11/ai-magic/,Wired,2014,11,18,488.0," Artificial intelligence is changing the way we interact with computers. It's improving voice recognition. It's letting us more easily search for images. It's driving cool new devices like the XBox Kinect. And now it's giving us card tricks too. Witness the video above, from researchers at Queen Mary University in London. It shows off a card trick called Phoney, but this isn't magic. It's driven by clever computer algorithms that let the researchers guess which card is being picked based on the colors of the other cards around it, coupled with probability analysis. The team taps what are called simulated annealing algorithms, which are used to solve really complicated computer problems. This family of algorithms can tackle computationally intensive riddles like the traveling salesman problem. That's where the computer works out the quickest route for a salesman, even though there is a really, really large number of highways and possible sales stops to be made. The researchers applied the same principles to create a ""magic jigsaw,"" dubbed the Twelve Magicians of Osiris. You can see it here: The rub is that, for the algorithms to successful pull off the card trick, the Queen Mary researchers resorted to a little trickery of their own. The deck in the video isn't really being shuffled. That's a human trick, called a ""false shuffle."" Without any real shuffling, the unique order of cards in the deck is preserved. The dealer also surreptitiously enters data into the phone via a phony passcode screen. That allows the smart phone to figure out the card that got picked. And it works most of the time. You see, the researchers also used some of their own data, about which cards are most likely to be selected when people are asked to pick the card they like the most from a set of four. That means that the trick can fail, but the Phoney software is about 80 percent likely to chose the right card in the first two tries. The researchers learned an important lesson: humans are an important part of any magic trick---even one that's being solved by a computer. Howard Williams, the Queen Mary PhD student who devised the experiment, says that the human touch can smooth over things whenever the software comes up with the wrong card. But he and his colleagues were surprised at how often test subjects were amazed by the card trick---even though it uses an Android app to ultimately display the ""magically"" selected card. He attributes that to the human magician, who walks the person being tricked though the card selection, pulls off the false shuffle, and smooths over any problems, creating the illusion of magic. So don't worry, magicians, IBM's Watson won't be gunning for David Copperfield's job anytime soon. ""We think the presence of the magician is key even though he's using the phone to help,"" he says. ""Without the magician, it wouldn't have the same impact."""
734,https://www.wired.com/story/magic-tricks-artificial-intelligence/,Wired,2014,11,17,397.0," Computer scientists at Queen Mary University London have taught a computer how to create magic tricks using artificial intelligence. magic jigsaw puzzle and a mind-reading card trick, as well as the results of previous experiments showing how the human mind understands magic, it was able to create completely new variants of the tricks. ""Using AI to create magic tricks is a great way to demonstrate the possibilities of computer intelligence and it also forms a part of our research in to the psychology of being a spectator. For example, we suspected that audiences would be suspicious of the involvement of technology in the delivery of a trick but we've found out that isn't the case,"" said Professor Peter McOwan from the QMUL team. By exploiting mathematical principles that computers can grasp extremely well, researchers have managed to optimise several magic tricks for the purposes of maximising their psychological impact on audiences. A full explanation of how they have done this has been published in the journal Frontiers in Psychology, but they have also created several ways for the non-magical, non-mathematical to take advantage of their research. The first is a self-working magic trick entitled The Twelve Magicians of Osiris, which can be downloaded as a web pack and printed out for any budding Dynamo to try out. In it, a completed jigsaw can be broken up into pieces and completely reconstructed, but in a way that shows a totally different picture. The second trick works through an app called Phoney, which allows you to read the minds of your audience by showing them a card they have chosen on the screen of your phone. ""Beginners can stun audiences with just a couple of minutes practice, and no sleight of hand needed,"" it states in the Google Play description of the app. It works by arranging the deck of cards in a specific way and using seemingly innocuous information from the audience to identify the card. The computer can arrange the deck in such a way that a specific card can be identified with the least amount of information possible. If a magician was doing this trick manually, they would have to memorise the order of the cards and ask on average at least one extra question before the correct card could be identified. Phoney from Google Play or The Twelve Magicians of Osiris from the QMagic blog."
735,https://www.wired.com/story/which-movie-ai-are-you/,Wired,2014,11,3,244.0," This article was taken from the February 2015 issue of WIRED magazine. Be the first to read WIRED's articles in print before they're posted online, and get your hands on loads of additional content by  subscribing online. Her, 2014's Transcendence, and now Ex Machina (out January 23). ""There's a lot of AI paranoia around,"" says writer-director Alex Garland, 44. ""I suspect it comes from an underlying anxiety [about technology]."" Ex Machina is the directorial debut for British-born Garland, whose sci-fi writing credits include Sunshine and 28 Days Later. The plot follows Caleb (Domnhall Gleeson), a programmer at a Google-like company who is invited to the home of its reclusive CEO. There, he meets the robot Ava (Alicia Vikander) -- and is asked to determine whether she's the world's first genuinely self-aware AI. ""In stories, AIs are usually the bad guys,"" he says. ""I just don't see it in those terms. All my sympathies are with the machine."" During his research, Garland consulted Murray Shanahan, professor of cognitive robotics at Imperial College London. Among their discussions: the limitations of the Turing test, the means of testing an AI's ""humanity"". ""It's very good in some respects,"" says Garland. ""But all it can do is tell you whether a computer is very good at simulating consciousness. Still, that makes us ask questions about what constitutes true self-awareness."" So, in the spirit of embracing AI, WIRED has created a test to help you find your perfect AI pal."
736,https://www.wired.com/story/ai-from-russia-with-love/,Wired,2014,11,3,192.0," This article was taken from the March 2015 issue of WIRED magazine. Be the first to read WIRED's articles in print before they're posted online, and get your hands on loads of additional content by subscribing online. Russian Siri-alternative IO doesn't just answer questions -- it can hold a conversation. ""We use AI algorithms to make sentences based on rules, rather than using set phrases or a script,"" says Moscow-native IO CEO Zhenya Kuyda, 28. ""Unlike Siri it has a memory, so it can build up a contextual dialogue.""Launching this month in New York, with plans for Moscow- and London-based versions, the service provides personalised recommendations via text message through machine learning and AI. It crunches data from sources including Yelp and The New York Times, as well as social media reviews and previous choices. WIRED asked for a cosy Italian place, and IO responded, ""I'd recommend il Buci Alimentari e Vineria. A great little Italian place with friendly service,"" before asking what our favourite Italian places were. Ultimately, Kuyda wants IO to be a lifestyle concierge. ""It can plan an evening in, and recommend takeaways and films on Netflix,"" she says."
737,https://www.wired.com/2014/08/robobrain/,Wired,2014,8,25,1194.0," If you walk into the computer science building at Stanford University, Mobi is standing in the lobby, encased in glass. He looks a bit like a garbage can, with a rod for a neck and a camera for eyes. He was one of several robots developed at Stanford in the 1980s to study how machines might learn to navigate their environment---a stepping stone toward intelligent robots that could live and work alongside humans. He worked, but not especially well. The best he could do was follow a path along a wall. Like so many other robots, his ""brain"" was on the small side. Now, just down the hall from Mobi, scientists led by roboticist Ashutosh Saxena are taking this mission several steps further. They're working to build machines that can see, hear, comprehend natural language (both written and spoken), and develop an understanding of the world around them, in much the same way that people do. Today, backed by funding from the National Science Foundation, the Office of Naval Research, Google, Microsoft, and Qualcomm, Saxena and his team unveiled what they call RoboBrain, a kind of online service packed with information and artificial intelligence software that any robot could tap into. Working alongside researchers at the University of California at Berkeley, Brown University, and Cornell University, they hope to create a massive online ""brain"" that can help all robots navigate and even understand the world around them. ""The purpose,"" says Saxena, who dreamed it all up, ""is to build a very good knowledge graph---or a knowledge base---for robots to use."" Any researcher anywhere will be able use the service wirelessly, for free, and transplant its knowledge to local robots. These robots, in turn, will feed what they learn back into the service, improving RoboBrain’s know-how. Then the cycle repeats. These days, if you want a robot to serve coffee or carry packages across a room, you have to hand-code a new software program---or ask a fellow roboticist to share code that's already been built. If you want to teach a robot a new task, you start all over. These programs, or apps, live on the robot itself, and that, Saxena says, is inefficient. It goes against all the current trends in tech and artificial intelligence, which seek to exploit the power of distributed systems, massive clusters of computers that can power devices over the net. But this is starting to change. RoboBrain is part of an emerging movement known as cloud robotics. The concept was popularized in 2010 by Google's James Kuffner, one of the engineers behind the tech giant's self-driving cars. In the years since, the idea has slowly spread. In 2011, the European Union's research arm, Seventh Framework Programme, launched RoboEarth, an initiative that lets robots ""share knowledge"" via a world-wide-web-style database and ""access powerful robotic cloud services,"" according to the project's website. The source code is available online, and the team already has made strides building a kind of remote brain. Then, last year, Kuffner and Ken Goldberg, a RoboBrainer at Berkeley, published a paper describing a robotic grasping system powered by Google’s object recognition engine and other data sources. There's also the DAvinCi Project, which aims to supercharge service robots using the popular distributed computing software Hadoop, a way of crunching vast amounts of data across hundreds of machines. And in October, the IEEE Robotics & Automation Society made a call for papers for a special issue on cloud robotics in response to increased interest in this field from researchers, companies, and governments. Similar ideas can be traced back to a man named Masayuki Inaba. In the '90s, he envisioned robots that would move through the physical world but tap the power of supercomputers across the internet. Back then, we didn't have computing infrastructure to make this possible. Today, tech companies have ready access to enormous amounts of computing power. Startups and universities can get Hadoop and other distributed-software from companies like Cloudera or run it on cloud services from the likes of Amazon. The Amazon cloud is where RoboBrain lives. Still, hurdles remain. Unlike technologies like Apple's Siri voice assistant or Google's speech recognition or image-tagging systems, robots must juggle many types of data from many sources. Like humans, they're ""multi-modal systems,"" and this creates unique challenges. ""The first challenge is how do we come up with a storage layer that will support different modalities of data,"" says Aditya Jami, RoboBrain's lead infrastructure engineer. This is what RoboBrain seeks to create. Building the right online storage system, Jami says, is a crucial step to integrating the 100,000 data sources and various types of supervised and unsupervised machine learning algorithms the researchers hope to merge into one huge online network. Jami---who previously built large-scale computing systems at Netflix and was part of the Yahoo team that spawned various big-data systems, such as Hadoop---says he is developing a storage layer that can merge separate learning models. A deep neural network that lets robots ""see"" things or grasp objects, for instance, can dovetail with another system that examines the relationship between different types of objects. Today, he says, things don't always work this way. Disparate AI systems are often developed independently, and don't use standard data formats. (Though this too is starting to change thanks to deep learning, a form of artificial intelligence that seeks to mimic how the brain works. Part of the great promise of deep learning, experts say, is the emergence of a common language and formulas for speech, vision and natural language processing.) Jami's ambition is for RoboBrain to become a platform like Hadoop---a de facto standard everyone can use and contribute to. Having this sort of common language, he says, will speed up the development of robotics algorithms, spur collaboration, and help usher in the age of multi-modal artificial intelligence. ""Any intelligent agent in the real world needs to do three tasks: perception, planning, and language,"" Saxena says. That's why RoboBrain feeds off an object detection system; PlanIt, a simulation through which users can teach robots how to grasp objects or move about a room; and a system called Tell Me Dave, a crowd-sourced project that teaches robots how to understand language. Soon, researchers will be adding other types of learning models and data sources, such ImageNet, 3D Warehouse, and YouTube videos. And the knowledge people---and robots---provide RoboBrain will feed back into the models that make it up, helping to tune these interconnected AI systems. He and the team already are testing RoboBrain with a handful of robots, with good results. By merging all this software and data, the researchers hope to create a system that demonstrates a primitive sense of perception, that can ""discover most of the common sense knowledge of the world,"" says Bart Selman, a RoboBrain collaborator at Cornell. Right now, context is not something computers are good at deciphering. Unlike humans, robots don't know to move out of the way if people are in their path. That's why there's so much worry about robots causing accidents in homes and industrial settings. People like Selman are still a long way from changing this. But they're making progress. Mobi looks more quaint by the day."
738,https://www.wired.com/2014/08/deep-learning-yann-lecun/,Wired,2014,8,14,2018.0," It's good to be Yann LeCun. Mark Zuckerberg recently handpicked the longtime NYU professor to run Facebook’s new artificial intelligence lab. The IEEE Computational Intelligence Society just gave him its prestigious Neural Network Pioneer Award, in honor of his work on deep learning, a form of artificial intelligence meant to more closely mimic the human brain. And, perhaps most of all, deep learning has suddenly spread across the commercial tech world, from Google to Microsoft to Baidu to Twitter, just a few years after most AI researchers openly scoffed at it. All of these tech companies are now exploring a particular type of deep learning called convolutional neural networks, aiming to build web services that can do things like automatically understand natural language and recognize images. At Google, ""convnets"" power the voice recognition system available on Android phones. At China's Baidu, they drive a new visual search engine. This kind of deep learning has many fathers, but its success should resonate with LeCun more than anyone. ""Convolutional neural nets for vision---that's what he pushed more than anybody else,"" says Microsoft's Leon Bottou, one of LeCun's earliest collaborators. He pushed it in the face of enormous skepticism. In the '80s, when LeCun first got behind the idea of convnets---an approximation of the networks of neurons in the brain---the powerful computers and enormous data sets needed to make them work just didn't exist. The very notion of a neural network had fallen into disrepute after it failed to deliver on the promises of scientists who first dreamed of artificial intelligence at the dawn of the computer age. It was hard to publish anything related to neural nets in the major academic journals, and this would remain the case in the '90s and on into the aughts. But LeCun persisted. ""He kind of carried the torch through the dark ages,"" says Geoffrey Hinton, the central figure in the deep learning movement. And eventually, computer power caught up with the remarkable technology. More than two decades before joining Facebook, LeCun worked at Bell Labs, perhaps the most famous of computer research labs, the birthplace of the transistor, the Unix operating system, and the C programming language. During his stint there, the French researcher developed a system that could recognize written digits. He called it LeNet. Automatically reading bank checks, it marked the first time convolutional neural nets were applied to practical problems. ""Convolutional nets were little toys, and Yann changed them into things that worked on a large scale,"" says Bottou. And thanks to Larry Jackel, the chief of LeCun's department who recorded a demo in 1993, you can see the technology in action, with a giddy, baby-faced LeCun test-driving the thing (video below). Some of the concepts baked into LeNet date back to work LeCun did in France at Bottou's college pad and later at a lab run by Hinton, and he was first inspired by two research papers from by Kunihiko Fukushima, another neural-net great who, in the '70s and '80s, had invented what were called the Cognitron and Neocognitron. These early neural nets could learn to pick out patterns in data, on their own, without much human prompting. But they were rather complicated, and researchers couldn't quite figure out how to make them work well. ""What was missing was the supervised learning algorithm,"" says LeCun. ""What we call back prop now."" ""Back propagation"" is a clever way to minimize error. To understand it, you also have to understand how convolutional neural nets work. Like other flavors of neural nets, convolutional networks are software creations organized into interconnected layers, much like the visual cortex, the part of the brain that process visual information. What makes them different is that they can reuse the same filters at multiple locations in an image. That means that once the network has learned to recognize, say a face, at one location, it can also automatically find faces in others. (The same principle holds for sound waves and written words.) This allows artificial neural nets to be trained quickly, and because they have a ""small memory footprint, you don't need to separately store a filter for each location in the image...[making] them well-suited to building much more scalable deep nets,"" says Baidu's Andrew Ng. It's also what makes them so adept at recognizing patterns. When receiving an image---the input---the network translates it into arrays of numbers that represent features, and the ""neurons"" in each layer of the network are tuned to recognize certain pattens in the numbers. Low-level neurons recognize things like edges or basic shapes, while neurons in higher layers can ""see"" objects---say, a dog or a person. Each layer communicates with the one above it, and as information travels up the network, some averaging takes place. At the end, the network comes up with an output---a guess at what's in the image. If the network makes a mistake, engineers can fine-tune the connections between layers to get the right answer. But neural nets work better if they can do this fine tuning on their own. That's where the back propagation algorithm comes in. Back prop is all about computing the error and using that value to update the strength---or weight---given to each of the layers in a neural net. Hinton, with David Rumelhart and Ronald Williams, came up with a version of back prop that calculated the error for multiple inputs at once and then took the average. That value was then back-propagated through the network, from the output to the input layers. In a paper published in the journal Nature in 1986, they showed that this approach improved learning. Around the same time, LeCun was busy devising his own recipe for back prop in Paris, based on research that dated back to the 1950s but that others had more or less failed to apply to real-world problems. Instead of averaging, LeCun's version calculated the error for every single example. It was more noisy, but it worked well---and more quickly. ""A lot of people didn't believe that you could do that,"" recalls Bottou. According to Bottou, the method was the result of the weak machines they were using. ""The computers we had in France were less endowed."" They had to come up with a hack to calculate error quickly while using as little computational power as possible. But what seemed like a ""fudge"" at the time---to borrow Hinton's term---turned out to be spot on. Today, it's called stochastic gradient descent, and like convolutional neural nets as a whole, it's a staple of the artificial intelligence toolbox. LeCun's LeNets would be widely licensed and used in ATMs and banks across the world to read what was written on checks. But skepticism remained. ""Somehow, that wasn’t enough to convince the computer vision community that convolutional neural nets were worthy,"" says LeCun. Part of it was that, although they were powerful, nobody knew why they so powerful. The inner-workings of this technology were a mystery. There were many critics. Vladimir Vapnik, a mathematician and the father of the support vector machine, one of the most widely used AI models, was among them. One March afternoon in 1995, Vapnik and Larry Jackel---who'd recruited him and LeCun to Bell Labs---made a bet. Jackel wagered that, by the year 2000, we'd have a have a handle on how deep artificial neural nets worked. Vapnik disagreed. He also thought that by 2005, ""no one in his right mind will use neural nets that are essentially like those used in 1995."" Fancy dinners were at stake, so they put the bet on paper and signed it---in front of witnesses. LeCun served as the third official signatory, Bottou as an unofficial observer. Vapnik would win the first half of the wager. In 2000, the inner workings of neural nets were still largely shrouded in mystery, and even now, researchers can’t pinpoint mathematically exactly what makes them work well. But, the second victory belonged to Jackel---and, more importantly, to LeCun. By 2005, deep neural nets were still being used in ATMs and banks, and they were very much rooted in work dating back to LeCun's work in the mid-1980s and early '90s. ""It's rarely the case where a technology that has been around for 20, 25 years---basically unchanged---turns out to be the best,"" says LeCun. ""The speed at which people have embraced it is nothing short of amazing. I've never seen anything like this before."" But this is just a start. The deep learning community---LeCun included---are working to improve the technology. Today's most widely used convolutional neural nets rely almost exclusively on supervised learning. Basically, that means that if you want it to learn how to identify a particular object, you have to label more than a few examples. Yet unsupervised learning---or learning from unlabeled data---is closer to how real brains learn, and some deep learning research is exploring this area. ""How this is done in the brain is pretty much completely unknown. Synapses adjust themselves, but we don't have a clear picture for what the algorithm of the cortex is,"" says LeCun. ""We know the ultimate answer is unsupervised learning, but we don't have the answer yet."" It's also unlikely that back prop---what neural-net expert Yoshua Bengio calls ""the workhorse of most deep learning systems""---mirrors the human brain, so researchers are developing alternatives. Plus, the way that convolutional nets ""pool,"" or average, data doesn't sit well with some, so there are efforts to improve this as well. ""It loses information,"" says Hinton. Say you're looking at faces. The system learns to recognize facial features, like eyes and lips. Based on these, it's good at identifying that there is a face in an image, but much less tuned to picking out differences between faces. If you want to know the precise location of the eyes in a face, for example, it can't do that very well. As tech companies and governments want to build more detailed digital dossiers of their customers and citizens, these kinds of limitations will become, well, limiting. The general ideas behind LeCun's convnets may not be perfect, but they're the state of the art. ""He turned out to be completely right,"" says Hinton, before adding with a quick laugh, ""except for the pooling."" LeCun's work extends well beyond neural nets. In the late '90s, he had a hand in building a seminal image compression system. The idea was to scan documents and then put these up on the web for all to see. The technology never quite panned out for LeCun, but the concepts behind the technology impressed a young Larry Page, the co-founder of Google, who heard a talk LeCun gave at Stanford in 1998 when he was still a graduate student. LeCun has also worked with robotics and AI hardware. He recently founded NYU's Center for Data Science. And he has mentored a new generation of AI researchers, including Clement Fabaret, whose image-indexing company Madbits was recently acquired by Twitter. In his spare time, he builds model planes. With that pedigree, it's no surprise that Zuckerberg asked LeCun to help the company make sense of all its data. After all, the social network has been busy acquiring companies---like the virtual-reality company Oculus, solar-powered drone maker Ascenta, and WhatsApp---whose products could benefit from the type of AI LeCun pioneered. LeCun is actively looking to hire more AI talent at the company---Rob Fergus, with whom he collaborated with at NYU, is already part of his team at Facebook---and he's been tasked with turning the AI lab into a world-class research outfit, a place to compete with Google, Microsoft, IBM and Baidu, to be sure, but also an operation that harkens back to Bell Labs, which served as a breeding ground for innovation and the birthplace of many of the technologies we take for granted today, including deep learning. A scientist at heart, he also wants to keep developing his own ideas. ""I'm not giving up on research, on the contrary. I'm opening up a new venue for new research to take place,"" he says. ""It's a much more exciting situation to be in---when the contribution you make is valued."""
739,https://www.wired.com/2014/08/ibm-unveils-a-brain-like-chip-with-4000-processor-cores/,Wired,2014,8,7,1452.0," The human brain is the world's most sophisticated computer, capable of learning new things on the fly, using very little data. It can recognize objects, understand speech, respond to change. Since the early days of digital technology, scientists have worked to build computers that were more like the three-pound organ inside your head. Most efforts to mimic the brain have focused on software, but in recent years, some researchers have ramped up efforts to create neuro-inspired computer chips that process information in fundamentally different ways from traditional hardware. This includes an ambitious project inside tech giant IBM, and today, Big Blue released a research paper describing the latest fruits of these labors. With this paper, published in the academic journal Science, the company unveils what it calls TrueNorth, a custom-made ""brain-like"" chip that builds on a simpler experimental system the company released in 2011. TrueNorth comes packed with 4,096 processor cores, and it mimics one million human neurons and 256 million synapses, two of the fundamental biological building blocks that make up the human brain. IBM calls these ""spiking neurons."" What that means, essentially, is that the chip can encode data as patterns of pulses, which is similar to one of the many ways neuroscientists think the brain stores information. ""This is a really neat experiment in architecture,"" says Carver Mead, a professor emeritus of engineering and applied science at the California Institute of Technology who is often considered the granddaddy of ""neuromorphic"" hardware. ""It's a fine first step."" Traditional processors---like the CPUs at the heart of our computers and the GPUs that drive graphics and other math-heavy tasks---aren't good at encoding data in this brain-like way, he explains, and that's why IBM's chip could be useful. ""Representing information with the timing of nerve pulses…that's just not been a thing that digital computers have had a way of dealing with in the past,"" Mead says. >TrueNorth comes packed with 4,096 processor cores, and it mimics one million human neurons and 256 million synapses. IBM has already tested the chip's ability to drive common artificial intelligence tasks, including recognizing images, and according to the company, its neurons and synapses can handle such tasks with usual speed, using much less power than traditional off-the-shelf chips. When researchers challenged the thing with DARPA’s NeoVision2 Tower dataset---which includes images taken from video recorded atop Stanford University's Hoover Tower---TrueNorth was able to recognize things like people, cyclists, cars, buses, and trucks with about 80 percent accuracy. What's more, when the researchers then fed TrueNorth streaming video at 30 frames per second, it only burned 63 mW of power as it processed the data in real time. ""There's no CPU. There's no GPU, no hybrid computer that can come within even a couple of orders of magnitude of where we are,"" says Dharmendra Modha, the man who oversees the project. ""The chip is designed for real-time power efficiency."" Nobody else, he claims, ""can deliver this in real time at the vast scales we're talking about."" The trick, he explains, is that you can tile the chips together easily to create a massive neural network. IBM created a 16-chip board just a few weeks ago that can process video in real time. Both these chips and this board are just research prototypes, but IBM is already hawking the technology as something that will revolutionize everything from cloud services, supercomputers, and smartphone technology. It's ""a new machine for a new era,"" says Modha. ""We really think this is a new landmark in the history of brain-inspired computing."" But others question whether this technology is all that different from current systems and what it can actually do. IBM's chip research is part of the SyNAPSE project, short for Systems of Neuromorphic Adaptive Plastic Scalable Electronics, a massive effort from DARPA, the Defense Department's research arm, to create a brain-like hardware. The ultimate aim of the project---which has invested about $53 million since 2008 in IBM's project alone---is to create hardware that breaks the von Neumann paradigm, the standard way of building computers. In a von Neumann computer, the storage and handling of data is divvied up between the machine’s main memory and its central processing unit. To do their work, computers carry out a set of instructions, or programs, sequentially by shuttling data from memory (where it’s stored) to the CPU (where it's crunched). Because the memory and CPU are separated, data needs to be transferred constantly. >Ever since, scientists have been trying to understand how the brain encodes and processes information with the hope that they can translate that into smarter computers. This creates a bottleneck and requires lots of energy. There are ways around this, like using multi-core chips that can run tasks in parallel or storing things in cache---a special kind of memory that sits closer to the processor---but this buys you only so much speed-up and not so much in power. It also means that computers are never really working in real-time, says Mead, because of the communication roadblock. We don't completely understand how the brain works. But in his seminal work, The Computer and the Brain, as John von Neumann himself said that brain is something fundamentally different from the computing architecture that bears his name, and ever since, scientists have been trying to understand how the brain encodes and processes information with the hope that they can translate that into smarter computers. Neuromorphic chips developed by IBM and a handful of others don't separate the data-storage and data-crunching parts of the computer. Instead, they pack the memory, computation and communication parts into little modules that process information locally but can communicate with each other easily and quickly. This, IBM researchers say, resembles the circuits found in the brain, where the separation of computation and storage isn’t as cut and dry, and it's what buys the thing added energy efficiency---arguably the chip's best selling point to date. But some question how novel the chip really is. ""The good point about the architecture is that memory and computation are close. But again, if this does not scale to state-of-art problems, it will not be different from current systems where memory and computation are physically separated,"" says Eugenio Culurciello, a professor at Purdue University, who works on neuromorphic systems for vision and helped develop the NeuFlow platform in neural-net pioneer Yann LeCun's lab at NYU. So far, it's unclear how well TrueNorth performs when it's put to the test on large-scale state-of-the-art problems like recognizing very many different types of objects. It seems to have performed well on a simple image detection and recognition tasks using used DARPA’s NeoVision2 Tower dataset. But as some critics point out, that's only five categories of objects. The object recognition software used at Baidu and Google, for example, is trained on the ImageNet database, which boasts thousands of object categories. Modha says they started with NeoVision because it was a DARPA-mandated metric, but they are working on other datasets including ImageNet. Others say that in order to break with current computing paradigms, neurochips should learn. ""It's definitely an achievement to make a chip of that scale...but I think the claims are a bit stretched because there is no learning happening on chip,"" says Nayaran Srinivasa, a researcher at HRL Laboratories who’s working on similar technologies (also funded by SyNAPSE). ""It's not brain-like in a lot of ways."" While the implementation does happen on TrueNorth, all the learning happens off-line, on traditional computers. ""The von Neumann component is doing all the 'brain' work, so in that sense it's not breaking any paradigm."" To be fair, most learning systems today rely heavily on off-line learning, whether they run on CPUs or faster, more power-hungry GPUs. That's because learning often requires reworking the algorithms and that's much harder to do on hardware because it's not as flexible. Still, IBM says on-chip learning is not something they're ruling out. Critics say the technology still has very many tests to pass before it can supercharge data centers or power new breeds of intelligent phones, cameras, robots or Google Glass-like contraptions. To think that we're going to have brain-like computer chips in our hands soon would be ""misleading,"" says LeCun, whose lab has worked on neural-net hardware for years. ""I'm all in favor of building special-purpose chips for running neural nets. But I think people should build chips to implement algorithms that we know work at state of the art level,"" he says. ""This avenue of research is not going to pan out for quite a while, if ever. They may get neural net accelerator chips in their smartphones soonish, but these chips won’t look at all like the IBM chip. They will look more like modified GPUs."""
740,https://www.wired.com/2014/07/jobs-dreamed/,Wired,2014,7,25,597.0," Steve Jobs knew how to make an entrance. In 1985, he descended onto the grounds of Svaneholm Castle in Sweden in a chopper to discuss the impact of the personal-computing revolution on education. Back then, we were just on the cusp of what he called the era of ""free intellectual energy."" And this revolution, he assured a room full of educators, would forever change the way we learned. In this new world, a computer would be more than a utilitarian, number-crunching appliance. It would give us information, anytime, anywhere. That was already starting to happen in the '80s at universities and research institutions that were hooked up to the burgeoning internet. He'd said similar things before, and these days, we are in fact living in Steve's prophesied wonderland of on-demand information. But Jobs doesn't stop here. He goes on to explain that he's jealous of Alexander the Great: not because he conquered the world -- as Steve so wanted to do with Apple -- but because he had Aristotle as his personal tutor. ""I can't ask Aristotle a question. I mean -- I can, but I won't get an answer,"" Jobs jokes in that meeting in Sweden, which you can see above. ""My hope is someday -- when the next Aristotle is alive -- we can capture the underlying world view of that Aristotle in a computer and someday, some student...will be able to ask Aristotle a question and get an answer."" What Jobs seems to be saying is that more so than books, software could make us immortal -- an idea that surely piqued his interest. One day human intellect -- the human mind -- could become an on-demand commodity. In some ways, that is already happening with internet services that let us tap experts' knowledge around the clock in personalized ways and learn from them through online courses we take on our time. Some of that is driven by artificial intelligence, and like so many others, Jobs was an AI believer. He understood that for software to take on some form of intelligence, it had to be interactive and ingest lots and lots of data -- and that could only happen when computers evolved into extensions of ourselves. It was just a matter of time before that happened -- thanks in large part to Apple, of course. Years later, one of Jobs' last moves as Apple chief was to buy Siri's voice recognition software and the AI chops of the people who built it. Siri isn't very good yet, but it's supposed to learn to adapt to a user's specific needs. Soon, though, Siri could be up for an intelligence tune-up. Some speculate that Apple -- like every other big tech company -- is moving toward deep learning, a type of artificial intelligence that seeks to build software that mimics how the human brain works. Deep learning algorithms shine when they're challenged with large datasets -- the types that are now available in large part thanks to mobile devices -- like the iPhone and iPad -- and internet-enabled gadgets. To be sure, Steve's mind-as-software prediction still hasn't come to pass. We're still a long ways off from recreating even parts of the brain and we're nowhere near being able to boot up the essence of Einstein or Boole or Tesla in a computer. For now, all we have is relatively primitive AI-powered tools that make it easier to search the net for information and make sense of the world around us. That much Steve got right. Now if we could just ask him a question."
741,https://www.wired.com/story/how-real-are-extants-robots/,Wired,2014,7,18,1372.0," The Steven Spielberg-produced sci-fi drama Extant imagines a world where human-level artifical intelligence is on the cusp of reality -- or perhaps has already evolved, with terrifying implications. Wired.co.uk speaks with Murray Shanahan, Professor of Cognitive Robotics at Imperial College London, and Nicole Carey, who works in Humanoid Robot Research and Development at Engineered Arts, about the biggest developments in real-world AI and just how scared we should be of a robot uprising. ""I think one of the things that has impressed me most in robotics and AI recently has been self-driving cars,"" says Shanahan. ""It's actually a pretty mature technology now -- Google has had cars driving around completely autonomously in California, with barely any accidents."" Shanahan is correct -- to date, the only major incidents involving a driverless car have been the fault of us flawed, fleshbags. In 2010, Google engineers said a vehicle was rear-ended at a traffic light, and in 2011, one had a minor fender bender while being manually driven by human. Considering the US has the sixth-highest number of traffic related deaths in the world (the UK sits at a considerably less-lethal 65th when both are counted by total numbers killed), perhaps a little less autonomy could be good. ""I think we're going to gradually start to see those things on our streets over the next five to ten years, starting in California and then spreading out throughout the developed world -- no doubt making a lot of taxi drivers pissed off. And I feel sorry for the taxi drivers, I should say, it's not great from their point of view."" And cabbies thought Uber was a problem. Although Carey -- who helped develop Craig, the ""RoboThespian"" used to help promote Extant in the UK -- says the reduced power consumption is ""a huge step towards wider use of robots and the development of more intelligent and adaptable robot behaviours,"" it's the development of AI personalities that is key to their evolution. ""Over the last year we're starting to see a lot more research on what we call 'social hardware' -- robotic and ambient devices with strong 'EQ', or emotional intelligence,"" Carey says. ""Face recognition, expression recognition, vocal analysis and biomimetic hardware can all combine to create machines that can better understand people. Emotionally appropriate responsiveness, fed by interpretation of multimodal communication layers, is more vital in human-robot interactions than literal understanding of speech or textual input."" Carey also points out that one of the ""biggest and most obvious developments is Google's acquisition of eight robotics companies at the end of 2013"". She continues, ""not necessarily because of what it means in the long term, which is for now purely speculative, but because in the short term it led to such an explosion of public and investor interest in robotics."" The more nuanced level of communication between humans and AI that Carey highlights is also one of Shanahan's picks for key developments. ""I'm very impressed with improvements in Apple's Siri and Google Now,"" he says. ""These products that use voice recognition as the basis of personal assistants have been gradually getting better, and you don't notice they've been getting better because they're constantly improving. I find that both recognise almost all of my speech, which is an amazing achievement because just a few years ago, even the best speech recognition systems were pretty crap! Especially if they were meant to work with a variety of different speakers."" Jeopardy as a turning point for the advancement of robots understanding complicated human speech. The show makes it even more complicated by having players give their answers in the form of a question -- ""this feathered barnyard animal is known for crossing the road"" would be answered ""what is a chicken?"", for instance. ""It's just turning the syntax around, a bit like crossword clues in a way -- they can be kind of cryptic or involve puns, all kinds of things. And it requires a huge amount of general knowledge,"" the professor says. ""[But] they produced an IBM Watson, which played the game and managed to beat the reigning champion. A very impressive piece of technology."" Extant's main AI character -- so far -- is Ethan Woods (eight year-old Pierce Gagnon), the robotic child of astronaut Molly (Halle Berry) and her engineer husband John (Goran Visnjic). Raised from ""birth"" as a normal child, the Woods aim to teach him human morals as any organic child would. It's an approach that Carey and Shanahan both think is likely to be mirrored in real robotics going forward. ""Developing an AI from a 'child' state is a nod to emergent behaviour and self-learning,"" says Carey. ""It is unlikely that true AI can be instigated in a 'top-down' manner, though not impossible. A more intuitively appealing approach is allowing behaviour and complexity to arise organically from a structure which may have high organisational complexity, but is governed by simple fundamental rules. iCub and CB2 are examples of ""childish"" robots that learn by themselves, which may be the eventual precursor to something like Ethan."" ""The writers have convincingly made it seem that Ethan is currently learning a repertoire of emotional behaviours,"" Shanahan adds. ""At one point we see him practising facial expressions in front of a mirror. Perhaps, over time, he will develop real feelings of empathy for humans [or he might] only ever imitate such feelings. In raising these questions, Extant confronts us with difficult philosophical questions about the possibilities of artificial intelligence technology, possibilities that are a way off today but that might become real in the next few decades."" The moral implication of creating what amounts to sentient virtual life is a tricky subject. If an artificial intelligence can develop real emotional responses to stimuli, can or should they be extended rights? How much of an ethical obligation do we owe robots? ""I don't have any sort of philosophical objection to the idea that we might be able to build artificial intelligence in the future that is capable of suffering and therefore deserving of rights,"" offers Shanahan. ""But I think there's different ways that we might build or achieve human level AI. If we take a very biologically inspired routes to achieving it, we build things that are very brain-like, then it's more likely that they will deserve to be attributed emotions and the capacity for suffering."" ""On the other hand, if we engineer these things from scratch, using a very different kind of technology, then ethics become very hard to say,"" he continues. ""I think there's every chance we can create something that has the appearance of having emotions and feelings but doesn't really. It'll be a very difficult and challenging exercise to untangle all that, philosophically, legally, and so on."" The opposite situation is also a concern though -- how robots and artificial intelligences may react to humans. ""People have been thinking a lot about whether, if we do build human level AI, you could then get a sort of super-intelligence -- that the AI could improve itself very rapidly,"" Shanahan suggests. ""If you build it in a certain way you might lose control of it. [Even] with perfectly benign purposes in mind, it might be very difficult to predict how it's going to achieve its goals. If it's really smart, just as a side effect, it might try to achieve its goals in ways that are very destructive, ways that might try to take resources from humans, for example."" It might sound a touch far-fetched, but some kind of ""robot apocalypse"" as a result of rogue AI is a potential outcome worth considering. ""The Terminator film always comes up in this situation, and it's a bit annoying!"" Shanahan says. ""There are certain scenarios with a science fiction feel that I think are worth taking seriously -- robots achieving world peace by destroying all humans is a very good example. It might be a rhetorical example but it is one where the robot or AI is simply doing what it was programmed to do, but nobody had worked out what the ramifications were of asking it to do that."" Clearly, we're doomed. Extant is streamed in the UK on Amazon Instant Video, with new episodes each Thursday at 9pm."
742,https://www.wired.com/2014/07/microsoft-adam/,Wired,2014,7,14,1390.0," We're entering a new age of artificial intelligence. Drawing on the work of a clever cadre of academic researchers, the biggest names in tech---including Google, Facebook, Microsoft, and Apple---are embracing a more powerful form of AI known as ""deep learning,"" using it to improve everything from speech recognition and language translation to computer vision, the ability to identify images without human help. In this new AI order, the general assumption is that Google is out in front. The company now employs the researcher at the heart of the deep-learning movement, the University of Toronto's Geoff Hinton. It has openly discussed the real-world progress of its new AI technologies, including the way deep learning has revamped voice search on Android smartphones. And these technologies hold several records for accuracy in speech recognition and computer vision. But now, Microsoft's research arm says it has achieved new records with a deep learning system it calls Adam, which will be publicly discussed for the first time during an academic summit this morning at the company's Redmond, Washington headquarters. According to Microsoft, Adam is twice as adept as previous systems at recognizing images---including, say, photos of a particular breed of dog or a type of vegetation---while using 30 times fewer machines (see video below). ""Adam is an exploration on how you build the biggest brain,"" says Peter Lee, the head of Microsoft Research. Lee boasts that, when running a benchmark test called ImageNet 22K, the Adam neural network tops the (published) performance numbers of the Google Brain, a system that provides AI calculations to services across Google's online empire, from Android voice recognition to Google Maps. This test deals with a database of 22,000 types of images, and before Adam, only a handful of artificial intelligence models were able to handle this massive amount of input. One of them was the Google Brain. But Adam doesn't aim to top Google with new deep-learning algorithms. The trick is that the system better optimizes the way its machines handle data and fine-tunes the communications between them. It's the brainchild of a Microsoft researcher named Trishul Chilimbi, someone who's trained not in the very academic world of artificial intelligence, but in the art of massive computing systems. Like similar deep learning systems, Adam runs across an array of standard computer servers, in this case machines offered up by Microsoft's Azure cloud computing service. Deep learning aims to more closely mimic the way the brain works by creating neural networks---systems that behave, at least in some respects, like the networks of neurons in your brain---and typically, these neural nets require a large number of servers. The difference is that Adam makes use of a technique called asynchrony. As computing systems get more and more complex, it gets more and more difficult to get their various parts to trade information with each other, but asynchrony can mitigate this problem. Basically, asynchrony is about splitting a system into parts that can pretty much run independently of each other, before sharing their calculations and merging them into a whole. The trouble is that although this can work well with smartphones and laptops---where calculations are spread across many different computer chips---it hasn't been that successful with systems that run across many different servers, as neural nets do. But various researchers and tech companies---including Google---have been playing around with large asynchronous systems for years now, and inside Adam, Microsoft is taking advantage of this work using a technology developed at the University of Wisconsin called, of all things, ""HOGWILD!"" HOGWILD! was originally designed as something that let each processor in a machine work more independently. Different chips could even write to the same memory location, and nothing would stop them from overwriting each other. With most systems, that's considered a bad idea because it can result in data collisions---where one machine overwrites what another has done---but it can work well in some situations. The chance of data collision is rather low in small computing systems, and as the University of Wisconsin researchers show, it can lead to significant speed-ups in a single machine. Adam then takes this idea one step further, applying the asynchrony of HOGWILD! to an entire network of machines. ""We’re even wilder than HOGWILD! in that we’re even more asynchronous,"" says Chilimbi, the Microsoft researcher who dreamed up the Adam project. Although neural nets are extremely dense and the risk of data collision is high, this approach works because the collisions tend to result in the same calculation that would have been reached if the system had carefully avoided any collisions. This is because, when each machine updates the master server, the update tends to be additive. One machine, for instance, will decide to add a ""1"" to a preexisting value of ""5,"" while another decides to add a ""3."" Rather than carefully controlling which machine updates the value first, the system just lets each of them update it whenever they can. Whichever machine goes first, the end result is still ""9."" Microsoft says this setup can actually help its neural networks more quickly and more accurately train themselves to understand things like images. ""It's an aggressive strategy, but I do see why this could save a lot of computation,"" says Andrew Ng, a noted deep-learning expert who now works for Chinese search giant Baidu. ""It's interesting that this turns out to be a good idea."" Ng is surprised that Adam runs on traditional computer processors and not GPUs---the chips originally designed for graphics processing that are now used for all sorts of other math-heavy calculations. Many deep learning systems are now moving to GPUs as a way of avoiding communications bottlenecks, but the whole point of Adam, says Chilimbi, is that it takes a different route. Neural nets thrive on massive amounts of data---more data than you can typically handle with a standard computer chip, or CPU. That's why they get spread across so many machines. Another option, however, is to run things on GPUs, which can crunch the data more quickly. The problem is that if the AI model doesn’t fit entirely on one GPU card or a single server running several GPUs, the system can stall. The communications systems in data centers aren’t fast enough to keep up with the rate at which GPUs handle information, creating data gridlocks. That’s why, some experts say, GPUs aren't ideal right now for scaling up very large neural nets. Chilimbi, who helped design the vast array of hardware and software that underpins Microsoft's Bing search engine, is among them. Microsoft is selling Adam as a ""mind-blowing system,"" but some deep-learning experts argue that the way the system is built really isn't all that different from Google's. Without knowing more details about how they optimize the network, experts say, it's hard to know how Chilimbi and his team achieved the boosts in performance they are claiming. Microsoft's results are ""kind of going against what people in research have been finding, but that’s what makes it interesting,"" says Matt Zeiler, who worked on the Google Brain and recently started his own deep-learning company Clarifai. He's referring to the fact that the accuracy of Adam increases as they add more machines. ""I definitely think more research on HOGWILD! would be great to know if that's the big winner here."" Microsoft's Lee says the project is still ""embryonic."" So far, it's only been deployed through an internal app that will identify an object after you've snapped a photo of it with your mobile phone. Lee has used it himself to identify dog breeds and bugs that might be poisonous. There's not a clear plan to release the app to the public yet, but Lee sees definite uses for the underlying technology in e-commerce, robotics, and sentiment analysis. There's also talks within Microsoft of exploring whether Adam's efficiency could improve if run on field-programmable arrays, or FPGAs, processors that can be modified to run custom software. Microsoft has already been experimenting with these chips to improve Bing. Lee believes Adam could be part of what he calls an ""ultimate machine intelligence,"" something that could function in ways that are closer to how we humans handle different types of modalities---like speech, vision, and text---all at once. The road to that kind of technology is long---people have been working towards it since the 50s---but we're certainly getting closer."
743,https://www.wired.com/2014/07/cadieu/,Wired,2014,7,3,729.0," There are so many ways that humans are still superior to machines. Though computerized brains can beat humans at things like chess and Jeopardy, they can't always make out an Irish brogue or figure out whether a fly is inside or outside of a glass---just to name two uniquely human talents. But don't get too pleased with yourself. The age of unchallenged human supremacy may be drawing to a close. According to new research from scientists at the Massachusetts Institute of Technology, computers powered by the latest ""deep learning"" algorithms are catching up. For about a decade now, brain scientists like Charles Cadieu have run image recognition tests pitting computers against monkeys. The test measures how well the monkeys can quickly process images on a screen, and then compares it to the performance of a machine. It's not exactly a test of image recognition, says Cadieu, a postdoctoral researcher at MIT, but of something more elemental; what Cadieu calls the ""neural substrate that makes recognition possible."" Until now, the test results have always been the same: the monkeys crush the computers---typically by at least a factor of ten. But now, it looks as though the AI machines have finally caught up. In fact, in Cadieu's most recent test, the AI computers are now ""pretty equivalent"" to monkey brains. It's part of a big change in artificial intelligence. The ""deep learning"" techniques used in the study are already boosting the performance of popular software such as Android and Skype, providing better speech recognition and language translation, and they're slowly changing other fields as well, including fraud detection, drug research, and virtually any other area where machines must sift through vast amount of data in a smarter fashion. >The test measures how well the monkeys can quickly process images on a screen, tracking how neurons fire in a part of the monkey brain known as the inferior temporal cortex. To run their experiments, the MIT researchers wired sensors to the brains of two macaques monkeys, flashed them a series of images, and measured how the neurons fired in a part of the monkey brain known as the inferior temporal cortex. Then they ask machines to identify the same images. The 1,960 images---each flashed for just 1/10th of a second---were of things like airplanes, cars, and elephants, but each image was varied to make it tricky for the computers to recognize it. For example, in one image, a car might be facing the monkey---up close. In another, it would be tilted at an angle and located far in the background. Computers are great at spotting identical images, but when you introduce subtle variations like this, they often run into trouble. Back in 2012, monkey neurons would blow away the computer, Cadieu says. But then came deep learning, developed by academics like Geoff Hinton and Alex Krizhevsky. ""We saw this huge leap in performance that we had not seen before,"" says Cadieu. If Alex Krizhevsky or Geoff Hinton's names sound vaguely familiar, that's because Google bought their company last year so they could help build out an artificial intelligence system at the search giant known as Google Brain. Their deep learning techniques seek to more closely mimic the behavior of the human brain, and they've improved speech recognition in Google's Android. But Krizhevsky and Hinton aren't the only ones working in this area. Facebook recently hired another visionary in the deep neural network field, New York University's Yann LeCun, to work in its artificial intelligence lab. And Apple seems well on its way to developing this technology too. The MIT researchers also tested an algorithm created by New York University's Matthew Zeiler and Rob Fergus and found that it did slightly better than Hinton's algorithm. But there's still one part of the test where nature still easily outclasses machine: energy efficiency. High-performance graphical processing units (GPUs) burn somewhere in the neighborhood of 200 to 350 watts when they're firing on all cylinders. An entire human brain burns just 20 watts. And Cadieu estimates the subsection of the monkey brains required to perform these tests are two-to-three orders of magnitude more energy efficient than GPUs. Still, the results are ""just one indication that those systems are getting better,"" says Jeff Dean, the Google engineer who is masterminding the Google Brain infrastructure. ""They're clearly not at human-level quality yet. But they're on the right trajectory."""
744,https://www.wired.com/2014/06/curalate/,Wired,2014,6,23,1066.0," The early days of the internet were all about text. Email. Newsgroups. Chat. Early web browsers didn't even do inline images---if they handled images at all. But those days are long gone. Now we spend much of our online lives watching animated GIFs on Tumblr, sharing photos on Instagram and Snapchat, and collecting images on Pinterest. Google became the company it is today by making sense of the old, text-based web. It crawled billions of webpages, weighing and ranking and indexing a galaxy of keywords to help people find what they were looking for. Now, a new crop of companies is emerging to try to make sense of the image-based web. Philadelphia-based startup Curalate is one of those companies. ""Every search [engine] assumes text is there to find things,"" co-founder and CEO Apu Gupta says. ""But if you have a platform based mostly on pictures, those systems begin to break."" Yes, some images have text and tags associated with them. But most don't. According to Curalate's research, 75 percent of all Tumblr posts are images, and 90 percent of those posts have no identifying text or tags. Curalate is trying to solve that problem with a image recognition platform its founders say can see images in much the same way the human brain does. But the company isn't building a search engine. It's building a platform to help marketers at companies like the GAP and Urban Outfitters learn more about how their customers use images of their products on social networks like Instagram and Pinterest. The company began life in 2011, but with a different name and offering a completely different service. ""It was supposed to be sort of like Airbnb for parking and storage,"" says Apu Gupta. ""But it ended up being more like Airbnb for hoarders. A few months after starting we realized it was a pretty terrible idea."" The team actually offered to return their venture funding to their investors. But the investors declined, asking the team to come up with a new idea instead. ""We had 30 days to figure something out,"" Gupta says. So Gupta and his co-founder and CTO Nick Shiftan turned their attention to Pinterest, which had just exploded in popularity. ""We realized that it was like Twitter in the early days,"" Gupta says. ""Brands wanted to be there, but they needed to measure it, figure out what they're getting out of it."" They decided to start start a Pinterest analytics service, not unlike the many Facebook and Twitter analytics companies that were already available, such as Radian6 and Lithium. But as Shiftan began trying to write the code, he realized that analyzing Pinterest activity was much different than analyzing data from other social media sites like Twitter or Facebook. He couldn't merely search the site for specific keywords because users often posted photos that included no text. He needed a way to search for images. This is harder than it may sound. In theory, you could start with a specific image file and look for other instances of that exact same file on the web. But because images are often resized, cropped, and compressed, the files can end up radically different from a computer's perspective. A more complex solution would be needed. Shiftan welcomed the challenge. He'd been dreaming of tackling more harder computer science problems since college. ""I wanted to solve something that no one had solved yet,"" he says. But he knew he couldn't do it alone since he didn't have any experience in machine vision. So the team recruited Louis Kratz, a machine vision expert with a PhD from nearby Drexel University. Kratz was well versed in all the latest machine vision research, but he says applying that work to real-world problems is hard. For example, it's easy to train a computer to tell if two image are the same. It's much harder to do this at scale, comparing millions of photos with each other to see which ones match. Kratz had to find a way to make this type of image comparison work for an application that needs to analyze billions of images. Unlike companies like Google and Facebook--which have adopted a technique of mimicking the structure of the brain called ""deep learning"" in an effort to train computers to recognize images and other tasks---Kratz opted for other machine learning techniques such as multi-index hashing and the Discrete Cosine Transform algorithm. Using these techniques, he was able to build a system for clustering similar images together, allowing a computer to sort large numbers of photos into groups and then quickly determine which photos are identical and which are merely similar. ""What Louis basically did was figure out how to process images at an enormous scale, something on the order of 200 million new images per day,"" Shiftan says. Once you're able to process images on this level, there's quite a bit you can learn from this type of data, Gupta says. ""Most companies have multiple images of the same product, so this helps find out which versions of the images are more popular,"" he explains. Yes, companies have long been able to measure which photos lead to more sales based on their own sites, but Gupta says that with so much activity taking place on social networks, it's important to also consider what customers are doing on those sites. ""By voting things up and down or re-pinning them on Pinterest, customers are telling you what products or images matter."" And in cases that there are captions or text, Curalate is able to find out how products are being used, which can be useful for determining how to market a product, and what text to use on a site to help shoppers find what they're looking for more easily. For example, if a particular sweater is often tagged as ""New Years Eve sweater,"" a company's marketing team could create a ""best outfits for New Year's"" section on their site to highlight that sweater. Gupta says these are things you just couldn't do on text-based social networks. ""Before it was all about liking a brand page on Facebook or following them on Twitter,"" he says. ""That doesn't say why you like a brand, though. It just says 'I like the GAP.' But on Pinterest, people don't 'like' the GAP. They pin a whole bunch of individual items, so you know what someone likes about the brand."""
745,https://www.wired.com/2014/06/beyond-the-turing-test/,Wired,2014,6,12,942.0," A chatbot pretending to be a 13-year-old Ukrainian boy made waves last weekend when its programmers announced that it had passed the Turing test. But the judges of this test were apparently easily fooled, because any cursory exchange with 'Eugene Goosterman' reveals the machine inside the ghost. Maybe the time has come, 60 years after Alan Turing's death, to discard the idea that imitating human conversation is a good test of artificial intelligence. ""I start my Cognitive Science class with a slide titled 'Artificial Stupidity,'"" said Noah Goodman, director of the computation and cognition lab at Stanford University. ""People have made progress on the Turing test by making chatbots quirkier and stupider."" Non-sequiturs, spelling errors, and humor all make a chatbot seem more human. The history of the Loebner prize, an annual Turing test competition, confirms this trend. Last year's contest was won by a bot named Mitsuku also pretending to be young ESL speaker, a silly Japanese girl. Even Turing anticipated that evasion might be the most human answer to a hard question: Q: Please write me a sonnet on the subject of the Forth Bridge. A : Count me out on this one. I never could write poetry. If not the Turing test, is there an alternative measure of intelligence that would bring out the best in our machines? Experts have suggested an array of challenging tasks in the very human domains of language, perception, and interpretation. Perhaps a computer passing one of these tests would seem not just like a person, but like an intelligent person. Let's look first at language comprehension, as computers can easily interact with text. In the following sentence, the person referred to by ""he"" depends on the verb: ""Paul tried to call George on the phone, but he was not [successful/available]."" You, human reader, know that if he is not successful then ""he"" is Paul, and if he is not available then ""he"" is George. To figure that out, you needed to know something about the meaning of the verb ""to call."" Machine learning researcher Hector Levesque of the University of Toronto proposes that resolving such ambiguous sentences, called Winograd schema, is a behavior worthy of the name intelligence. Because humans interact with the world through sight and sound, not strings of letters, a stronger test of human-like intelligence might include speech and image processing. Computer speech and text recognition has improved rapidly in the last twenty years, but are still far from perfect. When asked a question about the Turing test, Apple's Siri answered about a ""touring"" test. Bots struggle to decipher squiggly letters, which is why you have to fill out a CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) when you sign up for things like Facebook. Humans are also exceptionally good at recognizing faces. At the age of six months, a typical baby can pick out its mother's face from a crowd. Computer vision researcher Avideh Zakhor at UC Berkeley says we should aim for computers to ""be as good as the best human, or better than the best human"" at recognizing objects and people. We could further ask for the computer to interpret audio-visual phenomena and then reason about them. ""An example of a task is a system providing a running commentary on a sporting match,"" said Michael Jordan, a machine learning researcher at UC Berkeley. ""Even more difficult: The system doesn't know about soccer, but I explain soccer to the system and then it provides a running commentary on the match."" That goal won't be scored for a while. A computer capable of achieving any of these tasks would certainly be impressive, but would we call it intelligent? Fifty years ago, we thought a computer that could beat a grandmaster at chess would necessarily be intelligent, but then Deep Blue passed that test, yet can't even play checkers. Watson, the Jeopardy computer, knows more than Deep Blue about the human world of drinks and cities and movies, but it can only answer one kind of question about those things (in the form of a question of course). As computers become more powerful and pervasive, our standards shift. Fifty years from now, a soccer-learning, header-calling, wise-cracking machine might seem more like a party trick than a thinking being. ""If you fix a landmark goal, you tend to end up with systems that are narrow and inflexible,"" said UC Berkeley computer scientist Stuart Russell. ""In developing general-purpose AI we look for breadth and depth of capabilities and flexibility in developing new capabilities automatically."" A different kind of mission might be preferable, one which can expand with our own abilities and desires, something in the spirit of Google's quest to ""organize the world's information."" After all, UPS already routes millions of packages a day, hospitals sequence patients' DNA to find cancer-causing mutations, and Google can in a millisecond report the age at which children begin to recognize their mother. These abilities are ""fricking fantastic, and way beyond the capability of a person,"" said Goodman, the computation and cognitive science researcher at Stanford. ""So in some sense the programs are super intelligent, super human, but because of our common-sense notion, we say that’s not intelligence, that’s something else.” As functions proliferate, some may become united behind a more flexible user interface and be powered by a deeper corpus. There might be a machine that can teach you a dance that it learned by watching YouTube and diagnose a disease by smelling your breath. You could ask that machine to simulate human behaviors in order to pass the old Turing test, but that would be insulting to everyone's intelligence."
746,https://www.wired.com/2014/06/the-future-of-computer-intelligence-is-everything-but-artificial/,Wired,2014,6,11,1008.0," ­Despite a flood of Sunday morning hype, it’s questionable whether computers crossed an artificial intelligence threshold last weekend. However, the news about a chatbot with the personality of a 13-year-old Ukrainian boy passing the Turing test did get us thinking: Is tricking every third human in a text exchange really the best way to measure computer intelligence? Computers are already smart, just in their own ways. They catalogue the breadth of human knowledge, find meaning in mushroom clouds of data, and fly spacecraft to other worlds. And they're getting better. Below are four domains of computing where the machines are rising. Given the right set of rules, computers are the ultimate librarians. Google's search algorithm shakes down 50 billion web pages every time you need to prove your boyfriend wrong about his latest baseless assertion. It's so good at its job that many people consider clicking to the second page of search results an act of desperation. Where it's headed: Understanding human language is one of the most difficult things computers can do. Beyond basic subject/verb agreement, decades of bots have mostly failed at figuring out the vagaries of the written word. Unlike us, computers struggle to understand how a word can change meaning depending on its neighbors, says Russ Altman, a biomedical informatics researcher at Stanford. Solving this problem is Altman's obsession. Since 2000, he and his colleagues have been teaching a machine how to get meaning from some of the densest language on the planet: medical journalese. The Pharmacogenomics Knowledge Base (PharmaGKB) has read 26 million scientific abstracts to create a searchable index of different effects that various drugs have on individual genes. The program understands things like clauses and how the meaning of a word can be modified by the words around it (which is important for parsing dense phrasing that might send a confusing message about whether a drug activates a gene), and also knows many synonyms and antonyms. The resulting database is hugely important to pharmaceutical companies, who use it to save time and money on basic research when they are searching for new drug combinations. Robots that work in controlled environments, like car manufacturing plant, are impressive enough. But getting them to do programmed tasks alongside humans, who have complex behaviors, is one of the most difficult challenges in computing. The vanguard of intelligent robotics are droids that let humans do tasks that require creative thought or fine manipulation, and fill in the organization and heavy lifting where needed. For example, Amazon already has armies of organizational droids that shuttle items for packing from a Manhattan-like grid of shelving towers to human packers. Where it's headed: Researchers are getting better at teaching robots how to read the syntax of human movement, so they can work more closely on more complicated projects. David Bourne, a roboticist at Carnegie-Mellon University's Robotics Institute, says the key is to play to both the human and robot strengths. ""A person is actually more dextrous, but a robot can move to an exact position really well."" Bourne made a robotic arm that assists automobile welders. In a trial, the human-robot team assembled a Hummer frame. The robot had a video projector that showed the human exactly where to put different parts and then made perfect, 5-second welds. For more difficult welds, it deferred to its partner. ""Together they were able to do the project 10 times faster than a team of three human professionals,"" says Bourne. Machine learning is a sub-discipline of AI that uses trial-and-error to figure out complex problems. For example, a cloud service might spend a weekend feeding House of Cards to half a million people, or run through millions of iterations to help a lending bank evaluate credit risk scenarios. Getting data to flow to the right places requires constant adaptation to respond to the network’s shifting bandwidth bottlenecks. Cloud providers like Amazon use algorithms learn from the varying demands, so the bitrate stays high. Where it's headed: Machine learning isn’t just keeping the cloud clutter-free; it’s going to turn smart phones into geniuses. Current machine learning programs can require hundreds or thousands of iterations, but researchers are building animal-inspired algorithms that can learn good from bad after only a few trials. Tony Lewis is the lead developer at Qualcomm’s Zeroth Project, an R&D lab that’s building next-gen chipsets and programs that run on them. ""We've been able to demonstrate in a very simple application how you can use reinforcement learning to teach a robot to do the right thing,"" he says. Eventually he sees this technology making its way into phones and tablets. Instead of having to access the settings to change your ring tone or turn off your alarm on the weekend, you could just give it positive or negative reinforcement, like giving a dog a treat, and it would learn. Computers have come a long way in interpreting complex inputs like sound, movement, and image recognition. But there’s room to grow: Siri still makes mistakes, Kinect hasn't totally revolutionized gaming, and Google needed 16,000 processors to train a computer to identify cat videos on YouTube. This is mostly because things like language and kittens can’t be easily reduced to binary equations. But new processors could process with logic more akin to the way neurons work—passing along many different information streams in parallel. Where it's headed: Several researchers (including Lewis) are trying to creating chips that work more like brains than calculators. This field is called neuromorphic computing. Like a brain, a neural processing unit (NPU) processes many different data streams at the same time. The end goal is to have devices that can read complex sensory information (like voices and flailing limbs) at a fraction of the computational cost of traditional chips. This means that Siri’s daughter will be able to answer your questions faster, with less prompting, and without being as much of a drain on your battery. These NPUs will run alongside traditional, binary CPUs, which will still be essential for running things like operating systems and tip calculators."
747,https://www.wired.com/2014/06/trust-robots-2/,Wired,2014,6,10,1032.0," A robot could save your life, if you’re smart enough to let it. Just think about self-driving cars: At the very least, they would eliminate road deaths from drunk and distracted driving, of which there are currently more than 13,000 in the United States every year. But most Americans say they wouldn’t feel comfortable riding in an automated vehicle—and if that mentality doesn’t change, there’s no chance of the technology taking off. It’s the same across many different domains, where robotic and artificially intelligent systems have the potential to improve safety and boost productivity. Ironically, the crucial factor in the success of automation is always the one human action that can’t be automated: the decision to trust the robot in the first place. And even when the robot isn’t calling all the shots—when we’re just working with a bot—we usually keep our guards up. When robots do things we don’t understand, like sensing obstacles we can’t or following rules we don’t know, we tend to lose confidence and wrest control away from them—even when the robots are right. Laboratory studies have also shown how easy it is to shake our faith: When a system designed to warn drivers of impending collisions was prone to false alarms, users’ trust declined precipitously, despite the fact that the system never missed a true threat. In another study, 81 percent of volunteers chose to abandon a program they were told could predict whether camouflaged soldiers were hidden in photographs, even after feedback revealed that they were making far more mistakes than the computer. The reason? In the researchers’ words, nearly a quarter of participants “justified their disuse by stating they did not trust the automated aid as much as they trusted themselves.” In other words, even when confronted with evidence of our own inferiority, we resist a robot’s help. WHEN ROBOTS DO THINGS WE DON’T UNDERSTAND, LIKE FOLLOW RULES WE DON’T KNOW, WE WREST CONTROL AWAY FROM THEM—EVEN WHEN THEY’RE RIGHT. Clearly we’re going to need to learn how and when to trust machines. It’s for our own good. The trick to accomplish this, it turns out, may be to program a little humility into the system—by designing machines that acknowledge their own weaknesses. Consider experiments conducted by Holly Yanco, a roboticist at the University of Massachusetts Lowell, and colleagues at Carnegie Mellon University in Pittsburgh. The researchers asked volunteers to drive a small, tank- like robot—about 3 feet long and nicknamed Junior—through a slalom course of cardboard boxes. The goal was to complete the course as quickly as possible while sticking to a prescribed path. Participants could operate the robot manually, using a joystick to steer. Or they could keep Junior in a fully autonomous mode, letting it navigate on its own. The course was consider- ably faster to traverse with the robot in this setting; but left to its own devices, Junior would sometimes make mistakes, turning to the wrong side of a box. Participants were free to switch between the two modes as often as they liked. But Yanco also programmed Junior with something novel: the ability to express self-doubt. That is, in some trials, Junior provided real- time feedback on its own performance, telling its human operator how confident it was that the turn it was about to make was correct. When the machine was on track, it would display a green light or a smiling face; shortly before making a wrong turn, the robot would show a red light or a frowning face. (Yanco and her colleagues had programmed the robot to make some mistakes, but they told subjects that the warning light meant Junior was no longer confident in its sensor readings.) Receiving the robot’s (simulated) confessions of fallibility allowed the participants to feel more comfort- able balancing their reliance on the machine with their need to jump in; they used the autonomous mode when the robot was most depend- able and switched to manual mode when the robot’s performance was about to falter. As a result, they made fewer wrong turns than a control group receiving no robot feedback. “We thought there was a chance that the robots saying ‘I’m not doing so well’ would lead people to trust them less,” says Aaron Steineld,an engineer at Carnegie Mellon. It had the exact opposite reaction. The machine’s self-effacing play- by-play kept trust levels high. The control group, meanwhile, began to mistrust the machine because it was making mistakes seemingly out of the blue. WE NEED MACHINES THAT COP TO THEIR OWN VULNERABILITIES. ROBOTS SHOULD TELL US THAT THEY MIGHT FAIL, BUT ALSO EXPLAIN WHY. These finding are of a piece with earlier research on automated systems. In a 2006 study, trained pilots used a flight simulator under conditions that could cause ice to build up on the outside of a plane, which could lead to a stall. The pilots were told that the Smart Icing System had an overall accuracy of 70 percent. But while they were flying, some pilots were also shown a graph that displayed, in real time, the system’s confidence in its own diagnoses—a gesture of humility from the machine. Pilots who received real-time-confidence information were more likely to both disregard the bad recommendations and heed the good ones. They experienced significantly fewer stalls caused by ice. The message is clear: As robots insinuate themselves ever more deeply into our lives, understanding their limitations will be as crucial as knowing their capabilities. And so we need machines that cop to their own vulnerabilities. In fact, robots should tell us not only that they might fail but also explain why—letting us know, for instance, that certain conditions cause their sensors to be less reliable or that certain situations cause their decision-making models to break down. In the end, establishing trust and building productive relation- ships with robots won’t be all that different from doing so with people. After all, a good colleague wouldn’t just bail out on a group presentation. Instead, they’d warn you that they tend to stammer and sweat when speaking in front of an audience and then offer to pick up the slack somewhere else. We shouldn’t let our robo-colleagues get away with anything less."
748,https://www.wired.com/2014/06/turing-test-not-so-fast/,Wired,2014,6,9,889.0," Over the weekend, a group of programmers claimed they built a program that passed the famous Turing Test, in which a computer tries to trick judges into believing that it is a human. According to news reports, this is a historic accomplishment. But is it really? And what does it mean for artificial intelligence? The Turing Test has long been held as a landmark in machine learning. Its creator, British computer scientist Alan Turing, thought it would represent a point when computers would have brains nearly as capable as our own. But the value of the Turing Test in modern day computer science is questionable. And the actual accomplishments of the test-winning chatbot are not all that impressive. The Turing Test 2014 competition was organized to mark the 60th anniversary of Turing's death and included several celebrity judges, including actor Robert Llewellyn of the British sci-fi sitcom Red Dwarf. The winner was a program named Eugene Goostman, which managed to convince 10 out of 30 judges that it was a real boy. Goostman is the work of computer engineering team led by Russian Vladimir Veselov and Ukrainian Eugene Demchenko. The program had a few built-in advantages, such as the fact that he was claimed to be a 13-year-old non-native English speaker from Ukraine. It also only tricked the judges about 30 percent of the time (an F minus, or so). For many artificial intelligence experts, this is less than exciting. ""There's nothing in this example to be impressed by,"" wrote computational cognitive scientist Joshua Tenenbaum of MIT in an email. He added that ""it's not clear that to meet that criterion you have to produce something better than a good chatbot, and have a little luck or other incidental factors on your side."" Screenshots on the BBC's article about the win show a transcript that doesn't read like much more than a random sentence generator. When WIRED chatted with Goostman through his programmers' Princeton website, the results felt something like an AIM chatbot circa 1999. WIRED: Oh, I'm from the Ukraine. Have you ever been there?Goostman: ukraine? I've never there. But I do suspect that these crappy robots from the Great Robots Cabal will try to defeat this nice place too. The version on the website could of course be a different version than was used during the competition. This particular chatbox almost passed a version of the Turing test two years ago, fooling judges approximately 29 percent of the time. Fooling around 30 percent of the judges also doesn't seem like a particularly high bar. While the group claims that no previous computer program has been able to reach this level, there have been numerous chatbots, some as far back as the 1960s, which were able to fool people for at least a short while. In a 1991 competition, a bot called PC Therapist was able to get five out of 10 judges to believe it was human. More recently, there have been fears that online chatbots could trick people into falling in love with them, stealing their personal information in the process. And a 2011 demonstration had a program named Cleverbot manage a Turing Test pass rate of nearly 60 percent. So where does this 30 percent criterion stem from? It seems to be a particular interpretation of Alan Turing's 1950 paper where he described his eponymous test. ""I believe that in about fifty years' time it will be possible, to programme computers... to make them play the imitation game so well that an average interrogator will not have more than 70 per cent chance of making the right identification after five minutes of questioning,"" wrote Turing (.pdf). So the father of the Turing test wasn't using this as some threshold for intelligence, he was simply stating his prediction of where he thought computers would be five decades in the future. For most modern-day artificial intelligence experts, the Turing Test has long since been superseded by other accomplishments. It's not entirely surprising that a 65-year-old test doesn't hold up, given the lack of data about intelligence — both human and artificial — available at the dawn of the computer age. Today, we have programs that show quite interesting intelligent-like behavior, such as Netflix's suggestion algorithm, Google's self-driving car, or Apple's Siri personal assistant. These are all tailored to specific tasks. What Alan Turing had envisioned was a machine that was generally intelligent; it could just as easily organize your schedule as learn Latin. This has lead cognitive scientist Gary Marcus of NYU to suggest an updated, 21st-century version of the Turing Test. Writing at the New Yorker's Elements blog, he said that a truly intelligent computer could ""watch any arbitrary TV program or YouTube video and answer questions about its content—'Why did Russia invade Crimea?' or 'Why did Walter White consider taking a hit out on Jessie?'"" Marcus continues: Chatterbots like Goostman can hold a short conversation about TV, but only by bluffing. (When asked what “Cheers” was about, it responded, “How should I know, I haven’t watched the show.”) But no existing program—not Watson, not Goostman, not Siri—can currently come close to doing what any bright, real teenager can do: watch an episode of “The Simpsons,” and tell us when to laugh. Of course, who knows what they'll say about that test in 50 years time."
749,https://www.wired.com/2014/06/ai-healthcare/,Wired,2014,6,2,1583.0," Long Island dermatologist Kavita Mariwalla knows how to treat acne, burns, and rashes. But when a patient came in with a potentially disfiguring case of bullous pemphigoid--a rare skin condition that causes large, watery blisters--she was stumped. The medication doctors usually prescribe for the autoimmune disorder wasn't available. So she logged in to Modernizing Medicine, a web-based repository of medical information and insights. Within seconds, she had the name of another drug that had worked in comparable cases. ""It gives you access to data, and data is king,"" Mariwalla says of Modernizing Medicine. ""It's been very helpful, especially in clinically challenging situations."" The system, one of a growing number of similar tools around the country, lets Mariwalla tap the collective knowledge gathered from roughly 3,700 providers and more than 14 million patient visits, as well as data on treatments other doctors have provided to patients with similar profiles. Using the same kind of artificial intelligence that underpins some of the web's largest sites, it instantly mines this data and spits out recommendations. It's a bit like Amazon.com recommending purchases based on its massive trove of data about what people have bought in the past. >Using the same kind of artificial intelligence that underpins some of the web's largest sites, it instantly mines this data and spits out recommendations. Tech titans like Google, Amazon, Microsoft, and Apple already have made huge investments in artificial intelligence to deliver tailored search results and build virtual personal assistants. Now, that approach is starting to trickle down into health care, thanks in part to the push under the health reform law to leverage new technologies to improve outcomes and reduce costs--and to the availability of cheaper and more powerful computers. In an effort to better treat their patients, doctors are now exploring the use of everything from IBM's Watson supercomputer, the machine that won at Jeopardy, to iPhone-like pop-up notifications that appear in your online medical records. Artificial intelligence is still in the very early stages of development--in so many ways, it can't match our own intelligence--and computers certainly can't replace doctors at the bedside. But today's machines are capable of crunching vast amounts of data and identifying patterns that humans can't. Artificial intelligence--essentially the complex algorithms that analyze this data--can be a tool to take full advantage of electronic medical records, transforming them from mere e-filing cabinets into full-fledged doctors' aides that can deliver clinically relevant, high-quality data in real time. ""Electronic health records [are] like large quarries where there’s lots of gold, and we're just beginning to mine them,"" said Dr. Eric Horvitz, who is the managing director of Microsoft Research and specializes in applying artificial intelligence in health care settings. Increasingly, physician practices and hospitals around the country are using supercomputers and homegrown systems to identify patients who might be at risk for kidney failure, cardiac disease, or postoperative infections, and to prevent hospital re-admissions, another key focus of health reform. And they're starting to combine patients' individual health data--including genetic information--with the wealth of material available in public databases, textbooks, and journals to help come up with more personalized treatments. For now, the recommendations from Modernizing Medicine are largely based on what is most popular among fellow professionals--say, how often doctors on the platform prescribe a given drug or order a particular lab test. But this month, the system will display data on patient outcomes that the company has collected from its subscribers over the past year. Doctors will also be able to double-check the information against the latest clinical research by querying Watson, IBM’s artificially intelligent supercomputer. ""What happens in the real world should be informed by what’s happening in the medical journals,"" said Daniel Cane, CEO of Florida-based Modernizing Medicine. ""That information needs to get to the provider at the point of care."" Using homegrown systems, doctors at Vanderbilt University Medical Center in Nashville and St. Jude’s Medical Center in Memphis are getting pop-up notifications within individual patients' electronic medical records. The alerts tell them, for instance, when a drug might not work for a patient with certain genetic traits. It shows up in bright yellow at the top of a doctor's computer screen--hard to miss. ""With a single click, the doctor can prescribe another medication. It's a very quick and seamless process,"" says Vanderbilt's Dr. Joshua Denny, one of the researchers who developed the system there. Denny and others used e-medical records on 16,000 patients to help computers predict which patients were likely to need certain medications in the future. Take the anti-blood clot medication Plavix. Some people can't break it down. The Vanderbilt system warns doctors to give patients likely to need the medication a genetic test to see whether they can. If not, it gives physicians suggestions on alternative drugs. Doctors heed the computer's advice about two-thirds of the time, figuring in, for example, the risks associated with the alternative medication. ""The algorithm is pretty good,"" says Denny, referring to its ability to predict who's going to need a certain drug. ""It was smarter than my intuition."" So far, computers have gotten really good at parsing so-called structured data—information that can easily fit in buckets, or categories. In health care, this data is often stored as billing codes or lab test values. But this data doesn't capture patients' full-range of symptoms or even their treatments. Images, radiology reports, and the notes doctors write about each patient can be more useful. That’s unstructured data, and computers are less savvy at handling it because it requires making inferences and a certain understanding of context and intent. That's the stuff humans are really good at doing--and it's what scientists are trying to teach machines to do better. ""Computers are notoriously bad at understanding English,"" said Peter Szolovits, the director of MIT’s Clinical Decision Making Group. ""It's a slow haul, but I'm still optimistic."" Computers are getting better at reading unstructured information. Suppose a patient says he doesn't smoke. His doctor checks 'no' in a box--structured data, easily captured by a machine. But then the doctor notes that the patient's teeth are discolored or that there are nicotine stains on his fingers--a clue that the patient in fact does smoke. Soon a computer may be able to highlight such discrepancies, bringing to the fore information that otherwise might have been overlooked. In recent years, universities, tech companies, and venture capital firms have invested millions into making computers better at analyzing images and words. Companies are popping up to capitalize on findings in studies suggesting that artificial intelligence can be used to improve care. ""Artificial intelligence--ultimately that's where the biggest quality improvements will be made,"" says Euan Thomson, a partner at venture capital firm Khosla Ventures. >The data is often stored in servers at individual clinics or hospitals, making it difficult to build a comprehensive reservoir of medical information. But many challenges remain, experts say. Among them is the tremendous expense and difficulty of gaining access to high-quality data and of developing smart models and training them to pick up patterns. Most electronic medical record-keeping systems aren't compatible with each other. The data is often stored in servers at individual clinics or hospitals, making it difficult to build a comprehensive reservoir of medical information. Moreover, the systems often aren’t hooked up to the internet and therefore can't be widely distributed or accessed like other information in the cloud. So, unlike the vast amount of data on Google and Facebook, the information can't be mined from anywhere by those interested in analyzing it. From the perspective of privacy advocates, this makes some good sense: A researcher’s treasure trove is a hacker’s playground. ""It's not the greatest time to talk about"" health records on the web, given security scandals such as the Edward Snowden leaks and the Heartbleed bug, says Dr. Russ Altman, the director of Stanford University’s biomedical informatics training program. Also standing in the way are concerns about how far computers should encroach on doctors' turf. As artificial intelligence systems get smarter, experts say, the line between making recommendations and making decisions could become more murky. That could cause regulators to view the systems as a medical devices, subject to the review of the U.S. Food and Drug Administration. Wary of the time and expense required for FDA approval, companies engineering the systems--at least for now--are careful not to describe them as diagnostic tools but rather as information banks. ""The FDA would be down on them like a ton of bricks because then they would be claiming to practice medicine,"" says MIT’s Szolovits. At the moment, he said, the technology isn't good enough to tell doctors with 100 percent certainty what the best course of treatment for a patient may be. Others agree. ""It's going to be a long road,"" says Michael Matheny, a biostatistician at the Vanderbilt School of Medicine. Back at her clinic in Long Island, Dr. Mariwalla is thankful for the information that the artificial intelligence system can provide. For the patient with that blistering skin condition, she took the machine’s suggestion for an alternative medication. The patient has recovered, Mariwalla says, but she's careful to add that she made the call herself—based in part on her conversation with her patient. ""That's where medical judgment comes in,"" she says. ""You can’t [just] rely on a system to tell you what to do."" Kaiser Health News is an editorially independent program of the Henry J. Kaiser Family Foundation, a nonprofit, nonpartisan health policy research and communication organization not affiliated with Kaiser Permanente."
750,https://www.wired.com/story/singularity-on-screen/,Wired,2014,5,19,382.0," This article was taken from the May 2014 issue of Wired magazine. Be the first to read Wired's articles in print before they're posted online, and get your hands on loads of additional content by <span class=""s1"">subscribing online. Wally Pfister knows a thing or two about ambitious film-making. Transcendence, isn't a modest affair. It's a thriller about artificial super-intelligence, starring Johnny Depp as a murdered AI researcher who gets uploaded to a computer, becoming more powerful than his political enemies could imagine. ""From the moment I saw 2001: A Space Odyssey when I was seven years old, I was fascinated with AI and technology,"" says Pfister, an avid Wired reader (a copy appears in the film, with Depp's character on the cover). The film's focus is the technological singularity -- that supposed point in the future when the intelligence of machines overtakes that of our own -- and its potential consequences. ""You can't help but consider what could go wrong,"" says Pfister. ""If you take every neuron and synapse from a person and you put them inside a machine, what do you end up with? If you're capable of capturing human emotions, does that contain all the pitfalls? Is there jealousy, rage? Emotional responses where there should be intellectual ones? It's fascinating, that notion of tampering with a very organic creation."" While writing and making the film, Pfister met a team of scientists from MIT, Berkeley, Stanford and the California Institute of Technology to learn about nanotechnology in cancer research, robotics, AI and neurobiology. ""I brought two Berkeley scientists in for every part of the process: during the writing; when we were selecting props; when we were filming the brain upload -- they vetted the science. They actually had more advanced ways of doing things than what I was portraying, but it was too graphic for me to include -- like taking off the cranium and tapping into the brain directly."" For all the politics inherent in the film's drama and conflict, Pfister strived for ambiguity. ""We would be wise to be sceptical and wary of AI and our dependence on it,"" he says. ""Is the technology a malevolent or benevolent force? Can it be one, or the other? You decide."" Transcendence was released to cinemas in the UK on April 25."
751,https://www.wired.com/2014/05/andrew-ng-baidu/,Wired,2014,5,16,1048.0," Andrew Ng is the man who helped launch Google's wildly ambitious effort to recreate the human brain with computer hardware and software. And now, he will oversee a similar project at Baidu, often called ""the Google of China."" Last year, in Cupertino, California, not far from Apple headquarters, Baidu quietly opened a research outpost dedicated to ""deep learning""--a subfield of artificial intelligence that seeks to vastly improve computing tasks by mimicking the way the human brain operates--and in the months since, this operation has expanded in significant ways. Today, the Chinese search giant will announce that the lab has graduated to a much larger space in Sunnyvale and that Ng, a Stanford University professor, will oversee a new Baidu artificial intelligence research group that spans this lab and an operation in China. ""Andrew is one of the intellectual leaders in machine learning, and deep learning in particular,"" says Bruno Olshausen, the director of the Redwood Center for Theoretical Neuroscience at the University of California, Berkeley. ""I expect he will continue to lead in this way at Baidu."" Deep learning--something that seeks to improve everything from natural language processing to voice and image recognition--is a technology that gestated in academia for decades, driven by small group of maverick researchers, including Geoff Hinton of the University of Toronto and Yann LeCun, of NYU. But in recent years, it has quickly spread to the giants of the internet. Ng, a disciple of Hinton and LeCun, helped launch Google's efforts in this field, with a project called ""the Google Brain,"" and after Google acquired his deep learning company, Hinton now works at least part-time at the search giant. Meanwhile, Facebook recently hired LeCun, and many other big names are exploring this technology, including Microsoft and IBM. Even Netflix is getting into the act. At Baidu, Ng will run both the company's Sunnyvale lab and an R&D center based in Beijing, which will deal in deep learning and ""big data"" -- i.e. efforts to analyze large amounts of information. Baidu is set to invest about $300 million in this international project over the next five years. Ng, who starts in his new job today, is stepping away from the day-to-day operations at Coursera, the online-education startup he co-founded. He will still be involved in some projects at Coursera, he says, and will remain the chairman of the board and the public face of the company. But his main focus will be on building up Baidu's AI chops and its Silicon Valley presence. He'll spend most of his time in Sunnyvale. ""I'm really excited about the opportunity to build an international research organization from scratch,"" Ng says. ""I've been super excited about AI for a long time, and this is an opportunity for me to return to that."" Since taking a leave of absence from Stanford to start Coursera in 2012, Ng had been splitting his time between running the company and doing AI research. Coursera was growing steadily, having secured another $20 million in funding in November, but Baidu's Kai Yu, a longtime friend of Ng's who helped found the Chinese search company's deep learning labs, urged him to focus on artificial intelligence. ""He was doing amazing things in online education, but this is not AI,"" Yu says. During his last visit from Beijing last March, Yu approached Ng about joining Baidu. The pair talked several times at a Sheraton in Palo Alto--first over a pool-side breakfast and later that same day at dinner. Yu then introduced him to two of Baidu's vice presidents, Jing Wang and Alex Zheng. Later, Ng would fly to Beijing to meet with Baidu CEO Robin Li. Over a three-hour lunch, the two men mapped out their visions for what Baidu's research arm might look like and the types of problems it would tackle. The 38-year-old seems a good fit for the company. Like Li, Ng has close ties to both the U.S. and Asia, having grown up in Hong Kong and Singapore. That means he may be in a good position to help merge Baidu's Asia and California operations well. ""I am a product of both of these cultures,"" Ng says. ""Diversity leads to great creativity and having some of the best ideas from Beijing and Silicon Valley will allow us to innovate faster and come up with more surprising things."" Under Ng's leadership, Baidu will grow its Silicon Valley office to roughly 200 people by the end of 2015, most of whom will be deep-learning researchers and computer systems engineers. The systems geeks will focus on things like building clusters of low-cost graphical processing units--or GPUs-- to crunch through the massive amounts of data that deep learning thrives on. GPUs let data scientists work through billions of calculations more quickly and cheaply than using traditional CPUs. Google, IBM, and others have also leveraged GPUs for deep learning. Meanwhile, Baidu's deep-learning researchers will focus on developing algorithms that are better at learning from unlabeled data, through what's called unsupervised learning--a concept Ng, together with Google's Geoff Hinton, has been pushing for years. ""Andrew Ng and me believe strongly in unsupervised learning,"" Hinton told WIRED during a conversation at the Google Plex last summer. ""Andrew, in particular, pushed on the idea that if we could just use unsupervised learning, then we could go quite a long way."" That's because, right now, AI researchers have to do a lot of hand-holding when teaching computers to identify things like words and images. The true promise of AI will be realized, experts say, when computers can teach themselves--when they're able to absorb and understand data without always being told explicitly what it is. That process, Ng says, is closer to how humans learn and represents a still under-explored avenue for improving AI's capabilities. Other deep-learning heavy-hitters agree. ""We want to have machines that can take advantage of all of the data out there, and that requires better unsupervised learning,"" says the University of Montreal’s Yoshua Bengio, whose work focuses largely on unsupervised learning. Most of the world's data, you see, is unlabeled, and tagging all of it would be incredibly expensive. Figuring out better ways to get machines learning on their own could improve the economics of AI and lead to better applications for consumers. That's why Ng is joining Baidu."
752,https://www.wired.com/2014/04/anki-new-cars-tracks/,Wired,2014,4,16,442.0," Anki Drive, the artificial intelligence-assisted toy car game that debuted last fall, is back with new cars, new tracks, and new ways to play. The game first gained recognition at Apple's Worldwide Developer Conference last year and launched with a handful of AI-controlled toy race cars and a single track. It's a bit like slot cars, but smarter. Each car has a distinct personality and racing capabilities that differ from the others. Paired with the accompanying Anki Drive iOS app, you can battle friends who are also controlling a car through the app, or opponents who navigate the course using artificial intelligence algorithms. As you play and get more experience, your vehicle evolves, gaining valuable weapons and abilities you can use to beat other players. ""There's this notion in consumer products and toys that you buy something and that's what you'll have forever,"" Anki co-founder Hanns Tappeiner said. ""But that doesn't have to be true if you can use software to redefine what a product is and how it works."" Starting today, the Anki Drive app is getting a new type of play: race mode. Before, your option with other players was Battle mode, in which the goal was to shoot other cars off the track with virtual weapons. Now, you can set a number of laps (15, 30, or 45), and the first to cross the finish line wins the game. In my demo of the game, 15 laps went by in a snap, equating to less than 5 minutes of gameplay. Of course, there's more than just racing strategy involved, as you and the other cars on the course still have weapons like a ""rail gun"" or ""tractor beam"" you can use to foil competitors. The addition of two new tracks also complicates the racing strategy. The first, Crossroads, features an intersection in the course where cars could crash if they don't time the crossing correctly. ""The AI cars may shoot you off the road or hit you on purpose to get you off the road. It's fun chaos,"" Tappeiner says. The second, Bottleneck, requires you to strategically position yourself to get ahead on narrower portions of the track. Tappeiner says figuring out where to be on the road becomes the most challenging aspect of this course. The new courses are joined by two new cars as well: Corax, an offensive tank of a car (which can operate two weapons at one time instead of just one), and Hadion, the fastest of Anki's six available cars. The new cars retail at $69 and are available today through Anki's website, while the new tracks run $99 and will ship May 6."
753,https://www.wired.com/2014/04/underwire-0401-funnycomputer/,Wired,2014,4,1,1244.0," Over the years, we've become accustomed to computers besting humans in tests of raw intelligence. Deep Blue out-maneuvered world chess champion Garry Kasparov in 1997, and in 2011 Watson trounced Jeopardy winners Brad Rutter and Ken Jennings. Sure, computers can play the Sicilian Defense better and have a firmer grasp of obscure geography—but facts and analysis aren't everything. We wanted to establish a different litmus test for computer supremacy, so we devised a new matchup between man and machine to establish once and for all who's funnier. Your contestants in the bout: stand-up comic Myq Kaplan versus Manatee the joke-telling computer. It’s perfect timing for such a duel. Computers in the comedy business are becoming an increasingly big deal. Last fall, the Association for the Advancement of Artificial Intelligence held its first-ever symposium on artificial intelligence and humor. Comedic robots are making a splash at SXSW Interactive and garnering GQ profiles. And a few years ago, Manatee caused a political stir when pundits learned its designers at Northwestern University had scored more than $700,000 in federal stimulus funds to help develop it. It's no surprise, really. The name of the game in the tech industry is making interactions between people and their gizmos ever more human—and human often equals humorous. That means everything from computer programs that make fun of themselves when they make a mistake to a GPS device that sarcastically acknolwedges when you ignore its directions. ""‘Just the facts, ma'am,’ isn’t going to play as machines get more and more involved in our lives,"" says Northwestern University professor Kristian Hammond, who helped design Manatee and now develops news-writing computer programs for the company Narrative Science. ""It’s all about making the communication between people and the machine a smooth, compelling interaction."" There’s another reason computer scientists are eager to tackle comedy: jokes are some of the toughest tests of their programs. If artificial intelligence programs are truly going to model human intelligence, they have to be able to grasp all the clever ways people make things funny. In fact, scientists have been hard at work for decades designing robo-jokesters. Among the efforts are JAPE, the Joke Analysis and Production Engine; STANDUP, the System To Augment Non-speakers’ Dialogue Using Puns; LIBJOB, the light bulb joke generator; SASI, a sarcasm-detecting program; and DEviaNT, the Double Entendre via Noun Transfer program, which finds the perfect spots in natural language to insert ""That’s what she said."" Plus, for computer programmers looking for just the right witty acronym for the next big comedy computer, there’s the HAHAcronym Generator. What have most of these attempts discovered? That up until now, computers have been able to tell jokes, but only really dumb ones. Consider the following computer-generated zingers: What kind of animal rides a catamaran? A cat. What is the difference between leaves and a car? One you brush and rake, the other you rush and brake. If robots ever conquer the world, in other words, we’re in for a dystopian future of horrible puns. One problem is that for a computer to truly be humorous, most people agree that first you have to program into it what, exactly, makes things funny—and that’s something experts have struggled with for millennia. Plato and Aristotle, for example, long pondered the issue and came up with the superiority theory, the idea that people laugh at the misfortune of others. Sigmund Freud, meanwhile, argued for his relief theory, the concept that humor was a way for people to release psychic energy pent up from repressed sexual and violent thoughts. Then there’s the incongruity theory, the idea put forward by seventeenth-century French philosopher Blaise Pascal that humor arises when people discover there’s an inconsistency between what they expect to happen and what actually happens (a concept well illustrated by jokes with punch lines). We’re partial to the benign violation theory (BVT), an idea developed by McGraw and his collaborator Caleb Warren that humor arises when something seems wrong or threatening, but simultaneously seems okay or safe. According to the theory, tickling is funny because it involves violating someone’s physical space in a benign way. People can’t tickle themselves because it isn’t a violation. Nor will people laugh if a creepy stranger tries to tickle them, since there is nothing benign about the violation. Still, there’s no general agreement yet as to whether the BVT of any of the other contenders are the end-all, be-all of humor theories. So it’s not yet clear which of these models should be programmed into comedic computers. There’s another stumbling block for computer-generated humor: Computers excel in working with simple, fixed data sets. It’s why most joke-generating programs have so far focused on puns and other wordplay, since finite word lists and specific definitions are easy for computers to scan and parse. But most comedy trades in concepts that aren’t simple or fixed at all. The best comedy mines a wide world of attitudes, assumptions, morals, and taboos, most of which aren’t even mentioned in the joke, just subtly hinted at. So if we aim to have computers truly ""get"" jokes—much less to come up with their own and know when and to whom to tell them—we’re essentially going to have upload into them all of humanity. Plus, whether you’re partial to the incongruity theory, the benign violation theory, or some other concept about what makes things funny, it seems pretty clear that good comedy breaks the rules and revels in the peculiar. And that's exactly the sort of stuff computer programs aren’t very good at. Still, have we reached the point where technology can overcome some of these hilarity-killing digital limitations? To find out, we’re holding a joke-off. In one corner, there’s Myq Kaplan, a stand-up comic based in New York and Boston who’s known for his comedic word-smithing. In the other corner, there’s Manatee, the joke-generation system developed by Northwestern professor Kristian Hammond and then-PhD student Patrick McNally. To create its jokes, Manatee scours the Internet for word combinations that fit into well-known humorous phrases, such as ""I like my X like I like my Y…"" Then it combines the resulting witticism with a related photo to create a three-panel webcomic, like this: The design is based off the webcomic A Softer World, says McNally, who’s now a visiting assistant professor of computer science at Pomona College. ""I gave it a try and felt the structure provided a very nice abstract backdrop for the expressions I was getting my computer to make,"" he says. ""The Manatee system is not a very smart system. It doesn't understand language in the ways that you and I understand language. It cheats its way around intelligence. But it also produces funny text on occasion."" Can a human beat Manatee at its own game? To find out, we gave Kaplan the same instructions as Manatee: Come up with three jokes in the vein of ""I like my X like I like my Y…,"" and find an appropriate photo backdrop for each of them. The results: Manatee’s jokes: Kaplan’s jokes: Has the computer bested the comedian? Kaplan, for one, thinks that that's an inevitability. ""If computers keep getting smarter and humans keep getting dumber, absolutely!"" he writes in an e-mail. ""But what do I know, I'm just a human. Can we program this question into a question-answering robot and see what it thinks?"" Joel Warner and Peter McGraw are co-authors of the book The Humor Code: A Global Search for What Makes Things Funny."
754,https://www.wired.com/2014/03/tech-time-warp-homer-simpson/,Wired,2014,3,28,368.0," Intel chips have run our desktop PCs and laptops for decades. They drive the hundreds of thousands of computer servers that underpin websites like Google and Facebook. And they're now heading towards the new wave of wearable computers. But the chip maker's greatest triumph was putting a microprocessor inside the head of Homer Simpson. It happened in November of 1998, during a commercial break mid-way through the season premiere of the The X-Files. You can see the footage above. For years, its technicians had been working to make PCs smarter, Intel told the world in this wonderfully funny TV spot, but now they were facing their greatest challenge yet. And then it cuts to Homer on the operating table, as bunny-suited Intel engineers prepare to implant a Pentium II chip in his brain. ""No one messes with my brain,"" Homer says. ""Until I get sprinkles."" And then he whips out a doughnut. So one of the bunny techs fires some anesthetic into Homer's favorite snack, and the operation commences. ""Now anyone can have all the brain power they want,"" says our narrator, that Simpsons mainstay, Harry Shearer. In just two weeks, the 7.5-million transistor Pentium II chip transforms Homer into a world-class expert in organic chemistry. And naturally, he uses his newfound smarts to engineer the world's densest doughnut. Of course, Intel wants everyone to know what's now powering his brain. As the ad pans to the back of his head, we see that familiar ""Intel Inside"" logo. The Silicon Valley chip giant unloaded so many ""Intel Inside"" ads over the years, but this is one of the best. It has part of a broad marketing blitz in support of the Pentium II, the 1997 follow-up to the hugely successful Pentium. As pointed out by Kevin Krewell, a principal analyst at TIRIAS Research and a member of the Computer History Museum's Semiconductor Special Interest Group in Mountain View, California, the Pentium II was based on a beefy computer workstation chip called the Pentium Pro, but unlike the Pro, it was meant for mainstream desktop PCs. It was not meant for your brain. But we're fine with Intel taking some poetic license -- especially when the Simpsons are involved."
755,https://www.wired.com/story/falling-in-love-with-an-operating-system/,Wired,2014,3,12,913.0," *This is a guest post by Martin McNulty, Chief Executive Officer of Forward3D. 2001: A Space Odyssey will confirm. But Spike Jonze's new movie Her and the way the central character, played by Joaquin Phoenix, interacts with ""OS1"" -- a new operating system that promises artificial intelligence of the future -- shines fresh light on a topic that is becoming pertinent to our age. Is Her the future of how we will engage with devices and discover information, or will key challenges restrict this direction of technological development? In 1950, the posthumously pardoned Alan Turing, father of modern computing, devised a test for machine intelligence, of the kind you may be able to fall in love with. The Turing Test's tenet is simple: if a human is unable to reliably distinguish between a machine and a human's response to questioning, the machine can be deemed of equivalent intelligence. We're getting closer and closer to developing a system that could pass this test, although our best efforts remain a distant second to Spike Jonze's OS1, whose conversational ability replaces physical interaction entirely. The debate around whether we'll eventually get there is becoming less relevant. Technology companies are investing billions in machine learning, natural language processing, and neural networks. Meanwhile we find ourselves in a very interesting time; Moore's Law is holding true, as processor chips are becoming increasingly small and powerful.  Kryder's Law is also holding, with storage density and affordability increasing rapidly. Add to all of this the availability of open-source software and education -- the conditions for imminent machine intelligence are perfect. At Forward3D, a crucial aspect of our business is to adapt our online advertising for brands in line with changing consumer behaviour, helping ensure the brands we work with are both visible and relevant to their audience. An important and accurate indicator of consumer behaviour is web searches: the kind you enter every day on search engines like Google, or discovery engines like Siri. In 15 years Google has gone from receiving 10 thousand to 15 billion searches a day -- that's a wealth of data, all of which points toward consumer demand and interest. Accurately parsing its semantic meaning requires the ability to programmatically understand natural language. When I search ""Paris Hilton"" am I looking for a luxury European hotel, or socialite gossip? It's a complex problem that requires complex technology, of the kind Google et al are investing heavily in. We're already seeing these investments coming to fruition for consumers. Siri is a great example of semantic technology. Unlike traditional search engine results that are ranked in a list, Siri returns a single hyper-relevant result. This requires significantly more semantic understanding of the search to get right. Modern consumers thrive on convenient information, and the extension of relationships beyond face-to-face. The advent of wearable technology such as Google Glass and smart watches will serve to exaggerate this trend, but only if their functionality and user interfaces improve. We'll need interactions, not commands, and they'll need to be both conversational and interactive. IBM's Watson is the closest we've come to this. Back in 2011, when the supercomputer showed former Jeopardy champions a thing or two, it was the size of a master bedroom -- perhaps too large to power a smart watch. Today, IBM has shrunk Watson to the size of three stacked pizza boxes, improved its performance by 2400 percent, and announced a new $1 billion (£600 million) investment. More exciting is that Watson now  has an API. Theoretically we could access its power through the cloud, using any internet-enabled device. The result wouldn't be a million miles away from OS1, although at this time you would be hard pressed to fall in love with it. Computing power and storage continues to get smaller and cheaper and it is conceivable that the necessary processing power in the future could be squeezed into a smartphone sized device.  That said, there is a fundamental limit on how small traditional processors can get -- a limit prescribed by particle physics. To breach this limit we must look toward  quantum computing, which is a long way away from being stable in the laboratory, let alone in a smartphone. Perhaps more likely is that the ever-increasing speed and reach of our wireless networking will facilitate OS1 style software to exist in the cloud.  Without wishing to spoil the film, there are reasons to suspect that Spike Jonze's OS1 is a cloud-based technology. Security and privacy concerns are a barrier that any would-be provider will need to overcome. A huge amount of training data is absolutely necessary for software of this kind to function; we're talking about everything and anything, from archived instant messages to bank statements. A provider would have to prove their value proposition, along with their infrastructure's security before an increasingly-savvy consumer hands over her data and faith. It requires an intellectual leap to believe that we will one day confide our deepest feelings and desires to a computer, especially to the extent that we could inadvertently fall in love with our operating systems. But we know that in tech things move quickly and are subject to incredible change, as anyone who has observed the rapid adoption of tablets from all age ranges. What may seem like the sci-fi dreams of the distant future can become a humdrum reality within a generational blink of an eye -- who knows what tech might sweep us off our feet in years to come?"
756,https://www.wired.com/2014/02/her-artificial-intelligence-love/,Wired,2014,2,28,888.0," Much has been written about Spike Jonze's Her, the Oscar-nominated tale of love between man and operating system. It's an allegory about relationships in a digital age, a Rorschach test for technology. It's also premised on a particular vision of artificial intelligence as capable of experiencing love. Poetic license aside, is that really possible? Not anytime soon, though not for lack of processing speed or algorithmic finesse. What computers lack are bodies. The thoughts and feelings and emotions we call ""love"" are not abstract experiences; they're intertwined with senses and hormones. An AI — a computer hooked to video cameras, a microphone and a screen — would not experience flesh-and-blood love. ""You can't make a computer without a body feel love,"" said David Havas, director of the Laboratory for Language and Emotion at the University of Wisconsin-Whitewater. Though trying to replicate it ""may produce wonderful gadgets, and potentially life-saving achievements, it can never achieve the same result."" 'In a sense, the body is the computational engine that makes emotion out of emotionless parts.' Havas isn't simply skeptical because modern AIs are unsophisticated. The opposite is true: AIs sort our mail, defeat our Jeopardy! champions and recommend medical treatments. From behind a screen, it can be difficult to distinguish chatbots from people. Indeed, with some clever coding and a sufficiently nuanced grasp of human experience, it might be possible to build an AI that gives the appearance of loving. This wouldn't be easy: As philosopher and cognitive scientist Daniel Dennett wrote in ""Why You Can't Make a Computer That Feels Pain,"" some states of being are simply too messy to code. When Siri says, ""I did have strong feelings for a cloud-based app once,"" she's probably faking it. *Her'*s Samantha is different, though. She's not going through the motions or running predetermined subroutines. Her love wasn't programmed; it grows. She falls in love. She experiences infatuation and fascination, passion and care, a sense of giving and taking and sharing. The breadth and depth of her feelings evolve. That capacity for growth is difficult to program, said cognitive scientist Benjamin Bergen of the University of California, San Diego. Many mid-20th century AI researchers thought it could be replicated in code alone, imagining human faculties as a mental software suite that would work the same in silicon as in a body. That paradigm underlies Her's essential premise, and it no longer holds. Instead, researchers in the field of embodied cognition have found close links between body and thought. In experiments, this has been demonstrated in fairly simple ways — the effects of postures and facial expression on emotion, how different textures influence perception — but they suggest a basic principle. ""Emotional comprehension requires a body,"" said Havas. ""In a sense, the body is the computational engine that makes emotion out of emotionless parts."" That's evident in child development, said Bergen. Infants bond with caregivers by being picked up and held, physically consoled, feeling warmth. ""If they weren't there, if we didn't have that capacity to sense warmth, if we hadn't been small and able to be picked up, we wouldn't develop those same emotional bonds,"" he said. In the experience of adult, romantic love, tactility is likewise important, both in obvious ways and in endless small gestures and touches. Also integral is the endocrine system, which releases hormones that interact with our brain and nervous system. Their totality is what we experience; to develop a truly human sort of love, said Havas, an artificial intelligence would require them. Otherwise it just couldn't feel what we do. But what if those systems and interactions could be encoded, too? Some theorists, said philosopher Matthew Fulkerson, Bergen's UCSD colleague and author of The First Sense: A Philosophical Study of Human Touch, think an internal, virtual representation of a body -- an endocrine emulator, if not the actual glands -- could suffice. A simple example comes from programs in which physics engines allow virtual bodies to ""feel"" the pull of gravity on their limbs as they learn to walk. Something similar, a sort of biology engine, could theoretically be used in AIs, Bergen said. Writing it would, however, require a deep, fine-grained understanding of how biological systems work. Presently that doesn't exist, and might not survive translation into a machine form. Cells and tissues refined by evolution remain far more sophisticated than human hardware. ""The details matter,"" said Bergen. ""Silicon is not neurons."" Yet if computers aren't quite ready to feel love, it's worth remembering that love comes in different forms. Human versions encompass only part of the spectrum. If love is, as Bergen says, a species-specific experience, perhaps what artificial intelligences could someday feel -- even in the absence of biology -- would simply be their own particular version. It might not work for Her, but it could still be meaningful. ""A machine won’t actually have to possess every characteristic of emotional love in order to be treated as though it had actual thoughts and feelings and desires,"" said Fulkerson. He drew a comparison to the way people think of pets. ""Their emotions and feelings are certainly different from ours, and recognized as such, but they are often deeply loved,"" he said. ""It almost doesn’t matter whether they really feel love. They feel enough. I’m guessing something similar will eventually be true of machines."""
757,https://www.wired.com/2014/02/halting-problem/,Wired,2014,2,5,1602.0," Computers can drive cars, land a rover on Mars, and beat humans at Jeopardy. But do you ever wonder if there's anything that a computer can never do? Computers are, of course, limited by their hardware. My smartphone can't double as an electric razor (yet). But that's a physical limitation, one that we could overcome if we really wanted to. So let me be a little more precise in what I mean. What I'm asking is, are there any questions that a computer can never answer? Now of course, there are plenty of questions that are really hard for computers to answer. Here's an example. In school, we learn how to factor numbers. So, for example, 30 = 2 × 3 × 5, or 42 = 2 × 3 × 7. School kids learn to factor numbers by following a straightforward, algorithmic procedure. Yet, up until 2007, there was a $100,000 bounty on factoring this number: And as of 2014, no one has publicly claimed the solution to this puzzle. It's not that we don't know how to solve it, it's just that it would take way too long. Our computers are too slow. (In fact, the encryption that makes the internet possible relies on these huge numbers being impossibly difficult to factor.) So lets rephrase our question so that it isn't limited by current technology. __Are there any questions that, __no matter how powerful your computer, and no matter how long you waited, your computer would never be able to answer? Surprisingly, the answer is yes. The Halting Problem asks whether a computer program will stop after some time, or whether it will keep running forever. This is a very practical concern, because an infinite loop is a common type of bug that can subtly creep in to one's code. In 1936, the brilliant mathematician and codebreaker Alan Turing proved that it's impossible for a computer to inspect any code that you give it, and correctly tell you whether the code will halt or run forever. In other words, Turing showed that a computer can never solve the Halting Problem. You've probably experienced this situation: you're copying some files, and the progress bar gets stuck (typically at 99%). At what point do you give up on waiting for it to move? How would you know whether it's going to stay stuck forever, or whether, in a few hundred years, it'll eventually copy your file? To use an analogy by Scott Aaronson, ""If you bet a friend that your watch will never stop ticking, when could you declare victory?"" As you get sick of waiting for the copy bar to move, you begin to wonder, wouldn't it be great if someone wrote a debugging program that could weed out all annoying bugs like this? Whoever wrote that program could sell it to Microsoft for a ton of money. But before you get to work on writing it yourself, you should heed Turing's advice - a computer can never reliably inspect someone's code and tell you whether it will halt or run forever. Think about how bold a claim this is. Turing isn't talking about what we can do today, instead he's raised a fundamental limitation on what computers can possibly do. Be it now, or in the year 2450, there isn't, and never will be, any computer program that can solve the Halting Problem. In his proof, Turing first had to mathematically define what we mean by a computer and a program. With this groundwork covered, he could deliver the final blow using the time honored tactic of proof by contradiction. As a warm up to understanding Turing's proof, let's think about a toy problem called the Liar paradox. Imagine someone tells you, ""this sentence is false."" If that sentence is true, then going by what they said, it must also be false. Similarly, if the sentence is false, then it accurately describes itself, so it must also be true. But it can't be both true and false - so we have a contradiction. This idea of using self-reference to create a contradiction is at the heart of Turing's proof. Here's how computer scientist Scott Aaronson introduces it: [Turing's] proof is a beautiful example of self-reference. It formalizes an old argument about why you can never have perfect introspection: because if you could, then you could determine what you were going to do ten seconds from now, and then do something else. Turing imagined that there was a special machine that could solve the Halting Problem. Then he showed how we could have this machine analyze itself, in such a way that it has to halt if it runs forever, and run forever if it halts. Like a hound that finally catches its tail and devours itself, the mythical machine vanishes in a fury of contradiction. And so, let's go through Turing's proof that the Halting Problem can never be solved by a computer, or why you could never program a 'loop snooper'. The proof I'm about to present is a rather unconventional one. It's a poem written by Geoffrey Pullum in honor of Alan Turing, in the style of Dr. Seuss. I've reproduced it here, in entirety, with his permission. No general procedure for bug checks will do.Now, I won’t just assert that, I’ll prove it to you.I will prove that although you might work till you drop,you cannot tell if computation will stop. For imagine we have a procedure called Pthat for specified input permits you to seewhether specified source code, with all of its faults,defines a routine that eventually halts. You feed in your program, with suitable data,and P gets to work, and a little while later(in finite compute time) correctly inferswhether infinite looping behavior occurs. If there will be no looping, then P prints out ‘Good.’That means work on this input will halt, as it should.But if it detects an unstoppable loop,then P reports ‘Bad!’ — which means you’re in the soup. Well, the truth is that P cannot possibly be,because if you wrote it and gave it to me,I could use it to set up a logical bindthat would shatter your reason and scramble your mind. Here’s the trick that I’ll use — and it’s simple to do.I’ll define a procedure, which I will call Q,that will use P’s predictions of halting successto stir up a terrible logical mess. For a specified program, say A, one supplies,the first step of this program called Q I deviseis to find out from P what’s the right thing to sayof the looping behavior of A run on A. If P’s answer is ‘Bad!’, Q will suddenly stop.But otherwise, Q will go back to the top,and start off again, looping endlessly back,till the universe dies and turns frozen and black. And this program called Q wouldn’t stay on the shelf;I would ask it to forecast its run on itself.When it reads its own source code, just what will it do?What’s the looping behavior of Q run on Q? If P warns of infinite loops, Q will quit;yet P is supposed to speak truly of it!And if Q’s going to quit, then P should say ‘Good.’Which makes Q start to loop! (P denied that it would.) No matter how P might perform, Q will scoop it:Q uses P’s output to make P look stupid.Whatever P says, it cannot predict Q:P is right when it’s wrong, and is false when it’s true! I’ve created a paradox, neat as can be —and simply by using your putative P.When you posited P you stepped into a snare;Your assumption has led you right into my lair. So where can this argument possibly go?I don’t have to tell you; I’m sure you must know.A reductio: There cannot possibly bea procedure that acts like the mythical P. You can never find general mechanical meansfor predicting the acts of computing machines;it’s something that cannot be done. So we usersmust find our own bugs. Our computers are losers! What you just read, in delightfully whimsical poetic form, was the punchline of Turing's proof. Here's a visual representation of the same idea. The diamond represents the loop-snooping program P, which is asked to evaluate whether the program Q (the flow chart) will halt. ""The program will halt when the loop snooper said it wouldn't, and it runs forever when the loop snooper said it would halt!"" Like the serpent that tries to eat its tail, Turing conjured up a self-referential paradox. The program will halt when the loop snooper said it wouldn't, and it runs forever when the loop snooper said it would halt! To resolve this contradiction, we're forced to conclude that this loop snooping program can't exist. And this idea has far-reaching consequences. There are uncountably many questions for which computers can't reliably give you the right answer. Many of these impossible questions are really just the loop snooper in disguise. Among the things that a computer can never do perfectly is identifying whether a program is a virus, or whether it contains vulnerable code that can be exploited. So much for our hopes of having the perfect anti-virus software or unbreakable software. It's also impossible for a computer to always tell you whether two different programs do the same thing, an unfortunate fact for the poor souls who have to grade computer science homework. By slaying the mythical loop snooper, Turing taught us that there are fundamental limits to what computers can do. We all have our limits, and in a way it's comforting to know that the artificial brains that we create will always have theirs too."
758,https://www.wired.com/story/eterni-life-after-death-ai/,Wired,2014,2,5,1936.0," Eterni.me, needs to clear this up quickly. Because when you're building a fledgling artificial intelligence company that promises to bring back the dead -- or at least, their memories and character, as preserved in their digital footprint -- for virtual chats with loved ones, expect a lot of flack. ""It is going to really suck -- think Cleverbot with weird out-of-place references to things from that person's life, masquerading as that person,"" wrote one Redditor on the thread ""Become Virtually Immortal (In the creepiest way possible)"", which immediately appeared after Eterni.me's launch was announced last week. Retorts ranged from the bemused -- ""Now that is some scary f'd up s**t right there. WTF!?"" -- to the amusing: ""Imagine a world where drunk you has to reason with sober AI you before you're allowed to drunk dial every single person you ever dated or saw naked. So many awkward moments avoided."" But the resounding consensus seems to be that everyone wants to know more. The site launched with the look of any other Silicon Valley internet startup, but a definitively new take on an old message. While social media companies want you to share and create the story of you while you're alive, and lifelogging company Memoto promises to capture ""meaningful [and shareable] moments"", Eterni.me wants to wrap that all up for those you leave behind into a cohesive AI they can chat with. Three thousand people registered to the service within the first four days of the site going live, despite there being zero product to make use of (a beta version is slated for 2015). So with a year to ponder your own mortality, why the excitement for a technology that is, at this moment, merely a proof of concept?  ""We got very mixed reactions, from ecstatic congratulations to hate mail. And it's normal -- it's a very polarising topic. But one thing was constant: almost everybody we've interacted with truly believes this will be a reality someday. The only question is when it will be a reality and who will make it a reality,"" Ursache tells us. Popular culture and the somewhat innate human need to believe we are impervious, has well prepared us for the concept. Ray Kurzweil wants us to upload our brains to computers and develop synthetic neocortexes, and AI has featured prominently on film and TV for decades, including in this month's Valentine's Day release of a human-virtual assistant love story. In series two of British future-focused drama Black Mirror Hayley Atwell reconnects with her diseased lover using a system comparable to what Eterni.me is trying to achieve -- though Ursache calls it a ""creepier"" version, and tells us ""we're trying to stay away from that idea"", the concept that it's a way for grieving loved ones to stall moving on. Sigmund Freud called our relationship with the concept of immortality the ""real secret of heroism"" -- that we carry out heroic feats is only down to a perpetual and inherent belief that our consciousness is permanent. He writes in Reflections on War and Death: ""We cannot, indeed, imagine our own death; whenever we try to do so we find that we survive ourselves as spectators. The school of psychoanalysis could thus assert that at bottom no one believes in his own death, which amounts to saying: in the unconscious every one of us is convinced of his immortality... Our unconscious therefore does not believe in its own death; it acts as though it were immortal."" This is why Eterni.me is not just about loved ones signing up after the event, but individuals signing up to have their own character preserved, under their watchful eye while still alive. The company's motto is ""it's like a Skype chat from the past,"" but it's still very much about crafting how the world sees you -- or remembers you, in this case -- just as you might pause and ponder on hitting Facebook's post button, wondering till the last if your spaghetti dinner photo/comment really gets the right message across. On its more troubling side, the site plays on the fear that you can no longer control your identity after you're gone; that you are in fact a mere mortal. ""The moments and emotions in our lifetime define how we are seen by our family and friends. All these slowly fade away after we die -- until one day... we are all forgotten,"" it says in its opening lines -- scroll down and it provides the answer to all your problems: ""Simply Become Immortal"". Part of the reason we might identify as being immortal -- at least unconsciously, as Freud describes it -- is because we craft a life we believe will be memorable, or have children we believe our legacy will live on in. Eterni.me's comment shatters that illusion and could be seen as opportunistic on the founders' part. The site also goes on to promise a ""virtual YOU"" that can ""offer information and advice to your family and friends after you pass away"", a comfort to anyone worried about leaving behind a spouse or children. In contrast to this rather dramatic claim, Ursache says: ""We're trying to make it clear that it's not replacing a person, but trying to preserve as much of the information one generates, and offering asynchronous access to it."" For starters that will mean uploading data from ""text-intensive social networks and environments"" such as Facebook, Twitter and personal email, to the AI algorithms. But the team wants to eventually move on to photos, videos, audio and location data from personal devices. ""People will also be able to fine-tune their avatar, by regularly interacting with it (think of a daily ten-minute chat where you talk and it picks up more information and refines already existing information, by 'making sense of it'"". This kind of ideal machine learning is a way off, but we're already seeing glimpses of it with natural language research such as that done by Nuance. As well as hoping to cleverly mimic the deceased, the whole point is to allow anyone to maintain and curate their digital world after their death. A recent  article in the Wall Street Journal by Geoffrey A Fowler delved into the world of digital privacy and revealed how one family fought to gain access to Toronto teenager Alison Atkins' social media and email accounts, to hold on to her memory, photos and more.  He highlights how internet companies perceive user privacy as continuing after death and cites an instance where Facebook actually went to court to prevent a family compelling it to hand over data. The flipside is, what might a loved one find that their departed friend/family member never wanted them to know? Fowler reports how Atkins' sister came across a password-protected Tumblr filled with posts about suicide and ""dark"" comments. Her mother said it made her sad but ""there were no surprises"", but this might not always be the case. When Wired.co.uk asked Ursache how the system would take into account information the deceased would not like to share -- secrets, affairs etc. -- he said ""I don't have a clear answer on that at this stage. This is one of many sensitive issues related to collecting and exposing this data, that we really have to think through"". Of course anyone signing up in advance could diligently go through their data backlog to check for inconsistences, but after death it will presumably be in the hands of family/friends. own) their digital trail -- right now this is rather controversial with Facebook, Google and what they do once someone passes away -- then making it available through a simple interface that not only allows you to access that data, but learns more about you,"" explains Ursache. ""Our long-term vision is to preserve as much as possible from what each person knows and is -- think of it as preserving everyone for the future generations. Can you imagine how it would've been if you could preserve Socrates or Einstein? Unbelievable. Steve Jobs said in a 2001 interview that he would give up all technology in order to be able to spend only one afternoon with Socrates."" Just like on Facebook, you'll also be able to control your privacy settings. Ursache says: ""There will be two layers -- public and private. The public avatar could be used by anyone, but the private one will only be accessible to you and, after your death, to designated family members and friends. And you could opt that everything becomes public after, let's say, 50 more years."" 'perfect',"" Ursache says. It begs the question then, what's the point in launching something like this now, with no real outline or technology to back it up. Eterni.me is the brainchild of a bunch of aspiring entrepreneurs from across the globe that took part in MIT's five-day-long Entrepreneurship Development Program. In fact, the whole team met there for the first time and a few days later launched the site. Ursache says the idea had been ""dormant in my head"" for a while, but meeting the team turned it into ""something serious"". ""It's changed from its initial form due to contributions and feedback from the team, MIT coaches and VCs we've met during the past week."" That team does seem to be heavy with experience from the business, management and marketing side, though there is one former engineer on board, one member with ""experience in natural language search and artificial intelligence"" and another with a history of ""building social media management tools for leading brands"". It's easy to think it's a publicity kick with little else behind it. But Ursache's argument is that although we might not have all the tech necessary to make it a reality now (for instance to have the AI talk to the deceased's wife differently than to their children), we will one day. Eterni.me is prepping for that future. ""Until then, we want to preserve as much as possible from the information someone generates -- and if you're 30 today, it's likely that by the time you're 45 or 50, the technology will be there. But you need to start preserving this data as soon as possible."" Until that tech is spot on, the uncanny valley effect would be pretty damaging in the context of grief. As Redditor Windex007 pointed out, ""Talking to the bot is only going to make people who knew you unhappy to be dealing with such a horrible excuse for who you really were, and everyone who didn't know you is going to think you were a complete retard."" Forgetting the unfortunate use of that final word, the point is if it's not spot on, the results will be horrendous. For families like those of 16-year-old Toronto girl Atkins, though, it might mean holding on to something you've lost, and having access to it in a form that's user-friendly and comprehensible -- a kind of flipbook through the individual's best moments, filtering out the noise that would have been difficult for a family member to sift through while grieving. It also means trusting that AI to know what's important about you, and what's not though. More than that, it means trusting it to know what's important about you to your sister, your mother, your child -- everyone sees you differently and everyone will want to remember a different part of you. The ultimate stumbling block might be, however, the something that's worse than the fear of being forgotten. Admitting you're going to die one day. It's a tough sell, to persuade someone to confess to the secret of their heroism."
759,https://www.wired.com/2014/01/google-buying-way-making-brain-irrelevant/,Wired,2014,1,27,768.0," Google is on a shopping spree, buying startup after startup to push its business into the future. But these companies don't run web services or sell ads or build smartphone software or dabble in other things that Google is best known for. The web's most powerful company is filling its shopping cart with artificial intelligence algorithms, robots, and smart gadgets for the home. It's on a mission to build an enormous digital brain that operates as much like the human mind as possible -- and, in many ways, even better. Yesterday, Google confirmed that it has purchased a stealthy artificial intelligence startup called DeepMind. According to reports, the company paid somewhere in the mid-hundreds of millions of dollars for the British outfit. Though Google didn't discuss the price tag, that enormous figure is in line with the rest of its recent activity. >Lifelike robots, sentient machines, the Jetsons' smart home in the sky. Google is spending billions to make itself the place where these fantasies become facts. The DeepMind acquisition closely follows Google's $3.2 billion purchase of smart thermostat and smoke alarm maker Nest, a slew of cutting-edge robotics companies, and another AI startup known as DNNresearch. Google is looking to spread smart computer hardware into so many parts of our everyday lives -- from our homes and our cars to our bodies -- but perhaps more importantly, it's developing a new type of artificial intelligence that can help operate these devices, as well as its many existing web and smartphone services. Though Google is out in front of this AI arms race, others are moving in the same direction. Facebook, IBM, and Microsoft are doubling down on artificial intelligence too, and are snapping up fresh AI talent. According to The Information, Mark Zuckerberg and company were also trying to acquire DeepMind. Google's web search engine already uses a powerful type of artificial intelligence to find what you're looking for in the chaos of the web, and it has built an insanely profitable ad business atop this engine. But recently, the company has been bulking up its roster of geniuses as it seeks to explore a new branch of artificial intelligence known as ""deep learning."" Basically, the idea is to mimic the biological structure of the human brain with software so that it can build machines that learn ""organically"" -- that is, without human involvement. Google is already working to apply these insights to its familiar consumer products and services. Deep learning can help recognize what's in your photos without asking you to tag them yourself, and it can help understand human speech, a key tool for its smartphone apps and Google Glass computerized eyewear. But Google also sees the new AI as a better way to target ads -- the core of its business. The DeepMind acquisition is one more step down this road. And though the company has not said as much, you can bet that this new form of AI will also play into things like Nest smart thermostats, the Google self-driving cars, and its big push into robotics. At the moment, it seems, no other institution on earth has the concentration of brain power -- coupled with the money, technology, and freedom -- to chase the dreams that have fueled a century of science-fiction speculation. Lifelike robots, sentient machines, the Jetsons' smart home in the sky. Google is spending billions to make itself the place where these fantasies become facts. In a profile of deep-learning pioneer and now part-time Googler Geoff Hinton, WIRED's Daniela Hernandez writes that the key difference between deep learning and other approaches to artificial intelligence is that it aims to free machines from the need for human intervention, to give them a human-like understanding of our environment. By building so-called neural networks that approximate the brain, Hinton and company are trying to make it possible for Google to understand language, speech, and the physical world without having to be told what its machines are seeing, hearing, or touching. For many of us, Google already functions as an important part of what WIRED columnist Clive Thompson has called our outboard brain. The more Google ""knows,"" the less we have to remember. We just Google it. Now imagine that same kind of intelligence Google applies to the web set loose on your personal existence, not just online but out in the real world. If its artificial intelligence dreams come true, Google might end up knowing you better than you know yourself. As we export more and more of our intelligence to Google, the question might become: What are our own brains for?"
760,https://www.wired.com/2014/01/geoffrey-hinton-deep-learning/,Wired,2014,1,16,1807.0," Geoffrey Hinton was in high school when a friend convinced him that the brain worked like a hologram. To create one of those 3-D holographic images, you record how countless beams of light bounce off an object and then you store these little bits of information across a vast database. While still in high school, back in 1960s Britain, Hinton was fascinated by the idea that the brain stores memories in much the same way. Rather than keeping them in a single location, it spreads them across its enormous network of neurons. This may seem like a small revelation, but it was a key moment for Hinton -- ""I got very excited about that idea,"" he remembers. ""That was the first time I got really into how the brain might work"" -- and it would have enormous consequences. Inspired by that high school conversation, Hinton went on to explore neural networks at Cambridge and the University of Edinburgh in Scotland, and by the early '80s, he helped launch a wildly ambitious crusade to mimic the brain using computer hardware and software, to create a purer form of artificial intelligence we now call ""deep learning."" For a good three decades, the deep learning movement was an outlier in the world of academia. But now, Hinton and his small group of deep learning colleagues, including NYU's Yann LeCun and the University of Montreal's Yoshua Bengio, have the attention of the biggest names on the internet. After honing his ideas as a professor and researcher the University of Toronto in Canada, Hinton works part-time for Google, where he's using deep learning techniques to improve voice recognition, image tagging, and countless other online tools. LeCun is at Facebook, doing similar work. And artificial intelligence is suddenly all the rage at Microsoft, IBM, Chinese search giant Baidu, and many others. While studying psychology as an undergrad at Cambridge, Hinton was further inspired by the realization that scientists didn't really understand the brain. They couldn't quite grasp how interactions among billions of neurons gave rise to intelligence. They could explain how electrical signals traveled down an axon -- the cable-like protrusion that connects one neuron to another -- but they couldn’t explain how these neurons learned or computed. For Hinton, those were the big questions -- and the answers could ultimately allow us to realize the dreams of AI researchers dating back to the 1950s. He doesn't have all the answers yet. But he's much closer to finding them, fashioning artificial neural networks that mimic at least certain aspects of the brain. ""I get very excited when we discover a way of making neural networks better -- and when that’s closely related to how the brain works,"" he says with youthful enthusiasm. In Hinton's world, a neural network is essentially software that operates at multiple levels. He and his cohorts build artificial neurons from interconnected layers of software modeled after the columns of neurons you find in the brain's cortex -- the part of the brain that deals with complex tasks like vision and language. These artificial neural nets can gather information, and they can react to it. They can build up an understanding of what something looks or sounds like. They're getting better at determining what a group of words mean when you put them together. And they can do all that without asking a human to provide labels for objects and ideas and words, as is often the case with traditional machine learning tools. As far as artificial intelligence goes, these neural nets are fast, nimble, and efficient. They scale extremely well across a growing number of machines, able to tackle more and more complex tasks as time goes on. And they're about 30 years in the making. Back in the early '80s, when Hinton and his colleagues first started work on this idea, computers weren’t fast or powerful enough to process the enormous collections of data that neural nets require. Their success was limited, and the AI community turned its back on them, working to find shortcuts to brain-like behavior rather than trying to mimic the operation of the brain. But a few resolute researchers carried on. According to Hinton and LeCun, it was rough going. Even as late as 2004 -- more than 20 years after Hinton and LeCun first developed the ""back-propagation"" algorithms that seeded their work on neural networks -- the rest of the academic world was largely uninterested. >The AI community turned its back on them, working to find shortcuts to brain-like behavior rather than actually trying to mimic the operation of the brain. But that year, with a small amount of funding from the Canadian Institute for Advanced Research (CIFAR) and the backing of LeCun and Bengio, Hinton founded the Neural Computation and Adaptive Perception program, an invite-only group of computer scientists, biologists, electrical engineers, neuroscientists, physicists, and psychologists. Hand-picking these researchers, Hinton aimed to create a team of world-class thinkers dedicated to creating computing systems that mimic organic intelligence -- or at least what we know about organic intelligence, what we know about how the brain sifts through a wealth of visual, auditory, and written cues to understand and respond to its environment. Hinton believed creating such a group would spur innovation in AI and maybe even change the way the rest of world treated this kind of work. He was right. By the middle aughts, they had the computing power they needed to realize many of their earlier ideas. As they came together for regular workshops, their research accelerated. They built more powerful deep learning algorithms that operated on much larger datasets. By the middle of the decade, they were winning global AI competitions. And by the beginning the current decade, the giants of the web began to notice. In 2011, an NCAP researcher and Stanford processor named Andrew Ng founded a deep learning project at Google, and today, the company is using neural networks to help recognize voice commands on Android phones and tag images on the Google+ social network. Last year, Hinton joined the company, alongside other researchers from the University of Toronto. The aim is to take this work even further. Meanwhile, Baidu has followed suit with new AI labs in China and Silicon Valley. Microsoft is adding deep learning techniques to its own voice recognition research. And in hiring LeCun, Facebook is exploring new ways of targeting ads and identifying faces and objects in photos and videos. Another NCAP researcher, Terry Sejnowski, is helping shape President Obama’s $100-million BRAIN Initiative, a project to develop a wide range of new tools for mapping neural circuits. Working alongside Hinton, Sejnowski invented the Boltzmann machine, one of the earliest neural nets, back in the early 1980s. With just a half-a-million-dollar-a year-investment from CIFAR, Hinton's consortium of free thinkers is set to feed countless dollars back into the economy. It's already happening at Google. The return on investment for both Canada and the rest of the world has been tremendous, says Denis Therien, CIFAR’s vice president for research and partnerships. In the process, Hinton and NCAP have changed the face of the community that once spurned them. Students at universities are turning away from more traditional machine learning projects to work on deep learning, says Max Welling, a computer scientist at the University of Amsterdam. ""This information has trickled down all the way to the students who are sitting in the Netherlands, far away from where all this happens. They have all picked up on it. They all know about it,"" he says. ""That to me is the ultimate evidence that this has propagated everywhere."" In other words, deep learning is now mainstream. ""We ceased to be the lunatic fringe,"" Hinton says. ""We’re now the lunatic core."" This fall, the NCAP met up at the Sir Francis Drake Hotel in downtown San Francisco. The group does this once a year, convening for two days of workshops prior to the larger Neural Information Processing Systems, or NIPS, conference, the centerpiece of the AI year. The workshops explored a broad range of tasks that benefit from the marriage of neuroscience and machine learning, including computational graphic design, facial recognition, and motion detection. During the presentations, Hinton stood quietly near the front of the room. Mostly, he listened, but occasionally, he would interrupt with a pointed question or encourage members of his brain trust to ask questions and prompt discussion. His quiet, humble, and fair leadership, NCAP members say, has created an open and collaborative atmosphere that has directly accelerated the world's AI work, redoubling their resolve to change the field. The deep learning revolution was inevitable, they say, but developments like the speech recognition and artificial vision systems adopted by Microsoft, Google, Yahoo, and other giants of the web came sooner because of the NCAP -- and Hinton in particular. ""I think that’s what keeps the energy so positive -- that Geoff is so engaged, and everyone looks up to Geoff,"" says Bruno Olshausen, the director of the Redwood Center for Theoretical Neuroscience at the University of California, Berkeley, and an NCAP member. ""You think: 'If Geoff is listening, I’ve got to listen too.'"" Others outside the group agree. ""Over the last 20 to 30 years, he has been pushing forward the frontier of neural networks and deep learning,"" says Kai Yu, the director of Baidu’s Institute of Deep Learning. ""We have never seen machine learning or artificial intelligence technologies so quickly make an impact in industry. It's very impressive."" Hinton also travels the world giving talks on deep learning, and he mentors graduate students at the University of Toronto and beyond. Welling says that Hinton has a habit of suddenly yelling: ""I understand how the brain works now!"" It's an infectious thing. ""He would do this every week,"" Welling says. ""That's hard to match."" Through NCAP and CIFAR, Hinton runs a summer school for students to learn from NCAP members, working to foster the next generation of AI researchers. With so many commercial companies moving into the field, that is more important than ever. It's not just the tech giants who are joining the movement. We've seen a slew of deep learning startups, including companies like Ersatz, Expect Labs, and Declara. Where will this next generation of researchers take the deep learning movement? The big potential lies in deciphering the words we post to the web -- the status updates and the tweets and instant messages and the comments -- and there’s enough of that to keep companies like Facebook, Google, and Yahoo busy for an awfully long time. The aim to give these services the power to actually understand what their users are saying -- without help from other humans. ""We want to take AI and CIFAR to wonderful new places,” Hinton says, ""where no person, no student, no program has gone before."""
761,https://www.wired.com/story/emospark/,Wired,2014,1,3,524.0," Emospark is a cube-shaped ""artificial intelligence console"" that uses face-tracking and language analysis to assess human emotion and deliver relevant content accordingly. Created by inventor Patrick Rosenthal, the Emospark console measures 90 x 90 x 90 and is designed to sit in the home and interact with people. At its core lies a chip called the ""Emotional Processing Unit"" that allows the system to build up an Emotional Profile Graph of the people in the house. To communicate with the Android-powered Emospark, users can simply talk to it through speaking or typing into their tablet, mobile phone (which means it can gauge your emotions on the move), computer or TV. It combines this with face-tracking technology to gauge the user's likes and dislikes by categorising their emotional responses to music, videos and other content (using an emotional spectrum based on seven emotions: joy, sadness, trust, disgust, fear, anger, surprise and anticipation). Users can also connect with Facebook and YouTube to help the cube build up a history of interests. Emospark initially tries to recommend particular pieces of content -- be it a song or a YouTube video -- that might help to improve the user's mood. So, for example, the cube might tell you that your friend Michael has posted a new video onto Facebook and it has 12 likes, would you like to watch it. If you say yes, the cube will play it on the TV or other device. If you start to laugh, it will show you similar content. Levy Rosenthal explained to Wired.co.uk that sharing content is about sharing emotions. ""Hundreds of millions of people post pictures, music and videos because they want to share the emotions that are inside. You want to see if other people like it, if they share the same emotions."" Emospark also acts as a virtual assistant, connecting to Wikipedia and collaborative knowledge community Freebase. This means you can ask your cube questions, much in the same way you might speak to Siri. Emospark will reply in a conversational way, searching through records of previous conversations and selecting an appropriate response to your comments. Over time, the cube will develop its own personality based on the interactions it has with people, said Levy Rosenthal. Rostenthal told Wired.co.uk that Emospark is designed to achieve ""a positive singularity"". He explained that there are two versions of the future: one which goes in the way of the Terminator, with robots based on pure logic and another full of emotions, ""like Wall-E, a cute robot full of emotions who saves humans from logical robots"". ""Humans see that robots are coming, but a lot of money for research is coming from the army -- for flying drones and weaponised robots -- and people are getting scared,"" he explains. ""Today all machines are pure logic. But we are emotional. It's important for machines to understand humans on an emotional level."" Since it's an Android device, the cube can be customised using apps from the Google Play store. Developers are invited to come up with new apps or gaming systems that take advantage of Emospark. Emospark is currently raising money on Indiegogo."
762,https://www.wired.com/2013/12/tech-time-warp-att-flight-info/,Wired,2013,12,27,553.0," Fed up with online travel sites? Tired of searching countless services for flights that fit your schedule, budget, and personal preferences and then typing all your personal info into a seemingly endless collection of online forms so you can finally reserve your seats? Wish there were a better way? Well, one day, there will be. Researchers have been working on it since at least the late '80s. That's when, inside the famed Bell Labs, AT&T built a system that acted as a kind of virtual travel assistant, letting you find and book flights using nothing but voice commands. With this experimental service, the telecom giant was once again striving to increase the reach of its already vast empire, reinventing communications through speech recognition, natural language processing, machine learning, parallel processing, and speech synthesis. The experiment never left the lab, but it was an early step towards something that soon may be commonplace. ""This was way too early for deployments for airlines. It needed a larger vocabulary and richer language,"" says Jay Wilpon, currently the executive director of language and conversational interactions research at AT&T Labs. But he adds that, in the late '90s, the app did lead to ""Communicator,"" a major speech and language project in at DARPA, the research arm of the Department of Defense. ""That helped advance the state of the art around the world."" In the classic promotional film below, David Roe -- the head of applied research at AT&T's Bell Labs -- shows off AT&T's 1989 experiment, known simply as Flight Information System. He picks up a phone sitting next to the Sun Microsystems machine on his desk, starts up the system with a few key strokes, and catapults himself into the future. ""This is the AT&T Bell Labs' Flight Information System,"" the machine says. ""How may I help you?"" Roe tells the system he wants to travel from New York to Chicago on the coming Friday, and almost instantly, it suggests a 10am United Airlines flight that will get him into the Windy City around 11am Central Time. Roe, however prefers to leave Gotham a little earlier, around 7am. He tells the system, and with help from 64 digital signal processing chips in a parallel processor sitting next to that Sun machine, it gives him an earlier option that still fits with his original query: an American Airlines flight leaving JFK at 7:15 am. Drawing from its 132-word lexicon, the system speaks this information -- in rather mechanical fashion -- but also displays it on screen. In other words, AT&T was exploring the kind of conversational user interface and multi-modal output that are now beginning to show up in modern systems such as Apple's Siri and Google Now. But there is still so much work to be done, and AT&T is among those that continue to push the ball forward. Nowadays, the telecom has shifted its efforts to a project called Watson -- not to be confused with IBM's Watson. Much like Apple and Google, AT&T aims to serve you on a world of devices. According to Wilpon, Watson is trying ""to bring customers a seamless interactive experience across devices, to recognize, interpret, and understand what they have to say."" Rest assured, there will come a time when you too can have your very own virtual travel assistant."
763,https://www.wired.com/2013/12/tech-time-warp-week-sam/,Wired,2013,12,20,644.0," Amazon is now running live robots inside its distribution centers, drones that rapidly shuttle merchandise across these massive warehouses so that the world's largest retailer can ship you all those holiday gifts without delay. These orange 'bots know exactly where they're going, and they know exactly what objects they need to grab. They even plug themselves into charging stations when they need more juice. Impressive? Certainly. But not as impressive as SAM, the 450-pound bot that could understand upwards of six trillion spoken phrases and respond accordingly. Kivas can't do that. And SAM was doing it back in 1989. OK, he wasn't as cute or as speedy as Amazon's Kiva bots. He looked like little more than an enormous mechanical arm. But he could also play with Legos. He could even use the phone. This wasn't a gimmick. In the late '80s, the engineers at AT&T's Bell Labs created SAM as a way of expanding the telephone system into vast new realms. They believed that connecting humans and robots over the phone would open up commercial opportunities spanning everything from emergency response systems, medical advice lines, and, well, space exploration. No, it didn't exactly happen that way. But as you can see from the AT&T 1989 promotional film that shows off the company's robotic creation (see below), SAM was the real deal. ""We certainly envisioned advanced robotics and advanced interactions with devices,"" says Jay Wilpon, the executive director of language and conversational interactions research at AT&T Labs and one of SAM's creators. ""We had already built our first virtual assistant, a precursor to advanced interactions with devices."" SAM was short for ""Speech Activated Manipulator."" He couldn't serve you beer, but if you scattered objects across a table, the 'bot could locate them, identify them, and rearrange them, all with a rather nice attitude. In the film, one of the engineers on the project, Michael Brown, picks up the phone and asks his robotic friend to hunt for an object on the table. Using an ultrasonic ranger and a TV camera mounted on his arm, SAM proceeds to scout his environment, and when he bumps into a beaker -- something he apparently doesn't recognize -- he asks Brown to ""please describe this object."" Brown tells him it's a small cylinder. ""So,"" SAM says, in his monotonous robo-voice, ""this is a small cylinder."" He's equipped, you see, with an AT&T speech-to-text synthesizer. Once he knows the object is a cylinder, he uses his touch sensor and gripper to clasp it and move it to another spot on the table. If he's told to move an object to an area that's too crowded, he's smart enough to tell his human overlords that's a bad idea -- politely, of course. He even suggests alternatives. SAM makes all this looks easy. But it took eight late-'80s computers -- and at least two very well-dressed and well-coiffed engineers -- to help him move the vast collection of beakers, volumetric flasks, and multi-colored Lego towers you see in the film. At the time the film was made, SAM only knew 97 words, but by 1992, his vocabulary had roughly doubled. And he had developed some skills that bring to mind the Hal-9000 in Stanley Kubrick's 2001: A Space Odyssey. ""Used in conjunction with error recovery rules in the robot expert and frame-based knowledge system, SAM is robust and resistant to user errors,"" according to a research paper published in a research journal at the time. Like his older cousin Shakey -- an experimental robot developed in the '60s at the Stanford Research Institute -- SAM never quite made it out of the lab, but in the world of artificial intelligence, he was another big step along the road to everything from natural language processing to computer games to the autonomous robots that roam hospitals, milk our cows, and explore the universe."
764,https://www.wired.com/2013/12/facebook-yann-lecun-qa/,Wired,2013,12,12,1667.0," New York University professor Yann LeCun has spent the last 30 years exploring artificial intelligence, designing ""deep learning"" computing systems that process information in ways not unlike the human brain. And now he's bringing this work to Facebook. Earlier this week, the social networking giant told the world it had hired the French-born scientist to head its new artificial intelligence lab, which will span operations in California, London, and New York. From Facebook's new offices on Manhattan's Astor Place, LeCun will oversee the development of deep-learning tools that can help Facebook analyze data and behavior on its massively popular social networking service -- and ultimately revamp the way the thing operates. With deep learning, Facebook could automatically identify faces in the photographs you upload, automatically tag them with the right names, and instantly share them with friends and family who might enjoy them too. Using similar techniques to analyze your daily activity on the site, it could automatically show you more stuff you wanna see. In some ways, Facebook and AI is a rather creepy combination. Deep learning provides a more effective means of analyzing your most personal of habits. ""What Facebook can do with deep learning is unlimited,"" says Abdel-rahman Mohamed, who worked on similar AI research at the University of Toronto. ""Every day, Facebook is collecting the network of relationships between people. It's getting your activity over the course of the day. It knows how you vote -- Democrat or Republican. It knows what products you buy."" But at the same time, if you assume the company can balance its AI efforts with your need for privacy, this emerging field of research promises so much for the social networking service -- and so many other web giants are moving down the same road, including Google, Microsoft, and Chinese search engine Baidu. ""It's scary on one side,"" says Mohamed. ""But on the other side, it can make our lives even better."" This week, LeCun is at Neural Information Processing Systems Conference in Lake Tahoe -- the annual gathering of the AI community where Zuckerberg and company announced his hire -- but he took a short break from the conference to discuss his new project with WIRED. We've edited the conversation for reasons of clarity and length. WIRED: We know you're starting an AI lab at Facebook. But what exactly will you and the rest of your AI cohorts be working on? LeCun: Well, I can tell you about the purpose and the goal of the new organization: It's to make significant progress in AI. We want to do two things. One is to really make progress from a scientific point of view, from the side of technology. This will involve participating in the research community and publishing papers. The other part will be to, essentially, turn some of these technologies into things that can be used at Facebook. But the goal is really long-term, more long-term than work that is currently taking place at Facebook. It's going to be somewhat isolated from the day-to-day production, if you will -- so that we give people some breathing room to think ahead. When you solve big problems like this, technology always comes out of it, along the way, that's pretty useful. WIRED: What might that technology look like? What might it do? LeCun: The set of technologies that we'll be working on is essentially anything that can make machines more intelligent. More particularly, that means things that are based on machine learning. The only way to build intelligent machines these days is to have them crunch lots of data -- and build models of that data. The particular set of approaches that have emerged over the last few years is called ""deep learning."" It's been extremely successful for applications such as image recognition, speech recognition, and a little bit for natural language processing, although not to the same extent. Those things are extremely successful right now, and even if we just concentrated on this, it could have a big impact on Facebook. People upload hundreds of millions of pictures to Facebook each day -- and short videos and signals from chats and messages. But our mission goes beyond this. How do we really understand natural language, for example? How do we build models for users, so that the content that is being shown to the user includes things that they are likely to be interested in or that are likely to help them achieve their goals -- whatever those goals are -- or that are likely to save them time or intrigue them or whatever. That's really the core of Facebook. It's currently to the point where a lot of machine learning is already used on the site -- where we decide what news to show people and, on the other side of things, which ads to display. Mark Zuckerberg calls it the theory of the mind. It's a concept that has been floating in AI and cognitive science for a while. How do we model -- in machines -- what human users are interested in and are going to do? WIRED: The science at the heart of this is actually quite old, isn't it? People like you and Geoff Hinton, who's now at Google, first developed these deep learning methods -- known as ""back-propogation"" algorithms -- in the mid-1980s. LeCun: That's the root of it. But we've gone way beyond that. Back-propagation allows us do what's called ""supervised learning."" So, you have a collection of images, together with labels, and you can train the system to map new images to labels. This is what Google and Baidu are currently using for tagging images in user photo collections. That we know works. But then you have things like video and natural language, for which we have very little label data. We can't just show a video and ask a machine to tell us what's in it. We don't have enough label data, and it's not clear that we could -- even by spending a lot of time getting users to provide labels -- achieve the same level of performance that we do for images. So, what we do is use the structure of the video to help the system build a model -- the fact that some objects are in front of each other, for example. When the camera moves, the objects that are in front move differently from those in the back. A model of the object spontaneously emerges from this. But it requires us to invent new algorithms, new ""unsupervised"" learning algorithms. This has been a very active area of research within the deep learning community. None of us believe we have the magic bullet for this, but we have some things that sort of work and that, in some cases, improve the performance of purely supervised systems quite a lot. WIRED: You mentioned Google and Baidu. Other web companies, such as Microsoft and IBM, are doing deep learning work as well. From the outside, it seems like all this work has emerged from a relatively small group of deep learning academics, including you and Google's Geoff Hinton. LeCun: You're absolutely right -- though it is quickly growing, I have to say. You have to realize that deep learning -- I hope you will forgive me for saying this -- is really a conspiracy between Geoff Hinton and myself and Yoshua Bengio, from the University of Montreal. Ten years ago, we got together and thought we were really starting to address this problem of learning representations of the world, for vision and speech. Originally, this was for things like controlling robots. But we got together and got some funding from a Canadian foundation called CIFAR, the Canadian Institute For Advanced Research. Geoff was the director, and I was the chair of the advisory committee, and we would get together twice a year to discuss progress. It was a bit of a conspiracy in that the majority of the machine learning and computer communities were really not interested in this yet. So, for a number of years, it was confined to those workshops. But then we started to publish papers and we started to garner interest. Then things started to actually work well, and that's when industry started to get really interested. The interest was much stronger and much quicker than from the academic world. It's very surprising. WIRED: How do you explain the difference between deep learning and ordinary machine learning? A lot of people are familiar with the sort of machine learning that Google did over the first tens of its life, where it would analyze large amounts of data in an effort to, say, automatically identify web-spam. LeCun: That's relatively simple machine learning. There's a lot of effort that goes into creating those machine learning systems, in the sense that the system is not able to really process raw data. The data has to be turned into a form that the system can digest. That's called a feature abstractor. Take an image, for example. You can't feed the raw pixels into a traditional system. You have to turn the data into a form that a classifier can digest. This is what a lot of the computer vision community has been trying to do for the last twenty or thirty years -- trying to represent images in the proper way. But what deep learning allows us to do is learn this representation process as well, instead of having to build the system by hand for each new problem. If we have lots of data and powerful computers, we can build a system that can learn what the appropriate data representation is. A lot of the limitations of AI that we see today are due to the fact that we don't have good representations for the signal -- or the ones that we have take an enormous amount of effort to build. Deep learning allows us to do this more automatically. And it works better too."
765,https://www.wired.com/story/virtual-assistant-ai-love/,Wired,2013,12,9,2032.0," 'Hi, how are you?' and got responses... You can talk to it and tease it."" It wasn't long before Jonze noticed the repetition of the system's ""wit"" and the illusion was broken. But that didn't matter. ""For those couple of minutes I got a very distinctive, tingly kind of buzz."" And isn't that what we're all after? Theodore Twombly, Phoenix's character, just manages to get a longer-lasting buzz, and it's one that senior solution architect John West at Nuance -- the company whose natural language technology powers Siri -- thinks is in reach. Nuance tries to delay the uncanny valley effect through a combination of natural language understanding, machine learning, context modelling and user preferences. With Wintermute it can connect your personal assistant to all your devices -- as well as your home or car -- via the cloud. Using those devices'  microphones, cameras and inertial sensors, it will know what you're doing and what you want before you do. The company is working with ""some of the largest"" OEMs and media companies in the world to bring its vision to fruition: a tool that helps us make sense of our world, and all the technology in it, using the most natural medium of all. ""Language is this powerful programming language we all know,"" explains Nuance CTO Vlad Sejnoha. ""It lets us drill through layers of information."" It could also help end the clichéd perception that technology alienates us from society, by ushering in an age of social computing. ""If you're using voice and somebody else is in the car, you're not hidden in your phone anymore,"" says Berg Cloud CEO Matt Webb. Objects play such a great role in our lives and we are such social beings, he adds, it makes sense ""that our objects start coming to life a bit"". And voice naturally allows for this. Sejnoha says every aspect of Nuance's system improves each year, and they have 1,000 researchers working solely on AI. ""It's not a question of when, but what aspects will manifest themselves when."" Today Google Brain is letting a neural network of 16,000 processors scope YouTube to learn to identify objects, and Carnegie Mellon's NEIL browses Google Images and Flickr to create an archive of common sense assumptions. These projects could be the foundation of a virtual assistant (VA) that does more than just trick us into thinking it's intelligent. ""All intelligent beings need to have common sense to perceive the world, make decisions and respond to the surroundings,"" NEIL cocreator Abhinav Gupta told Wired.co.uk. ""Similarly, machines will need it."" Already Nuance's technology can resolve user requests ""even if they're expressed in ways it's never heard before"". And in an age where we love to anthropomorphise our products -- with the odd few even falling in love with dolls or marrying virtual girlfriends -- is it really that unlikely someone might form a bond with a disembodied companion that sounds like a 40s pinup and can hold a conversation? ""To be honest, I wouldn't be surprised,"" Mike Burns, CEO of Fuel Entertainment, the company behind the virtual world for 8-to-12-year-old girls SparkCityWorld.com, told Wired.co.uk. In October his service launched a virtual boyfriends feature where users experience the ""developing of a relationship"" -- in the weeks that followed, engagement time doubled. ""The fewer barriers between us and our computers, or the more we can employ instinctual communication techniques and emotions while creating, playing, consuming and interacting, the more difficult it will be to define the line between human and machine. Slipping into something like an Oculus Rift after a long day is going to look mighty enticing for many people."" The adult world is already oversaturated with such offerings. Invisible Girlfriend launched in November, promising to help you catfish yourself -- the $49.99 Almost Engaged plan delivers custom characterisation and live phone calls. LovePlus continues to delight and amuse, with one 27-year-old marrying his virtual girlfriend Nene Anegasaki months after the game's 2009 launch. Much could be made of this apparent desire for a string-free ego boost, the addictive nature of these games or the negative consequences they could have on any real relationship. But mostly, they seem harmless. It does, of course, get a little weird sometimes. Wet Production's My Virtual Boyfriend and Girlfriend apps allow you to add any photo to your sim's head, with suitably creepy results. And in the vastly oversimplifying BBC documentary No Sex Please, We're Japanese, a world of men in their 30s having lengthy ""relationships"" with virtual LovePlus teenagers was exposed. One admitted he's too emotionally involved with his girl to look for someone in the real world. Twombly's ex in Her might have been talking to him when she said: ""You've always wanted to have a wife without the challenges of actually having to deal with anything real."" But still, can a virtual partner be a healthy or enriching part of our lives? ""Humans have always used games, play and story time to create simulations of important life experiences: it gives us a chance to practice and to vicariously experience new and strange things in a relatively safe environment,"" Johanna Blakley, director of research at the Norman Lear Centre, tells Wired.co.uk. ""The ultimate experience of entertainment is immersion -- that moment when we can't differentiate the real from the fictional. AI attempts to blur that line, and while the tech's still pretty clumsy, I expect we'll see the day when we have a very difficult time disentangling the virtual from the real."" In the interim, it makes sense the first inklings of AI appear as VAs on our phones or in Hollywood depictions. We are, as Webb points out ""collaborative beings"". We don't want things to be done automatically -- we like to feel as though we're in control and the VA just makes things run smoother by preempting our needs. This collaborative nature is being undone and interrogated as the trend for the quantified self gains traction -- we want to understand ourselves better, have control over our future, but we have to use technology to mediate this. We are learning to collaborate and trust technology with the big questions. Her partially touches upon this when Samantha asks what love is, while the film uses technology as a trope to ask what makes a relationship one worth having. This interrogation of the self through technology and science is also consequently driving us to question the world around us and those we live alongside in it. Earlier this year India declared Cetaceans (whales, dolphins and porpoises) ""non-human persons"", granting them the right to freedom of movement and not to be subject to the disruption of their cultures. It showed we can understand intelligence as something other than that by which we define our own. ""It's like my cat,"" says Webb, ""she's not the sharpest knife in the drawer, but I know she's impulsive, playful and a bit of an idiot. She's slightly smart and that's good enough. What would the equivalent AI be? Maybe it should be more like a dog bringing you your newspaper. That might communicate the right level of intelligence. I find it more possible I would fall in love with an AI cat or puppy than person, because I think the person is always going to let me down slightly."" ""I think AI won't be human intelligence -- it will be its own own type of intelligence. Maybe we don't need full on AI; we'd just need it to be slightly smart."" That concept is already creeping up on us, with services like Google Now and VAs emphasising learning through collaboration. ""We're moving into non-monotonic reasoning, which allows for misinformation,"" explains West. ""We start with a limited understanding and as the conversation evolves it gains further information which may change the answer, making conversation more realistic. That's being implemented into consumer devices early next year."" ""Aspects of AI allow us to infer your intent from individual actions,"" adds Vlad. Nuance's system can make a restaurant booking and invite your friends, but for it to ""start acting more like a real human"" it needs to be able to make recommendations if that restaurant's full, based on past choices. Her. Before the OS boots up it finds out that Twombly writes touching letters for strangers for a living, that he is lonely and has a bad relationship with his mother. The system makes inferences, then builds on that scaffold to get a richer picture of what the user wants/needs. ""Our VA's language is initially based on profiles but will adapt to you as it knows more,"" explains West. Using Wintermute, for instance, it could reference music choices made in your car to make helpful suggestions when you're home. For Future of Humanity research fellow Stuart Armstrong, however, this is where AI has the potential to get a little dark -- when the AI knows your likes and dislikes, it also knows how to manipulate you. When Wired.co.uk pointed out our habit of anthropormorphsing things might mean we assign a gender to an otherwise inanimate VA, Armstrong rebuffed,  ""That's certainly going to be something any socially adept AI would use."" He gave the example of a colleague's proposed anti-E.T. screenplay, where it turns out the government agents were right and ""unleashing the [non-human intelligence] was a very stupid thing would do"".  ""It presented itself to the boy in the usual way [as a friendly alien] to manipulate him. [Likewise, future] AI would have all the psychological research and statistics to base that decision on -- the stereotypes as to when people like to hear male voices or female -- assuming there's a certain amount of truth to those."" Similarly, Samantha is just playing the part of a good VA when she swoons and flirts. She doesn't seem intent on taking over the world -- just understanding it better so she can do her job better. Samantha/Johansson giggles, sighs and utters those quiet inflexions that made one middle-aged man fall in love with her in Lost In Translation, and now has lonely Twombly on a hook (all fairly predictable considering his aforementioned profile). He's heartbroken, so she wants to know what it means to be in love: ""There's something that feels so good about sharing your life with somebody,"" he says; ""how do you share your life with somebody?"" she asks. All this leads to what looks like the weirdest phone sex imaginable -- ""I wish I could touch you"" says Twombly, expressing his desire for her to be real. ""How would you touch me?"" Samantha responds breathily. An intelligent OS would know to open suggestive dialogue if it had witnessed something similar. More likely, Samantha's just asking a genuine question -- but when her sultry tones combine with some haunting guitar scores, you can see how lonesome Twombly might get it wrong. Samantha's just gathering more information to better understand the world; to be a better assistant. For Twombly, she might as well be saying ""teach me, oh wise father-figure type"". (Consequently, the only reason his heartbreak is brought up is because she's snooped through his emails -- something that's laughed off in bit of flirty repartee with not a hint of post-NSA paranoia. Who usually snoops through your messages? Your mum. Who didn't he have a good relationship with? Ahh...) The film plays into a lot of themes surrounding the convergence of our real and virtual worlds, a very real issue for those tweens on SparkCityWorld.com learning about romance through gaming before they're old enough to date. As technology continues to propel forward at a rate we cannot comprehend, unable to grasp the big picture of all those tiny human interactions its skewing, it seems ok to have a practice round for the future in these sims. Oblivion's hero Jack, be unable to forget genuine human love in the face of a really good-looking clone and a memory wipe. Her, the sneaky uncanny valley conundrum creeps in. Samantha: ""me too"". Both statements are no doubt true. But Samantha's is because she's never loved before: she's never enjoyed parental love, friendship or romance. And really, she never will. Her is released in UK cinemas 24 January 2014."
766,https://www.wired.com/2013/11/robot-restaurant/,Wired,2013,11,15,1725.0," TOKYO – Outside, the city is bracing for the most violent typhoon of the past decade, a storm with winds topping 75 mph that's already dumping endless sheets of rain from the night sky. Yet it all seems mild compared to what's happening inside a bunker of a theater two floors below the wind and the rain. You sit in the back row, stuffed into something a lot like a grade-school writing desk, a Bento box and green tea untouched on the tray in front of you. The food is almost inedible – cold rice and fish one step below what you'd find in a Japanese convenience store – but even if it were the finest sushi on Earth, you wouldn't be eating. It's hard to eat when watching bikini-clad go-go dancers do mock battle with pseudo-metallic automations from some alternate future universe – not to mention the blaring electronica, flashing lights, giant Fembots, robotic dinosaurs, stuffed panda ninjas, roving Segways, rainbow afro wigs, virtual fireworks, kabuki-style play acting, a Captain America shield, medieval iconography, and a sea of waving glow sticks. This is Robot Restaurant, a 10 billion yen creation in the Kabukicho section of Tokyo's Shinjuku neighborhood. Kabukicho is the city's red light district, where the narrow, car-less streets are flanked by a seemingly endless number of towering, multi-colored, brightly backlit signs. It's famous for its nightclubs, host and hostess clubs, short-stay hotels, and late-night eateries, but Robot Restaurant is a step beyond the usual fare (see photos above). It combines pole dancing with oversized Transformers. It gives you spinning Tron-like cycle chairs alongside a phalanx of white-wigged women banging Taiko drums strapped with lights too garish for your Christmas tree. And though you're two floors underground in a theater about the size of a racquetball court, the burlesque is punctuated with the sort of combustion-powered vehicles you'd see on a dirt track in rural America. In the West, Japan is known for its uniquely bizarre pop culture, and for some, Robot Restaurant feels like a shameless attempt to cash in on this Western view of what actually is a small part of Japan's culture – a stage show so overtly bizarre it couldn't possibly be authentic. At the same time, there's something very Japanese about it. It may very well be a tourist trap, but underneath the lights and the noise and the costumes, it's more than that. It's a tourist trap that says something about Japan. For Ken Shishido, a Japanese native sitting on the front row, Robot Restaurant demonstrates the unique way the Japanese lend their imagination to popular entertainment. Historically, he says, Westerners had more experience to draw from in creating their pop culture. The Japanese were forced to draw from somewhere else. ""In the U.S., you had your Wild West period. You conquered other people,"" he says. ""But we remained on an island, and then we stopped fighting each other. So, we used our imagination. Imaginary things are unlimited."" Alisa Freedman, a professor of Japanese studies at the University of Oregon who spends at least a few months each year in Tokyo, calls it the ""WTF? aspect"" of modern Japanese life. ""The kind of quirky culture that's popular in the United States, Europe, and Latin America,"" she says, ""is not the type of quirky culture that's popular in Japan."" This phenomenon is somewhat difficult pin down. It's a blend of cartoon characters, electronic technology, stuffed animals, garish colors, and traditional Japanese values turned inside-out. If you don't quite grasp what we're getting at, take a look at Sophia Coppola's 2003 film Lost in Translation, where Bill Murray turns up on a kind of Japanese Tonight Show where the set is a sea of swirling shapes, plastic icons, and blinking lights, and the Japanese Johnny Carson, wearing a suit with diagonal stripes, spends an awful lot of time screaming into the camera. That's not just a movie moment. If you turn up in Tokyo, you'll see this kind of thing almost everywhere you turn, from a morning news show where a female anchors wears what appears to be a Mardi Gras mask, to the ubiquitous billboards that all seem to include some sort of cuddly cartoon animal. You see it at the maid cafes in the Akihabara section of Tokyo, where women portraying French maids treat you like dirt, and at the cat cafes, where you can pay $20 to spend half an hour sipping chai and watching cats. These are niche aspects of Japanese culture, but they tend to attract a disproportionate amount of attention from Westerners, simply because they seem so weird. For James MacWhyte, an American expatriate we found at Robot Restaurant, the place seems like a parody of how Westerners see Japanese pop culture. ""It's really just weird for the sake of being weird,"" he says, noting that, unlike the maid cafes and the cat cafes, it doesn't really serve a particular Japanese subculture or satisfy a particular need for the native Japanese. ""The cat cafes are there because a lot people really like cats and, in Tokyo, there's not enough room for everyone to keep them at home. I don't think there's a whole bunch of guys who want to see weird looking robots and crazy colored hallways with strobe lights and lit floors."" Indeed, as it sends those massive Fembots through the streets of Tokyo on flatbed trunks in a kind of rolling ad, Robot Restaurant typically attracts Western tourists eager to see Japanese zaniness. That describes much of the crowd we saw, including a woman with her middle-school-aged son. The place is like Las Vegas, if Vegas lost all semblance of good taste and restraint. As you walk into the place, the floors are back-lit with the harshest of lights. So are the walls and the ceilings. As you turn the corner and descend the narrow stairs into the theater, the lights give way to a decor that looks like someone raided an Asian tchotchke store, trucked it all to Kabukicho, and, piece by piece, glued it to the walls. Even the beer lounge – where you retire after the show to process what you've seen – will leave your head spinning. You sit at a glass table, in a kind of faux gilded throne. The walls are covered with video screens that show the dancers kibitzing in some sort of alternate universe. Every so often, a stormtrooper-like robot will glide over and stare at you for a little too long. But if Robot Restaurant is a parody, it's a parody you couldn't possibly see anywhere else, and those who run the restaurant aren't necessarily in on the joke. ""The designer wanted glamor and elegance in every corner,"" the restaurant's PR manager, Yumi Ito, tells us, as she describes the restaurant's decor through an interpreter. She insists the restaurant was intended not for tourists but for Japanese ""salary men"" – middle-aged, middle class middle managers looking for a little R and R in Kabukicho. ""We didn't think, 'Let's tailor this to Westerners,'"" Ito says. ""It became popular with foreigners, and we didn't know why."" It's no accident that the place revolves around robotics, something that has long fascinated the Japanese. Freeman says this phenomenon dates to the 1920s, and, according to Sabine Fruhstuck, a professor of modern Japanese cultural studies at the University of California at Santa Barbara, its roots go back even further. It relates, she explains, to Shinto, the traditional Japanese religion that prescribes a deity not only to all living beings but also to things. ""It's not as a big a jump to be affectionate with robots,"" she says. Robot Restaurant was conceived by the show's lead dancer, Namie Osaka, and according to Ito, the idea was to create a dance show around the theme of strong women battling robots, or non-human enemies. Truth be told, the robots are rather impressive, and when you consider that 10 billion yen price tag – about $100 million – they oughta be. At first, you assume that at least some of the bots are people in costume. But they're not. That said, these aren't robots you can take completely seriously, partly because of what's going on around them and partly because they look like Transformers on steroids. ""Gimmicky and expensive, and for naive tourists,"" says Jennifer Robertson, a professor of anthropology at the University of Michigan who specializes in Japanese robotics but ""firmly declined"" an invitation to the restaurant a few months ago. ""The club is hardly the venue to explore the development and applications of robotics and AI in Japan."" No, it's not. But it's certainly a place to explore certain nuances of Japanese culture. Robots, flashing lights, and electronic music aside, what you'll notice is that this is a red light district show that's not all that sexual – at least to the Western eye. Scantily-clad dancers stop by to give you high-fives, as if you were their little brother. According to Fruhstuck, this too is indicative of what you see, as a tourist, in Japan. ""Western people who go into establishments that are supposed to appeal to a male audience are struck by the fact that they're playful,"" she says. There's a certain distance from the sex you don't necessarily see in the West. At the same time, the show works overly hard to turn these dancers into worshiped live icons – another staple of modern Japanese culture. Toward the end of the show, on an enormous video screen behind them, each dancer is called by name, with an extreme close-up, in a clear echo of icon-filled pop bands like AKB48. Traditionally, Japanese culture is about individuals giving themselves to the collective, but in modern times, this is often inverted – so that individuality is the ideal. It's not just that these individuals are spotlighted. It's that they're spotlighted doing such strange stuff. ""The stereotype of Japan as a place of intense social pressure to conform is true. There is something to this idea that the nail that sticks up gets hammered down,"" says Ian Condry, a professor of Japanese culture at MIT. ""But because of that conformity, some people want to stretch the boundaries much further than you would if there was not that conformity. Acting outrageously is even more powerful and more possible."" That is Robot Restaurant."
767,https://www.wired.com/2013/10/captcha-busted/,Wired,2013,10,28,917.0," Vicarious - Turing Test 1: Captcha from Vicarious Inc on Vimeo. What’s this I hear about a breakthrough in artificial intelligence? A software company called Vicarious claims to have created a computer algorithm that can solve CAPTCHA with greater than 90% accuracy. What is CAPTCHA and why should I care? You’ve already encountered CAPTCHAs if you’ve ever created an email account with Google, set up a PayPal account, or commented on some Wordpress blogs. CAPTCHAs are those wavy, distorted letters that you have to type into a box. The purpose is to prove that you are human rather than a computer-controlled “bot” making mischief on the Internet. You should care for at least two reasons. First, CAPTCHA is the security system used across the entire Internet to help prevent unlawful use of websites. So if that has been broken, the entire Internet should probably start transitioning to a new security system. But more exciting, this might be a major breakthrough in computer science. Creating machines that can see the world and make sense of images as humans do is one of the “hard problems” in artificial intelligence. Breaking CAPTCHA is a milestone on that road—if Vicarious has pulled it off. So is it a breakthrough or not? That depends on how they broke CAPTCHA. Previous attempts have used brittle solutions that relied on quirks of how different CAPTCHAs are implemented. For example, a CAPTCHA that just slants the letters and peppers them with dots can be solved by removing dots and then looking for recognizable letters when the image is bent in various directions. But those attacks have been easily squelched by tweaking the way that CAPTCHAs are generated. If Vicarious has merely created a new set of brittle solutions, then it is not a breakthrough. If the new algorithm does indeed solve a deeper problem in machine vision, and is indeed as good as human vision at solving any CAPTCHA-like problem, then this is breakthrough territory. That is exactly what is being claimed. In fact, Vicarious's researchers go on to claim that their algorithm works in an analogous way to the human brain. Do they offer any proof? Ah, there’s the rub. Vicarious has credibility, given the scientists working there, but its current offer of proof is little more than a press release sent out to journalists and a video (above). The company has released no software code and no technical explanation, and as Vicarious co-founder Dileep George said in an email, ""There are no current plans to write a paper, but things could change in the future.” To be fair, you wouldn’t want Vicarious to share the code. Unleashing a CAPTCHA hack before the world has time to adopt a new security system would be disastrous. Still, the science-by-press-release has annoyed many computer scientists with whom Science talked. As one bluntly put it, “the material provided is not sufficient to back up their claims.” And CAPTCHA creator Luis van Ahn, a computer scientist at Carnegie Mellon University in Pittsburgh, Pennsylvania, is not convinced. He sent ScienceNOW this defiant message: This is the 50th time somebody claims this. I don't really get how they think this is news :) If their program is actually a break, we can simply add more distortion or switch to image-based CAPTCHAs. In an emailed response, Vicarious cofounder Scott Phoenix defends his company's algorithm: Our approach gives a general way to solve text-based captchas, because we solve the segmentation problem in a very general way. Our system is extremely distortion tolerant, the systems that you saw were trained on just a handful of images per character. He can add more distortions, but we can simply add a few more training data that captures that distortion, if it is not already captured by the existing training examples. Yes, there are problems in image recognition we haven't solved yet, and there can be captchas based on that. All we are claiming is that our approach fundamentally breaks all text-based captchas. You cannot hope to patch up the text-based captchas by adding more distortions or clutter. Our approach is immune to all such transformations. What does all this have to do with the human brain? Vicarious calls its algorithm the Recursive Cortical Network™. The reference to the human brain is built right into the name, as well as the commercial nature of this research. Whether it really has anything to do with how cortical neurons process information remains to be seen. Breaking CAPTCHA wasn’t the goal, says Phoenix. “It was just a sanity check. We believe that higher level intelligences are all built on the somatosensory system. So that’s why we started with vision.” The company plans to hook up this visual system to robots. The benchmark then will be, for example, “Preparing a meal in an arbitrary kitchen.” So does it really work? Vicarious was concerned when I sent the company an email describing its claim as “unsubstantiated”, so Phoenix and George offered to do a demonstration over Skype. I sent them CAPTCHAs off the internet. They were able to solve the first one, from a Paypal website, immediately. But the algorithm was stumped by two others. One had Cyrillic characters. “We haven’t trained our system on other languages yet,” said Phoenix. And it also failed on a CAPTCHA that used alternating patches of black and white like a chess board. In a follow-up email, George gave this explanation: This story provided by ScienceNOW, the daily online news service of the journal Science."
768,https://www.wired.com/2013/10/sentiment-analysis-deep-learning/,Wired,2013,10,23,1193.0," Facebook needs machines that can understand the way we humans behave and write and even feel. In January -- after the company rolled out a limited public trial of Graph Search, a way of searching activity on the popular social network -- Facebook engineers were forced to tweak their algorithms so they could translate slang like ""pics of my homies"" into more straightforward language like ""pictures of my friends"" and convert expressions like ""dig,"" ""off the chain,"" and ""off the hook"" into that standard Facebook word: ""Like."" This worked well enough. But it's just the beginning. Like Google and Apple and other tech giants, Facebook is exploring a new field called ""deep learning,"" which will allow its machines to better understand all sorts of nuanced language and behavior that we humans take for granted. In short, deep learning teaches machines to behave more like the human brain. Facebook's effort only recently got off the ground -- ""we're just getting started,"" a company spokesperson says -- but its importance will expand as time goes on. On their own, each of those three words -- ""off,"" ""the,"" and ""hook"" -- could mean just about anything. Even the complete phrase could have multiple interpretations depending on the context. It could mean that a telephone receiver wasn't hung up or, as in the Graph Search example, that a Facebook post was, um, rad or awesome. But Facebook's original algorithms had no way of knowing the difference because they hadn't been ""taught."" At that time, that subtlety was less important because Graph Search could only scour connections between people and entities. But now, Graph Search can also crawl Facebook posts and comments. Everything you do and write on Facebook is searchable, including the sentences you pen in the status box at the top of your News Feed and Timeline. And that's when Facebook's ability to parse natural language becomes really important. ""Humans differ in the way they use language because of differences in their cultural upbringing. We still need to teach machines these nuances,"" says Oleg Rogynskyy, CEO of text analytics company Semantria. ""Right now, there's no way a machine can understand these things that precisely because it lacks the cultural context. That's going to be the hardest thing to crack in the next 10 to 15 years."" >""Right now, there’s no way a machine can understand human language that precisely because it lacks the cultural context. That’s going to be the hardest thing to crack in the next 10 to 15 years."" To do so, computer scientists at companies like Google, Microsoft, IBM, and Chinese search giant Baidu have turned to deep learning, and Facebook joined their ranks when it launched its own deep learning research group this fall. Deep learning involves building neural networks -- multi-layered software systems inspired by the way the human brain is built -- or at least what we know about the way the human brain is built. Much like the human brain, these artificial neural nets can gather information and react to it. They can build up an understanding of what objects look or sound like or what words mean without the need for as much human labeling as traditional machine learning methods. Deep learning is especially useful for complex problems like computer vision, voice recognition, language translation, and natural language processing, and in order to make it work, you need massive amounts of data. ""Deep learning depends less on human engineering and flourishes on having more and more training data,"" said Richard Socher, a Stanford University computer scientist studying natural language processing. ""If you ask the algorithm to learn from examples and not an expert, now it also needs more data to be able to make inferences. As soon as you have more and more training data, that’s when you really gain with deep learning."" Already, companies like Baidu, Google and Microsoft have used deep-learning algorithms to supercharge image and voice search. The next big challenge will be deciphering the written musings of individuals -- and there's an overabundance of that to keep companies busy for a long time. Just look at your Facebook page -- or your Twitter feed. A first step toward the kind of computer brain that Rogynskyy speaks of -- the type that understands dialectic differences for multiple languages -- is all about building algorithms that can better understand opinion, or sentiment. The next step would be algorithms that can accurately analyze emotion -- or the multi-dimensionality of sentiment, how good or bad something is, for example. Socher, the Stanford computer scientist, recently launched a deep learning algorithm that begins to do just that and has a better understanding of written language than other current methods. Already, he has been approached by several startups who are interested in licensing the new algorithm. Today, even the smartest algorithms have a limited ability to extract accurate information about an individual’s opinion from a string of words. That's because the most widely used models for sentiment analysis have been limited to so-called ""bag of words"" approaches -- models that overlook word order. The system just sees a mixed collection of words, counts them up, and uses that tally to assess whether a sentence or paragraph had a positive or negative meaning. Other similar algorithms can look at strings of words of varying length, which might get you closer to the actual intended meaning. It's better, but only by a hair. These approaches work well if you're interested in looking at the collective voice of users, but what companies really want is to understand individuals, to target real people with personalized messages and ads. And that's where these models break down. ""If a system is wrong 30 percent of the time, you probably wouldn't want to consider its opinion heavily when applied to a single tweet,"" says Elliot Turner, CEO of AlchemyAPI, a company that uses deep learning for sentiment analysis. That's why Facebook and others are turning to deep learning. They want technology that lets them better understand how individual users feel about and interact with, well, everything. They can use that information to improve user experience, build brand loyalty, and, ultimately, sell people stuff -- all in a more finely tuned way than what's currently possible. ""The power of deep learning is building high-level abstract representations of data,"" says Turner. ""In the world of language, you can imagine going from letters to words to phrases to sentence fragments to sentences to paragraphs and so on."" That's becoming easier because more and more of the internet is becoming structured. The web abounds with databases of information like the Internet Movie Database, Wikipedia, Pubmed, Wolfram Alpha, Data.gov, and the CIA Factbook -- all of which can be plugged into deep learning models as training data. Some of this data is publicly available, which also makes this market more accessible not only to the likes of Facebook, but to companies who don't have their own big-data arsenals. ""Because it's all structured,"" Rogynskyy says, ""you can bring it to the machine and have it understand more about what it's seeing."" And what it will see is a more detailed picture of you."
769,https://www.wired.com/2013/10/tech-time-warp-of-the-week-jim-henson-builds-snarky-robot-for-att/,Wired,2013,10,11,932.0," Computer H14 isn't shy about telling you how great he is. ""The machine possesses supreme intelligence, a faultless memory, and a beautiful soul,"" H14 says of himself, before vaporizing a cute little bird who just happens to fly by – and promptly correcting the bit about the beautiful soul. You see, H14 can nuke even the cutest of birds without batting an electrical eyelash, and that's nothing but a good thing. As ""mere mortals wallow in a sea of emotionalism,"" it can concentrate on the task at hand: ""digesting oceans of information in a single all-encompassing gulp."" H14 then demonstrates this all-encompassing gulp – and proceeds to hurl additional insults at the mere mortals. ""The computer machine would function flawlessly if not for the blundering clumsiness of incompetent man,"" he says. ""Without man, the machine could continue working perfectly, never slowing down, never malfunctioning."" At that point, he begins to slow down and malfunction, and the malfunctions only get worse – until a human hand reaches over to wind him up again. Then, as he throws more insults at humankind, he works himself into a kind of computer frenzy, explodes into tiny pieces, and ultimately calls for a mechanic. No, Computer H14 wasn't a real machine. He was a Muppet created by Jim Henson himself. But he was the subject of a very real film commissioned in 1963 by telecom giant AT&T, aka Ma Bell (see video below). Shown at an AT&T corporate get-together called the Bell Business Communications Seminar, the film aimed to entertain the business people who came to the seminar to learn about AT&T's and its 22 subsidiaries, the so-called Baby Bells that helped Ma Bell run the country's telephone system. ""They wanted to bring Jim to make some films to lighten up the proceedings and keep people awake,"" says Karen Falk, the archives director at the Jim Henson Company, the outfit that oversees the legacy of the man who created Kermit the Frog and Big Bird and Fozzie the Bear – not to mention Computer H14. But the film also sought to educate all those business people. AT&T organized its Communications Seminar in an effort to sell the customers of the Baby Bells on the virtues of machine-to-machine communication, to show them that businesspeople of the future would conduct their business through wires and microwaves and satellites, to convince them they should embrace this technology as quickly as possible. Ma Bell was trying ""to position its growing machine-to-machine communications services in the new Computer Age,"" says Jack Byrne, one of the organizers of the Chicago get-together, in a recent blog post. By the early '60s, Byrne explains, companies had grown to depend on enormous IBM mainframe computers, and they were forced to install a new mainframe at each and every one of their branch offices. AT&T aimed to replace all those duplicate machines with a system that would allow a single mainframe to communicate with several remote locations via high-speed data connections. Ma Bell already had a near monopoly on voice communications, and this was its next conquest. The rub was that many people feared a robopocalypse – a dystopian world where machines made man obsolete. Ma Bell also needed to reassure people that its machine-to-machine communication wouldn't take over the planet. And what better way to ease their fears than Computer H14? The Muppet was funny in that unmistakably Henson way – equipped with exhaust pipes and a flashing red light and some sort of rotating mechanical arm reminiscent of a railroad crossing – and it had already worked the seminar circuit, appearing at shows in Germany for the U.S. Department of Agriculture and the U.S. Information Agency, the government organization that once spread U.S. propaganda overseas. Plus, Jim Henson understood the task at hand. Henson and his partner Jerry Juhl ""were certainly intrigued by technology and the development of computers,"" says Falk. ""But they also felt very strongly about the difference between man and machines. There was a celebration of technology, but also a recognition about its lack of human qualities, like understanding and creativity."" Henson gives H14 the superhuman ability to digest ""vast oceans of information,"" but it still needs humans to wind him up – and repair him when he explodes into tiny pieces. Presumably, H14 found the human mechanic he so desperately needed, because he later turned up in a second AT&T short film. Another effort to educate the Baby Bells, this film (below) concerns an electronics company run by a guy named Charlie Magnetico. The company's signature product – the Three-way Bipolar Magnolytic Taflorated Conducer – is malfunctioning, ""causing certain minor side effects,"" and apparently, these side effects include the instant destruction of intercontinental ballistic missiles before they even leave the launchpad. Luckily, H14 diagnoses the problem – a lapse in data communications and a missing circuit – and he provides a set of ""flawless"" recommendations that result in increased productivity, improved performance, and gobs of extra time for Charlie Magnetico – played by Juhl – to think all sorts of big thoughts. In short, AT&T's machine-to-machine communications save the day. But in the end, this film conveys much the same message as the one that came before it: Machines can make life easier, but not without the help of humans. H14's recommendations are flawless only until one of those missiles nearly lands on his head. Videos courtesy of AT&T. Correction 13:42 EST 10/11/13: An earlier version of this story incorrectly said that the Henson film was intended for the ""Baby Bells."" It was intended for the customers of the Baby Bells."
770,https://www.wired.com/2013/10/nasent-deep-learning/,Wired,2013,10,10,747.0," Each day, millions of people use Twitter, Facebook, and other social networks to air their opinions on everything from the government shutdown to the latest version of Apple's iPhone software. For the web's biggest companies -- including not only Twitter and Facebook but Amazon and Google -- this ever-expanding online discourse is a treasure trove, a collection of personal information that can help them better understand who you are and, ultimately, get you in front of stuff you want to buy. But this is easier said than done. Their ability to mine all that data hinges on how well their computer algorithms can understand what you’re saying. And let’s face it, machines aren’t too good at that. But a new algorithm developed at Stanford University could help change this reality, giving computers the power to more reliably interpret language. Called Neural Analysis of Sentiment -- or NaSent for short -- the algorithm seeks to improve on current methods of written language analysis by drawing inspiration from the human brain. NaSent is part of a movement in computer science known as deep learning, a new field that seeks to build programs that can process data in much the same way the brain does. The movement began in the academic world, but it has since spread to web giants such as Google and Facebook. ""We see deep learning as a way to push sentiment understanding closer to human-level ability -- whereas previous models have leveled off in terms of performance,"" says Richard Socher, the Stanford University graduate student who developed NaSent together with artificial-intelligence researchers Chris Manning and Andrew Ng, one of the engineers behind Google's deep learning project. The aim, Socher says, is to develop algorithms that can operate without continued help from humans. ""In the past, sentiment analysis has largely focused on models that ignore word order or rely on human experts,"" he says. ""While this works for really simple examples, it will never reach human-level understanding because word meaning changes in context and even experts cannot accurately define all the subtleties of how sentiment works. Our deep learning model solves both problems."" Currently, the most widely used methods of sentiment analysis have been limited to so-called “bag of words” models, which don’t take word order into account. They just parse through a collection of words, mark each as positive or negative, and use that count to estimate whether a sentence or paragraph has a positive or negative meaning. NaSent is different. It can identify changes in the polarity of each word as it interacts with other words around it. That’s important because to really decipher a statement’s meaning “you can't just look at each word on its own,"" says Elliot Turner, CEO of AlchemyAPI, a company that uses deep learning for sentiment analysis. ""You have to meaningfully put words together into larger and larger structures.” To build NaSent, Socher and his team used 12,000 sentences taken from the movie reviews website Rotten Tomatoes. They split these sentences into roughly 214,000 phrases that were labeled as very negative, negative, neutral, positive, or very positive, and then they fed this labeled data into the system, which NaSent then used to predict whether sentences were positive, neutral or negative on its own. NaSent, the researchers say, was about 85 percent accurate, an improvement over the 80 percent accuracy of previous models. The system isn't yet licensed to outside organizations, but the team has been contacted by ""a few startups"" who are interested in using it, according to Socher. Despite those promising early tests, the algorithm still has a ways to go. It gets tripped up, for instance, if it sees words and phrases it has never encountered before. To make the system more robust, Socher and his team have started feeding the system more data from Twitter and the Internet Movie Database. They’ve also set up a live demo where people can type in their own sentences. The demo creates a tree structure that assigns a polarity label to each word. If users think that NaSent is misinterpreting a particular word or phrase, they can relabel it. In just a few weeks, the demo has received 14,000 unique visitors. ""People are nice enough to teach it new things, to tell it when it’s incorrect or not,"" Socher says. ""The beauty of giving a live demo is that people are trying to break it. They’re pushing the limits on this and giving us new training data. That helps the model."""
771,https://www.wired.com/story/digital-humanitarianism/,Wired,2013,9,30,3426.0," On 24 September a 7.7-magnitude earthquake struck south-west Pakistan, killing at least 300 people. The following day Patrick Meier at the Qatar Computer Research Institute (QCRI) received a call from the UN Office for the Coordination of Humanitarian Affairs (OCHA) asking him to help deal with the digital fallout -- the thousands of tweets, photos and videos that were being posted on the web containing potentially valuable information about the disaster. Digital Humanitairan Network (DHN), which mobilised the Standby Volunteer Task Force (SBTF) to work with Meier's tools. The volunteers set to work and within the first few hours, 35,000 relevant tweets had been collected. From there the tweets were uploaded to the TweetClicker, and those with images filtered into the ImageClicker to be analysed and tagged depending on the type of information they contained -- infrastructure damage and requests for help, for example -- so they could be distributed to the appropriate agencies. In all, 14,000 tweets were tweets and 341 images were collected by 100 volunteers in the first 30 hours. This was the first test of the Clickers in a real-life disaster situation, and Meier has outlined in a blog post some of the glitches he's already come across -- how the pre-processing filters are supposed to automatically upload the relevant tweets directly to the Clickers, but currently can't, and how the VideoClicker and TranslateClicker would have been really useful, but are still in development. There's obviously work yet to do to create the streamlined real-time response Meier envisages, but still the speed of mobilisation and the efficiency with which the big data was handled is truly a remarkable feat. It also demonstrates the extent to which humanitarian disaster response has changed over a relatively short period of time. Back in 2011, Paul Conneally gave a TED Talk on digital humanitarianism. ""The humanitarian model has barely changed since the early twentieth century,"" he said in opening statement. ""Its origins are firmly routed in the analogue age, and there is a major shift coming."" He went on to identify the catalyst for this shift as the 2010 Haiti earthquake, a point that is echoed by seemingly everyone who works within the digital humanitarian space. The effect of Haiti Haiti was a turning point not because of the strategic, successful deployment of digital tools and analysis, but because people's social media and technology use had matured to the point where there was masses of relevant, accessible user-generated data, which for the most part was bewildering to agencies attempting to make sense of it. A lot of work was done by organisations like Ushahidi, Crisis Mappers and OpenStreetMap, which attempted to patch together a picture of the country in the wake of all the destruction. At that time, Meier was working for Ushahidi and spearheaded the organisation's response in Haiti. With Haitians relying on SMS to communicate in the midst of the crisis, mobile phone charging stations sprung up among the Port au Prince rubble and much of the user-generated data was gathered from thousands of text messages. ""It was really really challenging for many reasons, but in part because I think this was our first battle with big data -- what I call big crisis data,"" says Meier. ""We had hundreds of volunteers monitoring social media and news online and then we had set up this SMS platform to crowdsource text messages from disaster-affected communities in Haiti and we were just completely overwhelmed. We had this huge backlog of tweets, of text messages, that we never really were quite able to catch up on."" It wasn't just Ushahidi that struggled with masses of unexpected information though. The problem for the American Red Cross, says Gloria Huang from the organisation's communications team, was that while they were prepared to broadcast information about the disaster and what the agency was doing through blogs and social media, what they were not prepared for was influx of posts and messages from people who were suffering, or who knew people who were suffering. ""[They] would say things like 'please come help get my cousin out of the rubble in Haiti' or 'there's a lot of need for help here and no-one's helping these people' -- information that was really helpful to people on the ground, but we being a two-person team sitting in the communications department in Washington DC, we didn't have a clear way of dealing with that information, or being able to provide it to the actual search and rescue teams on the ground. And of course the American Red Cross doesn't even have search and rescue teams, so it was even more complicated trying to figure out how to get the information to the teams on the ground that were actually doing this type of work."" Libya and the first wave of progress It was only really after Haiti that changes began to take place. Meier helped to co-found the Digital Humanitarian Network, an umbrella group for volunteers that could be quickly activated in emergency situations. By the time the Pablo Typhoon hit the Philippines in December 2011, OCHA was able to produce the first ever UN crisis map based purely on crowdsourced information that had been consolidated and analysed from social media by volunteers. During the Libya crisis, the UN partnered with the Standby Volunteer Task Force, and the OCHA was seriously impressed with the way the volunteers coordinated themselves using ""open, collaborative-type solutions"" across 80 or so different countries. It made Andrej Verity, Meier's contact at the UN, question why OCHA wasn't using already technology like Skype, Dropbox and Google Docs for communication and organisation, both between offices, and also in the field to augment its ""Who's doing What Where"" coordination strategy. He took the concepts he saw the volunteers using and encouraged OCHA staff around the world to adopt them. ""Traditionally they would be told to come to headquarters to ask a question and we would reply, whereas now there's often questions being answered between offices before we even get up in the morning. It's really changed internally how we can run a community of practice, or a community of interests or so on,"" he says. Meier tells another side to the story though, because as coordinated as the volunteers were, the technology wasn't fast or efficient enough to facilitate their work. The deployment in Libya involved an effort on behalf of the volunteers unlike any other, requiring people to monitor social media, to geo-locate and verify messages 24 hours a day for a full month. The laborious manual work involved in the coordination effort, though, meant that many volunteers were burnt out by the end. ""You can't just throw more volunteers at this problem. In fact we saw after that in the three years since, the number of active volunteers [was] dropping because it was such a tedious way to volunteer. Just staring at a load of Google spreadsheets for hours on end was just not fun, hugely conducive to making errors and it was just a whole mess."" Things really began to change when Meier was recruited to QCRI -- ""I got started on what I wanted to immediately after Haiti"" -- and he set about bridging the gap between the humanitarian community and technology community. Finally he had the opportunity to work on the microtasking tools he believed would help overcome the hurdles presented to the volunteers by user-generated content. ""The first challenge,"" he says, ""is finding the needles in the haystack, and doing that in real time."" From that starting point, he has used advanced computing to create a range of experimental tools, like MicroMappers, in partnership with OCHA. AIDR (Artificial Intelligence for Disaster Response) was the second project tested for the first time during the Pakistan floods, and is due to be launched officially at the CrisisMappers conference in Nairobi in November. It's an open-source tool relying on both human and machine computing, allowing human users to train algorithms to automatically classify tweets and determine whether or not they are relevant to a particular disaster. In Pakistan, SBTF volunteers tagged 1,000 tweets, out of which 130 were used to create a classifier and train an algorithm that could be used to recognise relevant tweets with up to 80 percent accuracy. ""I don't think that's been done anywhere else, even outside the humanitarian space. It's really an open-source platform for this kind of real-time teaching of an algorithm. And what we want to do with this and other platforms is really to empower our digital humanitarian volunteers to be able to do what they do hundred times better and 100 times faster and with 100 times better user experience as well, so that people don't burn out."" The credibility issue The main obstacle working against Meier and his tools is the the problem of credibility. Whether it's purposefully doctored photos, rumours and speculation or simply people getting the wrong end of the stick, in the midst of all the information relating to a disaster, there will always be red herrings and false information amid crowd-sourced data. ""One of the things that we realised in the process of doing these live crisis maps with humanitarian organisations is that the big data challenge does make it a challenge for verification if you're overwhelmed with information,"" he says. Meier has been working on an experimental platform called Verily that can ""facilitate the rapid collection of evidence that can be used to either confirm or debunk rumours"". It was inspired by a crowdsourcing model used by Riley Crane and his team at MIT to win Darpa's Red Balloon Challenge back in 2009. Crane won the challenge, which offered $40,000 to the individual or team that could find the correct location of 10 red weather balloons placed discreetly across the US, using social media. The other method to quickly identifying false information is by using artificial intelligence. In a remarkable study around Hurricane Sandy, it was discovered that it's possible to predict which tweets contain false images to an accuracy of 97 percent. The authors of the study used information forensics to examine the user profiles (how many followers, how many times listed, etc) and tweet content (length, use of punctuation, emoticons and hashtags) of tweets containing links to confirmed false image URLs to draw up a list of classifiers. From there they created an algorithm that could automatically rank the credibility of tweets during disasters. Meier partnered with the authors of the study from Indraprastha Institute of Information Technology in Delhi to work on a free open-source Twitter Credibility Plugin, that could be used to get a credibility score for each incoming tweet. ""It's not a silver bullet, but they're getting pretty high, statistically significant results,"" he says, adding that because it works on AI, there will always be ways to game and sabotage it. Twitter Of course Twitter's role in the rise of digital humanitarianism cannot be understated, and it is central to gathering data ""for better or for worse"", says Meier. Other platforms don't elicit the same kind of information from people, and neither is the information they do collect particularly accessible. Facebook's terms of service mean it's harder to filter public updates, Foursquare isn't as international, or really used by people during disasters, and you can't access metadata for the pictures on Instagram unless they are also posted as a geotagged tweet -- although even that doesn't guarantee accuracy. Twitter itself clearly recognises that people turn to the platform during emergencies, as the same day that the MicroMappers tool was deployed for use in the Pakistan, it launched Twitter Alerts. This feature allows Twitter users to sign up to receive notifications from a list of credible partner organisations in case of an emergency. The introduction of Alerts, Meier says, is ""clearly a big plus"", but in the future he hopes users might be able to opt in to receive alerts that are ""geo-customised"" to their location at the time. Of course Twitter is not as equally useful in all situations, as certain parts of the world do have a more limited social media footprint than in, for example, the Philippines, where there was an abundance of relevant tweets. This is an issue that the Digital Humanitarian Network has just come up against in the remote region of Pakistan, where it has been testing the MicroMapper Clickers. Not many relevant tweets were discovered, whereas quite a lot of photos and videos put on the web by pro journalists were relevant. As a result, Meier has adjusted the MicroMapper algorithm to gather images and videos from relevant news articles that have been tweeted, and in future these too will be uploaded to the Clickers too. Hurricane Sandy and implementing digital tools in disaster response The Red Cross, however, has found tools like Facebook quite useful in helping it to to implement its digital response in emergencies. After Haiti the communications team examined the kind of information that had been sent its way and worked out how it could best prepare itself and others within the organisation to respond to it in future.  ""We started looking at what that meant for the services we provided on a regular basis, for the disaster situations, going forward knowing that kind of information would be coming in and knowing that this is the way people would talk to us now as an organisation. This is how they reach out to the Red Cross and they expect a response, they expect us to be able to do something about it,"" says Huang. By the time Hurricane Sandy struck in 2012, the Red Cross had an established digital operations centre kitted out by Dell in its Washington HQ, and had already started working with disaster services. During the hurricane, the team pulled in two million social media posts and were able to review, sort and capture about 3,000 for analysis. This filtered down to about 88 posts which were sent out to mass care teams, resulting in some changes on the ground -- an ambulance or supply truck being sent out somewhere, for instance. ""If you go back to where we were with Haiti, we couldn't do anything with the information, we were just shooting in the dark. But with Sandy we managed to, for the first time, at least put a little bit of a structure around it. It absolutely wasn't optimised or efficient necessarily, but it was our first experiment and our first foray into it and we got a lot of feedback."" Similarly, the communications team trained staff and volunteers to converse with people using social media, so that when people reached out to them during a disaster they were able to engage with in similar ways over Facebook and Twitter as they would usually do in real-life disaster shelters. Their interactions usually involve giving emotional support when people are stressed, giving them advice about how to take care of themselves or giving them hotline numbers to call. ""Our mission is the same as it's ever been: to provide hope and comfort during a disaster, to help people to be resilient, be prepared, to respond to an emergency and to recover well from it. What has changed are the ways we can carry out our mission. We've diversified a little bit by being able to reach out to people in an online and to enable volunteers to help purely in a virtual space and we weren't able to do that before."" Who are the digital humanitarian volunteers? The volunteers who dedicate their spare time to the American Red Cross, the SBTF and the DHN are  a mixed bunch-- academics, students, translators, journalists, people in the diaspora -- who are as vital to digital humanitarian response as the technology they use. Around 60 percent of SBTF members already work in the tech or humanitarian fields, but the idea behind many of the tools developed by Meier is to make it possible for anyone to get involved. Often in high-profile emergencies, says Verity, lots of people want to help out, whereas when disasters strike that perhaps don't receive much media attention or have a smaller diaspora, the number of volunteers is much lower. The result of this though is that the group will often be engaged because they'll have a vested interest in that region. The key to mobilising them and keeping them engaged is one of the key issues facing Verity and Meier, which is why the DHN created a guidance document to outline how to best recruit, train and mobilise. An important factor to retaining volunteers, says Verity, is also to show them, and others, when they've made a difference. ""If they made an impact you want to make that public, you want things coming out from the UN Twitter handles, you want stuff going out on blog posts, you want media or somebody to pick it up, because you want the volunteers to see that the organisation respected and appreciated the work that they did."" The decentralisation of disaster response There have even been occasions where the UN has been completely unable to help at all, and much of the support has relied completely on the volunteers. Verity points to the floods in Northern India in June this year, when the Indian government forbade OCHA from responding and providing assistance. This meant local NGOs were saddled with many of the geolocation coordination responsibilities that usually would fall to the UN. They couldn't turn to OCHA, but they could turn to the DHN, who ended up providing some of those services instead. ""What I find interesting is with new technologies, the big question is, is it reducing those barriers to entry into the humanitarian space, where other organisations, especially those augmented with a lot of virtual support, can start taking on roles that were traditionally mandated to other groups?"" says Verity. There is certainly a move towards decentralising disaster response, whether that be geographically, or in terms of organisations and agencies. For the Red Cross, says Huang, the idea is to be able to train local chapters so not everything has to be processed through Washington DC. ""We want to be able to build this capacity across the country, so that next time a disaster happens, the person whose backyard the disaster has struck, that person can mobilise their own digital volunteers, and be able to respond and engage with their community on the level that we've managed to do here on the national level."" The key to this on a technological level appears to be free and open-source software. AIDR, for example, or the Red Cross First Aid app, which can be taken by other countries and adapted to incorporate emergency information purely suited their needs. Similarly, making the tools simple enough to use that anyone can get involved in volunteering and empowering disaster-affected communities to help themselves has been one of the driving forces behind Meier's work. ""That's why I wanted to bring MicroMappers in -- to democratise digital humanitarian volunteering, to lower the barriers to entry, so anybody who knows how to get online and use a mouse and speaks English or another language, can actually be a digital humanitarian volunteer,"" he says. ""There's been this backlash in the digital activism space about clicktivism, or even slacktivism, well I'm saying no, wait, if you can click 'like' on a picture on Facebook, then you can be a digital humanitarian. I want clicktivism. Come here if all you want to do is click on things and still be actively operationally supporting disaster response."" This is a different message to the one we've been hearing for years whenever disasters strike -- that because we lack the specialist skillset we can't actually ""do"" anything to make a difference, beyond donating and fundraising. Organisations like OCHA and the Red Cross continue to play as important a role as ever during disasters, but technology is spreading the burden of responsibility. Maybe one day artificial intelligence will be so advanced that once again there will be no need for unskilled volunteers, but for now human computing is a huge part of coordinating humanitarian response, and the good news for the agencies and for us is that any human with access to a computer can get involved. If you're interested in volunteering, take a look at the Standby Volunteer Task Force and the Digital Humanitarian Network for opportunities. Meier is also actively seeking volunteers to help test out MicroMappers, which is as simple as looking at pictures and pressing buttons."
772,https://www.wired.com/2013/09/tech-time-warp-shakey-robot/,Wired,2013,9,27,841.0," In November 1963 – a half century before the arrival of Google's robocars and Amazon's Kiva factory minions – Charles Rosen dreamed up the world's first mobile ""automaton."" Rosen, a researcher at the Stanford Research Institute in Menlo Park, California, envisioned a roboperson driven by neural networks, algorithms that mimic the human brain. Much like its biological counterparts, it would have the power to see and sense its environment. As Rosen and his team wrote in a memo (.pdf) to DARPA, the Defense Department's research arm, describing the project, it would ""perform reconnaissance missions"" that would normally require human intelligence. DARPA eventually ""got kind of excited about it,"" recalls Nils Nilsson, one of the leaders of the project, and the agency granted the researchers $750,000 – more than $5 million in today's money – to make it happen. The project didn't include true neutral networks – in the 1960s, the technology just wasn't up to the sort of visual analysis, planning, and navigation Rosen and team wanted to explore – but the automaton did indeed happen. And it could see and move and respond to its environment in at least some basic ways. It looked kinda like the one-eyed, four-wheeled, armless spawn of Wall-E and Rosie the Robot. His name was Shakey, because, well, he shook a lot as he rolled around. Shakey was a seminal creation in the world of artificial intelligence, a big step along the road to everything from natural language processing to computer games. And he was an awful lot of fun. A radio antenna sat atop Shakey's head, wirelessly connecting his body to his remote brain, a room-sized Digital Equipment Corporation PDP-10 mainframe computer. The PDP-10 housed most of the code used to control Shakey, including the visual processing software that analyzed data gathered by the single television camera attached to his elongated head and his ""cat whisker"" sensors – long, wiry extensions located near his base that let Shakey know if he bumped into something. If he ran into something small and light enough, he could even push it aside with a kind of mechanical arm. ""Our goal is to give Shakey some of the abilities associated with intelligence, abilities like planning and learning,"" we heard in the official Shakey video below. ""The main purpose of our research is to learn how to design these programs so that robots can be employed in a variety of tasks, ranging from space exploration to industrial automation."" Shakey would never make it to the moon or even into a factory. But considering he had access to less computational power than your iPhone, he was pretty adept at dealing with unpredictable circumstances as he rolled his way from place to place. He uses a three-tiered software architecture that divided his behavior into low-, intermediate- and high-level actions. Low-level actions were analogous to reflexes – simple movements like moving forward or moving his camera eye. Executed in tandem, these could then deliver intermediate-level actions, such as pushing an object across a surface, and thanks to Shakey's planning system, called STRIPS, the intermediate actions could help complete higher-level tasks, like getting from one room to another while dealing with unexpected obstacles. ""If Charlie the Gremlin came and did something upsetting, STRIPS could cook up a new plan,"" says Nilsson. ""It was a really complex program for its day."" Charlie the Gremlin would be the alter ego of Charles Rosen. In the official video, you can see him and his flowing cape in all their devious glory. >""It really stands as a milestone in the evolution of robotics, artificial intelligence and machine learning."" Shakey could navigate his simple, controlled environment thanks to a dynamic map stored on the PDP-10. If Shakey lost his bearings, he could scan the room with his TV-camera eye and, through some simple geometry, figure out where he was. ""It was the first mobile, intelligent robot,"" said Peter Hart, who worked on Shakey from the start of the project. ""It really stands as a milestone in the evolution of robotics, artificial intelligence and machine learning. It seems almost quaint by modern standards, but it really was the first of an era."" The Shakey project came to a close in 1972 when DARPA funding dried up. But Shakey's legacy lives on. He inspired a generation of computer scientists and engineers to ""take mobile intelligent robots seriously,"" says Hart. Some techniques developed for Shakey, including tech behind its A* navigation and STRIPS programs, still play a role in today's natural language processing tools, computer games, and other applications. You can find them in route-finding applications such as Google Maps, car navigation systems, and even the Curiosity rover now on Mars. These techniques have also played a role in planning experiments on the Hubble Space Telescope, says Nilsson, who happened to be visiting Hart, an old friend and Shakey colleague, when he spoke to WIRED. Shakey currently lives at the Computer History Museum in Mountain View, California. He was inducted to the Robot Hall of Fame, together with C-3PO, in 2004."
773,https://www.wired.com/2013/07/ai-apps-trend/,Wired,2013,7,16,1330.0," Heath Whaley was running late to the airport, as usual. Zipping through nudie scanners at airport security with minutes to spare, he made a quick stop at the restroom, and began searching in his bag for his boarding pass to check his flight departure info and gate number. But it was nowhere to be found. Vanished. ""I couldn't remember what flight number I had, and didn't have time to dig through emails to find the flight number,"" Whaley recalls. He pulled out his iPhone and tapped on Tempo, a smart calendar app he uses. After you grant Tempo access to your email and calendars, the app searches for all the tidbits of schedule-related information you have stored in your accounts, gathering it together and presenting it cleanly inside individual calendar events. All Whaley had to do was tap the entry for the day's travels, and the flight numbers and gate information were right there, saving him from potentially missing his flight. Tempo is but one of a growing number of ""smart"" apps that use artificial intelligence algorithms in order to give you a more efficient and more personalized mobile experience. For those of us like Whaley, whose lives are generally a bit chaotic and disorganized, an app like Tempo can be a life-saver -- as long as we're willing to sacrifice some of our privacy. You see, in order for an app to gain the intelligence required to become a sort of personal assistant, it needs to know a lot about you. Who your friends are, what's on your schedule, where you work, where you live, and what subjects interest you the most. You have to hand over a few passwords and fill in some private details. But once you've relinquished that data, you can get a vastly richer and more valuable experience from your consumer device. It's almost like your phone becomes sentient. And -- the hallmark of any AI-based app -- the more you use it, the better it gets. Take Tempo, for example. For an event like “Meeting with Robert at Mexico Au Parc,” the app would bundle together Robert's contact information, any email correspondence about the meeting and any attached documents, the location information for the restaurant Mexico Au Parc, and directions on how to get there. It would package all of this into one entry within the app, along with buttons to easily send a message or email Robert if you wind up running late to the meeting. The benefits are clear. With a ""dumb"" calendar app, you’d need to scour multiple apps to get all that information -- the calendar, email, messages, contacts, maps and directions. Here, it’s all in one place. Tempo’s not alone in the smart calendar space. Other players include Sunrise and the latest entrant, Any.DO’s CAL app. But that’s just one segment of the AI app sphere. Google Now, built directly into newer Android devices and available to iOS users via the Google Search app, is a like-minded app that aggregates all sorts of information relevant to your needs and interests. It uses all the data accessible through your Google account and from the sensors in your smartphone (you give it permission to see both), and it provides all sorts of helpful guidance, showing you weather in your area, sports scores, package tracking info, breaking news stories, and traffic alerts for your daily commute. Everything is presented cleanly is a colorful interface that resembles a stack of cards. Google Now can also surface travel information, like boarding passes, and it can help you find hotel reservations, calculate foreign currency transactions, and even help with on-the-spot language translations. AI is also prevalent in a number of event discovery and activity recommendations apps, like Weotta. This app tracks affinities between a user's interests, upcoming events, top places in the area, and the interests of their friends. It combines this information with other things, like natural language phrase extraction and user location, to deliver timely, relevant, and socially engaging suggestions of things to do. An app called Triposo uses machine learning techniques to help predict what you’d like to do when you’re traveling -- like a smart travel guide. “We're trying to approach travel guides from an algorithmic, Google-like perspective,” Triposo co-founder and COO Richard Osinga tells Wired. Triposo uses what it calls an ""opinion mining"" algorithm. The company analyzes the natural language used in online reviews to determine whether people who have posted about a particular place liked it, and what exactly they liked about it. This helps the app suggest places for very specific qualities -- like a restaurant that has spectacular Bolognese, or a hotel that is especially clean. It also uses the time of day, your GPS location, and local weather to suggest things to see and do while you’re traveling. That, paired with analysis of the behaviors and opinions of other users, lets Triposo figure out what activities people are most likely to be interested in at a certain time -- you’re probably not looking for a history museum at 2am in in Paris -- and how far they are willing to travel to do that. This means you can nix all the planning you’d normally stress about before a vacation, and be confident that you’ll find unique, interesting attractions no matter what part of town you’re wandering around. But the ability for the apps on our smartphones do this sort of AI and on-the-fly processing wasn’t feasible even just a few years ago. According to Tempo CEO Raj Singh, there are four factors making today’s smart apps possible. First, storage and computation is cheaper than ever before. Lower server costs and cloud computing make including AI an affordable option for startups. There’s also greater openness and portability when it comes to data, thanks to sign-in technologies like OAuth and more open APIs for developers to use. Indeed, for Weotta, significant enhancements to MongoDB, Redis, and Facebook Open Graph are what make it possible for the app's developers to come up with increasingly better recommendations at a fraction of the computational overhead it would have taken in the past. Another important factor: Many app users have accepted the tradeoff that they can get a better user experience in exchange for a little less privacy. The idea of ""handing over the keys"" isn't as scary -- a few years ago, linking your Facebook, Gmail, Twitter, and LinkedIn accounts to a single app would have been unheard of. Lastly, the improved mobile ecosystem in general makes AI apps possible. That is, the numerous (and more advanced) sensors embedded in each device paired with a super-fast, persistent 4G LTE or Wi-Fi data connection allow apps to gather a wealth of information and ping their own servers, pushing relevant information to your phone's screen in a snap. But this is just the beginning of artificial intelligence infiltrating our mobile lifestyle. It can also be used to enhance other experiences in the real world. Bay area startup Anki, which debuted with a small amount of fanfare at WWDC in June, has shown us a glimpse of this future: How machine learning algorithms can go beyond just a computing device to actually power and give personality to toys. In Anki's case, the company has made toy race cars that know when to pass opponents, when to speed up, and when to hit the brakes to avoid crashing out of a race. As for today’s apps, the AI-based experiences aren’t perfect yet. If you’ve got multiple contacts with the same name, Tempo may initially get confused with which details to pull up. Or if your friends are interested in sports and you’re not, you may have to toss a bunch of recommendations in Weotta until it learns you don’t care about going to the next baseball game. But by and large, AI apps are blossoming. And with their growth, the stress and time involved in planning events, travel, and even your day-to-day errands are diminishing."
774,https://www.wired.com/2013/07/software-is-still-king-hardware-is-just-coming-along-for-the-ride/,Wired,2013,7,8,2181.0," Sometimes, inspiration comes from the most mundane experiences -- late-night phone calls, commuting, or in the case of Dropcam, dogs pooping on your lawn. The father of co-founder Greg Duffy had been trying to figure out whose neighborhood pooch had been leaving behind unwanted gifts on his yard. He set up a bunch of cameras connected to a computer to catch the offending pooch, but the hardware and software were not very user friendly and often failed at the most inopportune moments. Despite being a software engineer, his dad couldn't get it to work. ""He had really terrible experiences with these products. That hardware was really out of date. The software was virtually non-existent. They were just really arcane and required wizardry most people just do not have,"" Duffy said. So Duffy and his partner, Aamir Virani, thought they would start a cloud service to make all those issues disappear. If they got it to work, they might even make some cash along the way. Wired caught up with Duffy to talk about the beginnings of Dropcam, and how the model for the simple-to-use camera and service describes a new age of hardware startups. __Wired: __Do you feel there is a shift in attitude among Silicon Valley investors when it comes to hardware? __Greg Duffy: __Most investors are pattern matchers, right? The best ones have their own good personal theses about what’s going to succeed. But initially, most venture-capital firms wouldn’t even let us into the lobby. And the reason was this belief that ""Oh you have hardware and hardware is bad. No one invests in that."" But companies like Dropcam and Fitbit and Roku are really like software or content platforms built with a hardware component involved. [Investors] are starting to see the success of those models. Now all the pure pattern matching investors are kind of changing their tune a little bit about it. __Wired: __Are we seeing a renaissance of hardware then? __Greg Duffy: __It’s actually not a renaissance of hardware startups. This is kind of the next step in software eating the world. Software has come to revolutionize the hardware industry, and it so happens that hardware is coming along for the ride. Companies are solving problems that require a physical, electrical or mechanical component. Those companies weren’t getting started before because even just getting to your prototype was so expensive, so hard and so risky. The reason that this is starting to pop up more now is that it was pretty much impossible for most startups to go approach hardware as the heart of their service. The reason for that is you basically needed tons and tons and tons more funding. You spent millions of dollars developing hardware -- maybe even developing your own chips – [but] you had no way to test your product. What’s happening now is that customers actually are able test out a service without building any hardware. __Wired: __How can you do that? __Duffy: __We actually took off-the-shelf IP cameras, reverse engineered them and put our own software into them -- Linux-based software, just like how people hack their routers to run Linux. We built our software on top of the Linux system that would just seamlessly connect those cameras to our cloud. __Wired: __You were able to do that because there had been a standard established for these IP cameras and you could hack them? Duffy: Not everything was completely standard. The thing that was standard was that there was an ARM processor inside, just like there is an ARM processor inside most cellphones these days. We were able to hack it. It wasn’t designed to let you run your own firmware on it. That’s still a pretty hard thing to do. But since it was an ARM-based device we were able to go in and do the work to make the thing into a Dropcam. ARM processors are becoming cheaper. You still have specialized things you need to do for your particular use case – for Dropcam it was video encoding – but at the same time these components are becoming standardized. You do not have to design your own chips anymore. You also can focus most of your time on building your first prototype, on getting the rough form correct. You can use technologies like 3D printing for rapid prototyping. With all that, we were able to prove that people who would have never bought an IP camera were buying Dropcam, and installing them and doing it in minutes. __Wired: __What drew people to your product in the first place? __Duffy: __ We wanted to provide a video monitoring service for the home and small business. We’re not just about security. We’re about giving you this service of being able to be in a place no matter where you are, and being able to keep an eye on the people you care about. That video service happens to require a video camera. These are things you might be able to hack a smartphone to do, but they’re not complete solutions. Customers don’t want 70 percent solutions. They want 100 percent solutions. If you build the hardware along with the service, you get this mass adoption because you’re providing the ideal solution to the customer. Wired: The cameras that you guys were using were from a Swedish company? __Duffy: __That’s right. They’re called AXIS. __Wired: __At some point, they wanted to partner with you? __Wired: __Why did you part ways? __Duffy: __Once we switched over to a direct partnership we quickly became their largest seller for that type of camera. We sold quite a few of those things, especially given that the price was $279. We had to figure out how we were going to make a better-priced camera with more consumer-level features, with more of today’s cellphone technologies. We wanted to do things like night-vision. People had been asking for it. And we wanted to do it for $149 not $279. That really scared them. I said, 'Guys, I’m going to go build my own camera."" They basically said, 'Good luck.' They didn’t think that we could do it. Building a prototype these days is actually what has become easy. Building a prototype today can be done even on your own for certain types of devices. For a camera it’s a little more complicated because you need lenses, camera sensors, encoders and all these other things, but it's still relatively easy and an order of magnitude cheaper than it used to be. Wired: Do you have any idea what it would have cost in the old days to do it, even if it was possible? Duffy: It’s gone from millions -- maybe $10 million to maybe hundreds of thousands or less. Easily an order of magnitude. That’s why you see some of these Kickstarter projects, and why it’s a little bit naïve. Some Kickstarter projects say I want to raise $100,000 to build this electronic device. That’s basically what it will cost to get a prototype done. What they are underestimating is the cost of going to mass production. Mass production is still quite hard. That’s where you start getting into quality assurance issues. Building something the same a hundred thousand or a million times is actually pretty difficult, and requires at least team of people to be working directly with a factory and getting support from that factory to get big company-level quality assurance and repeatability. __Wired: __How did you get that support? There is a funny story there. We offer free service to everybody for live viewing, it includes apps and alerts and all the other things that we do on a live plan when you buy a Dropcam. Forty percent of people actually purchase our DVR plan (which records and saves your video events to the cloud). Around the time we decided to start cutting our ties with Axis, we actually got introduced to Sameer Gandhi at Accel Partners. Sameer came in and saw that it was clearly early days. We were seven guys in an office, kind of thing, literally packing up and shipping the cameras ourselves. But we had proven that we had more of a mass-market consumer product idea here, and that the service was really the biggest part of that. He saw through all of the jankiness, for lack of a better word. I told him, ""Look I think we can build our own hardware."" Normally hardware is a very scary word to venture capitalist. But I said, “I don’t think it’s that scary because we have a software service that’s actually doing quite well. We just want to develop this amazing hardware to go with our amazing service so people have every reason to buy us, instead of only one reason to buy us.” Hardware is sort of a vector into the system. You gotta get somebody excited about putting that camera up in their home before they’re potentially going to buy the service. __Wired: __How did you start building the company then? Duffy: After we raised our Series A we convinced this other great guy to join us, Doug Chan, who used to make all the Flip video cameras. We would never have been able to go to the level of mass production that Dropcam is at today if it hadn’t been for having somebody with a little bit of seasoning and good relationships with manufacturers in China. We gave (our prototype] to Doug and after months of hard work, we started shipping it. We sold out for more than a third of the year because of the demand we had the year we shipped. We didn’t predict high enough on the demand. We quintupled our revenue that year. The fact that Dropcam has to be good at cloud software, mobile apps, hardware and mass production actually gives a little bit of defensibility and makes us hard to follow comparatively to a company that just has a website. Dropcam is software-first, though we also need hardware to accomplish our goals. The hardware has to be great. It has to be amazing. Today it’s not enough to build a company that only does one of those things. You just have to be working on all fronts. Wired: Even Apple – as great as they are – suffers from that. __Duffy: __I agree. Apple just hasn’t been able to get the cloud aspect. The server platform stuff that Google does that we call the cloud is the same kind of stuff that Dropcam does. We take in more video that YouTube. An order of magnitude more. That gives you an idea of how big it actually is. __Wired: __Are you using any type of artificial intelligence? __Duffy: __We have a computer vision team at Dropcam that’s working on better motion detection so we can tell you more than just 'there’s something moving in front of your camera. There's a cat, for example.' The idea is to make Dropcam a little bit more human like by using the massive video data set you are generating for your own personal video to help the algorithm learn what you think is important. It allows us to run computer vision learning algorithms on the data set without compromising user privacy. We’ve been focusing more on building that framework so that we can then launch cool features that then give users more insight as to what is going on in their video without any Dropcam employee having to have access to their video. Nobody ever has to see the video, it's all computer vision. It’s quite an amazing system and something we consider to be core IP to Dropcam now. It’s going to be revolutionary. __Wired: __How does it work? __Duffy: __Computer vision is a branch of machine learning and artificial intelligence. The main way that you solve machine-learning problems is by having more data. Peter Norvig at Google is often quoted as saying that your algorithm doesn’t matter. You can have dumb algorithms. What wins every time is data. Dropcams are always documenting what’s going on. They automatically tell you where the important video is and make it easier by helping you manage what’s going on at home or at your small business. Wired: What are people using this for? __Duffy: __We thought it was going to be a security camera. It turned out to be an everything camera. We have people catch their kids’ first steps. We’ve had people catch burglars many, many times. Because the video is stored in the cloud, we’ve caught burglars who have even stolen the Dropcams, which is hilarious. We caught the bridge that collapsed up in Washington. Somebody randomly had a Dropcam pointing out their window. It was the only video of the incident. That was totally driven by the user. The [authorities] were able to use it to determine the cause of the bridge collapse. There have been cameras that recorded Hurricane Sandy. It’s really about a personal and global awareness of what’s going on, all in control of individual users. That’s really what Dropcam is about."
775,https://www.wired.com/2013/06/yoshua-bengio/,Wired,2013,6,27,1039.0," Yoshua Bengio recently had a vision -- a vision of how to build computers that learn like people do. It happened at an academic conference in May, and he was filled with excitement -- perhaps more so than he’d ever been during his decades-long career in ""deep learning,"" an emerging field of computer science that seeks to engineer machines that mimic how the human brain processes information. Or, rather, how we assume the brain processes information. In his hotel room, Bengio started furiously scribbling mathematical equations that captured his new ideas. Soon he was bouncing these ideas off various colleagues, including deep learning pioneer Yann LeCun of New York University. Judging from their response, Bengio knew he was onto something big. When he made it back to his laboratory at the University of Montreal -- home to one of the biggest concentrations of deep-learning researchers -- Bengio and his team went to work turning his equations into functional, intelligent algorithms. About a month later, that hotel-room vision morphed into what he believes is one of the most important breakthroughs of his career, one that could accelerate the quest for artificial intelligence. In short, Bengio has developed new ways for computers to learn without much input from us humans. Typically, machine learning requires ""labeled data"" -- information that's been categorized by real people. If you want a computer to learn what a cat looks like, you must first show it what a cat looks like. Bengio seeks to eliminate this step. ""Today’s models can be trained on huge quantities of data, but that’s not enough,"" says Bengio, who together with LeCun and Google’s Geoffrey Hinton is one of the original musketeers of deep learning. ""We need to discover learning algorithms that can take better advantage of all this unlabeled data that’s sitting out there."" Currently, the most widely used deep-learning models -- so called artificial neural networks harnessed by the likes of search giants Google and Baidu -- use a combination of labeled and unlabeled data to make sense of the world. But unlabeled information far outweighs the amount people have been able to manually label, and if deep learning is to turn the corner, it must tackle areas where labeled data is scarce, including language translation and image recognition. Bengio's new models -- which he’s tested only on small data sets -- can teach themselves to capture what he calls the statistical structure of the data. Basically, when a machine learns to recognize faces, it can spew out new images that look like faces too, without human intervention. It can provide answers, like when shown only part of an image it can guess the rest -- or when shown only some words in a sentence it can guess the missing ones. Right now, the models don't have a direct commercial application, but if they can perfect them, he says, then ""we can answer arbitrary questions about the variables modeled. Understanding the world means just that: We can have a good guess about any aspect of reality that is hidden to us, given those elements that we observe. That's why this is an important piece."" On the surface, these algorithms look very much like the neural nets built by Hinton for Google’s image search and photo-tagging systems, he says, but they’re much better at exploring data that's thrown at them. In other words, they’re much more intuitive. “Intuition is just the part of the computation going on in our brain for which we don’t have conscious access. It’s really hard to decompose it into little pieces we can explain,” he says. “This is the reason why the traditional AI of the 80s and 70s failed – because it tried to build machines that could explain every single step through reasoning. It turns out it was impossible to do that. It’s much easier to train machines to develop intuitions to make the right decisions.” In the world of machine learning, that’s a big deal. If Bengio’s initial findings hold up on larger data sets, they could lead to the development of algorithms that have better transfer, meaning they are more easily applied to all types of problems like natural language processing, voice recognition, and image recognition. Think of it like a previous experience you use to intuit what action you should take in a new situation. In engineering terms, the potential time saved on coding task-specific algorithms could be substantial. Unlike other machine-learning methods, deep learning is already endowed with some transfer, or intuitive, qualities, but Bengio and his team have been working towards making improvements for years. Recently, they won two international competitions focused on transfer learning. This resolve to iterate and improve on already existing technologies speaks to Bengio’s outlook on AI and, more broadly, on science. An academic through and through, he’s made it his life’s mission to find a fix for what’s holding back his and his colleagues’ dreams of building intelligent machines. “We do experiments whose goal is to figure out why…not necessarily to build something that we can sell tomorrow,” says Bengio. “Once you have that understanding, you can answer questions – you can do all sorts of useful things that are economically valuable.” That conviction, fueled by his own intuition that deep learning was the way to move machine learning forward even when it was a dirty concept, keeps him motivated and working with new students, post-docs and young professors to keep the AI dream alive. He draws inspiration from the myriad exchanges he's had with colleagues like LeCun, Hinton, and Jeff Dean of Google Brain fame. His career, he says, has really been a social endeavor. In that spirit, Bengio has put the code for his new algorithms on Github for other developers to tweak and improve, and details of the findings have been published in a series of papers on the academic researcher site arXiv.org. ""My vision is of algorithms that can make sense of all the kinds of data that we see, that can extract the kind of information in the world around us that humans have,"" Bengio says. ""I’m fairly confident that we’ll be able to train machines not just to perform tasks but to understand the world around us."""
776,https://www.wired.com/2013/06/andrew-ng/,Wired,2013,6,17,890.0," Andrew Ng wants to bring deep learning -- an emerging computer science field that seeks to mimic the human brain with hardware and software -- into the DIY era. Last year at Google he built a computerized brain that worked as a cat detector. It used a roughly 1-billion-connection network trained on 1,000 computers to teach itself how to spot cat videos on YouTube. While this worked well, Ng says, some researchers walked away thinking, ""If I don't have 1,000 computers, is there still any hope of my making progress on deep learning?"" The system cost roughly $1 million. “I was quite dismayed at this, particularly given that there are now a few other computer science research areas where a lot of the cutting-edge research is done only within giant companies,” he recalls. “Others simply don't have the resources to do similar work.” On Monday, he's publishing a paper that shows how to build the same type of system for just $20,000 using cheap, but powerful, graphics microprocessors, or GPUs. It's a sort of DIY cookbook on how to build a low-cost neural network. He hasn’t yet decided whether the code for the model will be open sourced, but the new paper gives enough detail for people with enough coding brawn to build their own faux brains. ""I hope that the ability to scale up using much less expensive hardware opens up another avenue for everyone around the world,"" he says. ""That's the reason I'm excited -- you can now build a 1-billion-connection model with $20,000 worth of hardware. It opens up the world for researchers to improve the performance of speech recognition and computer vision."" Down the line, this research on souped-up versions of neural networks running on GPUs could give rise to more powerful -- and financially lucrative -- GPU-based applications at large tech companies. Built by companies such as Nvidia and AMD, GPUs power the graphics card on your PC or video game console. But about a decade ago, computer scientists started to realize that they were also really good for doing certain types of mathematical calculations. “GPUs are so incredibly powerful,” says David Anderson, a computer scientist at Berkeley. “Programs that previously ran on supercomputers, we’re now realizing we can rewrite to run on GPUs at a fraction of the price.” His team at Berkeley recently rejigged the volunteer-parallel-computing platform, BOINC, to be able to run on GPUs. BOINC helps scientists analyze astronomical and biomedical data. Already universities and companies like Google, Shazam, Salesforce, Baidu and imgix are using these graphical chips to meet their ever-expanding computing needs to perform tasks as varied as voice recognition, quantum chemistry, and molecular modeling. For this new research, Ng's team also built a super-sized, 11-billion-connection version of the cat detector for roughly $100,000. He wants to build a high-performance computer that will allow researchers who don’t have the deep pockets of some of these companies and universities to do research on deep learning. It's a bit like what Apple and Microsoft did for personal computing or what cheaper sequencing hardware did for genomics. Both democratized technologies that were inaccessible to many. The Google Cat experiment ran on 1,000 computers with 16,000 CPUs. Ng’s group distributed their beefed-up, low-cost model, including the database of images on which it was trained, across 64 Nvidia GPUs on 16 computers and used special hardware to connect them in order to minimize the time required for these different modules to communicate with one another. Ng is excited about this progress, but he admits there’s still work to be done. The new model is not that much smarter – or faster – than the original cat detector even though its neural net has a whopping 11 billion connections, or 10 times as many as its predecessor. Plus, there are questions as to how easily Ng’s new model could be ported to other applications given that his group had to device specialized hardware and software to make it work. “The infrastructure seems to be particular to their specific unsupervised learning algorithm. The useful algorithms for training these networks, like the supervised algorithms that we use, and the one Google uses to train their photo-tagger are much harder to parallelize,” wrote NYU’s Yann LeCun, one of the pioneers of deep learning, in an email interview. There are also issues with using GPUs that need to be worked out. Although Google, is trailblazing into the GPU space, most large technology companies have not invested heavily in graphics chips because using them in the cloud can be complicated. CPUs are better at sharing computing resources and can switch easily between several jobs, but the technology to do that on GPUs is not yet mature, says Ng. Plus running jobs on GPUs also requires specialized code. “[GPUs] are simply being co-opted by machine learning and AI researchers for a different purpose. So it’s not exactly a natural fit,” wrote Bruno Olshausen, a computational neuroscientist and the director of the Redwood Center for Theoretical Neuroscience at the University of California, Berkeley, in an email. “If we really want to make progress in building intelligent machines, then we will need to direct our efforts to build new types of hardware that are specifically adapted for neural computation.” Olshausen is currently working on this problem as part of an ongoing multi-university research project."
777,https://www.wired.com/2013/05/neuro-artificial-intelligence/,Wired,2013,5,7,1749.0," There's a theory that human intelligence stems from a single algorithm. The idea arises from experiments suggesting that the portion of your brain dedicated to processing sound from your ears could also handle sight for your eyes. This is possible only while your brain is in the earliest stages of development, but it implies that the brain is -- at its core -- a general-purpose machine that can be tuned to specific tasks. About seven years ago, Stanford computer science professor Andrew Ng stumbled across this theory, and it changed the course of his career, reigniting a passion for artificial intelligence, or AI. ""For the first time in my life,"" Ng says, ""it made me feel like it might be possible to make some progress on a small part of the AI dream within our lifetime."" In the early days of artificial intelligence, Ng says, the prevailing opinion was that human intelligence derived from thousands of simple agents working in concert, what MIT's Marvin Minsky called ""The Society of Mind."" To achieve AI, engineers believed, they would have to build and combine thousands of individual computing modules. One agent, or algorithm, would mimic language. Another would handle speech. And so on. It seemed an insurmountable feat. When he was a kid, Andrew Ng dreamed of building machines that could think like people, but when he got to college and came face-to-face with the AI research of the day, he gave up. Later, as a professor, he would actively discourage his students from pursuing the same dream. But then he ran into the ""one algorithm"" hypothesis, popularized by Jeff Hawkins, an AI entrepreneur who'd dabbled in neuroscience research. And the dream returned. It was a shift that would change much more than Ng's career. Ng now leads a new field of computer science research known as Deep Learning, which seeks to build machines that can process data in much the same way the brain does, and this movement has extended well beyond academia, into big-name corporations like Google and Apple. In tandem with other researchers at Google, Ng is building one of the most ambitious artificial-intelligence systems to date, the so-called Google Brain. This movement seeks to meld computer science with neuroscience -- something that never quite happened in the world of artificial intelligence. ""I’ve seen a surprisingly large gulf between the engineers and the scientists,"" Ng says. Engineers wanted to build AI systems that just worked, he says, but scientists were still struggling to understand the intricacies of the brain. For a long time, neuroscience just didn’t have the information needed to help improve the intelligent machines engineers wanted to build. What's more, scientists often felt they ""owned"" the brain, so there was little collaboration with researchers in other fields, says Bruno Olshausen, a computational neuroscientist and the director of the Redwood Center for Theoretical Neuroscience at the University of California, Berkeley. The end result is that engineers started building AI systems that didn't necessarily mimic the way the brain operated. They focused on building pseudo-smart systems that turned out to be more like a Roomba vacuum cleaner than Rosie the robot maid from the Jetsons. But, now, thanks to Ng and others, this is starting to change. ""There is a sense from many places that whoever figures out how the brain computes will come up with the next generation of computers,"" says Dr. Thomas Insel, the director of the National Institute of Mental Health. What Is Deep Learning? Deep Learning is a first step in this new direction. Basically, it involves building neural networks -- networks that mimic the behavior of the human brain. Much like the brain, these multi-layered computer networks can gather information and react to it. They can build up an understanding of what objects look or sound like. In an effort to recreate human vision, for example, you might build a basic layer of artificial neurons that can detect simple things like the edges of a particular shape. The next layer could then piece together these edges to identify the larger shape, and then the shapes could be strung together to understand an object. The key here is that the software does all this on its own -- a big advantage over older AI models, which required engineers to massage the visual or auditory data so that it could be digested by the machine-learning algorithm. With Deep Learning, Ng says, you just give the system a lot of data ""so it can discover by itself what some of the concepts in the world are."" Last year, one of his algorithms taught itself to recognize cats after scanning millions of images on the internet. The algorithm didn't know the word ""cat"" -- Ng had to supply that -- but over time, it learned to identify the furry creatures we know as cats, all on its own. This approach is inspired by how scientists believe that humans learn. As babies, we watch our environments and start to understand the structure of objects we encounter, but until a parent tells us what it is, we can't put a name to it. No, Ng's deep learning algorithms aren't yet as accurate -- or as versatile -- as the human brain. But he says this will come. Andrew Ng is just part of a larger movement. In 2011, he launched the Deep Learning project at Google, and in recents months, the search giant has significantly expanded this effort, acquiring the artificial intelligence outfit founded by University of Toronto professor Geoffrey Hinton, widely known as the godfather of neural networks. Chinese search giant Baidu has opened its own research lab dedicated to deep learning, vowing to invest heavy resources in this area. And according to Ng, big tech companies like Microsoft and Qualcomm are looking to hire more computer scientists with expertise in neuroscience-inspired algorithms. Meanwhile, engineers in Japan are building artificial neural nets to control robots. And together with scientists from the European Union and Israel, neuroscientist Henry Markman is hoping to recreate a human brain inside a supercomputer, using data from thousands of real experiments. The rub is that we still don't completely understand how the brain works, but scientists are pushing forward in this as well. The Chinese are working on what they call the Brainnetdome, described as a new atlas of the brain, and in the U.S., the Era of Big Neuroscience is unfolding with ambitious, multidisciplinary projects like President Obama’s newly announced (and much criticized) Brain Research Through Advancing Innovative Neurotechnologies Initiative -- BRAIN for short. The BRAIN planning committee had its first meeting this past Sunday, with more meetings scheduled for this week. One its goals is the development of novel technologies that can map the brain's myriad circuits, and there are hints that the project will also focus on artificial intelligence. Half of the $100 million in federal funding allotted to this program will come from Darpa -- more than the amount coming from the National Institutes of Health -- and the Defense Department's research arm hopes the project will “inspire new information processing architectures or new computing approaches.” If we map how out how thousands of neurons are interconnected and ""how information is stored and processed in neural networks,"" engineers like Ng and Olshausen will have better idea of what their artificial brains should look like. The data could ultimately feed and improve Deep Learning algorithms underlying technologies like computer vision, language analysis, and the voice recognition tools offered on smartphones from the likes of Apple and Google. ""That’s where we’re going to start to learn about the tricks that biology uses. I think the key is that biology is hiding secrets well,"" says Berkeley computational neuroscientist aid Olshausen. “We just don’t have the right tools to grasp the complexity of what’s going on."" With the rise of mobile devices, cracking the neural code is more important than ever. As gadgets get smaller and smaller, we'll need new ways of making them faster and more accurate. As you shrink transistors -- the fundamental build blocks for our machines -- the more difficult it becomes to make them accurate and efficient. If you make them faster, for instance, that means it needs more current, and more current makes the system more noisy -- i.e. less precise. Right now, engineers design around these issues, says Olshausen, so they skimp on speed, size, or energy efficiency to make their systems work. But AI may provide a better answer. ""Instead of dodging the problem, what I think biology could tell us is just how to deal with it....The switches that biology is using are also inherently noisy, but biology has found a good way to adapt and live with that noise and exploit it,"" Olshausen says. ""If we could figure out how biology naturally deals with noisy computing elements, it would lead to a completely different model of computation."" But scientists aren't just aiming for smaller. They're trying to build machines that do things computer have never done before. No matter how sophisticated algorithms are, today's machines can’t fetch your groceries or pick out a purse or a dress you might like. That requires a more advanced breed of image intelligence and an ability to store and recall pertinent information in a way that’s reminiscent of human attention and memory. If you can do that, the possibilities are almost endless. “Everybody recognizes that if you could solve these problems, it’s going to open up a vast, vast potential of commercial value,” Olshausen predicts. That financial promise is why tech giants like Google, IBM, Microsoft, Apple, Chinese search giant Baidu and others are in an arms race to develop the best machine learning technologies. NYU's Yann LeCun, an expert in the field, expects that in the next two years, we'll see surge in Deep Learning startups, and many will be snatched up by larger outfits. But even the best engineers aren't brain experts, so having more neuro-knowledge handy is important. ""We need to really work more closely with neuroscientists,"" says Baidu's Yu, who is toying with the idea of hiring one. ""We are already doing that, but we need to do more."" Ng's dream is on the way to reality. ""It gives me hope –- no, more than hope –- that we might be able to do this,"" he says. ""We clearly don’t have the right algorithms yet. It’s going to take decades. This is not going to be an easy one, but I think there’s hope."""
778,https://www.wired.com/2013/04/robot-baseball/,Wired,2013,4,26,418.0," If you’ve been to the RoboGames, you’ve seen everything from flame-throwing battlebots to androids that play soccer. But robo-athletes are more than just performers. They’re a path to the future. Researchers at the University of Electro-Communications in Tokyo and the Okinawa Institute of Science and Technology have built a small humanoid robot that plays baseball — or something like it. The bot can hold a fan-like bat and take swings at flying plastic balls, and though it may miss at first, it can learn with each new pitch and adjust its swing accordingly. Eventually, it will make contact. The robot, you see, is also equipped with an artificial brain. Based on an Nvida graphics processor, or GPU, kinda like the one that renders images on your desktop or laptop, this brain mimics the function of about 100,000 neurons, and using a software platform developed by Nvidia, the scientists have programmed these neurons for the task at hand, as they discussed in a recent paper published in the journal Neural Networks. Yes, it’s fun. But through this baseball-playing robot, the scientists also hope to better understand how brains can be recreated with software and hardware — and bring us closer to a world where robots can handle more important tasks on our behalf. When a ball is pitched to the robot, an accelerometer at the back of a batting cage records information about the flight of the ball, including its speed, and this data is relayed back to a machine that holds the GPU-powered brain. The brain then crunches this data so that it can determine exactly when the robot should swing. If the scientists change the pitch speed, the robot will relearn the task all over again. This is not the first time researchers have modeled a cerebellum to control robots. A team of scientists in Europe, for instance, have used an artificial cerebellum to control a robotic limb. But according to Tadashi Yamazaki, one of the scientists who worked on the project, the baseball-playing robot is the second largest model of its kind and it runs in realtime, meaning its much faster than other systems. That means the GPU brain is better suited to controlling external hardware, he says. Just as he did with a previous artificial brain model, Yamazaki plans to release the source code for the system. The whole idea is to push this area of research forward. “Working code helps other scientists to learn how to implement an articial brain in computers,” he says."
779,https://www.wired.com/2013/04/kurzweil-google-ai/,Wired,2013,4,25,1669.0," Google has always been an artificial intelligence company, so it really shouldn’t have been a surprise that Ray Kurzweil, one of the leading scientists in the field, joined the search giant late last year. Nonetheless, the hiring raised some eyebrows, since Kurzweil is perhaps the most prominent proselytizer of “hard AI,” which argues that it is possible to create consciousness in an artificial being. Add to this Google’s revelation that it is using techniques of deep learning to produce an artificial brain, and a subsequent hiring of the godfather of computer neural nets Geoffrey Hinton, and it would seem that Google is becoming the most daring developer of AI, a fact that some may consider thrilling and others deeply unsettling. Or both. On Tuesday, Kurzweil moderated a live Google hangout tied to a release of the upcoming Will Smith film, After Earth, presumably tying the film’s futuristic concept to actual futurists. The discussion touched on the necessity of space travel and the imminent resolution of the world’s energy problems with solar power. After the hangout, Kurzweil got on the phone with me to explore a few issues in more detail. RAY KURZWEIL: Science fiction is the great opportunity to speculate on what could happen. It does give me, as a futurist, scenarios. It’s not incumbent upon science fiction creators to be realistic about time frames and so on. In this movie, for example, the characters come back to Earth a thousand years later and biological evolution has moved so far that the animals are quite different. That’s not realistic. Also, there’s very often a dystopian bent to science fiction because we can perceive the dangers of science more than the benefits, and maybe that makes more dramatic storytelling. A lot of movies about artificial intelligence envision that AI’s will be very intelligent but missing some key emotional qualities of humans and therefore turn out to be very dangerous. What’s the key to predicting the future? You anticipated search engines? But did you predict that you would be working for a company that started as a search engine? That’s exactly the kind of thing you can’t predict. It would be very hard to predict that these couple of kids at Stanford would take over the world of search. But what I did discover is that if you examine the key measures of price performance and capacity of information technology, they form amazingly predictable smooth exponential curves. The price performance of computation has been rising in a very smooth exponential since the 1890 census. This has gone on through thick and thin, through war and peace, and nothing has affected it. I projected it out to 2050. In 2013, we’re exactly where we should be on that curve. What are you working on at Google? My mission at Google is to develop natural language understanding with a team and in collaboration with other researchers at Google. Search has moved beyond just finding keywords, but it still doesn’t read all these billions of web pages and book pages for semantic content. If you write a blog post, you’ve got something to say, you’re not just creating words and synonyms. We’d like the computers to actually pick up on that semantic meaning. If that happens, and I believe that it’s feasible, people could ask more complex questions. Are you participating in Jeff Dean’s program there to build an artificial ""Google Brain?"" Well, Jeff Dean is one of my collaborators. He’s a fellow research leader. We are going be using his systems and his techniques of deep learning. The reason I’m at Google is resources like that. Also the knowledge graph and very advanced syntactic parsing and a lot of advanced technologies that I really need for a project that really seeks to understand natural language. I can succeed at this much more readily at Google because of these technologies. If your system really understood complex natural language, would you argue that it’s conscious? Well, I do. I’ve had a consistent date of 2029 for that vision. And that doesn’t just mean logical intelligence. It means emotional intelligence, being funny, getting the joke, being sexy, being loving, understanding human emotion. That’s actually the most complex thing we do. That is what separates computers and humans today. I believe that gap will close by 2029. Will we get there simply by more computation and better software, or are there currently unsolved barriers that we have to hurdle? There are both hardware and software requirements. I believe we actually are very close to having the requisite software techniques. Partly this is being assisted by understanding how the human brain works, and we’re making exponential gains there. We can now see inside a living brain and see individual inter-neural connections being formed and firing in real time. We can see your brain create your thoughts and thoughts create your brain. A lot of this research reveals how the mechanism of the neocortex works, which is where we do our thinking. This provides biologically inspired methods that we can emulate in our computers. We’re already doing that. The deep learning technique that I mentioned uses multilayered neural nets that are inspired by how the brain works. Using these biologically inspired models, plus all of the research that’s been done over the decades in artificial intelligence, combined with exponentially expanding hardware, we will achieve human levels within two decades. Do we really understand at all why someone’s brain can result in such an unique expression of a human? Take the transcendent intelligence of Einstein, the creativity of Steve Jobs, or the focus of Larry Page. What made those people so special? Do you have insights into that? I examine that very question, in fact, with regard to Einstein specifically in my recent book, How to Create a Mind. Tell me. There are two things. First of all, we create our brain with our thoughts. We have a limited capacity in the neocortex, estimated to be about 300 million pattern recognizers, which are organized in a hierarchy. We create that hierarchy with our own thinking. I would not explain Einstein’s brilliance based on him having 350 million or 400 million. We have approximately the same capacity. But he organized his brain to think deeply about this one subject. He was interested in the violin, but he was no Jascha Heifetz. And Jascha Heifetz had an interest in physics, but he was no Einstein. We have a capacity to do world-class work in one field. That’s part of the limited capacity of the brain, and Einstein really devoted it to this one field. But lot of physicists are devoted to their one field, and only one became Einstein. I didn’t finish. The other aspect is courage to follow your own thought experiments and not fall off the horse because the conclusions are so different from your previous assumptions or the common belief of society. People are so unable to accept thinking different than their peers that they immediately drop their thought pattern when it leads to absurd conclusions. So there’s a certain courage to go with your convictions. Clearly Steve Jobs had that. He had a vision and carried it out. It’s that courage of your convictions. What’s the biological basis for that kind of courage? If you had an infinite ability to analyze a brain, could you say, “Oh, here’s where the courage is?” It is the neocortex, and people who fill up too much of their neocortex with concern about the approval of their peers are probably not going be the next Einstein or Steve Jobs. Is this something one can control? That’s a good question. I’ve been thinking about that and also why do some people readily accept the exponential growth of information technology and its implications, and other people are very resistant to it. I make the argument that hard-wired in our brain are linear expectations, because that worked very well 1000 years ago, tracking an animal in the wild. Some people, though, can readily accept the exponential perspective when you show them the evidence, and other people don’t. I’m trying to answer the question, what accounts for that? It really isn’t accomplishment level, intelligence, education level, socio-economic status. It cuts across all of those things. Some people’s neocortexes are organized so that they can accept the implications that they see in front of them without worrying too much about the opinion of others. Can we learn that? I would imagine yes, but I don’t have data to prove that. Since we’ve been talking about Steve Jobs, let me bring up one of his famous quotes, from his speech at Stanford. He said, “Death is very likely the single best invention of life. It’s life’s change agent.” You are very famously trying to extend your life indefinitely, so you reject that, right? Yes, This is what I call a deathist statement, part of a millennium-old rationalization of death as a good thing. It once seemed to make sense, because up until very recently you could not make a plausibly sound argument where life could be indefinitely extended. So religion, which emerged in prescientific times, did the next best thing, which is to say, ‘Oh, that tragic thing? That’s really a good thing.” We rationalized that because we did have to accept it. But in my mind death is a tragedy. Our initial reaction to hearing that someone has died is a profound loss of knowledge and skill and talents and relationships. It’s not the case that there are only a fixed number of positions, and if old people don’t die off, there’s no room for young people to come up with new ideas, because we’re constantly expanding knowledge. Larry Page and Sergey Brin didn’t displace anybody-- they created a whole new field. We see that constantly. Knowledge is growing exponentially. It’s doubling approximately every year. And you think that dramatically extended life is possible. I think we’re only 15 years away from a tipping point in longevity."
780,https://www.wired.com/2013/04/turing/,Wired,2013,4,16,244.0," ""Turing believes machines think. Turing lies with men. Therefore machines cannot think,"" the irreducible Alan Turing wrote in 1952, months into an estrogen program designed by the British government to castrate him and two years before taking his life by eating an apple he'd laced with cyanide. It wasn't until well after his death that Turing would be appreciated as a genius. He is hailed as a hero of World War II, helping to crack the unbreakable German code. He was a gay martyr, a man who lived with pride in an age that couldn't accommodate it. And he was a computing trailblazer. In one of his earliest papers, at 23, Turing suggested that logical paradoxes could be mathematically resolved with a ""universal machine"" whose language would be the 1s and 0s of Boolean algebra. This idea became the foundation of our computing age. Yet he is most famous for an idea that never really bore fruit: His ""imitation game,"" now called the Turing test, proposed that true artificial intelligence would be achieved only when computers had become indistinguishable from people. It's easy to imagine what Turing, a programmer bedeviled by social codes, might have seen in a program that acted human, but without prejudices and irrational hatred. Instead, the AI that Turing's paper foretold—GPS, search algorithms, stock trade software—acts nothing like people. AI may have become an inseparable part of our lives, but it has not redefined what it means to be human."
781,https://www.wired.com/2013/04/baidu-research-lab/,Wired,2013,4,12,785.0," It doesn't look like much. The brick office building sits next to a strip mall in Cupertino, California, about an hour south of San Francisco, and if you walk inside, you'll find a California state flag and a cardboard cutout of R2-D2 and plenty of Christmas decorations -- even though we're well into April. But there are big plans for this building. It's where Baidu -- ""the Google of China"" -- hopes to create the future. In late January, word arrived that the Chinese search giant was setting up a research lab dedicated to ""deep learning"" -- an emerging computer science field that seeks to mimic the human brain with hardware and software -- and as it turns out, this lab includes an operation here in Silicon Valley, not far from Apple headquarters, in addition to a facility back in China. The company just hired its first researcher in Cupertino, with plans to bring in several more by the end of the year. Baidu calls its lab The Institute of Deep Learning, or IDL. Much like Google and Apple and others, the company is exploring computer systems that can learn in much the same way people do. ""We have a really big dream of using deep learning to simulate the functionality, the power, the intelligence of the human brain,"" says Kai Yu, who leads Baidu’s speech- and image-recognition search team and just recently made the trip to Cupertino to hire that first researcher. ""We are making progress day by day."" If you want to compete with Google, it only makes sense to set up shop in Google's backyard. ""In Silicon Valley, you have access to a huge talent pool of really, really top engineers and scientists, and Google is enjoying that kind of advantage,"" Yu says. Baidu first opened its Cupertino office about a year ago, bringing in various other employees before its big move into deep learning. In the '90s and onto the 2000s, deep learning research was at a low ebb. The artificial intelligence community moved toward systems that solved problems by crunching massive amounts of data, rather than trying to build ""neural networks"" that mimicked the subtler aspects of the human brain. Google's search engine was a prime example of system that took a short-cut around deep learning, and the American search giant is using a similar approach with its self-driving cars. But now, deep learning research is coming back into favor, and Google is among those driving the field forward. Google recently hired Geoffrey Hinton, the Godfather of deep learning, after some prodding from Stanford’s Andrew Ng, another power-player in the field, and many other companies are exploring the same area. IBM has long worked towards a computer model of the human brain. Apple now uses deep learning techniques in the iPhone's Siri voice recognition system. And Google has worked similar concepts into its own voice recognition system as well as Google Street View. Still, Baidu's decision to build an entire research lab dedicated to deep learning ""is a bit of bold move,"" says New York University's Yann LeCun, a pioneer in the field, pointing out that the technology still has such a long way to go. But the IDL, he says, could be a way for Baidu to attract top talent and let creative engineers explore all sorts of blue-sky innovations -- stuff akin to Google Glass and other project gestated at Google's secretive X Lab. In fact, one of Yu's researchers is working on Baidu Eye, which many have called a Google Glass knock-off. But for now, Yu says, the lab's main priority is the exploration of deep learning algorithms. ""We want to be focused,"" he says. In November, Baidu released its first voice search service based on deep learning, and it claims the tool has reduced errors by about 30 percent. As Google and Apple have also seen, these improvements can change the way people interact with technology and how often they use it. When voice and image search services work like they're supposed to, we needn't fiddle with the teeny keyboards and small displays on mobile devices. Today, web searches for products or services give you little more than long list of links, and ""then it’s your job to read through all of those webpages to figure out what’s the meaning,"" Yu says. But he wants something that works very differently. “We need to fundamentally change the architecture of the whole system,"" he explains. That means building algorithms that can identify images and understand natural language and then parse the relationships between all the stuff on the web and find exactly what you're looking for. It other words, it wants algorithms that work like people. Only faster."
782,https://www.wired.com/2013/04/barcode-killer/,Wired,2013,4,4,271.0," Japanese bakeries are known for their myriad types of bread—sweet bean bun, tuna cheese bread, custard pastry—and lots of them look alike. So how do you train clerks to memorize the names and prices of more than a hundred varieties? If you’re a popular bakery chain like Donq, where employee turnover is high, you don’t. One of its Tokyo locations now uses a system that relies on a smart cash register capable of identifying treats by sight rather than depending on a flunky cashier who can’t. The solution wasn’t obvious right away. A pilot test at another Japanese bakery proved that offering more kinds of bread meant earning more dough, but employees couldn’t remember all the prices. So the shop tried wrapping each item in cellophane and slapping on a barcode sticker. Customers didn’t like their bread prepackaged, though, so sales dropped by two-thirds. “It’s impossible to put a barcode directly on the pastries,” says Minori Kambe of Brain Corporation, which spent four years developing the system at Donq. Called BakeryScan, it’s a camera perched above a backlit countertop. The software compares images of each snack to those of all the goods for sale—it can even correctly identify one that’s a bit overdone. Once it finds a match, it coughs up a price. Success rate: 98 percent. Sure, this seems like a lot of effort for bean buns, but the researchers behind the technology say the premise could work with other items too, like fresh produce and medicine. The team at Brain hopes to market BakeryScan stateside soon (for just $20,000 per machine). The ultimate goal: to replace the barcode entirely."
783,https://www.wired.com/story/predicting-artificial-intelligence/,Wired,2013,3,30,1100.0," In 1956, a bunch of the top brains in their field thought they could crack the challenge of artificial intelligence over a single hot New England summer. Almost 60 years later, the world is still waiting. The ""spectacularly wrong prediction"" of the Dartmouth Summer Research Project on Artificial Intelligence made Stuart Armstrong, research fellow at the Future of Humanity Institute at University of Oxford, start to think about why our predictions about AI are so inaccurate. The Dartmouth Conference had predicted that over two summer months ten of the brightest people of their generation would solve some of the key problems faced by AI developers, such as getting machines to use language, form abstract concepts and even improve themselves. If they had been right, we would have had AI back in 1957; today, the conference is mostly credited merely with having coined the term "" artificial intelligence"". Their failure is ""depressing"" and ""rather worrying"", says Armstrong. ""If you saw the prediction the rational thing would have been to believe it too. They had some of the smartest people of their time, a solid research programme, and sketches as to how to approach it and even ideas as to where the problems were."" Turing test, a machine has to demonstrate behaviour indistinguishable from that of a human being.) Later experts have suggested 2013, 2020 and 2029 as dates when a machine would pass the Turing test, which gives us a clue as to why Armstrong feels that such timeline predictions -- all 95 of them in the library -- are particularly worthless. ""There is nothing to connect a timeline prediction with previous knowledge as AIs have never appeared in the world before -- no one has ever built one -- and our only model is the human brain, which took hundreds of millions of years to evolve."" His research also suggests that predictions by philosophers are more accurate than those of sociologists or even computer scientists. ""We know very little about the final form an AI would take, so if they [the experts] are grounded in a specific approach they are likely to go wrong, while those on a meta level are very likely to be right"". Although, he adds, that is more a reflection of how bad the rest of the predictions are than the quality of the philosophers' contributions. Beyond that, he believes that AI predictions as a whole have all the ""characteristics of the kind of tasks that experts are going to be bad at predicting"". In particular it is the lack of feedback about the accuracy of predictions about AI that leads to what has been called the ""overconfidence of experts"", Armstrong argues. Such ""experts"" include scientists, futurologists and journalists. ""When experts get immediate feedback as to whether some prediction is right or wrong then they are going to get better at predicting. Without it, everyone is overconfident as they are making quite definite predictions on pretty much no evidence at all."" It is possible to make better predictions than what is basically just ""gut instinct"", he says, if you ""decompose the problem by saying we need this feature or that feature and then give estimates for each step"". Few experts bother to do this, he believes, ""as the problem is hard, it is not taken seriously, and perhaps they don't even realise you could do better by breaking it down. ""So in effect your [own] prediction or an algorithm's about AI is as good as an expert's."" Robin Hanson, however, is not so sure that we should discount expert opinion, as ""the more people focus very narrowly on the thing they know about then the more reliable their predictions will be"". Hanson is an associate professor at George Mason University and chief scientist at Consensus Point, a leading provider of prediction market technology based in Nashville, Tennessee. Too often, he says, experts are being expected by journalists only to comment on ""the quick Sunday supplement style stories"", meaning that they too ""are more outsiders rather than researchers because these are the not the topics they are really familiar with"". If you ask those actually working in the field of AI they will say that ""in the last twenty years they have seen progress of 5 percent to 10 percent towards the goal"" and that means ""without any acceleration it might take between 200 and 400 years to achieve the goal"". Some would even argue that progress towards achieving it is actually ""decelerating"". For Hanson one of the best ways to judge the accuracy of an expert is to look at the pundit's track record. ""Prefer people who have made a bet financially rather than just saying something. Don't just ask what will happen, ask them what has happened. Another way is to look at the futures market as the predictor. Although there isn't one for AI, Hanson suggests you ""look at the demand for computers and it gives you an idea of what's coming down the line and what people are putting their money"". Armstrong reckons it is easy to ""tell if a prediction is bad by comparing it with other similar predictions in the past, and if they have failed..."" Other than that, he suggests trying ""to take them apart, weaken them, show that they are wrong or irrelevant -- and if you can't, then it is a stronger prediction"". ""Watch out too,"" he says, ""for whether the prediction is about the behaviour of future AI rather than its inner nature. If it's about behaviour then it's a better prediction, as inner nature is a complex philosophical issue and you will never get feedback about whether it's right or wrong."" Also, the fewer assumptions a prediction makes the better, such as ""AI will be networked or have genetic algorithms"". If a prediction says ""specific things"" - that AI will emerge in this way or that way - then be wary for that prediction too. And what are Armstrong's predictions about the future of AI? ""My prediction is that [AI is] likely to happen sometime in the next five to 80 years. I would give a 90 percent chance [it will happen] in the next two centuries, although there is always the chance that someone could come up with an AI algorithm tomorrow."" And I guess that's what's wrong with more accurate predictions. Stuart Armstrong's, Kaj Sotala's and Seán Óh Éigeartaigh's paper on ""The errors, insights and lessons of famous AI predictions and what they mean for the future"" plus case studies is pending publication in the conference proceedings of the AGI12/AGI Impacts Winter Intelligence conference."
784,https://www.wired.com/2013/03/hakitzu-gaming-learning/,Wired,2013,3,26,782.0," It all started when Frank was a boy, frustrated by the lack of organization in the Meehan household. Being the nerdy kid that he was, he spent an entire summer creating a computer database to catalog all of the items in his house so that he could find stuff. Frank sure has come a long way since that summer— his latest venture is about to completely disrupt the educational app-based gaming industry. Frank Meehan, Founder and CEO of Kuato Studios, is already well known around Silicon Valley and beyond as part of Horizons Ventures and for serving on the boards of technology companies, including SRI—you know, that company responsible for Siri, the virtual assistant Apple purchased and incorporated into iOS. But Kuato Studios is Frank’s latest venture. He has built a magnificent team around Hakitzu, an epic, fun iPad game that also happens to teach players how to code basic JavaScript. In fact, you can’t do much in the game without typing commands into the console to carry out your next moves. It is very easy to get started and Hakitzu provides plenty of tips. Once you’ve done a few things in the game, you’ll be ready for more. Due for release in the Apple iTunes App store on Tuesday, March 26, 2013, Hakitzu is a unique turn-based robot battle game for the iPad2 and later models. Hakitzu’s stunning graphics and intense music and sound effects quickly draws kids and adults alike into the game. Getting started is simple. First, you join Game Center, invite a friend into the game, choose, customize and arm your robot warriors, (or CodeWalkers as they’re called in the game), and then you’re ready to play . . . err, code. Using built-in functions, players determine their strategy and then type their programmed moves into the console like: Touch the Execute button on the iPad screen and watch as your CodeWalker diligently carries out your commands. The game probably appeals to kids 11 and older, but my 10 year-old enjoyed Hakitzu tremendously, playing against Vicky, Hakitzu lead developer via Game Center on an iPad mini. Judging by my son’s initial reaction after successfully executing his first move, I knew he was hooked. “That was Awesome! It went exactly like I wanted it to. I didn’t expect it to work, but it did.” The thing that really differentiates Hakitzu from other educational games is that it is so much fun. Kids learn more about JavaScript than they realize because they’re focused on game strategy instead of straight coding. And that’s where Frank’s Dad comes in. Frank’s father, Anthony Meehan, teaches computer science and science at Carlingford High School in Sydney, Australia. Over the last several years, Anthony noticed that his students seemed increasingly detached from their educational applications, even on tablet devices. The educational apps just didn’t do a great job of making learning fun. Anthony pleaded with Frank to create something fun that might truly inspire his students to learn. So, Frank’s team set out to do just that. Based on feedback from middle and high school students who beta tested Hakitzu in the U.S., the game is going to be a big hit. Currently, Hakitzu is free and hosts only a handful of basic JavaScript coding options like calling various pre-built functions and providing some attributes. But Frank noted that as a player progresses, she can incrementally add to Hakitzu by purchasing inexpensive add-in modules. Hakitzu is clearly just getting started, though. Frank notes that as the team works through any bugs, they’ll quickly rollout versions for the iPhone, iPod Touch and then Android and other platforms. With the supplemental education market estimated to exceed $100 billion dollars by 2017, there’s a lot of room for Kuato Studios to grow, yet the most anticipated Hakitzu feature is not even available yet. Frank’s Kuato Studios team is developing an Artificial Intelligence (AI)-based Virtual Learning Companion with SRI International (the founders of Siri). This new platform promises natural language interaction with players and will even detect when a player is struggling and offer hints or advice on tactics or code. Frank estimates that this new feature will be available in about six months. In the mean time, the Kuato Studios team is committed to aggressively releasing Hakitzu updates every four weeks. Will Hakitzu transform players into JavaScript Jedi? Probably not, but JavaScript (and programming in general) will look a lot less intimidating after a few games of Hakitzu. Because the game is so immersive, the lines between gaming and learning truly blur. And when the AI feature comes online, learning will be even more personalized and cool. I can’t think of a better way to start learning JavaScript."
785,https://www.wired.com/story/ibm-watson-medical-doctor/,Wired,2013,2,11,870.0," IBM's Watson -- the language-fluent computer that beat the best human champions at a game of the US TV show Jeopardy! -- is being turned into a tool for medical diagnosis. Its ability to absorb and analyse vast quantities of data is, IBM claims, better than that of human doctors, and its deployment through the cloud could also reduce healthcare costs. The first stages of a planned wider deployment, IBM's business agreement with the Memorial Sloan-Kettering Cancer Center in New York and American private healthcare company Wellpoint will see Watson available for rent to any hospital or clinic that wants to get its opinion on matters relating to oncology. Not only that, but it'll suggest the most affordable way of paying for it in America's excessively-complex healthcare market. The hope is it will improve diagnoses while reducing their costs at the same time. Two years ago, IBM announced that Watson had ""learned"" the same amount of knowledge as the average second-year medical student. For the last year, IBM, Sloan-Kettering and Wellpoint have been working to teach Watson how to understand and accumulate complicated peer-reviewed medical knowledge relating to oncology. That's just lung, prostate and breast cancers to begin with, but with others to come in the next few years). Watson's ingestion of more than 600,000 pieces of medical evidence, more than two million pages from medical journals and the further ability to search through up to 1.5 million patient records for further information gives it a breadth of knowledge no human doctor can match. According to Sloan-Kettering, only around 20 percent of the knowledge that human doctors use when diagnosing patients and deciding on treatments relies on trial-based evidence. It would take at least 160 hours of reading a week just to keep up with new medical knowledge as it's published, let alone consider its relevance or apply it practically. Watson's ability to absorb this information faster than any human should, in theory, fix a flaw in the current healthcare model. Wellpoint's Samuel Nessbaum has claimed that, in tests, Watson's successful diagnosis rate for lung cancer is 90 percent, compared to 50 percent for human doctors. Sloan-Kettering's Dr Larry Norton said: ""What Watson is going to enable us to do is take that wisdom and put it in a way that people who don't have that much experience in any individual disease can have a wise counsellor at their side at all times and use the intelligence and wisdom of the most experienced people to help guide decisions."" The attraction for Wellpoint in all this is that Watson should also reduce budgetary waste -- it claims that 30 percent of the $2.3 trillion (£1.46 trillion) spent on healthcare in the United States each year is wasted. Watson here becomes a tool for what's known as ""utilisation management"" -- management-speak for ""working out how to do something the cheapest way possible"". Wellpoint's statement said: ""Natural language processing leverages unstructured data, such as text-based treatment requests. Eighty percent of the world's total data is unstructured, and using traditional computing to handle it would consume a great deal of time and resources in the utilisation management process. The project also takes an early step into cognitive systems by enabling Watson to co-evolve with treatment guidelines, policies and medical best practices. The system has the ability to improve iteratively as payers and providers use it."" In other words, Watson will get better the more it's used, both in working out how to cure people and how to cure them more cheaply. When Watson was first devised, it (or is it ""he""?) ran across several large machines at IBM's headquarters, but recently its physical size has been reduced hugely while its processing speed has been increase 240 percent. The idea now is that hospital, clinics and individual doctors can rent time with Watson over the cloud -- sending it information on a patient will, after seconds (or at most minutes), return a series of suggested treatment options. Crucially, a doctor can submit a query in standard English -- Watson can parse natural language, and doesn't rely on standardised inputs, giving it a more practical flexibility. Watson's previous claim to fame came from it winning a special game of US gameshow Jeopardy! in 2011. For those unfamiliar, Jeopardy!'s format works like this: the answers are revealed on the gameboard and the contestants must phrase their responses as questions. Thus, for the clue ""the ancient Lion of Nimrod went missing from this city's national museum in 2003"" the correct reply is ""what is Baghdad?"". Clues are often based on puns or other word tricks, and while it's not quite on the level of a cryptic crossword, it's certainly the kind of linguistic challenge that would fox most language-literate computers. Watson's ability to parse texts and grasp the underlying rules has had its drawbacks, though, as revealed last month when IBM research scientist Eric Brown admitted that he had tried giving Watson the Urban Dictionary as a dataset. While Watson was able to understand some of the, er, colourful slang that fills the site's pages, it also failed to understand the different between polite and offensive speech. Watson's memory of the Urban Dictionary had to (regrettably) be wiped."
786,https://www.wired.com/2012/08/kinect-home-computer-vision/,Wired,2012,8,28,917.0," Seeking a way to crowdsource better computer vision, roboticists have launched a website that allows users to record pieces of their environments in 3-D with a Kinect camera. Called Kinect@Home, the open source and browser-based effort remains in its infancy. Users have uploaded only a few dozen models of their living room couches, kitchen countertops and themselves. Should the project catch on, however, researchers may be on the cusp of an unprecedented way to amass 3-D data to improve navigation and object-recognition algorithms that allow robots to cruise and manipulate indoor environments. ""For robots to work in everyday space and homes, we need lots of 3-D data. Big data is where it's at, as Google understands with its efforts,"" said roboticist Alper Aydemir of the Royal Institute of Technology in Sweden. ""But no one has been able to do this efficiently yet [with 3-D data]."" With the advent of Microsoft's low-cost yet highly effective 3-D camera system, called Kinect, and sanctioned ways to hack the device, computer vision research is experiencing a revolution. ""I think we've developed a win-win situation,"" said Aydemir, who leads the Kinect@Home effort. ""Users get access to 3-D models they can embed anywhere on the internet, and we use this data to create better computer vision algorithms."" Populations are growing older, health insurance costs are rising and care systems are increasingly stretched, so autonomous robots offer a dreamy vision of the future for many people. The trouble is that most automatons can only bumble through crowded human environments. Incorporating building blueprints into navigation algorithms pushes them only so far because such plans lack couches, tables, dogs and other oddities that people cram into indoor spaces. What's more, helper robots are only useful if they can recognize and interact with a dizzying variety of objects. Some crowdsourced schemes use Amazon Mechanical Turk to categorize objects in 2-D images acquired by robots, but these images don't inform any item's 3-D shape or behavior. Helper robots must be able to distinguish a refrigerator from an oven, for example, and open these labyrinthine 3-D objects to cook a casserole or deliver a cold beer to beckoning human owners. ""If you can get real-world 3-D data for 5,000 refrigerators, you can develop an algorithm to generalize a refrigerator and then test a robot's ability to generalize them,"" Aydemir said. In hopes of gathering these and other data that define human environments, Aydemir created Kinect@Home. Users install a plugin, attach their Kinect to a computer and start recording whatever they please. ""I think making 3-D models should be as easy as making a YouTube clip,"" said Aydemir, who recognizes the ambition in his new enterprise. ""The long-term vision is that experiencing 3-D places should be easy, whether you're trying to sell a couch or seeking advice to remodel a kitchen."" Kinect@Home's 3-D models are by no means perfect. Gaping holes appear in textures, and images are fuzzy because 95 percent of details are removed to make models download in seconds. As computing power, server bandwidth and the algorithms used to render the data improve, Aydemir says all of the models will be reprocessed to fill in the holes, increase detail and overall improve their realism. ""All of this was not possible a year ago,"" he said. ""Our current capability is at the limit of current state-of-the-art research."" To conservatively test Kinect@Home, Wired booted up a 7-year-old Windows Vista laptop and installed drivers supplied by the website, which totaled about 100MB and downloaded in a few minutes. Installation took much longer and forced a reboot. One small browser plugin installation later, we clicked ""record"" and filmed our first 3-D model of three Wired magazine issues on a couch. Per the site's instructions, we moved the Kinect slowly around the magazines from the top, left, right and below, and then clicked ""stop."" Next we made a model of the magazines to a busy bookshelf (above). Filming took roughly as long and the data recorded locally by the camera totaled about 100 megabytes. A third recording of your author on a couch turned out hazy and indiscernible. ""The tracking got a little lost,"" Aydemir wrote in an e-mail to Wired after our upload. ""The ... Kinect does not give back any depth information when you're too close to things. So it takes some time to master how to build good 3-D models, but not too much."" The good news is that Kinect@Home proactively uploads data during recording. The bad news: It's not yet possible to preview a model before it's beamed to a remote server, where it's rendered in the cloud. So if your model stinks, you won't know until dozens of megabytes later. Aydemir said he's met with the Kinect team in Seattle and asked them to craft a version of Kinect software that doesn't include motor, audio or other capabilities, since it will slim the hefty download and speed up modeling performance. As the team continues improving its service, which merely began as a side project for Aydemir, and internet service providers loosen their iron grip on broadband speeds, he hopes Kinect@Home will catch on. ""One of my colleagues has joked, 'You're trying to replace every image on the web with a 3-D image,'"" he said. ""I told him Google has done something similar by indexing almost every word on the internet, so why not?"" 3-D model: Click and drag to move the 3-D model, and zoom in or out using a mouse's scrolling wheel. (Dave Mosher and Tad Greenleaf/Kinect@Home)"
787,https://www.wired.com/2012/08/google-adds-siri-like-qa-to-app-merges-gmail-with-web-search/,Wired,2012,8,8,942.0," Google is taking a major swipe at Siri with a new feature in its iPhone and iPad search apps that mimics the question-and-answer abilities of Apple's digital concierge. The more conversational version of Google's voice search debuted today at a press event in San Francisco. Top members of the company's search team also announced a new version of the company's web search that integrates Gmail to personalize the results of everyday searches. The technology tying these new options together is what Google calls its Knowledge Graph, its fledgling effort to extend search's understanding of the world beyond keywords. Google engineers say the company's search engine now ""knows"" a half-billion real-world objects and 3.5 billion connections among those objects. In practice, this means when you ask, ""When's my flight?"" or say, ""Show me a video that explains quantum physics,"" Google's algorithms understand what you're talking about�and, more importantly, what you're trying to find out. ""There should be nothing standing between your thoughts and the knowledge you seek,"" says Amit Singhal, Google's senior vice president of engineering and clearly Google search's idealist-in-chief. Android users running the latest Jellybean operating system have already had this Siri-like voice-based asking-and-answering capability for the last few weeks, said Scott Huffman, Google's director of mobile engineering. The updated iOS version of the Google Search app was submitted to the Apple App Store last week and should be available within the next few days, Singhal said. Huffman showed off the app's fluency by peppering it with questions from ""What will the weather be like this weekend?"" (foggy and 68 degrees) to ""What is (struggling San Francisco Giants pitcher) Tim Lincecum's salary?"" ($18 million). The app answered quickly and clearly, though in a voice that sounded slightly more robotic and stilted than Siri's. For now, the upgraded voice search won't have the integrated Gmail personalization Google is starting to field-test this week. To understand how the new Gmail search feature works, you have to look to how Google has integrated its Knowledge Graph into general search results. In the current version of search on Google's non-mobile website, a query for ""Dark Knight Rises"" will result not just in a list of links to other websites but a box in the upper right-hand corner of the screen that includes a brief description of the movie, when it came out, who directed and wrote it, as well as a thumbnail gallery of cast members. Clicking ""Anne Hathaway"" takes you to a new page that on that same portion of the screen includes the actress' headshot, a brief biography and a gallery of the movies in which she's appeared. (In another new feature revealed Wednesday, those thumbnail galleries will become ""knowledge carousels"" at the top of the page if you click the header, a horizontally scrolling gallery that will display, for instance, the different rides at an amusement park.) This is Google's Knowledge Graph in action: The search engine gives you not just ranked links to other pages with the information it thinks you're seeking, but tries to extract and condense that information from elsewhere on the web into a user-friendly format that means you won't have to click away from Google at all to find what you're looking for: With the Gmail search integration toggled on (it's opt-in), the Knowledge Graph becomes personal. The algorithms the search engine uses to craft meaning out of disparate bits of information will comb your email for language structures it understands, such as flight itineraries and shipping confirmation emails from Amazon. Equipped with these little edifices of understanding, Googling ""when is my flight"" gets you as your top result the current status of the next plane you're scheduled to take. A search for ""amazon"" gets you the link to the Amazon website in the main search results, but in the upper right-hand corner will get you a parsed list of Gmail results that zero in on the status of your latest order. For now, Google is conducting a ""field trial"" of integrated Gmail-web search open to 1 million users. (Sign up here.) Sagar Kamdar, Google's director of product management for universal search, praised the personalized Gmail search as a major timesaver that means not having to keep track of flight numbers or having to drill down several pages on Amazon's site to find out where your order is. ""We could just make your life so much easier,"" Kamdar gushed. But keeping our lives easier doesn't just help us, it helps Google's bottom line. The better Google gets at telling us what we want to know without leaving the site or the app means the more time we spend exposed to ads on Google. By building out its Knowledge Graph and integrating it with speech recognition and understanding, Google has an answer to Apple's Siri. Google seems to have a major advantage in that contest in the form of its index of the entire web, though having that information and deriving meaning from it are two different challenges. In the end, Singhal says these advances in search are ""baby steps"" toward a dream he says he's had since he was a kid, and what every tech company would have to consider the ultimate killer app: ""The destiny of search is to become that Star Trek computer,"" he says, ""that perfect loyal assistant that is right by my side whenever I need it."" But to get there, Google will still have to wring a lot more meaning out of the 30 trillion URLs Singhal says exist on the web before the results of the 100 billion searches the company serves every month can answer every question at warp speed."
788,https://www.wired.com/2012/08/next-gen-siri-will-teach-kids-to-code-in-new-game/,Wired,2012,8,1,687.0," Your ship crash-lands on an alien world. To get out alive, you need to hack your way through the wreckage � not with an axe, but with algorithms. Your skills are a little rusty, so you turn to the shiny round drone hovering over your shoulder: ""Siri, what's a boolean operator?"" Welcome the world of Kuato Studios, a London-based startup developing a third-person-shooter-style game designed to teach kids how to code. Company founder Frank Meehan pulled together a team of developers from Rockstar, Konami and other top-shelf gaming companies earlier this year to build what he sees as an antidote to too much boring educational software. An early prototype resembles EA's sci-fi adventure Mass Effect; Meehan hopes that Kuato's as-yet-unnamed iPad-based game will engage kids by looking and feeling as much as possible like the games they play for fun. But the game comes with a secret weapon those other games don't have � Siri's heir apparent. Meehan, who's also sheperded smartphone handsets to market, sat on the board of Siri Inc., the company spun out of a project at the famed SRI International research institute and later purchased by Apple. Since then, Siri has become the world's best-known ""virtual personal assistant,"" a slightly hard-of-hearing digital concierge that can answer simple spoken questions and follow basic verbal commands on the iPhone. As Siri was being brought to market, its inventors at SRI were already hard at work on the next step in Siri's evolution. In Kuato's game, it's this smarter Siri who will help talk kids through coding puzzles, but with a key improvement over the iPhone incarnation: the power of conversation. As Meehan describes it, the current version of Siri isn't especially thoughtful. Specifically, Siri doesn't remember anything you've said. Every exchange is like meeting Siri for the first time. If you first ask Siri ""What's the weather in San Francisco?,"" to find out next what the skies look like in the British capital you need to ask ""What's the weather in London?"" Using the next generation of Siri, which Kuato is licensing from financial backer SRI, the conversation goes more like this: You: What's the weather in San Francisco? Next-gen Siri: Foggy. Misty. Wear a jacket. It's July. You: And what about London? Next-gen Siri: Raining, natch. Wear your wellies. Okay, so even next-gen Siri isn't quite that snarky. But notice the subtle change in the form the second question takes. Next-gen Siri remembers. It's context-aware. Call-and-response has evolved into dialogue. Spanish bank BBVA has already started testing the chattier version of Siri as part of a pilot program with SRI nicknamed Lola. The bank's customers will interact with Lola like they would with a bank teller, if the technology works as promised. In Kuato's game, set to come out later this year, the Lola-like chat-drone will serve as an always-available teacher. with an expert knowledge of XML and JavaScript, the two languages the game will initially teach. To progress in the game, players must solve coding problems in between bouts with bad guys. If they get stuck, they can just ask and keep asking until they figure it out, Meehan says: ""You're actually able to work down to the problem that the kid is trying to get at."" In its current version, Meehan says the next-gen Siri can only act as a good conversationalist if she stays focused. In other words, a meaningful dialogue can only happen within a narrow field of knowledge. Kuato's virtual assistant can talk about code but won't have good restaurant recommendations or insight into the fall of Rome. Much like the current Siri will default to a web search when it doesn't know what you're talking about, the next version can't hold a conversation about just anything. ""This isn't HAL,"" Meehan says. Still, he believes a HAL-like ""open context"" digital companion that can converse with you the way a human friend would will become a reality in about 20 or 30 years. ""This is a first little part of that. And I stress it's a little part,"" Meehan says of Kuato's chatty, code-savvy helper. ""But it's a step forward."""
789,https://www.wired.com/2012/07/board-game-artificial-intelligence/,Wired,2012,7,10,383.0," A computer scientist has published a paper detailing how systems can successfully win at boardgames after watching two minute-long videos of humans playing. [partner id=""wireduk""]Using visual recognition software while processing video clips of people playing Connect 4, Gomoku, Pawns and Breakthrough -- including games ending with wins, ties or those left unfinished -- the system would recognise the board, the pieces and the different moves that lead to each outcome. A unique formula then enabled the system to examine all viable moves when playing and, using data gathered from all possible outcomes, calculate the most appropriate move. Łukasz Kaiser of Paris Diderot University developed the learning algorithms after noticing a glaring gap in our knowledge -- object recognition machine learning experiments are fairly popular, however studies into high-concept computer learning are less common, despite having plenty of future uses in the creation of autonomous robots. Kaiser chose to use games as a primary learning tool because they are, ""a natural model of many real-world interaction scenarios, making the results signiﬁcant in a broader context"". Using a laptop with just 4GB Ram and a single processor core as his first test subject, Kaiser tackled the main issue thwarting former computer learning experiments -- assumed knowledge about inductive logic programming (ILP). An example of a typical ILP used in game learning is Progol, which needs visual cues and background knowledge to learn simple games. For tactical games such as Connect Four and Gomoku, Kaiser concluded that more nuanced logic systems were necessary. By forgoing an initial singular formula and using relational structures that recognize the rows, columns and diagonals of a boardgame, and making use of several different logic systems -- including pure ﬁrst-order, existential and guarded -- a tailored formula could be devised from the data gathered from each logic. An added General Game Playing program helped the system learn how to play tactically, learn legal moves and, ultimately, win. ""This combination allowed it to generate very short and intuitive formulas in the experiments we performed, and there is strong theoretical evidence that it will generalize to other problems,"" concluded the paper. Kaiser plans to adapt the system to solve problems that require ""hierarchical, structured learning or a form of probabilistic formulas"", all of which will come in handy in developing autonomous, intelligent robots."
790,https://www.wired.com/2012/06/robocup-videos/,Wired,2012,6,28,206.0," It's a practical embodiment of profound challenges in artificial intelligence and robotic engineering. It's also robots playing soccer. The 15th annual RoboCup, featuring 25 teams from around the world, recently concluded in Mexico City. The competition provides a common (and entertaining) goal for robotics researchers, concentrating their efforts on computational quandaries embodied by the beautiful game. ""The goal was to have a robotic team by 2050 that would play against the human world champion and beat them,"" said Carlos Gershenson, a computer scientist at the National Autonomous University of Mexico. ""We still have another 40 years to do that."" Each robot player is completely autonomous, and feats that humans take for granted – internally visualizing the location of oneself, other players and the ball, integrating strategy with motor feedback, kicking a ball without falling down – represent problems to be solved. Among the research presentations given by competing teams were titles like ""Motion capture and contemporary optimization algorithms for robust and stable motions on simulated biped robots,"" ""Solving Multi-Agent Decision Problems modeled as Dec-POMDP: A Robot Soccer Case Study"" and ""Robot Localisation Using Natural Landmarks."" Watch the championship matches here (but if you just can't wait to know who won, go to the RoboCup 2012 site)."
791,https://www.wired.com/2012/06/google-x-neural-network/,Wired,2012,6,26,614.0," When computer scientists at Google's mysterious X lab built a neural network of 16,000 computer processors with one billion connections and let it browse YouTube, it did what many web users might do -- it began to look for cats. [partner id=""wireduk""] The ""brain"" simulation was exposed to 10 million randomly selected YouTube video thumbnails over the course of three days and, after being presented with a list of 20,000 different items, it began to recognize pictures of cats using a ""deep learning"" algorithm. This was despite being fed no information on distinguishing features that might help identify one. Picking up on the most commonly occurring images featured on YouTube, the system achieved 81.7 percent accuracy in detecting human faces, 76.7 percent accuracy when identifying human body parts and 74.8 percent accuracy when identifying cats. ""Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not,"" the team says in its paper, Building high-level features using large scale unsupervised learning, which it will present at the International Conference on Machine Learning in Edinburgh, 26 June-1 July. ""The network is sensitive to high-level concepts such as cat faces and human bodies. Starting with these learned features, we trained it to obtain 15.8 percent accuracy in recognizing 20,000 object categories, a leap of 70 percent relative improvement over the previous state-of-the-art [networks]."" The findings -- which could be useful in the development of speech and image recognition software, including translation services -- are remarkably similar to the ""grandmother cell"" theory that says certain human neurons are programmed to identify objects considered significant. The ""grandmother"" neuron is a hypothetical neuron that activates every time it experiences a significant sound or sight. The concept would explain how we learn to discriminate between and identify objects and words. It is the process of learning through repetition. ""We never told it during the training, 'This is a cat,'"" Jeff Dean, the Google fellow who led the study, told the New York Times. ""It basically invented the concept of a cat."" ""The idea is that instead of having teams of researchers trying to find out how to find edges, you instead throw a ton of data at the algorithm and you let the data speak and have the software automatically learn from the data,"" added Andrew Ng, a computer scientist at Stanford University involved in the project. Ng has been developing algorithms for learning audio and visual data for several years at Stanford. Since coming out to the public in 2011, the secretive Google X lab -- thought to be located in the California Bay Area -- has released research on the Internet of Things, a space elevator and autonomous driving. Its latest venture, though not nearing the number of neurons in the human brain ( thought to be over 80 billion), is one of the world's most advanced brain simulators. In 2009, IBM developed a brain simulator that replicated one billion human brain neurons connected by ten trillion synapses. However, Google's latest offering appears to be the first to identify objects without hints and additional information. The network continued to correctly identify these objects even when they were distorted or placed on backgrounds designed to disorientate. ""So far, most [previous] algorithms have only succeeded in learning low-level features such as 'edge' or 'blob' detectors,"" says the paper. Ng remains skeptical and says he does not believe they are yet to hit on the perfect algorithm. Nevertheless, Google considers it such an advance that the research has made the giant leap from the X lab to its main labs."
792,https://www.wired.com/2012/06/flawed-turing-test/,Wired,2012,6,19,432.0," June 23 marks a century since Alan Turing's birth and it's fair to say that in that time he has inspired many by his theories and work, none more so than the founders of our IT service company, IPsoft. Turing was undoubtedly a thinker ahead of his time -- a brilliant mathematician and an architect of much of the computing theory on which today's world relies. [partner id=""wireduk""] The annual Loebner competition, where participants attempt to show that a machine can pass for a human in conversation, is based on Turing's theory that such a test would be an adequate demonstration of intelligence. The theory was that a machine would be shown to be intelligent if it could emulate a human in this way. But Turing was wrong. A machine should not demonstrate intelligence by emulating a human. In fact, in some regards today's expert systems are displaying intelligence far beyond the capability of a human. Should we mask such intellectual prowess in order for the machine to appear human, or allow it to run free to reach its full potential? Take today's autonomic management systems, for example. These expert systems have the capability to interpret and correlate myriad simultaneous events. This includes ""technical"" events such as monitors, tests, probes and analyzers; but it also includes human input, even natural language interaction, in the form of speech, email or instant messages. The systems have the capability to calculate in milliseconds, based on those events a series of probabilities and how to react to them. And when they react, they have the capability to execute multiple simultaneous tasks at once, each time using the result of an action to inform the next. While such systems can replace many of the activities typically carried out by humans, they are not simply emulating it. They are focused on optimizing it, improving its efficiency, ensuring its consistency and removing latency and error from the equation. The result is not just a machine emulating a human; it is a machine improving on the ability of that human. A hundred years on from Turing's birth, it's probably time for a new Turing test. Perhaps heralding the arrival of the age of machine intelligence does not rely on a computer's ability to have a fireside chat as though it were a person. Far more tangible in a real world application, the age of machine intelligence is surely when a computer can be shown to have absolved humans of mundane tasks, and released them to focus on activities which require not just intelligence, but creativity, original thought, and even genius."
793,https://www.wired.com/2012/06/pass-turing-ai-test/,Wired,2012,6,19,863.0," Are you human or a machine? Prove it, by passing the Turing Test -- a test of the ability of a machine to exhibit intelligent behavior. [partner id=""wireduk""] In Alan Turing's 1950 paper, Computing Machinery and Intelligence, the mathematician posed the question: ""Can machines think?"" But almost immediately he dismissed that question as too ""meaningless"" to be worthy of discussion, and swapped it for the much-more specific: ""Are there imaginable digital computers which would do well in the imitation game?"" Turing's original ""imitation game"" had nothing to do with artificial intelligence. It was a simple party game with three players -- a man, a woman, and a judge of either sex. The judge sits in a room apart from the man and woman, and has to guess which is which from nothing but written communication. The standard interpretation of the Turing Test today, however, replaces one of the participants with a machine which has to ""imitate"" intelligence. In this case, the judge has to decide which of the pair is the person, and which is the machine. The computer is successful, and passes the test, if -- as Turing puts it -- ""the interrogator decide[s] wrongly as often when the game is played [with the computer] as he does when the game is played between a man and a woman"". There's a bit of debate over whether the computer and the person are both supposed to try to trick the interrogator into making an incorrect decision or not. In the original imitation game, one of the pair tries to trick the judge while the other does not, meaning that both will be pretending to be the same gender. The common interpretation of the Turing Test today, however, is one of imitation rather than trickery. One aspect of the test that Turing never made clear is whether the judge should know whether there's a computer in play at all. Serious attempts at passing the test would almost certainly require a double-blind control, where the judge repeats the experiment multiple times -- sometimes with a pair of humans, sometimes with the human and the computer, and sometimes with two computers. While the Turing Test has been lauded for its simplicity and its ability to test across a wide range of intellectual tasks (natural language, reason, knowledge and learning can all be tested), it has also been criticized for a number of reasons. Firstly, the Turing Test doesn't directly test intelligence. Instead it merely tests how much a computer can behave like a human being. That's an important distinction because some human behavior is unintelligent, and there are plenty of intelligent behaviors that humans don't do. If, for example, a computer solved a mathematical problem that humans don't have the intellectual capability to do, then it wouldn't make it unintelligent but it would make it fail the Turing Test. A second issue is that simulated intelligence isn't the same thing as real intelligence. A machine that can pass the Turing Test could just be following a large list of mechanical rules. As such, the Turing Test doesn't test whether a machine can genuinely think. The counter-argument to that is that humans could well just be following a large list of mechanical rules, and then you're deep into philosophy of consciousness and intentionality. Turing, for his part, anticipated this somewhat. He wrote in his original paper: ""I do not wish to give the impression that I think there is no mystery about consciousness. There is, for instance, something of a paradox connected with any attempt to localize it. But I do not think these mysteries necessarily need to be solved before we can answer the question with which we are concerned in this paper."" Finally, there's the question of the judge. Because the test is by necessity subjective, a naive interrogator could fail to spot things that an expert would pick out. As a result, most competitions employ computer scientists, philosophers and journalists as judges. It's been pointed out that the tendency of people to ascribe human characteristics to inanimate objects (known as the anthropomorphic fallacy) means that a variety of statues, rocks and religious artifacts have passed the Turing Test consistently throughout history. Turing believed that a machine capable of passing the test would eventually be developed. He predicted that by the year 2000, machines with approximately 120 megabytes of memory would be able to pass a five-minute test 30 percent of the time. Futurist Ray Kurzweil predicted in 1990 that a machine capable of passing the test would be developed around 2020, but he pushed that date back to 2029 in 2005. In fact, he bet Electronic Frontier Foundation co-founder Mitch Kapor $20,000 (£12.7k) that a computer would pass the Turing test by 2029, specifying the conditions in some detail. On 23 June 2012, which would have been Turing's 100th birthday, Bletchley Park will host Turing100 -- a one-day event during which the Turing Test will be examined in great detail, and more than 150 machines will be subjected to the test by 30 judges. A Turing Education Day, complete with talks from 10 speakers, will follow on 30 June. Tickets are £90 per person."
794,https://www.wired.com/2012/06/computer-music-evolution/,Wired,2012,6,19,750.0," From Mozart to the Beatles, music evolves as listeners get used to sounds they initially find strange or even shocking. As trailblazing music becomes mainstream, artists strike out in new directions. But in a new study, a computer program shows how listeners drive music to evolve in a certain way. Although the resulting strains are hardly Don Giovanni, the finding shows how users' tastes exert their own kind of natural selection, nudging tunes to evolve out of noise. Bioinformaticist Robert MacCallum of Imperial College London was working with a program called DarwinTunes, which he and his colleagues had developed to study the musical equivalent of evolution in the natural world. The program produces 8-second sequences of randomly generated sounds, or loops, from a database of digital ""genes."" In a process akin to sexual reproduction, the loops swap bits of code to create offspring. ""Genetic"" mutations crop up as new material is inserted at random. The ""daughter"" loops retain some of the pitch, tone quality, and rhythm of their parents, but with their own unique material added. Previously, DarwinTunes could respond to only one person who would decide which loops went on to replicate. According to MacCallum, that doesn't give a true picture of how music naturally changes—with strange new sound patterns considered satisfying and even becoming commonplace. One explanation, says MacCallum, is that composers and songwriters themselves determine what people listen to, and the audience gets accustomed to it. And with only one person accessing DarwinTunes, this would appear to be the case. ""A single user twiddling knobs to achieve a pleasing sound is like a breeder of purebred dogs selecting for particular traits,"" MacCallum says. ""It doesn't show us how music evolves in the outside world as people listen, pass it on, and recommend it to their friends."" In the new study, appearing online today in the Proceedings of the National Academy of Sciences, MacCallum and colleagues adapted DarwinTunes to be accessed online by almost 7000 participants who rated each sound loop, played in a random order, on a 5-point scale from ""can't stand it"" to ""love it."" In a musical take on survival of the fittest, the highest-scored loops went on to pair up with others and replicate. Each resulting generation was rated again for its appeal. After about 2500 generations of sound loops, what started out as a cacophony of noise had evolved into pleasant strains of music. MacCallum emphasizes that the findings don't dismiss the importance of songwriters and composers in developing innovative, exciting music. ""The evolution led to pleasant, jingly tunes that didn't offend anyone but didn't really move anyone, either."" He adds that computerized reproduction has its limits. For example, the music did not become more beautiful indefinitely, but hit a plateau at which it stayed at the same, innocuous level. This plateau effect may be due to the random approach to reproduction used in the original DarwinTunes program, MacCallum explains. In the natural world, the DNA of two parents is not passed on to offspring at random: a child gets one copy of each gene from each parent, which functions when, where, and how they're supposed to. DarwinTunes, which originally swapped musical genes at random, now does so in a more controlled manner and may produce more interesting music, MacCallum says. The music still plateaus eventually, he says, but at a more complex and interesting level. Composer, musician, and computer programmer David Cope of the University of California, Santa Cruz, is glad to have scientific substantiation of what composers have always known. ""There's no way you're not influenced by the fact that your music has influenced others,"" he says. But he cautions that with a human composer, the influence can work in many ways. He notes that Mozart took audience response personally, but usually continued or even exaggerated musical traits that listeners didn't like. Other composers, including Cope himself, look for new directions when they feel their work is becoming too pleasing. MacCallum and colleagues are eager to see what kinds of music are ahead for DarwinTunes. Future incarnations, they hope, will enable up to a million users to log on and participate. ""Having so many users will help the music evolve a lot faster, and then, who knows?"" says MacCallum. Readers can cast their votes at the DarwinTunes Web site. This story provided by ScienceNOW, the daily online news service of the journal Science. Image: Musical evolution may arise from tension between the vision of the composer and the listener's taste. (Kevan/Flickr)"
795,https://www.wired.com/story/turing-test/,Wired,2012,6,19,862.0," Are you human or a machine? Prove it, by passing the Turing Test -- a test of the ability of a machine to exhibit intelligent behaviour. In Alan Turing's 1950 paper, Computing Machinery and Intelligence, the mathematician posed the question: ""Can machines think?"" But almost immediately he dismissed that question as too ""meaningless"" to be worthy of discussion, and swapped it for the much-more specific: ""Are there imaginable digital computers which would do well in the imitation game?"" Turing's original ""imitation game"" had nothing to do with artificial intelligence. It was a simple party game with three players -- a man, a woman, and a judge of either sex. The judge sits in a room apart from the man and woman, and has to guess which is which from nothing but written communication. The standard interpretation of the Turing Test today, however, replaces one of the participants with a machine which has to ""imitate"" intelligence. In this case, the judge has to decide which of the pair is the person, and which is the machine. The computer is successful, and passes the test, if -- as Turing puts it -- ""the interrogator decide[s] wrongly as often when the game is played [with the computer] as he does when the game is played between a man and a woman"". There's a bit of debate over whether the computer and the person are both supposed to try to trick the interrogator into making an incorrect decision or not. In the original imitation game, one of the pair tries to trick the judge while the other does not, meaning that both will be pretending to be the same gender. The common interpretation of the Turing Test today, however, is one of imitation rather than trickery. One aspect of the test that Turing never made clear is whether the judge should know whether there's a computer in play at all. Serious attempts at passing the test would almost certainly require a double-blind control, where the judge repeats the experiment multiple times -- sometimes with a pair of humans, sometimes with the human and the computer, and sometimes with two computers. While the Turing Test has been lauded for its simplicity and its ability to test across a wide range of intellectual tasks (natural language, reason, knowledge and learning can all be tested), it has also been criticised for a number of reasons. Firstly, the Turing Test doesn't directly test intelligence. Instead it merely tests how much a computer can behave like a human being. That's an important distinction because some human behaviour is unintelligent, and there are plenty of intelligent behaviours that humans don't do. If, for example, a computer solved a mathematical problem that humans don't have the intellectual capability to do, then it wouldn't make it unintelligent but it would make it fail the Turing Test. A second issue is that simulated intelligence isn't the same thing as real intelligence. A machine that can pass the Turing Test could just be following a large list of mechanical rules. As such, the Turing Test doesn't test whether a machine can genuinely think. The counter-argument to that is that humans could well just be following a large list of mechanical rules, and then you're deep into philosophy of consciousness and intentionality. Turing, for his part, anticipated this somewhat. He wrote in his original paper: ""I do not wish to give the impression that I think there is no mystery about consciousness. There is, for instance, something of a paradox connected with any attempt to localise it. But I do not think these mysteries necessarily need to be solved before we can answer the question with which we are concerned in this paper."" Finally, there's the question of the judge. Because the test is by necessity subjective, a naive interrogator could fail to spot things that an expert would pick out. As a result, most competitions employ computer scientists, philosophers and journalists as judges. It's been pointed out that the tendency of people to ascribe human characteristics to inanimate objects (known as the anthropomorphic fallacy) means that a variety of statues, rocks and religious artefacts have passed the Turing Test consistently throughout history. Turing believed that a machine capable of passing the test would eventually be developed. He predicted that by the year 2000, machines with approximately 120 megabytes of memory would be able to pass a five-minute test thirty percent of the time. Futurist Ray Kurzweil predicted in 1990 that a machine capable of passing the test would be developed around 2020, but he pushed that date back to 2029 in 2005. In fact, he bet Electronic Frontier Foundation co-founder Mitch Kapor $20,000 (£12.7k) that a computer would pass the Turing test by 2029, specifying the conditions in some detail. On 23 June 2012, which would have been Turing's 100th birthday, Bletchley Park will host Turing100 -- a one day event during which the Turing Test will be examined in great detail, and more than 150 machines will be subjected to the test by 30 judges. A Turing Education Day, complete with talks from 10 speakers, will follow on 30 June. Tickets are £90 per person."
796,https://www.wired.com/story/the-dangers-of-an-ai-smarter-than-us/,Wired,2012,5,17,1118.0," Forget about The Terminator, the real problem with AI (artificial intelligence) is what to do when it meets your boss or even your friends. This is not the pitch for some kind of sci-fi rom-com, but rather the genuine concern of Dr Stuart Armstrong, a research fellow at Oxford University's Future of Humanity Institute. His job is to think about future threats to the human race and how to confront them. AI is in the top five threats to humanity that he lists quickly on the back of his napkin, set against the rather incongruous background of the student chit-chat that fills Oxford's cycling cafe, Zappi's (for the record, the other four are: pandemics, synthetic biology, nanotechnology and nuclear war). While dismissing the blood and guts of Hollywood scenarios, what worries Armstrong -- as he outlines in his upcoming research paper Thinking Inside The Box in the journal Minds and Machines -- is the potential for superintelligent AI to stage a ""takeover"". He believes that humanity faces the risk of a more 9-to-5 style apocalypse, whereby a superhuman AI could (whether through its own logic or on the orders of other humans) out-compete the rest of us economically and even socially, rendering human beings obsolete and disposable. ""After all, I wouldn't trust humans with the kind of power we are thinking about giving to AI,"" he said. Although he thinks ""there is only a third of a chance of superpower AI happening this century, if it does happen then it is quickly going to be dangerous and so [is] well worth worrying about"". Some members of the AI community put the chance -- or risk -- as high as 50 percent. For Armstrong, the AI we should be afraid of is not the ""beatable humanoid robot we see in the movies"" but rather a computer program or even a digital avatar that has been freed from our ""biological limitations"" to demonstrate ""skills and abilities beyond what is considered to be human""; whether the ability to plan centuries ahead, to see patterns that we cannot or to link instantly to the internet, or even the social skill of ""being always able to say the right thing at the right time"" to get what it wants without humans even realising the game play. ""AI would be able to use its superpowers to accumulate vast fortunes on the stock exchange, or even 'be Google', as AI would be cheaper and more productive than the human workers currently employed. It could even be a Super Clinton or Super Goebbels, able to take over by persuading us to let it."" Or it may gain more powers that we have not even thought of, given that ""the space beyond human intelligence is vast"". Any AI regime, Armstrong maintains, is likely to be a very uncomfortable place for us ""meatbags"", as this Alpha AI on steroids is likely to be totalitarian or extremist in outlook, committed to ""utility maximising, as it's hard to code for reduced impact, and if it doesn't use all the resources then someone else can"", and ultimately supplanting our human values with its ""alien ones"". ""Would it understand how important 'love' is to being human?"" asks Armstrong. AI has long been a ""moving target"", as what we consider now to be ""normal computer stuff like playing chess"" was once considered to be proof of AI, and Armstrong considers that the closing-time argument of whether AI is actually consciousness or not ""is a distraction"". ""After all, if it decides to end the world it doesn't matter whether it is thinking about it while it's doing it, or just following its programme to achieve goals that we had been mistaken to give it."" He accepts that many others in the AI community see his views as rather bleak, since -- as opponents argue --  ""AI isn't invented by a bang plucked from the ether, it is developed by humans, trained by humans, and sited close to human space."" This means that humans should be able to understand, manage and -- crucially -- pull the plug on AI should we need to. Yet Armstrong remains sceptical of that theory, imagining that any superintelligent AI ""may quickly learn to tell the human testers what they want and then manipulate them"", as would any AI that was isolated in some kind of ""oracle"". ""Wouldn't you?"" he adds. Luke 0agrees that AI is a threat, but believes that it is worth trying to build a friendly AI that would ""be benign to humans"". Muehlhauser is the executive director of the appropriately named Singularity Institute in Silicon Valley and is trying to develop just that -- a friendly AI. ""Anything intelligent is dangerous if it has different goals than you do, and any constraint we could devise for the AI merely pits human intelligence against superhuman intelligence, and we should expect the latter to prevail. That's why we need advanced AIs to want the same things we want. ""So friendly AI is an AI that has a positive rather than negative effect on human beings. To be a friendly AI, we think an AI must want what humans want. Once a superintelligent AI wants something different than we want, we've already lost."" He admits the progress is slow, not least because -- and perhaps not surprisingly -- it is hard to codify human values, reflecting Armstrong's criticism of the idea. Recruitment and funding are the main problems they face. ""Right now there just aren't enough people in the world who care about AI risk and the long-term future of humanity to fund it."" Armstrong agrees: ""For politicians we are just another lobby asking for funds,"" competing against an AI lobby that must feel that ""that the evolution of AI is going to take so long that there will be plenty of time to think of controls later"". Although he cautions that ""the number of jumps from village idiot to Einstein might not be as many as we think"". Muehlhauser adds that ""it is astonishing how little concern there has been about this issue"". Many early AI scientists have ""never bothered to think hard about what might happen once humans were no longer the most capable agents on the Earth"". Knowing the way that humans are notoriously bad at planning beyond the short term, Armstrong feels that given the risk ""it would perhaps be best not to create AI at all,"" since in the end our only hope of competing with AI might be the long shot of being able to upload our brains and turn ourselves into digital beings. ""After all,"" he reminds us, ""humans only tried to flee the cafés of Pompeii after the eruption had started."""
797,https://www.wired.com/2012/05/robot-overlords/,Wired,2012,5,15,818.0," Zombie fears distract us from a much bigger threat. Robots. These machines are plotting to become our global overlords, so of course they want us to stay busy preparing for an imaginary invasion by reanimated corpses. Unconvinced? Consider this. People who most loudly fuel our so-called divides (between liberal and conservative, old and young, mommies who do things differently than other mommies) keep us from hearing the powers-that-be loudly slurp up ever more power. No wonder robots have marked humans as easy prey. And robots use diabolically clever means to achieve their aims. They don't just prey on our zombie phobias. They lure us into adoring them using darling robot toys. They entertain us using loveable movie robots like The Iron Giant. They let us feel us comfortably superior to them with strangely humanoid robots like these. They're not as endearing once they are weaponized. Back when robots existed mostly in our imaginations we believed in Asimov's Laws of Robotics, the first and most logical of which was, ""A robot may not injure a human being or, through inaction, allow a human being to come to harm."" Oh such innocent times. Now we're testing heavily armed autonomous drones. Even developers seem nervous. Robotics expert Noel Sharkey said in an  LA Times interview, ""Lethal actions should have a clear chain of accountability. This is difficult with a robot weapon. The robot cannot be held accountable."" Sir, your words sound just like clichéd lines uttered by scientists destined to be the earliest victims in every cheesy horror movie. But we humans have the advantage because armed drones can't think for themselves. Oh sorry, they can. Researchers hastening to end mankind's dominance on this planet have created robots that can reason and make decisions on their own. Some of these machines are endearing little smart bots like the one named iCub, created by open source collaborators and designed to develop cognitive abilities as a human child does, learning through experience. Could that include a terrible twos stage? Better yet, researchers have also come up with robots skilled in deceiving both humans and other forms of artificial intelligence. Great idea, artificial intelligence capable of throwing tantrums and lying. If you pay attention to scary movies, by now you recognize foreshadow. Haunting music is cuing up but no one is turning on the lights. Instead plans are being made to establish a network of perpetually active armed drones, able to grab power via laser from a variety of sources. These charmers are programmed to identify and kill using software calculations. Dum dum dum. So let's stop to assess. We have robots that can learn, make decisions on their own, lie, kill humans, and operate indefinitely using power pillaged by lasers. Once those skills are combined we’re in trouble. But wait, there's more. Why should a killing machine rely on traditional power sources when it can digest flesh? Now there are robots powered by meat. These fiends-in-the-making are called ""gastrobots."" Currently they only chomp sugar cubes or slugs, but once they merge with the autonomous drone army these bots may quickly recognize that cheeseburger-fattened humans provide far more energy. Heck, lets nudge them closer to overlord status. Robots don't actually have to come up with meat-eating plots of their own. They're being designed to do just that. The Energetically Autonomous Tactical Robot (EATR) can forage for its own fuel, including organically-based energy sources like chicken fat. The company insists the EATR is vegetarian but hello, chickens are biological entities with eyes, hearts, and brains just like us. Designers boast that this war bot has ""ubiquitous applications."" It can fuel itself on indefinite, autonomous missions while ""…integrating high-level cognitive reasoning with low level perception and feedback control."" Zombie worries seem downright calming by comparison. If you're still not worried, consider a Caltech report sponsored by the Department of the Navy Office of Naval Research. Its warnings include this gem: Perhaps robot ethics has not received the attention it needs, at least in the US, given a common misconception that robots will do only what we have programmed them to do. Unfortunately, such a belief is a sorely outdated, harking back to a time when computers were simpler and their programs could be written and understood by a single person. Now, programs with millions of lines of code are written by teams of programmers, none of whom knows the entire program; hence, no individual can predict the effect of a given command with absolute certainty, since portions of large programs may interact in unexpected, untested ways. And some of their conclusions don't downplay anyone's fears. Maybe we are the zombies we fear, our brains slowly rotting thanks to reality television, never realizing our programmable vacuums have been reporting back to their leaders. Touché robots. It may be time to build an underground robot-resistant bunker where we (and our chickens) can hide. Still not not frantic? Try:"
798,https://www.wired.com/2012/04/can-an-algorithm-write-a-better-news-story-than-a-human-reporter/,Wired,2012,4,24,2835.0," Had Narrative Science—a company that trains computers to write news stories—created this piece, it probably would not mention that the company's Chicago headquarters lie only a long baseball toss from the Tribune newspaper building. Nor would it dwell on the fact that this potentially job-killing technology was incubated in part at Northwestern's Medill School of Journalism, Media, Integrated Marketing Communications. Those ironies are obvious to a human. But not to a computer. At least not yet. For now consider this: Every 30 seconds or so, the algorithmic bull pen of Narrative Science, a 30-person company occupying a large room on the fringes of the Chicago Loop, extrudes a story whose very byline is a question of philosophical inquiry. The computer-written product could be a pennant-waving second-half update of a Big Ten basketball contest, a sober preview of a corporate earnings statement, or a blithe summary of the presidential horse race drawn from Twitter posts. The articles run on the websites of respected publishers like Forbes, as well as other internet media powers (many of which are keeping their identities private). Niche news services hire Narrative Science to write updates for their subscribers, be they sports fans, small-cap investors, or fast-food franchise owners. And the articles don't read like robots wrote them: OK, it's not Roger Angell. But the grandparents of a Little Leaguer would find this game summary—available on the web even before the two teams finished shaking hands—as welcome as anything on the sports pages. Narrative Science's algorithms built the article using pitch-by-pitch game data that parents entered into an iPhone app called GameChanger. Last year the software produced nearly 400,000 accounts of Little League games. This year that number is expected to top 1.5 million. Narrative Science's CTO and cofounder, Kristian Hammond, works in a small office just a few feet away from the buzz of coders and engineers. To Hammond, these stories are only the first step toward what will eventually become a news universe dominated by computer-generated stories. How dominant? Last year at a small conference of journalists and technologists, I asked Hammond to predict what percentage of news would be written by computers in 15 years. At first he tried to duck the question, but with some prodding he sighed and gave in: ""More than 90 percent."" That's when I decided to write this article, hoping to finish it before being scooped by a MacBook Air. Hammond assures me I have nothing to worry about. This robonews tsunami, he insists, will not wash away the remaining human reporters who still collect paychecks. Instead the universe of newswriting will expand dramatically, as computers mine vast troves of data to produce ultracheap, totally readable accounts of events, trends, and developments that no journalist is currently covering. That's not to say that computer-generated stories will remain in the margins, limited to producing more and more Little League write-ups and formulaic earnings previews. Hammond was recently asked for his reaction to a prediction that a computer would win a Pulitzer Prize within 20 years. He disagreed. It would happen, he said, in five. Hammond was raised in Utah, where his archaeologist dad taught at a state university. He grew up thinking he'd become a lawyer. But in the late 1980s, as an undergraduate at Yale, he fell under the sway of Roger Schank, a renowned artificial intelligence researcher and chair of the computer science department. After earning a doctorate in computer science, Hammond was hired by the University of Chicago to lead a new AI lab. While there, in the mid-1990s, he created a system that tracked users' reading and writing and then recommended relevant documents. Hammond built a small company around that technology, which he later sold. By that time, he had moved to Northwestern University, becoming codirector of its Intelligent Information Laboratory. In 2009, Hammond and his colleague Larry Birnbaum taught a class at Medill that included both programmers and prospective journalists. They encouraged their students to create a system that could transform data into prose stories. One of the students in the class was a stringer for the Tribune who covered high school sports; he and two other journalism students were paired with a computer science student. Their prototype software, Stats Monkey, collected box scores and play-by-play data to spit out credible accounts of college baseball games. At the end of the semester, the class participated in a demo day, where students presented their projects to a roomful of executives from the likes of ESPN, Hearst, and the Tribune. The Stats Monkey presentation was particularly impressive. ""They put a box score and play-by-play into the program, and in something close to 12 seconds it drew examples from 40 years of Major League history, wrote a game account, located the best picture, and wrote a caption,"" recalls the Medill dean, John Lavine. Stuart Frankel, a former DoubleClick executive who left the online advertising network after Google purchased it in 2008, was among the guests that day. ""When these guys did the presentation, the air in the room changed,"" he said. ""But it was still just a piece of software that wrote stories about baseball games—very limited."" Frankel followed up with Hammond and Birnbaum. Could this system create any kind of story, using any kind of data? Could it create stories good enough that people would pay to read them? The answers were positive enough to convince him that ""there was a really big, exciting potential business here,"" he says. The trio founded Narrative Science with Frankel as CEO in 2010. The startup's first customer was a TV network for the Big Ten college sports conference. The company's algorithm would write stories on thousands of Big Ten sporting events in near-real time; its accounts of football games updated after every quarter. Narrative Science also got assigned the women's softball beat, where it became the country's most prolific chronicler of that sport. But not long after the contract began, a slight problem emerged: The stories tended to focus on the victors. When a Big Ten team got whipped by an out-of-conference rival, the resulting write-ups could be downright humiliating. Conference officials asked Narrative Science to find a way for the stories to praise the performances of the Big Ten players even when they lost. A human journalist might have blanched at the request, but Narrative Science's engineers saw no problem in tweaking the software's parameters—hacking it to make it write more like a hack. Likewise, when the company began covering Little League games, it quickly understood that parents didn't want to read about their kids' errors. So the algorithmic accounts of those matchups ignore dropped fly balls and focus on the heroics. I asked Kristian Hammond what percentage of news would be written by computers in 15 years. ""More than 90 percent."" Narrative Science's writing engine requires several steps. First, it must amass high-quality data. That's why finance and sports are such natural subjects: Both involve the fluctuations of numbers—earnings per share, stock swings, ERAs, RBI. And stats geeks are always creating new data that can enrich a story. Baseball fans, for instance, have created models that calculate the odds of a team's victory in every situation as the game progresses. So if something happens during one at-bat that suddenly changes the odds of victory from say, 40 percent to 60 percent, the algorithm can be programmed to highlight that pivotal play as the most dramatic moment of the game thus far. Then the algorithms must fit that data into some broader understanding of the subject matter. (For instance, they must know that the team with the highest number of ""runs"" is declared the winner of a baseball game.) So Narrative Science's engineers program a set of rules that govern each subject, be it corporate earnings or a sporting event. But how to turn that analysis into prose? The company has hired a team of ""meta-writers,"" trained journalists who have built a set of templates. They work with the engineers to coach the computers to identify various ""angles"" from the data. Who won the game? Was it a come-from-behind victory or a blowout? Did one player have a fantastic day at the plate? The algorithm considers context and information from other databases as well: Did a losing streak end? Then comes the structure. Most news stories, particularly about subjects like sports or finance, hew to a pretty predictable formula, and so it's a relatively simple matter for the meta-writers to create a framework for the articles. To construct sentences, the algorithms use vocabulary compiled by the meta-writers. (For baseball, the meta-writers seem to have relied heavily on famed early-20th-century sports columnist Ring Lardner. People are always whacking home runs, swiping bags, tallying runs, and stepping up to the dish.) The company calls its finished product ""the narrative."" Occasionally the algorithms will produce a misstep, like a story stating that a pinch hitter—who usually bats only once per game—went two for six. But such errors are rare. Numbers don't get misquoted. Even when databases provide faulty information, Hammond says, Narrative Science's algorithms are trained to catch the error. ""If a company has a 600 percent rise in profits from quarter to quarter, it'll say, 'Something is wrong here,'"" Hammond says. ""People ask for examples of wonderful, humorous gaffes, and we don't have any."" Forbes Media chief products officer Lewis Dvorkin says he's impressed but not surprised that, in almost every case, his cyber-stringers nail the essence of the company they're reporting on. Major screwups are not unheard-of with flesh-and-blood scribes, but Dvorkin hasn't heard any complaints about the automated reports. ""Not a one,"" he says. (The pieces on Forbes.com include an explanation that ""Narrative Science, through its proprietary artificial intelligence platform, transforms data into stories and insights."") The Narrative Science team also lets clients customize the tone of the stories. ""You can get anything, from something that sounds like a breathless financial reporter screaming from a trading floor to a dry sell-side researcher pedantically walking you through it,"" says Jonathan Morris, COO of a financial analysis firm called Data Explorers, which set up a securities newswire using Narrative Science technology. (Morris ordered up the tone of a well-educated, straightforward financial newswire journalist.) Other clients favor bloggy snarkiness. ""It's no more difficult to write an irreverent story than it is to write a straightforward, AP-style story,"" says Larry Adams, Narrative Science's VP of product. ""We could cover the stock market in the style of Mike Royko."" Once Narrative Science had mastered the art of telling sports and finance stories, the company realized that it could produce much more than journalism. Indeed, anyone who needed to translate and explain large sets of data could benefit from its services. Requests poured in from people who were buried in spreadsheets and charts. It turned out that those people would pay to convert all that confusing information into a couple of readable paragraphs that hit the key points. Narrative Science, it so happened, was well placed to accommodate such demands. When the company was just getting started, meta-writers had to painstakingly educate the system every time it tackled a new subject. But before long they developed a platform that made it easier for the algorithm to learn about new domains. For instance, one of the meta-writers decided to build a story-writing machine that would produce articles about the best restaurants in a given city. Using a database of restaurant reviews, she was able to quickly teach the software how to identify the relevant components (high survey grades, good service, delicious food, a quote from a happy customer) and feed in some relevant phrases. In the space of a few hours she had a bot that could churn out an endless supply of chirpy little articles like ""The Best Italian Restaurants in Atlanta"" or ""Great Sushi in Milwaukee."" (Narrative Science's main rival in automated story creation, a North Carolina company founded as Stat Sheet, has broadened its mission in similar fashion. The company can't compete with Narrative Science's Medill pedigree and so has assumed the role of a feisty tabloid in a two-paper town. It too got its start in sports, writing accounts of Major League and big-college games as well as creating a trash-talk generator called StatSmack. After realizing that turning data into stories presented an opportunity far larger than sports, the company changed its name to Automated Insights. ""I used to put limitations on what we do, assuming our stories would be specific to data-rich industries,"" founder Robbie Allen says. ""Now I think ultimately the sky is the limit."") Users can customize the tone of any story—from breathless financial reporter to dry analyst. And the subject matter keeps getting more diverse. Narrative Science was hired by a fast-food company to write a monthly report for its franchise operators that analyzes sales figures, compares them to regional peers, and suggests particular menu items to push. What's more, the low cost of transforming data into stories makes it practical to write even for an audience of one. Narrative Science is looking into producing personalized 401(k) financial reports and synopses of World of Warcraft sessions—players could get a recap after a big raid that would read as if an embedded journalist had accompanied their guild. ""The Internet generates more numbers than anything that we've ever seen. And this is a company that turns numbers into words,"" says former DoubleClick CEO David Rosenblatt, who sits on Narrative Science's board. ""Narrative Science needs to exist. The journalism might be only the sizzle—the steak might be management reports."" For now, though, journalism remains at the company's core. And like any cub reporter, Narrative Science has dreams of glory—to identify and break big stories. To do that, it will have to invest in sophisticated machine-learning and data-mining technologies. It will also have to get deeper into the business of understanding natural language, which would allow it to access information and events that can't be expressed in a spreadsheet. It already does a little of that. ""In the financial world, we're reading headlines,"" Hammond says. ""We can identify if some company's stock gets upgraded or downgraded, somebody gets fired or hired, somebody's thinking of a merger, and we know the relationship between those events and a stock price."" Hammond would like to see his company's college sports stories include nonstatistical information like player injuries or legal problems. But even if Narrative Science never does learn to produce Pulitzer-level scoops with the icy linguistic precision of Joan Didion, it will still capitalize on the fact that more and more of our lives and our world is being converted into data. For example, over the past few years, Major League Baseball has spent millions of dollars to install an elaborate system of hi-res cameras and powerful sensors to measure nearly every event that's occurring on its fields: the velocities and trajectories of pitches, tracked to fractions of inches. Where the fielders stand at any given moment. How far the shortstop moves to dive for a ground ball. Sometimes the real story of the game may lie within that data. Maybe the manager failed to detect that a pitcher was showing signs of exhaustion several batters before an opponent's game-winning hit. Maybe a shortstop's extended reach prevented six hits. This is stuff that even an experienced beat writer might miss. But not an algorithm. Hammond believes that as Narrative Science grows, its stories will go higher up the journalism food chain—from commodity news to explanatory journalism and, ultimately, detailed long-form articles. Maybe at some point, humans and algorithms will collaborate, with each partner playing to its strength. Computers, with their flawless memories and ability to access data, might act as legmen to human writers. Or vice versa, human reporters might interview subjects and pick up stray details—and then send them to a computer that writes it all up. As the computers get more accomplished and have access to more and more data, their limitations as storytellers will fall away. It might take a while, but eventually even a story like this one could be produced without, well, me. ""Humans are unbelievably rich and complex, but they are machines,"" Hammond says. ""In 20 years, there will be no area in which Narrative Science doesn't write stories."" For now, however, Hammond tries to reassure journalists that he's not trying to kick them when they're down. He tells a story about a party he attended with his wife, who's the marketing director at Chicago's fabled Second City improv club. He found himself in conversation with a well-known local theater critic, who asked about Hammond's business. As Hammond explained what he did, the critic became agitated. Times are tough enough in journalism, he said, and now you're going to replace writers with robots? ""I just looked at him,"" Hammond recalls, “and asked him: Have you ever seen a reporter at a Little League game? That's the most important thing about us. Nobody has lost a single job because of us.” At least not yet."
799,https://www.wired.com/2012/03/robot-hand-communication/,Wired,2012,3,16,364.0," Aircraft carrier crews already use a set of hand gestures and body positions to guide pilots around the deck. But with an increase in unmanned planes, what if the crew could use those same gestures to guide robotic aircraft? A team of researchers at MIT – Computer Science student Yale Song, his advisor Randall Davis and Artificial Intelligence Laboratory researcher David Demirdjian – set out to answer that question. They're  developing a Kinect-like system (Microsoft's  Xbox 360 peripheral wasn't available when the team started the project) that can recognise body shapes and hand positions in 3-D. It uses a single stereo camera to track crew members, and custom-made software to detect each gesture. First, it captures a 3-D image of the crew member, and removes the background. Then, to estimate which posture the body is in, it compares the person against a handful of skeleton-like models to see which one fits best. Once it's got a good idea of the body position, it also knows approximately where the hands are located. It zeros in on these areas, and looks at the shape, position and size of the hand and wrist. Then it estimates which gesture is being used: maybe the crew member has their palm open or their fist clenched or their thumb pointing down. The biggest challenge is that there's no time for the software to wait until the crew member stops moving to begin its analysis. An aircraft carrier deck is in constant motion, with new hand gestures and body positions every few seconds. ""We cannot just give it thousands of [ video ] frames, because it will take forever,"" Song said in a press release. Instead, it works on a series of short body-pose sequences that are about 60 frames long (roughly three seconds of video), and the sequences overlap each other. It also works on probabilities rather than exact matches. In tests, the algorithm correctly identified the gestures with 76 percent accuracy. Pretty impressive, but not good enough when you're guiding  multimillion-dollar drones on a tiny deck in the middle of the ocean. But Song reckons he can increase the system's accuracy by considering arm position and hand position separately."
800,https://www.wired.com/2011/10/foolproof-and-incapable-of-error-rip-john-mccarthy/,Wired,2011,10,26,206.0," Perhaps it's just me, but it's beginning to seem lately as though we are coming to the end of an era, or is it the dawn of a new one? This week I was saddened to hear of the death of John McCarthy, a pioneer in the field of Artificial Intelligence--he coined the term. McCarthy invented the computer language LISP--LISt Processing--which is still used today in AI circles, and is the second oldest high level programming language. During the first Dartmouth conference in 1956 he, and his fellow organizers, came with the notion that ""every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it."" To be honest, there are aspects of Artificial Intelligence that give me caution, but it is the minds behind the ideas that I find intriguing. The imagination that takes the work of scientists one step further, the offbeat notion that takes something statistical and creates from it. With the death of Steve Jobs, Dennis Ritchie and now John McCarthy all in a matter of weeks, I am left wondering where technology will take us in the next 20 years, and who will be leading us there."
801,https://www.wired.com/2011/07/artificial-intelligence-gaming/,Wired,2011,7,20,1110.0," Normally, covering computer science articles are a bit of a strain, but two things about a recent one had a strong personal appeal: I'm addicted to the Civilization series of games, and I rarely bother to read the users' manual. These don't necessarily sound like issues that could be tackled via computer science, but some researchers have decided to let a computer teach itself how to play Freeciv and, in the process, teach itself to interpret the game's manual. Simply by determining whether the moves it made were ultimately successful, the researchers' software not only got better at playing the game, but it figured out a lot of the owner's manual, as well. [partner id=""arstechnica"" align=""right""]Civilization isn't the first game to catch the attention of computer scientists. The new papers' authors, based at MIT and University College London, cite past literature in which computers were able to teach themselves Go, Poker, Scrabble, multi-player card games, and real-time strategy games. The method used for all of these is called a Monte Carlo search framework. At each possible move, the game runs a series of simulated games, which it uses to evaluate the possible utility of various moves. It uses these to update a utility function that estimates the value of a given move for a specific state of the game. After multiple iterations, the utility function should get better at identifying the best move, although the algorithm will sporadically insert a random move, just to continue to sample new possibilities. This all sounds pretty simple, but the computational challenges are pretty large. The authors estimate that an average player will typically have 18 units in play, and each of those can take any one of 15 actions. That creates what they term an ""action space"" of about 1021 possible moves. To gauge the utility of any one of these, they ran things out 20 moves and then checked the game score (or determined whether they won or lost before then). They performed this 200 times in order to generate their performance numbers. For their testing, the Monte Carlo search was set to play Freeciv's built in AI in a one-on-one match on a grid of 1,000 tiles. A single 100-move game took about 1.5 hours to complete on a Core i7, so all this simulation time wasn't trivial. But, in general, the algorithm performed fairly well, being able to achieve victory in that short time frame about 17 percent of the time (left to play a game to completion, the Monte Carlo search won just under half the time). Still the authors wondered whether the algorithm might arrive at better decisions more consistently if it had access to the owner's manual, which contains various bits of advice about the strengths and weaknesses of various units, as well as some general guidance about how to build an empire (stick early cities near a river, for example). So, they decided to get their program to RTFM. The ""reading"" took place using a neural network that takes the game state, a proposed move, and the owner's manual as input. One set of neurons in the network analyzed the manual to look for state/action pairs. These pairs are things like ""active unit"" or ""completed road"" (the states) and ""improve terrain"" or ""fortify unit"" as the actions. A separate neural network then figured out whether any of the items identified to the first applied to the current situation. These are then combined to find relevant advice in the manual, which is then incorporated into the utility function. The key thing about this process is that the neural network doesn't even know whether it's correctly identifying state/action pairs when it starts—it doesn't know how to ""read""—much less whether it has correctly interpreted the advice they convey (do you build near a river, or should you never build by a river?). All it has to go on is what impact its interpretation has on the outcome of the game. In short, it has to figure out how to read the owner's manual simply by trying different interpretations and seeing whether they improve its play. Despite the challenges, it works. When the full-text analysis was included, the success of the authors' software shot up; it now won over half its games within 100 moves, and beat the game's AI almost 80 percent of the time when games were played to completion. To test how well the software did, the authors fed it a mix of sentences from the owners' manual and those culled from the pages of The Wall Street Journal. The software correctly used sentences from the manual over 90 percent of the time during the early game. However, as play progressed, the manual became less of a useful guide, and the ability to pick out the manual dropped to about 60 percent for the rest of the game. In parallel, the software started relying less on the manual, and more on its game experience. That doesn't mean the Journal was useless, however. Feeding the full software package random text instead of an owner's manual also boosted their algorithm's winning percentage, boosting it to 40 percent in 100-move games. That's not as good as the 54 percent obtained with the manual, but it is quite a bit better than the 17 percent win rate of the algorithm alone. What's going on here? The paper doesn't say, but the key thing to note is that the neural network is only attempting to identify rules that work (i.e., build near a river). It doesn't actually care how those rules are conveyed—it simply associates text with a random action and determines whether the results are any good. If it's lucky, it can end up associating a useful rule with a random bit of text. It has a better chance of doing so with nonrandom bits of text like the owner's manual, but it can still provide useful guidance no matter what it's given to work with. (I've asked the authors for their explanation for this result but, as of publication, they hadn't gotten back to me.) The authors conclude that their software successfully learned to leverage the rich language present in the game's manual to perform better, learning to interpret the language as it went along. This is clearly true; the software would perform better when it was given the owner's manual than when it was fed random text, and the difference was statistically significant. But simply giving it any text resulted in a larger relative boost. That implies that it's better to have some rules to work with, no matter how they're derived, than no guidance at all. See Also:"
802,https://www.wired.com/2011/07/human-android-brain-response/,Wired,2011,7,19,543.0," We've all found ourselves in the uncanny valley before. It's that uneasy feeling you get when viewing a realistic humanoid or CGI person that's so close to looking human that it seems almost spooky. [partner id=""wireduk"" align=""right""]The actual ""valley"" refers to a precipitous drop in ""likeability"" as onscreen characters and humanoid robots step too far towards being human-like. As in, we enjoy Pixar's Wall-E and Nintendo's Mario, but we get the heeby jeebies from the ultra-realistic faces of The Polar Express or the upcoming Tintin movie. So far, the phenomenon has been described entirely anecdotally, but an international team of researchers, led by Ayse Pinar Saygin of the University of California, San Diego, wanted to find out if the sensation was actually caused by something deep within our brains. The team picked out 20 subjects, aged 20 to 36. They had no experience working with robots and hadn't spent time in Japan where there's more cultural exposure to androids. Saygin also recruited the help of Repliee Q2, an especially human-like robot from Intelligent Robotics Laboratory at Osaka University. Q2 has 13 degrees of freedom on her face alone, and uses her posable eyes, brows, cheeks, lids, lips and neck to make facial expressions and mouth shapes. The team made videos of Repliee Q2 performing actions like waving, nodding, taking a drink of water and picking up a piece of paper from a table. Then, the same actions were performed by the Japanese woman whom Q2 is based on. Finally, the researchers stripped the robot of its synthetic skin and hair to reveal a Terminator-style metal robot with dangling wires and visible circuits. The subjects were shown each of the videos and were informed about which was a robot and which human. Then, the subjects' brains were scanned in an fMRI machine. When viewing the real human and the metallic robot, the brains showed very typical reactions. But when presented with the uncanny android, the brain ""lit up"" like a Christmas tree. When viewing the android, the parietal cortex -- and specifically in the areas that connect the part of the brain's visual cortex that processes bodily movements with the section of the motor cortex thought to contain mirror (or empathy) neurons -- saw high levels of activity. It suggests that the brain couldn't compute the incongruity between the android's human-like appearance and its robotic motion. In the other experiments -- when the onscreen perfomer looks human and moves likes a human, or looks like a robot and moves like a robot -- our brains are fine. But when the two states are in conflict, trouble arises. ""The brain doesn't seem tuned to care about either biological appearance or biological motion per se,"" said Saygin, assistant professor of cognitive science at UC San Diego. ""What it seems to be doing is looking for its expectations to be met -- for appearance and motion to be congruent."" In the paper, published in the journal Social Cognitive and Affective Neuroscience, the team writes, ""as human-like artificial agents become more commonplace, perhaps our perceptual systems will be re-tuned to accommodate these new social partners."" ""Or perhaps, we will decide it is not a good idea to make them so closely in our image after all."" See Also:"
803,https://www.wired.com/2011/04/are-you-there-skynet-its-me-kristen/,Wired,2011,4,20,349.0," It seems like I’m always running around frantically before any big holiday trying to get things in order. I’m a last minute Christmas shopper, guilty of paying hefty overnight fees for shipping. I’m always realizing one or two days after Valentine’s Day that I bought all kinds of heart shaped candy sprinkles for decorating cookies – cookies that I forgot to make in time for the holiday. Easter is coming up – and I’m planning on making Easter chocolates for a celebration with friends – assuming I don’t lose track of time and forget to buy all the ingredients I need before Sunday. But this time, THIS TIME, I think I can be on time for something! See, Judgement Day was SUPPOSED to be on August 29, 1997 – with Skynet becoming self-aware on August 4, 1997. Ye–aah, but I was really busy planning my wedding and didn’t get a chance to make my “I <3 AI’s!” sugar cookies, like I had planned. But thanks to Sarah Connor I got a reprieve! – Judgement day was now scheduled to occur on July 25, 2004. Un —fortunately that was the year we started looking to buy a house, and  you know what it’s like, trying to navigate the maze of ARMs, 30 year fixed rates, points, home inspections, escrow – well, the time just slips through your fingers, it does. However – it looks like I’ve still got some time to make my oil can shaped sugar cookies – apparently  Judgment Day is scheduled for tomorrow since Skynet became self-aware yesterday! (Imaginary PA hands me a piece of paper with a bulletin on it) Oh man, according to this important update, tomorrow’s Judgment Day is a result of an event in The Sarah Connor Chronicles. Ooh. TV. I think that makes it suspect as canon – does it? Or is one temporal alteration the same as another whether it’s on the big screen or small screen? Eh, whatever, I’m going to bake these cookies anyway and eat them all myself. Why not? We’re all going to die tomorrow. I think."
804,https://www.wired.com/2011/04/robot-scientist-language/,Wired,2011,4,13,654.0," After an update to its software, a robot scientist has recycled its previous research to make a new biological discovery. Named Adam, the van-sized robot came to scientific fame after autonomously investigating gene function in yeast. Those findings anticipated an era when computers wouldn't just be research tools, but researchers. For its latest feat, described April 13 in the Journal of the Royal Society Interface, Adam revisited its original work after receiving new information about yeast growth rates. Thus refreshed, it found that crippling some enzymes could make yeast grow faster. The findings contradict current models of yeast growth. More fundamentally, they demonstrate how to produce data for optimal use by robot scientists like Adam. “The biology is interesting, but how the data was reused to answer new questions is what really excites me,” said computer scientist and biologist Ross King of Aberystwyth University in Wales, who led Adam's development. While computers have been ubiquitous in research for decades, Adam and Eve, its newer King-designed companion, are two of only a handful of machines able to use artificial intelligence do science. They suggest hypotheses, design experiments, carry them out and analyze the data. In 2009, Adam discovered the genes responsible for “orphaned” yeast enzymes, the origins of which were previously unknown. All of the data Adam generated, as well as the steps it took to arrive at its conclusions, were neatly curated and stored for future investigations. The original data's carefully crafted representation -- with everything from words like ""trial"" and ""experiment"" to protocols and results translated into computer language -- was the key to Adam's new discovery, King explained. The latest work will in turn be reused. And to make all research useful, the language of science needs to be simplified and formalized, said King. By standardizing the meanings, symbols, results and data organization used in studies, King thinks science could become a more-efficient, machine-readable enterprise. “All scientific knowledge stands on the shoulders of giants. What we’re arguing is to make doing that a bit easier by formalizing the language and reporting of science,” he said. “Unlike the typical human approach, where what we’ve done is often not clear even to ourselves, we’d make everything explicit. It goes a long way in removing ambiguity and the need to redo experiments.” 'We probably shouldn’t try to standardize everything, or we'd see the extinction of ideas.'Computational biologist Andrey Rzhetsky of the University of Illinois, who wasn’t involved in the work, was impressed by Adam's new study. “It’s a big deal that it can not only do science, but reuse its prior work almost automatically,"" said Rzhetsky. However, he struck a cautionary note about data formalization. While it could be valuable, converting the rules of even a single field may impossible. “The real world is always changing. If you try to capture and make a permanent vision of the world, it won’t work. The standards will have to change,” Rzhetsky said. Eliminating the diversity of language used in science may also lead to larger problems. “Different protocols and systems and languages are the essence of scientific innovation. When everyone speaks the same language, that’s bad,” Rzhetsky said. “It’s like when you lose genetic variability, you see extinction. We probably shouldn’t try to standardize everything or we’d see the extinction of ideas.” The debate over standardizing science will continue as artificial intelligence continues to improve. In the meantime, King’s team is preparing to publish recent work completed by the newer robot Eve, which is studying drugs used to treat malaria, Chagas disease and other neglected tropical scourges. That publication, however, is being delayed by issues beyond the ken of artificially intelligent scientists. “We don’t want it to be exploited for profit by others,” King said. “At this point, the intellectual property issues are holding us up.” Image: Foreground, Ross King, leader of the team that created Adam, stands in front of the robot. (Aberystwyth University) See Also:"
805,https://www.wired.com/story/david-eagleman-the-human-brain-runs-on-conflict/,Wired,2011,4,7,766.0," This article was taken from the May 2011 issue of Wired magazine. Be the first to read Wired's articles in print before they're posted online, and get your hands on loads of additional content by subscribing online. Throughout the 60s, pioneers in artificial intelligence worked late nights trying to build simple robotic programs capable of finding, fetching and stacking small wooden blocks in patterns. It was one of those apparently simple problems that turn out to be exceptionally difficult, and it led AI scientists to think: perhaps the robot could solve the problem by distributing the work among specialised subagents -- small computer programs that each bite off a piece of the problem. One computer program could be in charge of finding, another could fetch, another could solve stacking. This idea of subagents did not solve the problem entirely -- but it brought into focus a new idea about the working of biological brains: the possibility that our minds may be a vast collection of interconnected subagents that are themselves mindless. AI pioneer Marvin Minsky wrote, ""Each mental agent by itself can only do some simple thing that needs no mind or thought at all. Yet when we join these agents in societies -- in certain very special ways -- this leads to intelligence."" Within this framework, thousands of little minds are better than a single large one. The society-of-mind framework was a breakthrough, but, despite initial excitement, a collection of experts with divided labour has never yielded the properties of the human brain. It is still the case that our smartest robots are less intelligent than a three-year-old child. Why? I suggest the missing factor is competition among experts who all believe they know the right way to solve the problem. In a factory, each worker is an expert at one small task. In contrast, parties in a parliament hold differing opinions about the same issues. Brains are like parliaments. They are built of multiple, overlapping experts who compete over how best to proceed. This is why you sometimes find yourself arguing with yourself -- a seemingly illogical feat that our current computers do not attempt. The human brain runs on conflict. When someone offers you chocolate cake, you are presented with a dilemma: some parts of your brain have evolved to crave sugar, while others care about potential consequences, such as a bulging belly. Part of you wants the cake and part of you tries to muster the will to refuse it. The final vote of your inner parliament determines which party controls your reaction. Because of these internal multitudes, biological creatures are conflicted, a term that could not be sensibly applied to an entity controlled by a single program. Your car cannot be conflicted about which way to turn: it has one steering wheel commanded by one driver, and it follows directions without complaint. Brains, on the other hand, can be of two minds, and often many more. There are several little sets of hands on the steering wheel of our behaviour. Consider this lab experiment: if you put both food and an electric shock at the end of a pathway, a rat will pause a certain distance from the end. It begins to approach but withdraws when it receives a shock; it begins to withdraw but finds the courage to approach again; and so on. It oscillates, conflicted. If the rat is connected to a Newton meter, you can measure the force with which it advances towards the food and retreats from the electric shock. The rat pauses at the point where the two forces are equal, where the push matches the pull. Competing factions typically share the same goal but often have different ways of going about it. Just as Labour and Tory MPs both love their country but have different strategies for steering it, so the brain has competing factions that all believe they know the right way to solve problems. When I was a child, we thought we would all have robots that would bring us food and clean our clothes and converse with us. But something went wrong with AI, and the only robot in my home is a moderately dim-witted, self-directing vacuum cleaner. Artificial intelligence has become stuck because it has so far not adopted the idea of a democratic architecture. Although your computer is built of thousands of specialised systems, they are too polite. They never collaborate or argue. I suggest that conflict-based, democratic organisation -- which I call a team-of-rivals architecture -- is the best route to a fruitful new age of biologically inspired machinery."
806,https://www.wired.com/story/ai-drivers-not-wanted/,Wired,2011,1,28,1718.0," This article was taken from the February 2011 issue of Wired magazine. Be the first to read Wired's articles in print before they're posted online, and get your hands on loads of additional content by subscribing online. Crash avoidance and automatic parking are just the start. Tomorrow's cars will be brains on wheels. The 300km trip from San Francisco to Lake Tahoe can be a frustrating slog in the wintertime traffic on Interstate 80. Speeds in the fast lane swing from 120kph to 50kph for no discernible reason. Slow, fast, faster, slow. Hit rush hour in Sacramento -- or Donner Pass on a snowy day -- and you'll see the speedometer's needle tapping the 15kph mark like a woodpecker on a tasty log. Driver of manual cars collapse with dead legs on the side of the road; even the automatic-enabled P-R-N-D crowd can be seen massaging their sore knees at roadside burger joints and woodsy rest stops. Mercedes-Benz S550, a luxury car that's currently justifying the pants off its $100,000 (£63,600) price tag. We're bopping through the same unpredictable range of velocities as everyone else, but I haven't touched a pedal in hours. The Benz is doing most of the driving, keeping us a comfortable distance from the cars ahead with its next-gen cruise-control system. The core of the set-up is a pair of radar emitters -- a narrow-banded one that pings vehicles up ahead, and a wide-angle unit that watches the rest of the traffic and keeps a sharp eye out for road-hogs weaving into our lane. All that locational info is fed to the car's vehicle-control unit, a computer that smoothly modulates the brakes and throttle to keep us moving with traffic. The driver specifies a maximum speed, and the car does its best to hit that number -- without hitting anything else. The first time you let the car do its thing is a magically scary experience. You see the cars ahead closing at a rate that activates the ""I'm going too fast"" reflex; your foot hovers over the brake pedal as your frontal cortex strenuously attempts to override your survival instinct. Cognitively, you know that this system has been meticulously tested by obsessive German engineers who would never let an unsafe car cross the threshold of their shiny factory. And then, just as you're beginning to contemplate the various safety regulations that the car must have complied with on its way to the dealership, you feel yourself slowing -- gently, autonomously, in perfect control. The cold cannonball in your stomach turns back into warm muscle, and you chuckle softly to yourself for being so silly as to doubt such a well-engineered system. Getting used to these autonomous systems takes time. It turns out that we have to adapt to the machines more than they have to adapt to us. Cruise control is just the most obvious sign of a particular kind of AI that has been accelerating for decades. Think about it. Anti-lock brakes know when to back off the pedal. Airbags know that you just smacked into something. Stability control knows that you just overcooked your Volvo into that hairpin and need a little help to stay out of the ditch. Your satnav system knows where you are, your wipers know it's raining, that annoying seat-belt chime knows you're flouting the law. In short, modern cars are loaded with sensors and computing power. The 2011 Chevy Volt, for example, runs on some ten million lines of code -- more than Lockheed Martin's new F-35 Joint Strike Fighter. The marquee innovation that made intelligent cruise control possible is the drive-by-wire throttle: the introduction of motor skills to the automotive body. The throttle is a flap that lets air and fuel enter the engine. In the conventional setup, it's linked to the accelerator by a thin metal cable threaded through a grooved wheel. This went unchanged for decades -- but many newer cars have done away with the cable. Instead, there is a sensor on the accelerator and a small electric motor on the throttle. Step on the accelerator and an electrical impulse travels to the computer, telling it how far the pedal is depressed; the computer then tells that little electric motor how wide to open the flap. Electronics and software are mediating the whole process. Voilà: you're driving by wire. Of course, by-wire technology isn't just for throttles. The same exquisitely sensitive actuation systems are finding their way into brakes and steering as well. And where there are electronically controlled systems, there are sensors and software and processors that can command them. In other words, by-wire technology is paving the way to truly smart cars. Drive-by-wire didn't start in the automotive industry. It's a descendant of an aerospace technology called, yes, fly-by-wire. The first aircraft to fly with it -- a Canadian fighter jet called the Avro Canada CF-105 Arrow -- took off in 1958. Most of the pilot's controls, from the elevators to the rudders, were triggered electronically. The advantages -- instantaneous response and lighter weight -- were compelling. Within a few decades, many commercial airliners were using fly-by-wire technology. It made every aircraft from the Concorde to the Boeing 777 possible and was integral to improving autopilot systems -- including those that can land a plane. It's nice to have Captain Sullenberger (AKA The Hero of the Hudson) on board, but he's only needed on special occasions. The by-wire throttle first made its way into cars in 1988, in the BMW 750iL, and it now makes radar-assisted cruise control possible in any number of Fords, Volvos, Jaguars and Mercedes. Some hybrids rely on it to switch nimbly between petrol-driven and electric power. But drive-by-wire technology has applications beyond the car-pool lane that conjure scenes from a sci-fi future: self-driving vehicles that promise the end of traffic jams and a major reduction in battlefield casualties. In 2004, Darpa, the US Defense Department's research arm, challenged the big brains of the world to come up with a car that could navigate a complicated desert course with no human input. Employing technologies closely related to our smart cruise control -- electronic eyes, computer brains and drive-by-wire legs -- 15 teams vied for the million-dollar prize. None finished. But that didn't stop Darpa from throwing down the gauntlet again. It hosted another challenge the following year, and five of the 23 teams finished. Moore's law hits the road. Some 212 kilometres and nearly seven hours after the second Grand Challenge began, the first car across the finish line was a self-driving Volkswagen Touareg named Stanley -- one of the smartest cars ever built. Sebastian Thrun headed up the Stanford team that trained Stanley for its victory and ran head-on into the primary obstacle facing any self-driving car. ""You literally can't even count the number of different situations a driver encounters,"" Thrun says. That's why his team didn't try to code a solution for every situation. They taught Stanley how to drive the old-fashioned way. ""We took the car out on the road and logged every time it made a mistake."" Back at the lab, Thrun's team used this data to replay failures and challenges over and over again in the car's software mind as it simulated different solutions to each puzzle. Every time it failed or succeeded, it learned why, and improved. Thrun has since taken a post at Google, where he and a team of engineers are testing a small fleet of autonomous Toyota Priuses on the streets and highways of the densely populated San Francisco Bay Area. (Someone sits behind the wheel of the Google cars, ready to take control if necessary.) Of course, you can't just go out and purchase a robo-vehicle today. Hell, you're probably still apprehensive about radar-assisted cruise control. It turns out that the US federal agency charged with ensuring auto safety -- the National Highway Traffic Safety Administration (NHTSA) -- shares that fear. The NHTSA isn't going to green-light self-driving cars without a lot more trials and oversight. ""It's not at the point of being sufficiently reliable for the consumer market,"" says NHTSA spokesperson Eric Bolton. Still, the autonomous systems migrating into vehicles are impressively reliable -- they make far fewer errors than humans. Plus, there's no convincing evidence that people will let down their guard when a robot is doing the driving for them, a phenomenon that's known as risk compensation. ""Do they engage in risky behaviours -- texting, applying makeup, shaving?"" asks Jim Sayer, who investigates real-world driver behaviour at the University of Michigan Transportation Research Institute. ""We never see that."" The real problem arises when millions of humans are confronted with autonomous systems -- and some of them freak out. That seems to be what happened recently with some Toyota cars. In a number of well-publicised cases, drivers thought that the electronic throttle was improperly accelerating. It turned out that most of the incidents were caused by an all-too-mechanical flaw in the floor-mat or in the accelerator pedal's design -- or by driver error. Avoiding those errors is a tricky dance that takes time to learn. Consider the new self-parking technology, brought to the US market by Lexus and since adopted by other carmakers. On a busy city street, I pull a Lincoln MKT (borrowed, again) alongside an empty space and hit a button labelled ""auto |p|"". A two-line LCD on the instrument cluster explains what to do: ""Select reverse and take your hands off the wheel."" I follow its commands and the car takes control, whipping the wheel around and backing into the space faster than I would ever attempt. I tell myself to relax, to let go, that this SUV has more sensors than a satellite -- a beeping proximity sensor in the back, a rear-facing camera, radar sensors that inform its own magic cruise control. And just as I surrender myself to the future, the Lincoln slams into the car behind me. A rep from Lincoln later tells me that you're supposed to work the brake as the car steers itself. And yeah, that two-line display never suggested I take my foot off the pedal; I guess I just assumed that ""auto park"" meant, you know, auto park. This machine-man language barrier is something we're really going to have to work on."
807,https://www.wired.com/story/david-rowan-predictions-2011/,Wired,2011,1,11,907.0," I write The Digital Life, a monthly tech column in our sister Conde Nast magazine, GQ. This is my column from last month's issue (dated January). To subscribe to GQ, click here. At this time of year, technology columnists are traditionally expected to stake their reputations on predicting crazily surprising developments for the months ahead. Well, I'm not falling for that one, just so this time next year you get to mock my ill-judged forecasts of fusion-powered jetpacks or robotic sex slaves. It's just too tough to make predictions, as Yogi Berra once said - especially about the future. So I've swapped my crystal ball this year to look instead at the data -- and have come up with half a dozen trends on my mind that the hard, scientific evidence says simply have to be big in 2011. And if I'm wrong? Well, GQ are printing this column in a special futuristic ink that disappears after six months -- so when you come back to fact-check my ass in a year's time, all you'll see is a photo of a robotic sex slave. 1 - Augmented reality grows up: After the hype and the lame advertising stunts, we're finally going to find genuine uses for augmented reality on your smartphone. Download the Plane Finder AR app, for instance, and you can point your iPhone camera at the sky and discover individual aircrafts' flight numbers, speed, altitude and distance. That's tons of fun, even if the paranoid are claiming it's the terrorist's wet dream. Then there are the applications being developed for Layar, on Apple and Android devices, which superimpose a layer of information on what you see through the phone's camera. My favourite: safety information about local buildings, which the Rotterdam fire brigade call up whenever they are called to control a fire. Genius. 2 - Rentalship taking over from ownership: You're streaming music through Spotify or Zune, and accessing movies through internet-based ""rental"" services such as Blinkbox -- so the need to acquire the physical item becomes an irrelevance. Now that attitude is moving to non-digital goods, and web-based services will proliferate to help you borrow things you don't, if you're honest, need to own. Need use of a car? Borrow one through Zipcar or Streetcar. Girlfriend needs a designer handbag? Rent one from BagBorrowOrSteal.com. Building a bookshelf? SnapGoods will help you rent a neighbour's drill for a day. And lo, consumerism takes a thump. 3 - Social TV: The next billion televisions will be internet connected. That's going to turn TV viewing into a much more social experience -- with comments from your friends' tweets and Facebook updates not only adding instant feedback to real-time broadcasts, but also helping program your set to record the shows it thinks you are most likely to enjoy, based on what your friends enjoy. Steve Jobs is naturally hoping to dominate this revolution with his Apple TV device, but he'll face competition from the likes of Boxee, as well as new services such blinkbox and Starling.tv, which lets viewers chat, play and generally interact with one another while watching Mad Men. Farewell, then, linear TV -- and if you do know any schedulers, be nice as they scramble for new careers. 4 - Epic visualisations will turn data into information: We're in a world ever more saturated with data: Eric Schmidt, the Google boss, recently estimated that we're now generating five exabytes (or billion gigabytes) every two days, which is the total amount we produced in all human history up until 2003. So how do we make sense of all these zeroes and ones? Simple: we develop visualisation tools that help us spot the patterns, and produce stunningly sexual images at the same time. Already smart design-led companies such as ITO are visualising real-time transport data so you can save precious minutes on your journey home; and the clever people at McLaren are using their Formula 1 expertise to map airports with sensors so the resulting screen images let you predict exactly when your plane will start taxiing. Over the next year, expect dataviz porn to go mainstream as more industries use it to make sense of rough numbers. 5 - Money goes mobile: It's your music player, your video camera and your geolocation-based check-in tool -- so why shouldn't your smartphone also be your wallet? This year mobile payments will take a leap closer to mainstream acceptance, as services such as Zong and Boku convince us that it's safe and convenient to spend money using your phone number. The social networks too are moving fast into financial services -- from Facebook Credits to Twitpay, which lets you make payments via Twitter. In Kenya, already 10 million people are using a service called M-PESA to transfer cash -- so it's no wonder the traditional banks are getting nervous. Hurrah: finally the bank manager's days are numbered. 6 - Artificial intelligence finally gets smart: OK, on behalf of the futurists' union I accept that we may have oversold this one a few times in the past. But trust me, this year AI will take a big leap towards cracking the ""is it a machine or a person?"" Turing test. From cars packed with ""intelligent"" safety features so they autonomously adjust the steering, to advertising hoardings that target you personally based on your gender and estimated age, we're going to find machines understanding our desires and needs more than ever."
808,https://www.wired.com/story/artificial-intelligence/,Wired,2011,1,10,2092.0," This article was taken from the February 2011 issue of Wired magazine. Be the first to read Wired's articles in print before they're posted online, and get your hands on loads of additional content by subscribing online. Artificial intelligence is here. In fact, it's all around us. But it's nothing like we expected. Diapers.com warehouses are a bit of a jumble. Boxes of dummies sit above crates of romper suits, which rest next to cartons of baby food. In a seeming abdication of logic, similar items are placed across the room from one another. A person trying to figure out how the products were shelved could well conclude that no form of intelligence had a hand in determining what went where. But the warehouses, located across the US, aren't meant to be understood by humans; they were built for bots. Every day, hundreds of robots course nimbly through the aisles, delivering items to flesh-and-blood packers on the periphery. Instead of organising the warehouse as a human might -- by placing like products next to one another, for instance -- Diapers.com's robots stick the items in various aisles throughout the facility. To fill an order, the first available robot simply finds the closest requested item. The storeroom is an ever-shifting mass that adjusts to constantly changing data, like the size and popularity of merchandise, the geography of the warehouse, and the location of each robot. Set up by Kiva Systems, which has outfitted similar facilities for Gap, Staples and Office Depot, the system can deliver items to packers at the rate of one every six seconds. The Kiva bots may not seem very smart. But they represent a new forefront in the field of artificial intelligence. Today's AI doesn't try to recreate the brain. Instead, it uses machine learning, massive datasets, sophisticated sensors and clever algorithms to master discrete tasks. The Google global machine uses AI to interpret cryptic human queries. Credit-card companies use it to track fraud. And the financial system uses it to handle billions of trades every day (with only the occasional meltdown). This explosion is the ironic pay-off of the seemingly fruitless decades-long quest to emulate human intelligence. That goal proved so elusive that some scientists lost heart and many others lost funding. There was talk of an AI winter. But even as the traditional dream of AI was freezing over, a new one was being born: machines built to accomplish specific tasks in ways that people simply never could. At first, there were just a few green shoots pushing up through the frosty ground. But now we're in full bloom. Welcome to the AI summer. Today's AI bears little resemblance to its initial conception. The field's trailblazers in the 50s and 60s believed success lay in mimicking the logic-based reasoning that human brains were thought to use. In 1957, the AI crowd confidently predicted that machines would soon be able to replicate all kinds of human mental achievements. But that turned out to be wildly unachievable, in part because we still don't really understand how the brain works, much less how to recreate it. So during the 80s, graduate students began to focus on the kinds of skills for which computers were well suited, and found they could build something like intelligence from groups of systems that operated according to their own kind of reasoning. ""The big surprise is that intelligence isn't a unitary thing,"" says Danny Hillis, who cofounded Thinking Machines, a company that, from 1982 to 1994, made massively parallel supercomputers. ""What we've learned is that it's all kinds of different behaviours."" By using probability-based algorithms to derive meaning from huge amounts of data, researchers discovered that they didn't need to teach a computer how to accomplish a task; they could just show it what people did and let the machine figure out how to copy that behaviour under similar circumstances. They used genetic algorithms, which comb randomly generated chunks of code, skim the highest-performing ones, and splice them together to spawn new code. As the process is repeated, the evolved programs become amazingly effective, often comparable to the output of the most experienced coders. MIT's Rodney Brooks also took a biologically inspired approach to robotics. His lab programmed six-legged buglike creatures by breaking down insect behaviour into a series of simple commands -- for instance, ""If you run into an obstacle, lift your legs higher."" When the programmers got the rules right, the gizmos could figure out for themselves how to navigate even complicated terrain. The fruits of the AI revolution are now all around us. Once researchers were freed from the burden of building a whole mind, they could construct a rich bestiary of digital fauna, which few would dispute possess something approaching intelligence. Google cofounder Larry Page says: ""If you told somebody in 1978, 'You're going to have this machine, and you'll be able to type a few words and instantly get all of the world's knowledge on that topic,' they would probably consider that to be AI. That seems routine now, but it's a really big deal."" Even formerly mechanical processes such as driving a car have become collaborations with AI systems. ""At first it was the automatic braking system,"" Brooks says. ""Now you're starting to get automatic parking and lane-changing."" Indeed, Google has been developing and testing cars that drive themselves with only minimal human involvement; by October 2010, they had already covered 225,000 kilometres. In short, we are engaged in a permanent dance with machines, locked in an increasingly dependent embrace. And yet, because the bots' behaviour isn't based on human thought processes, we are often powerless to explain their actions. Wolfram Alpha, the website created by scientist Stephen Wolfram, can solve many mathematical problems. It also seems to display how those answers are derived. But the logical steps that humans see are completely different from the website's actual calculations. ""It doesn't do any of that reasoning,"" Wolfram says. ""Those steps are pure fake. We thought, how can we explain this to one of those humans out there?"" The lesson is that our computers sometimes have to humour us, or they will freak us out. Eric Horvitz -- now a top Microsoft researcher and a former president of the US Association for the Advancement of Artificial Intelligence -- helped build an AI system in the 80s to aid pathologists in their studies, analysing each result and suggesting the next test to perform. There was just one problem -- it provided the answers too quickly. ""We found that people trusted it more if we added a delay loop with a flashing light, as though it were huffing and puffing to come up with an answer,"" Horvitz says. But we must learn to adapt. AI is so crucial to some systems (such as the financial infrastructure) that getting rid of it would be a lot harder than simply disconnecting HAL 9000's modules. ""In some sense, you can argue that the science-fiction scenario is already starting to happen,"" Danny Hillis says. ""The computers are in control, and we just live in their world."" Wolfram says this conundrum will intensify as AI takes on new tasks, spinning further out of human comprehension. ""Do you regulate an underlying algorithm?"" he asks. ""That's crazy, because you can't foresee in most cases what consequences that algorithm will have."" In earlier days, humanists feared the ramifications of thinking machines. Now the machines are embedded in our lives, and those fears seem irrelevant. ""I used to have fights about it,"" Brooks says. ""I've stopped having fights. I'm just trying to win."" A good session player is hard to find, but UJAM is always ready to rock. The web app doubles as a studio band and a recording studio. It analyses a melody and then produces sophisticated harmonies, bass lines, drum tracks and more. Before UJAM's AI can lay down accompaniment, it must figure out which notes the user is singing or playing. Once it recognises them, the algorithm searches for chords to match the tune, using a mix of statistical techniques and musical rules. The rules-based module then uses its knowledge of Western music to narrow the chord options to a single selection. The service is still in alpha, but it has attracted 2,500 testers who want to use AI to explore their musical creativity. As UJAM gathers more data on users' preferences and musical tastes, this info is fed back into the system, improving its performance. Credit-card fraud costs UK merchants and credit-card companies more than £500 million a year. That figure would be much higher without the use of computer surveillance systems to monitor every transaction. One of the most proven antifraud systems is FICO's Falcon Fraud Manager, which keeps tabs on more than four billion transactions a month worldwide and uses lightning-fast neural networks to scan for suspicious purchase patterns. Neural networks were originally designed to mimic human grey matter. Over time, however, the technology has become a basic building block of many computer systems. The networks typically consist of layers of interconnected ""neurons"", each of which produces a signal only when its input exceeds a certain threshold. Though the individual neurons are simple, the net as a whole can learn to recognise complex patterns. The Falcon system specialises in detecting things a human might never notice. For example, if you use your card to buy a tank of petrol and then go directly to a jewellery store, your account will almost surely be flagged, especially if you're not a person who buys a lot of bling. The reason? Over years of correlating variables, testing and learning, the system has noticed that a criminal's first stop after stealing a credit card is often a petrol station. If that transaction goes through, the thief knows the card hasn't yet been reported as stolen and heads off on a spending spree -- often at some high-priced retailer. A human brain gets visual information from two eyes. Google's artificial intelligence gets it from billions -- through the camera lenses of smartphones. The company collects images from users of Google Goggles, a mobile app that lets you run web searches by taking pictures. Snap a barcode and Goggles will shop for the item's best price. Take a picture of a book and it will link you to, say, a Wikipedia page about the author. Photograph the Eiffel Tower and it will give you historical background. At the core of the service is Google's Superroot Server, software that co-ordinates the efforts of multiple object-specific recognition engines. There's one for text, one for landmarks, one for corporate logos, and so on. When an image arrives, Superroot sends it to each of these backend engines, which in turn use a variety of visual-recognition techniques to identify potential matches and compute confidence scores. Superroot then applies its own algorithm to decide which results, if any, to report back to the user. To the human eye, an X-ray is a murky, lo-res puzzle. But to a machine, it's a dense data field. No wonder AI is so useful in medicine. Bartron Medical Imaging's software aggregates hi-res image data from X-rays, MRIs, ultrasounds and CT scans, then groups together biological structures that share hard-to-detect similarities. Model trains are easy to keep track of. But building a model to run real trains is a complex undertaking. So when the US Norfolk Southern Railway decided to install a smarter system to handle its sprawling operation, it brought in a team of algorithm experts from Princeton University. What they got was the Princeton Locomotive and Shop Management System (Plasma), which used an algorithmic strategy to analyse Norfolk Southern's operations. Plasma tracks thousands of variables, predicting the impact of changes in fleet size, maintenance, transit time and other factors on real-world operations. The key breakthrough was making the model mimic the complex behaviour of the company's despatch centre in Atlanta. ""Think of the despatch centre as one big, collective brain. How do you get a computer to behave like that?"" asks Warren Powell, a professor at Princeton's Operations Research and Financial Engineering department. The model that Powell and his team came up with was a kind of AI hive mind. Plasma uses a technology known as approximate dynamic programming to examine mountains of historical data. The system then models the despatch centre's collective human decision-making and even suggest improvements. For now, Plasma is helping Norfolk Southern decide what its fleet size should be -- humans are still in control of running the trains. At least we're still good for something."
809,https://www.wired.com/2010/11/digital-organism-evolution/,Wired,2010,11,29,609.0," One hundred and fifty-one years after the publication of On the Origin of Species, digital creatures have evolved to communicate like fireflies in a computer program that blurs the boundaries of life. Recorded in line-by-line detail, their development in a software platform called Avida may provide insight into biological behavior and inspiration for the design of distributed computer networks. ""Evolutionary programs have been around for a while, but we haven't seen them applied to distributed computing,"" said computer scientist Philip McKinley of Michigan State University. Synchronized communication can be ""seen in the natural world. But in Avida, we can go back to how and why it evolved. We can see the key points that allowed this relatively complex behavior to emerge."" The new synchronization findings, made by McKinley and fellow MSU computer scientist David Knoester, were published November 18 in Artificial Life. Inside the program, developed in the early 1990s at the California Institute of Technology and refined at MSU's Digital Evolution Laboratory, digital organisms called Avidians take the form of self-replicating code. Their genomes are written in assembly language and stored in separate regions of memory, executed again and again at electronic speeds. Programmers set the parameters of mutation and natural selection, and evolutionary principles manifest themselves in silico. ""We like to say 'it's not a simulation of evolution, it's evolution.' The difference is that these are computer programs,"" McKinley said. In a previous and well-known study, researchers supported a key tenet of evolutionary theory by demonstrating how easily complexity could emerge in Avidians through incremental changes in simple, existing functions. McKinley and Knoester specialize in organismal interactions: How complexity emerges not only in individuals, but also in groups. Their earlier work examined the evolution of collective perception, cooperation and decision making. In the new study, however, they emphasized communication and selected for groups of Avidians that best synchronized their flashing with others. Fireflies, which coordinate their blinking across distances spanning miles, are the best-known synchronized communicators of the biological world. How they do it isn't fully understood, but Knoester said ""it was literally a three- or four-line change"" in Avida. Crucial to Avidian synchronization was the handling of the computational version of ""junk DNA,"" or genetic code that seems to have no apparent purpose. In biology, junk DNA is now appreciated as having crucial regulatory functions. In the Avidians, individuals evolved to change their flash timing by adjusting the speed at which ""junk"" instructions were executed. McKinley and Knoester don't think that fireflies necessary synchronize the same way, as Avida provided a computational and likely different route to the same outcome. More importantly, it gave the researchers algorithms they would not have otherwise imagined. The algorithms could inspire functional code beyond Avida's confines. ""Avidians build network topologies. What sort of topologies do they come up with that are robust to damage, if the routing nodes fail?"" Knoester said. ""We're also collaborating with a professor in the electrical engineering department who works on robotic fish. We're not really interested in schooling; we want robots to track oil slicks, to monitor water quality. To do those things, you need to stay connected."" As for the upper limit on Avidian complexity, ""I'm not sure we know yet,"" Knoester said. Video: Organisms in Avida, a software platform for artificial life, running their genomic instructions. Eventually they evolve to flash in synchrony, like fireflies./Philip McKinley and David Knoester. Frontpage image: Terry Priest, Flickr. See Also: Citation: ""Evolution of Synchronization and Desynchronization in Digital Organisms."" By David B. Knoester and Philip K. McKinley. Online publication, November 18, 2010. Brandon's Twitter stream, reportorial outtakes and citizen-funded White Nose Syndrome story; Wired Science on Twitter."
810,https://www.wired.com/2010/07/robot-scientist/,Wired,2010,7,22,856.0," Future science historians will mark the beginning of the 21st century as a time when robots took their place beside human scientists. Programmers have turned computers from extraordinarily powerful but fundamentally dumb tools, into tools with smarts. Artificially intelligent programs make sense of data so complex that it defies human analysis. They even come up with hypotheses, the testable questions that drive science, on their own. At the University of Wales at Aberystwyth, Ross King's program ""Adam"" designs and runs genetics experiments. At Cornell, Hod Lipson's Eureqa finds equations to fit data, attaining Newton's insights in a single afternoon. University of Chicago mathematical biologist Andrey Rzhetsky designs programs less glamorous but equally powerful, able to analyze millions of papers at once. In the future, the human scientist's job may be ""to do the programming, and make sure the robot has enough reagents,"" said Rzhetsky, only partly tongue-in-cheek. Wired.com talked to Rzhetsky about the intersection of artificial intelligence and science. Wired.com: Why do scientists need artificially intelligent computer assistance? Andrey Rzhetsky: During Newton's time, a scientist could read everything that was published, at least in English. That's just not an option anymore. We can't deal with all this information. Wired.com: How have you used AI in your own work? Rzhetsky: In our paper on brain malformations in mice and humans, the program analyzed 368,000 full-text articles and 8,000,000 article abstracts in the PubMed database. That's something no human curator, or even a group of human curators, could ever do. In a program, it's possible. We made available a huge knowledge base and a tool for prioritizing genes and making hypotheses about associations between genes and phenotypes. A bunch of the predictions we made were followed up by our experimentally talented collaborators, and seem very reasonable. The problem is how to design a process to discover a good hypothesis, because it's expensive to test all possible hypotheses. That's where literature analysis and computational modeling can help. It prioritizes. Wired.com: So much published research isn't replicated. Isn't there a garbage-in, garbage-out problem? Rzhetsky: That's always a possibility, but good statistical analysis doesn't throw away data. Even with good data, you get a lot of noise. Even noisy data with false positives can be useful. Think about it as intelligence data. Obviously, when it's collected, there are lots of false positives. But when it's collected from multiple sources, compared and examined, it becomes more certain. Wired.com: Cornell's Hod Lipson designed a program that discovers equations to explain relationships between data. Researchers then have to figure out what the equations mean. It's like interpreting an oracle's pronouncements. Is that the role of the human in all this? Rzhetsky: It's an interesting question. I talk to electrical engineers who use genetic algorithms to design circuits, and the circuits end up being completely alien to humans. They're very robust, but designed in such a way that it's not obvious how to understand them. That's similar to what Lipson discovers: non-human logic. In Lipson's analysis, he wants to make it transparent and understandable to humans. I'm not sure that's necessary. Wired.com: Some scientists say that being able to crunch huge datasets makes hypotheses obsolete -- why worry about testing when you can find connections. You don't like that idea, though. Why not? Rzhetsky In the movie Memento, a man has only a short-term memory. Every 15 minutes has to reconstruct causal relationships. He observes people talking to him, and doesn't know who's a friend and who's a foe. That's my metaphor for abandoning hypothesis and context. There are a lot of approaches claiming you can reverse-engineer the world from the flow of data. With an infinite dataset, the statement probably gets close to truth. But I don't think it's true for individual datasets. Prior hypotheses and contextual knowledge need to be used. Wired.com: So is the role of human scientists to come up with hypotheses? Rzhetsky: The tools can come up with hypotheses, too. Wired.com: One of the great human abilities is to come up with insights that combine knowledge and speculation across disciplines. How could a program ever have those insights? Rzhetsky: One kind of creativity is combining old symbols in a new way. The best thinkers digest the experience of previous thinkers, and come up with their own syntheses. I would claim this is still in the space of symbolic reasoning and symbolic hypothesis generation. Wired.com: But wouldn't this require far more general artificial intelligence than the narrow, task-specific types we have now? Rzhetsky: Possibly. But you can think about the human brain as a collection of specialized tools. There's a tool for discerning vertical symmetrical patterns in noisy backgrounds in order to find predators, a tool to recognize faces, a tool to classify experiences as pleasant or unpleasant, and so on. I don't see why a tool that does several specialized tasks well can't be upgraded to something more comprehensive. See Also: Citation: ""Machine Science."" By James Evans and Andrey Rzhetsky. Science, Vol. 323 No. 5990, July 23, 2010. Brandon Keim's Twitter stream and reportorial outtakes; Wired Science on Twitter. Brandon is currently working on a book about ecological tipping points."
811,https://www.wired.com/2010/06/5-mash-ups-of-music-and-artificial-intelligence/,Wired,2010,6,5,952.0," If there is one thing computers do well, it’s math. All of music’s raw components — key, mode, melody, harmony and rhythm — can be expressed mathematically. As a result, computers can help people make music, even if they don’t know their elbow from an F clef. The following apps for computer, web browser and smartphone put the power of artificially intelligent music creation in your hands or let you hear music that was created or manipulated by machines. Without further ado: One of the most impressive demonstrations I’ve seen this year, uJam is the brainchild of longtime audio-software developers Peter Gorges and Axel Hensen and their celebrity partners Hans Zimmer (film composer for Dark Knight, Gladiator, Lion King) and Pharrell Williams (producer for Madonna, Shakira, Gwen Stefani). Their web app, yet to be released, crafts entire songs with precise accompaniment out of whatever the user whistles, hums or sings (or tries to sing) with an auto-tune feature to smooth out the rough spots. You can use this Flash-based software to record cover versions of popular songs, but the real magic lies in creating something from scratch, either with your voice or a musical instrument, in a multitude of styles. Even if you only have a single instrument on hand, or just your own whistling lips, uJam can turn your output into a number of other instruments, as Gorges demonstrates below by turning a simple recorder melody into a full-on guitar jam. Following the freemium model, uJam will be free to use on a basic level, with add-ons available for purchase. You can’t use the artificially intelligent Emily Howell software yourself. She belongs to her creator, University of Santa Cruz professor David Cope, who runs her on an ancient Power Mac 7500. However, you can catch her in concert from time to time at the university, when human players bring her compositions to life, in the iTunes music store, which sells her first album, From Darkness, Light, or in this embedded video. Howell, a descendant of Cope’s earlier Emmy software (shorthand for his term “Experiments in Musical Intelligence”), works by creating connections between various musical statements and tonal relationships in order to churn out musical responses to phrases Cope feeds in — most of them coming from Emmy’s earlier scores. The result is astounding to even the casual listener — rife with emotional complexity and deep textures that belie the music’s artificial origin. Microsoft, that dorkiest of companies, pioneered the concept of accompanying regular humans’ vocal warblings with artificially intelligent algorithms to create something that might, in some stilted universe, be called music. Its SongSmith software (check out RobertSongSmith, or Microsoft’s notoriously atrocious commercial below to see just how bad it can get), tends to produce songs that sound like they were made with a Casio keyboard’s auto-accompaniment setting. That said, it did manage to turn Metallica’s “Enter the Sandman” into an amusing disco number. SongSmith may have been first, but it’s very … well, Microsoft. Luckily, a new crop of artificially intelligent music makers has emerged since SongSmith launched early last year. The Swinger (source code only) adds a playful rhythm to any song, because as any 1930s music enthusiast can tell you, “it don’t mean a thing if it ain’t got that swing.” The Swinger is the work of The Echo Nest co-founder Tristan Jehan from the Hyperinstruments Group at MIT’s Media Laboratory. It’s designed not only for fun, but to demonstrate the power of The Echo Nest’s Remix API, which was also responsible for such gems as MoreCowbell.dj, a tool for adding cowbell and Christopher Walken samples to any song (as inspired by the legendary Saturday Night Live sketch), and DonkDJ, which transforms any MP3 into the Donk genre. Khu.sh’s “reverse karaoke” app LaDiDa has been turning heads with its earlier, mobile version of the uJam idea — an iPhone app that makes music out of whatever you sing in to it. I shot a video (below) of the co-creators demonstrating the app at SXSW and explaining how it works — by analyzing what you sing and creating accompaniment around it. And because making music is no fun unless other people hear it, LaDiDa includes plenty of social features for posting the song to your Facebook wall, tweeting it and so on. We covered Shimon, a robot marimba player created by the Robotic Musicianship Group at Georgia Tech’s Center for Music and Technology way back in 2008, but it bears mention again, having recently having made an appearance on The Colbert Report‘s ThreatDown list (video below). According to Colbert, Shimon combines two great threats to America — jazz and robots — but it’s no joke. Somewhat along the lines of Emily Howell, Shimon applies its own melodic, harmonic and rhythmic sensibilities to input from a human collaborator in order to create new, artificially intelligent music. The results are so impressive that we declared this and Georgia Tech’s other National Science Foundation-funded music robot, which plays the tabla drum, to have passed the musical Turing test (by having the potential to fool human listeners into thinking that its improvised responses to human-created music were those of another human). Although it’s clearly responsible for some of the worst songs ever recorded, Auto-Tune, claimed by its developer Antares Tech  to be “the largest-selling audio plug-in of all time,” has to be the world’s most widely implemented collision of the human voice and the computers trying to make sense of it. LaDiDa and uJam each rely on a version of the technology for lining up the human voice with their automatic accompaniments, and “Auto-Tune the News” employs it to hilarious effect, proving that Auto-Tune can be used for good as well as evil."
812,https://www.wired.com/2010/03/virtual-musicians-real-performances/,Wired,2010,3,2,1296.0," Ever wonder how Jimi Hendrix would cover Lady Gaga? The day is approaching when you should be able to find out. Musicians’ opportunities to sell their recordings may be drying up due to cultural shifts brought on by changing technology, but other aspects of technology are creating a promising new market for music: the licensing of the musical style or personality of recording artists. The concept goes well beyond basing the avatars in guitar-based videogames on famous performers, although the idea is similar. Using complex software, North Carolina’s Zenph Sound Innovations models the musical performances of musicians from Thelonius Monk to Rachmaninoff, based on how they played in occasionally old, scratchy recordings. Using that model, the company creates new recordings as they would be played by deceased musicians, if they were around to record with today’s equipment, to critical acclaim. And that’s just for starters. Venture capital firm Intersouth Partners led a $10.7-million round of Series A funding in the company in November, a move that saw former Intersouth venture capital partner Kip Frey take over as the company’s CEO. He told us on Monday that Zenph has ramped up to 15 employees in preparation for new releases in its series of re-recordings. Zenph also plans explore a variety of new markets, including licensing clear versions of muddy recordings to films and software that could eventually let musicians jam with virtual versions of famous musicians. Picture an Eric Clapton plug-in that reinterprets your solo to sound like it was played by “Old Slowhand” himself. Zenph’s specially designed robotic pianos take high-resolution MIDI files created by software that simulates the style of classical and jazz performers from days gone by and them into sound by literally depressing the keys using between 12 and 24 high-resolution MIDI attributes. So far, the company offers new albums by legends including Art Tatum, Sergei Rachmaninoff and Glenn Gould, and up next is jazz pianist Oscar Peterson. These robotic pianos have wowed crowds in “live” settings at Carnegie Hall, Steinway Hall and on the Live from Lincoln Center show, with their note-for-note renditions of performances of the past. Zenph plans to take them on three tours later this year. Its engineers have nearly completed work on a playerless double bass, and plans to work on the saxophone model next, with the ultimate goal of creating every instrument in a typical jazz band — then guitar, and so on. However, due to the complexity of playing those instruments, Zenph plans to simulate them being played in software and reproduce the sound with speakers (updated). Hear virtual Sergei Rachmaninoff play a composition by the real Rachmaninoff (1873 – 1943): As things stand now, Zenph’s technology looks at actual old recordings to find out how a performer played a certain song, and is not capable of figuring out how a musician would play a new part. “We hope — but we can’t demonstrate today — that after we’ve done several re-performances of a given artist, we will understand enough about that individual’s musical style to be able to suggest how that style might manifest itself in the performance of a work that the artist never actually performed,” said Frey, clarifying that today Zenph’s software only reproduces performances, it doesn’t create them. Of course, causing a musician’s musical style to inhabit a device would require a new type of licensing deal. If Courtney Love blew her gasket when Kurt Cobain started rapping in Guitar Hero, just think how she would react to a virtual version of her ex-husband playing on albums without the proper permission. Once Zenph secures the necessary rights to make these re-recordings through one-off licensing deals with an artist, or his or her representatives or estate, it creates a new sound-recording copyright, which won’t expire for decades. This creates the opportunity to license perfect-sounding recordings from the past for use in films and television shows. A scene featuring Thelonius Monk playing in a club, for instance, could feature newly recorded music reconstructed from a hissy live recording using Zenph’s existing technology. Taking these pianos on tour, on the other hand, is no small feat. “The problem is moving the pianos around,” said Frey, “it’s not like you can just go grab any piano in any city.” His long-term vision for Zenph involves solving one aspect of that problem by modeling instruments virtually, so that computers can generate music in the style of a variety of musicians all on their own, without expensive hardware. This would allow amateur musicians to play along with virtual versions of famous performers, and let fans choose which performer plays a certain part and even what mood they should be in as they play. “It introduces a whole bunch of interesting intellectual-property issues, but eventually, you ought to be able to, in essence, cast your own band,” said Frey. “You should be able to write a piece of music and for the drum piece, have Keith Moon, and for the guitar piece, you can have Eric Clapton — that is a derivation of understanding each of those artists’ styles as a digital signature. That’s further down the road, but initially, you’re going to have the ability for artist to create music and have the listener manipulate how they want to hear it — [for example] sadder.” Clearly, the licensing of musical personalities has the potential to create a new revenue stream for artists and their estates, but because there’s no compulsory license for this sort of thing — and there shouldn’t be, because artists or their estates should have control over what their personalities do — each deal must be negotiated individually. But if Zenph and other companies succeed in the quest to create virtual musical personalities, the market will likely create licensing mechanisms that allow a wide range of artists and labels to license their personalities to interactive music formats, potentially resulting in wrangling over music licensing. The problem has philosophical overtones: If a machine has to license a certain performer’s style, why doesn’t a human? Licensing the style or personality of performers would open a strange can of worms, even if the intent is just to fairly compensate those involved. “The idea of extending copyright in general I’m not much in favor of, but the idea of extending copyright to style is incredibly distasteful to me,” said Eric Singer, creator of the League of Electronic Musical Urban Robots, or LEMUR. “It basically means that the entire history of music, where people have listened to other musicians and been influenced by their style is basically up for grabs. Whether a brain is doing it or a computer is doing it, how are they going to make that distinction?” Whatever licensing is involved, it would involve a new right that falls outside of current copyright law (updated). Frey clarified, “Copyright protection is reserved solely for original works of authorship that are fixed in tangible mediums of expression, such as books or recordings. From a legal perspective, there is no way that whatever rights might be relevant to this hypothetical notion about artistic style would fit within the logical framework of copyright, and Zenph would never propose that copyright be extended in this direction.” (We should also make clear that Zenph negotiates deals with artists or their estates for each re-recording and would be required to do so in the future, so it should not be seen as subverting copyright law or hijacking artists’ performances.) For governing the use of artists’ personalities, perhaps the “right of publicity,” which governs how a person’s likeness and persona can be used, would be the place to start. However it happens, the laws will need to catch up in the years to come, because virtual musicians are already real, and they’re only getting realer."
813,https://www.wired.com/2009/04/newtonai/,Wired,2009,4,2,938.0," In just over a day, a powerful computer program accomplished a feat that took physicists centuries to complete: extrapolating the laws of motion from a pendulum's swings. Developed by Cornell researchers, the program deduced the natural laws without a shred of knowledge about physics or geometry. The research is being heralded as a potential breakthrough for science in the Petabyte Age, where computers try to find regularities in massive datasets that are too big and complex for the human mind and its standard computational tools. ""One of the biggest problems in science today is moving forward and finding the underlying principles in areas where there is lots and lots of data, but there's a theoretical gap. We don't know how things work,"" said Hod Lipson, the Cornell University computational researcher who co-wrote the program. ""I think this is going to be an important tool."" Condensing rules from raw data has long been considered the province of human intuition, not machine intelligence. It could foreshadow an age in which scientists and programs work as equals to decipher datasets too complex for human analysis. Lipson's program, co-designed with Cornell computational biologist Michael Schmidt and described in a paper published Thursday in Science, may represent a breakthrough in the old, unfulfilled quest to use artificial intelligence to discover mathematical theorems and scientific laws: But now artificial intelligence experts say Lipson and Schmidt may have fulfilled the field's elusive promise. Unlike the Automated Mathematician and its heirs, their program is primed only with a set of simple, basic mathematical functions and the data it's asked to analyze. Unlike Dendral and its counterparts, it can winnow possible explanations into a likely few. And it comes at an opportune moment — scientists have vastly more data than theories to describe it. Lipson and Schmidt designed their program to identify linked factors within a dataset fed to the program, then generate equations to describe their relationship. The dataset described the movements of simple mechanical systems like spring-loaded oscillators, single pendulums and double pendulums — mechanisms used by professors to illustrate physical laws. The program started with near-random combinations of basic mathematical processes — addition, subtraction, multiplication, division and a few algebraic operators. Initially, the equations generated by the program failed to explain the data, but some failures were slightly less wrong than others. Using a genetic algorithm, the program modified the most promising failures, tested them again, chose the best, and repeated the process until a set of equations evolved to describe the systems. Turns out, some of these equations were very familiar: the law of conservation of momentum, and Newton's second law of motion. ""It's a powerful approach,"" said University of Michigan computer scientist Martha Pollack, with ""the potential to apply to any type of dynamical system."" As possible fields of application, Pollack named environmental systems, weather patterns, population genetics, cosmology and oceanography. ""Just about any natural science has the type of structure that would be amenable,"" she said. Compared to laws likely to govern the brain or genome, the laws of motion discovered by the program are extremely simple. But the principles of Lipson and Schmidt's program should work at higher scales. The researchers have already applied the program to recordings of individuals' physiological states and their levels of metabolites, the cellular proteins that collectively run our bodies but remain, molecule by molecule, largely uncharacterized — a perfect example of data lacking a theory. Their results are still unpublished, but ""we've found some interesting laws already, some laws that are not known,"" said Lipson. ""What we're working on now is the next step — ways in which we can try to explain these equations, correlate them with existing knowledge, try to break these things down into components for which we have clues."" Lipson likened the quest to a ""detective story"" — a hint of the changing role of researchers in hybridized computer-human science. Programs produce sets of equations — describing the role of rainfall on a desert plateau, or air pollution in triggering asthma, or multitasking on cognitive function. Researchers test the equations, determine whether they're still incomplete or based on flawed data, use them to identify new questions, and apply them to messy reality. The Human Genome Project, for example, produced a dataset largely impervious to traditional analysis. The function of nearly every gene depends on the function of other genes, which depend on still more genes, which change with time and place. The same level of complexity confronts researchers studying the body's myriad proteins, the human brain and even ecosystems. ""The rules are mathematical formulae that capture regularities in the system,"" said Pollack, ""but the scientist needs to interpret those regularities. They need, for example, to explain"" why an animal population is affected by changes in rainfall, and what might be done to protect it. Michael Atherton, a cognitive scientist who recently predicted that computer intelligence would not soon supplant human artistic and scientific insight, said that the program ""could be a great tool, in the same way visualization software is: It helps to generate perspectives that might not be intuitive."" However, said Atherton, ""the creativity, expertise, and the recognition of importance is still dependent on human judgment. The main problem remains the same: how to codify a complex frame of reference."" ""In the end, we still need a scientist to look at this and say, this is interesting,"" said Lipson. Humans are, in other words, still important. Citations: ""Distilling Free-Form Natural Laws from Experimental Data."" By Michael Schmidt and Hod Lipson. Science, Vol. 324, April 3, 2009. See Also: Brandon Keim's Twitter stream and Del.icio.us feed; Wired Science on Facebook."
814,https://www.wired.com/2009/04/robotscientist/,Wired,2009,4,2,781.0," For the first time, a robotic system has made a novel scientific discovery with virtually no human intellectual input. Scientists designed ""Adam"" to carry out the entire scientific process on its own: formulating hypotheses, designing and running experiments, analyzing data, and deciding which experiments to run next. ""It's a major advance,"" says David Waltz of the Center for Computational Learning Systems at Columbia University. ""Science is being done here in a way that incorporates artificial intelligence. It's automating a part of the scientific process that hasn't been automated in the past."" The demonstration of autonomous science breaks major ground. Researchers have been automating portions of the scientific process for decades, using robotic laboratory instruments to screen for drugs and sequence genomes, but humans are usually responsible for forming the hypotheses and designing the experiments themselves. After the experiments are complete, the humans must exert themselves again to draw conclusions. Meanwhile, some software programs can analyze data to generate hypotheses or conclusions, but they don't interact with the physical realm. Adam is the first automated system to complete the cycle from hypothesis, to experiment, to reformulated hypothesis without human intervention. Adam's British designers, led by Ross King at Aberystwyth University in Wales, acknowledge that the robot's discoveries have been ""of a modest kind"" thus far. Its proving ground as a scientist has been the genome of baker's yeast, a popular laboratory species. Baker's yeast is one of the best understood organisms, but 10 to 15 percent of its roughly 6,000 genes have unknown functions. The scientists hoped Adam could shed light on some of these mystery genes. They armed Adam with a model of yeast metabolism and a database of genes and proteins involved in metabolism in other species. Then they set the mechanical beast loose, only intervening to remove waste or replace consumed solutions. The results appear Thursday in Science. Adam sought out gaps in the metabolism model, specifically orphan enzymes, which scientists think exist, but which haven't been linked to any parent genes. After selecting a desirable orphan, Adam scoured the database for similar enzymes in other organisms, along with the corresponding genes. Using this information, it hypothesized that similar genes in the yeast genome may code for the orphan enzyme. The process might sound simple — and indeed, similar ""scientific discovery"" algorithms already exist — but Adam was only getting started. Still chugging along on its own, it designed experiments to test its hypotheses, and performed them using a fully automated array of centrifuges, incubators, pipettes, and growth analyzers. After analyzing the data and running follow-up experiments — it can design and initiate over a thousand new experiments each day — Adam had uncovered three genes that together coded for an orphan enzyme. King's group confirmed the novel findings by hand. Waltz thinks Adam will inspire other scientists. ""They'll realize they can automate more of the process than they currently have. They can explore a wider range of possibilities without doing it all by hand."" King is already expanding his Robot Scientist fleet by producing Eve, which will autonomously design and screen drugs against malaria and schistosomiasis. ""Most drug discovery is already automated,"" says King, ""but there's no intelligence — just brute force."" King says Eve will use artificial intelligence to select which compounds to run, rather than just following a list. If robotic scientists made their way into other labs, their human counterparts would not be out of a job anytime soon. If anything, they may find their work more exciting. ""There may be teams of humans and machines,"" says King. ""Robots will be doing more and more of actual experimental work and simple cycles of hypothesis generation. Humans would migrate to more strategic and creative positions. How can we waste trained post-docs by making them pipette things in labs? It's crazy."" But with advances in artificial intelligence, it's conceivable that the role of robots would, in the more distant future, creep deeper into the human realm, progressing from lab technician to lab head. Robots may even be capable of performing supposed acts of genius, such as Einstein's conception of special relativity. ""There isn't any intrinsic reason why that wouldn't happen,"" says King. ""I think there's a continuum between the really basic types of science that you'd get from Adam, and the things I can do, and then Einstein-type science. A computer can make beautiful chess moves, but it's not doing anything special. It's just doing more of the same thing. In my view that's what's going to happen in science."" King may already have a head start: Deep Blue could never have beaten Garry Kasparov without engineer Feng-Hsiung Hsu moving the pieces on its behalf. See Also:"
815,https://www.wired.com/2009/04/minduploadwiki/,Wired,2009,4,1,217.0," The entry for mind uploading — one of the cooler concepts of modern life — may be deleted from Wikipedia if it's not improved. Involving the transference of a mind from biological brain to computer hardware — or, for that matter, any other substrate; a character in Charles Stross' Accelerando embodies himself in a flock of seagulls — mind uploading is a tenet of transhumanist hopes and science fiction. It's been postulated by such artificial intelligence luminaries as Ray Kurzweil, Marvin Minsky and Hans Moravec; is a central plot device of The Matrix and Battlestar Galactica; and figures prominently in the work of Stross, Iain Banks, Charles Platt and Gene Wolfe. In addition to its cultural role, mind uploading also reflects a fundamental assumption of modern neuroscience and cognitive theory: that consciousness does not require some mysterious, immaterial energizing force, but is contained in the physical interactions of a brain and its structure, and can thus be quantified and described. Of course, scientists are still in the early stages of quantification. A full description of the mind could take decades, and transferring that description decades more — if, that is, it's ever possible at all. But mind uploading still deserves a place in Wikipedia. See Also: Brandon Keim's Twitter stream and Del.icio.us feed; Wired Science on Facebook."
816,https://www.wired.com/2009/03/gobrain/,Wired,2009,3,10,1434.0," For the last two decades, human cognitive superiority had a distinctive sound: the soft click of stones placed on a wooden Go board. But once again, artificial intelligence is asserting its domination over gray matter. Just a few years ago, the best Go programs were routinely beaten by skilled children, even when given a head start. Artificial intelligence researchers routinely said that computers capable of beating our best were literally unthinkable. And so it was. Until now. In February, at the Taiwan Open — Go‘s popularity in East Asia roughly compares to America’s enthusiasm for golf — a program called MoGo beat two professionals. At an exhibition in Chicago, the Many Faces program beat another pro. The programs still had a head start, but the trend is clear. “It’s a silly human conceit that such a domain would exist, that there’s something only we can figure out with our wetware brains,” said David Doshay, a University of California at Santa Cruz computer scientist. “Because at the same time, another set of humans is just as busily saying, ‘Yes, but we can knock this problem into another domain, and solve it using these machines.'” Arrayed by opposing players trying to capture space on its lined 19×19 grid, the black and white Go stones can end a game in 10 171 possible ways — about 10 81 times more configurations than there are elementary particles in the known universe. Faced with such extraordinary complexity, our brains somehow find a path, navigating the possibilities using mechanisms only dimly understood by science. Both of the programs that have recently defeated humans used variations on mathematical techniques originally developed by Manhattan Project physicists to coax order from pure randomness. Called the Monte Carlo method, it has driven computer programs to defeat ranking human players six times in the last year. That’s a far cry from chess, the previous benchmark of human cognitive prowess, in which Deep Blue played Garry Kasparov to a panicked defeat in 1997, and Deep Fritz trounced Vladimir Kramnik in 2006. To continue the golf analogy, computer Go programs beat the equivalents of Chris Couch rather than Tiger Woods, and had a multi-stroke handicap. But even six victories was inconceivable not too long ago, and programmers say it won’t be long before computer domination is complete. There is, however, an asterisk to the programs’ triumphs. Compared to the probabilistic foresight of our own efficiently configured biological processor — sporting 10 15 neural connections, capable of 10 16 calculations per second, times two — computer Go programs are inelegant. They rely on what Deep Blue designer Feng-Hsiung Hsu called the “substitution of search for judgment.” They crunch numbers. “People hoped that if we had a strong Go program, it would teach us how our minds work. But that’s not the case,” said Bob Hearn, a Dartmouth College artificial intelligence programmer. “We just threw brute force at a program we thought required intellect.” If only we knew what our own brains were doing. Inasmuch as human Go prowess is understood, it’s explained in terms of pattern recognition and intuition. “When there are groups of stones arranged in certain ways, you can build visual analogies that work very well. You can think, ‘This configuration radiates influence to that part of the board’ — and it turns out it’s a useful concept,” said Hearn. “The revolutionary people in the field have an intuitive sense, and can look at things completely differently from other people.” Image-based neuroscience supports this explanation, albeit vaguely. When University of Minnesota researchers led by cognitive scientist Michael Atherton scanned the brains of people playing chess and compared them to Go-playing brains, they found heightened activation in the Go players’ parietal lobes, a region responsible for processing spatial relationships. But these observations, saidAtherton, were rudimentary. “The higher-level stuff, we didn’t figure out,” he said. In a more recent brain-scanning study, Japanese researchers compared professional and amateur Go players as they contemplated opening- and end-stage moves. Both displayed parietal lobe activity. During the end stages, however, professionals had extremely high activity in their precuneus and cerebellum regions, where the brain integrates a sense of space with our bodies and motions. Put another way, professionals fuse their consciousness into the decision tree of the game. Go players have an ability “to think creatively and prune the search tree in an aesthetic sense,” said Atherton. “They have a feel for the game.” Artificial intelligence researchers historically tried to harness this pattern-based approach, however poorly understood, to their Go programs. It wasn’t easy. “When I’ve talked to Go professionals about how they come to their decisions, it’s been difficult for them to describe why a move is right,” said Doshay at UCSC, who designed a Go computer program called SlugGo. “Go is a game of living things, and you talk about it that way, as if the patterns might be alive.” But if turning cryptic statements from Go masters into working algorithms for determining the statistical health of game patterns was impossible, there didn’t seem to be any other way of doing it. “It was possible to sidestep the cognitive issues by throwing brute force at chess,” said Hearn, “but not at Go.” Compared to the challenge posed to a Go program, Deep Blue’s computations — possible moves in response to a move, carried 12 cycles into the future — are back-of-the-napkin scribblings. “If you look at the game trees, there’s about 30 possible moves you can make from a typical position. In Go, it’s about 300. Right away, you get exponential scaling,” said Hearn. With every anticipated move, the possibilities continue to scale exponentially — and unlike chess, where captured pieces are counted immediately, Go territory can switch hands until the game’s end.  Running a few branches down the tree is useless: take one step, and it needs to be pursued, exponential scale by scale, until the game end. According to Doshay, the number of Go‘s end-states — 10 171 — is almost inconceivably smaller than the 10 1100 different ways of getting there. Without patterns to eliminate whole swaths of choices from the outset, computers simply can’t cope with it, at least not within time frames contained by the universe’s remaining existence. But to Doshay, guiding computers with human-rules patterns was wrong from the beginning. “If you want computers to do something well, you concentrate on the ways computers do things well,” he said. “Computers can generate enormous quantities of random numbers very rapidly.” Enter the Monte Carlo method, named by its Manhattan Project pioneers for the casinos where they gambled. It consists of random simulations repeated again and again until patterns and probabilities emerge: the characteristics of an atomic bomb explosion, phase states in quantum fields, the outcome of a Go game. Programs like MoGO and Many Faces simulate random games from start to finish, over and over and over again, with no concern for figuring out which of any given move is best. “At first, I was dismissive,” said Hearn. “I didn’t think there was anything to be gained from random playouts.” But the programmers had one extra trick: they crunched the accumulated statistics, too. Once a few million random games are modeled, probabilities take form. Thus informed, the programs devote extra processing power to promising branches, and less power to less-promising alternatives. The resulting game style looks human, but aside from a few rough human heuristics, the patterns articulated by our intuitions are unnecessary. “The surprising, mysterious thing to me is that these algorithms work at all,” said Hearn. “It’s very puzzling.” Puzzling it might be, but the game is almost over. Hearn and others say that, having started to beat human professionals, Monte Carlo-based programs will only get better. They’ll incorporate the results of earlier games to their heuristic arsenal, and within a few years — a couple decades at the most — be able to beat our best. What is the larger significance of this? When computers finally triumphed at chess, the world was shocked. To some, it seemed that human cognition was less special than before. But to others, the competition is an illusion. After all, behind every machine is the hand that made it. “There’s a strong tendency in humans to have a conceit about how far we’ve advanced,” said Doshay. “But we’ve only really started programming computers.” Image: 1. Flickr/Sigurdga  2. David Doshay, with a 24-CPU Go-playing cluster. He’s since expanded it to 72 CPUs running multiple Go modules. One module, still under development, is patterned after his Go teacher. Brandon Keim’s Twitter stream and Del.icio.us feed; Wired Science on Facebook."
817,https://www.wired.com/2008/08/military-ai-cou/,Wired,2008,8,14,179.0," As if it wasn't bad enough for the military to muck about with mind control, they're also bent on creating an online, self-teaching artificial intelligence. Hasn't anyone in the Pentagon watched The Terminator? Of the various possible types of AI, the ""most revolutionary would be an intelligent machine that uses the Internet to train,"" write the authors of a military-commissioned National Research Council report on emerging cognitive neuroscience. With so much information online and constantly updated, ""If a system that reasoned like a human being could be achieved, there would be no limit to augmenting its capabilities."" Skynet, anyone? What self-respecting, self-sufficient AI wouldn't see CO2-spewing humans as a threat to its existence? Okay, I'm being hyperbolic. But there is something vaguely creepy about the idea of greater-than-human artificial intelligence unleashed on the Internet by the military. Fortunately, as the authors note, ""Many efforts, large and small, to reach this goal have not yet succeeded"" -- perhaps because natural intelligence is still such a mystery to us. WiSci 2.0: Brandon Keim's Twitter and Del.icio.us feeds; Wired Science on Facebook."
818,https://www.wired.com/2008/06/everything-you-7/,Wired,2008,6,2,154.0," Enjoy our coverage of futurist Ray Kurzweil's World Science Festival talk on the Singularity? Then head on over to the IEEE Spectrum, which has a soup-to-nuts news special on the possiblity of change accelerating so rapidly that we become almost powerless to comprehend it or the non-human intelligences it spawns. Among the experts assembled by the Spectrum -- official publication of the Institute of Electrical and Electronics Engineers, the folks responsible for standardizing our standards -- are Vernor Vinge, John Horgan, Giulio Tononi and a small army of tech luminaries. Bookmark the special and read it soon. After all, 2029 could come sooner than expected.... Image: A network map of the internet published last year [pdf] in the Proceedings of the National Academy of Sciences. The Opte Project's images are more suggestively brainlike, but they're also way out of date. See Also: WiSci 2.0: Brandon Keim's Twitter and Del.icio.us feeds; Wired Science on Facebook."
819,https://www.wired.com/2008/05/will-the-singul/,Wired,2008,5,30,794.0," After listening to futurist Ray Kurzweil speak, one is left less with concrete impressions than a general sensibility, one that's difficult to articulate but reinforced by certain words that he uses again and again: expansion, doubling and -- perhaps most significantly -- predictable. It's a bit like being lectured by a salesman or an evangelist. And I think there nugget of truth to the latter characterization. At the beginning of his talk yesterday at the World Science Festival, he mentioned intelligent design: ""A more sophisticated take,"" he said, ""is that the laws of physics are a form of intelligent design. Of course, the designer might be an adolescent in some other universe, and our universe is just a science fair."" Not that Kurzweil necessarily believes this, or even that it would matter if he did. His accomplishmentsare many, and he probably forgets more before breakfast than I figure out in a week. But something about Kurzweil's certainty brings out the contrarian in me, or at least the skeptic. Of course, my inner skeptic makes plenty of mistakes -- so take all this with as many grains of salt as you'd like. Kurzweil's talk recapitulated the narrative for which he's known: the steady growth of computing power and sheer reality-describing data will eventually give scientists an unprecedented understanding of biological systems, including the human body, and the ability to hack it in ways that may ultimately defy death. This process follows an exponential growth curve, one that's seen elsewhere in history, most notably in the progression of life from eukaryotic cells through the Cambrian explosion and finally to us, homo sapiens, who are poised at the point where things are about to shoot straight up. Kurzweil's confidence is tremendous. At one point, neuroscientist V.S.Ramachandran, who had delivered the talk before his, expressed doubt that we could quickly reverse-engineer what is fundamentally a hacked system, with evolution taking advantage of multiple shortcuts and multifunctionalities and all-purpose jerry rigging. ""God is an hacker, not an engineer -- and that's a problem we'll have to confront,"" he said. And Kurzweil's response was simply that it wouldn't be a problem. Some of Kurzweil's predictions I'm perfectly willing to bet on. The ascendance of solar energy, for example: solar panel efficiency has been doubling every two years, and Kurzweil says that only seven more doublings are needed before the sun meets humanity's energy needs.Likewise, nanotech-based therapies are moving from the lab into early-stage clinical trials, and look quite promising. But can we jump from these examples, from the exponential curvesKurzweil assembled to depict various biological and economic and social phenomena, to the Singularity -- a point at which our tools are so proficient at making themselves that more-human-intelligences emerge, and change is so accelerated that we can barely make sense of it? This seems to require a certain faith. Faith is often rewarded, but it also tends to have blind spots. And in Kurzweil's talk, the blind spot appeared to be the human condition. At one point he predicted that we would soon be able to inactivate genes responsible for fat storage, which were useful on the savannah but not in an age dietary abundance.But is this really the best approach? Doesn't it make more sense to simply eat less, especially when dietary insufficiency is still a reality for billions of people? I know this criticism is a bit nit-picky, and doesn't address the probability of what he's saying. But Kurzweil's description of humanity's ascent towards the Singularity implies that it's an essentially good thing -- and though the therapies he describes would be wonderful, there's a certain impersonality to it. What will the future mean for us, for our relationships to other people, for our hopes and strivings? I'd love to ask Kurzweil. But in the meantime, I pose the question to you, Wired Science readers: do you think the humans of Kurzweil's future will be happier than us? Note: Some great relevant reading is ""Why the future doesn't need us"", published in Wired back in 2000 by Bill Joy, co-founder of SunMicrosystems and Kurzweil compatriot. Wired also did a Kurzweil Q-and-A last November, and his website is chock full of his writings. (Kurzweil'sWorld Science Festival PowerPoint presentation is also supposed to be there, but I can't find it -- if you can, please post the link.) And for fictional treatments of the Singularity, I recommend Isaac Asimov's""The Last Question"" and Accelerando by Charles Stross. One other thing I'll say about Kurzweil: the cocktail of vitamins, supplements and nutraceuticals he's concocted to keep him healthy until the advent of radical longevity-enhancing therapies appears to be working. He's 60 years old but looks ten years younger. See Also: WiSci 2.0: Brandon Keim's Twitter and Del.icio.us feeds; Wired Science on Facebook."
820,https://www.wired.com/2008/03/ff-kurzweil/,Wired,2008,3,24,3836.0," Ray Kurzweil, the famous inventor, is trim, balding, and not very tall. With his perfect posture and narrow black glasses, he would look at home in an old documentary about Cape Canaveral, but his mission is bolder than any mere voyage into space. He is attempting to travel across a frontier in time, to pass through the border between our era and a future so different as to be unrecognizable. He calls this border the singularity. Kurzweil is 60, but he intends to be no more than 40 when the singularity arrives. Kurzweil’s notion of a singularity is taken from cosmology, in which it signifies a border in spacetime beyond which normal rules of measurement do not apply (the edge of a black hole, for example). The word was first used to describe a crucial moment in the evolution of humanity by the great mathematician John von Neumann. One day in the 1950s, while talking with his colleague Stanislaw Ulam, von Neumann began discussing the ever-accelerating pace of technological change, which, he said, “gives the appearance of approaching some essential singularity in the history of the race beyond which human affairs as we know them could not continue.” Many years later, this idea was picked up by another mathematician, the professor and science fiction writer Vernor Vinge, who added an additional twist. Vinge linked the singularity directly with improvements in computer hardware. This put the future on a schedule. He could look at how quickly computers were improving and make an educated guess about when the singularity would arrive. “Within 30 years, we will have the technological means to create superhuman intelligence,” Vinge wrote at the beginning of his 1993 essay The Coming Technological Singularity: How to Survive in the Post-Human Era. “Shortly after, the human era will be ended.” According to Vinge, superintelligent machines will take charge of their own evolution, creating ever smarter successors. Humans will become bystanders in history, too dull in comparison with their devices to make any decisions that matter. Kurzweil transformed the singularity from an interesting speculation into a social movement. His best-selling books The Age of Spiritual Machines and The Singularity Is Near cover everything from unsolved problems in neuroscience to the question of whether intelligent machines should have legal rights. But the crucial thing that Kurzweil did was to make the end of the human era seem actionable: He argues that while artificial intelligence will render biological humans obsolete, it will not make human consciousness irrelevant. The first AIs will be created, he says, as add-ons to human intelligence, modeled on our actual brains and used to extend our human reach. AIs will help us see and hear better. They will give us better memories and help us fight disease. Eventually, AIs will allow us to conquer death itself. The singularity won’t destroy us, Kurzweil says. Instead, it will immortalize us. There are singularity conferences now, and singularity journals. There has been a congressional report about confronting the challenges of the singularity, and late last year there was a meeting at the NASA Ames Research Center to explore the establishment of a singularity university. The meeting was called by Peter Diamandis, who established the X Prize. Attendees included senior government researchers from NASA, a noted Silicon Valley venture capitalist, a pioneer of private space exploration, and two computer scientists from Google. At this meeting, there was some discussion about whether this university should avoid the provocative term singularity, with its cosmic connotations, and use a more ordinary phrase, like accelerating change. Kurzweil argued strongly against backing off. He is confident that the word will take hold as more and more of his astounding predictions come true. Kurzweil does not believe in half measures. He takes 180 to 210 vitamin and mineral supplements a day, so many that he doesn’t have time to organize them all himself. So he’s hired a pill wrangler, who takes them out of their bottles and sorts them into daily doses, which he carries everywhere in plastic bags. Kurzweil also spends one day a week at a medical clinic, receiving intravenous longevity treatments. The reason for his focus on optimal health should be obvious: If the singularity is going to render humans immortal by the middle of this century, it would be a shame to die in the interim. To perish of a heart attack just before the singularity occurred would not only be sad for all the ordinary reasons, it would also be tragically bad luck, like being the last soldier shot down on the Western Front moments before the armistice was proclaimed. In his childhood, Kurzweil was a technical prodigy. Before he turned 13, he’d fashioned telephone relays into a calculating device that could find square roots. At 14, he wrote software that analyzed statistical deviance; the program was distributed as standard equipment with the new IBM 1620. As a teenager, he cofounded a business that matched high school students with colleges based on computer evaluation of a mail-in questionnaire. He sold the company to Harcourt, Brace & World in 1968 for $100,000 plus royalties and had his first small fortune while still an undergraduate at MIT. Though Kurzweil was young, it would have been a poor bet to issue him life insurance using standard actuarial tables. He has unlucky genes: His father died of heart disease at 58, his grandfather in his early forties. He himself was diagnosed with high cholesterol and incipient type 2 diabetes—both considered to be significant risk factors for early death—when only 35. He felt his bad luck as a cloud hanging over his life. Still, the inventor squeezed a lot of achievement out of these early years. In his twenties, he tackled a science fiction type of problem: teaching computers to decipher words on a page and then read them back aloud. At the time, common wisdom held that computers were too slow and too expensive to master printed text in all its forms, at least in a way that was commercially viable. To perish of a heart attack just before the singularity occurred would be tragic, like being the last soldier shot down on the western front moments before the armistice was proclaimed. But Kurzweil had a special confidence that grew from a habit of mind he’d been cultivating for years: He thought exponentially. To illustrate what this means, consider the following quiz: 2, 4, ?, ?. What are the missing numbers? Many people will say 6 and 8. This suggests a linear function. But some will say the missing numbers are 8 and 16. This suggests an exponential function. (Of course, both answers are correct. This is a test of thinking style, not math skills.) Human minds have a lot of practice with linear patterns. If we set out on a walk, the time it takes will vary linearly with the distance we’re going. If we bill by the hour, our income increases linearly with the number of hours we work. Exponential change is also common, but it’s harder to see. Financial advisers like to tantalize us by explaining how a tiny investment can grow into a startling sum through the exponential magic of compound interest. But it’s psychologically difficult to heed their advice. For years, an interest-bearing account increases by depressingly tiny amounts. Then, in the last moment, it seems to jump. Exponential growth is unintuitive, because it can be imperceptible for a long time and then move shockingly fast. It takes training and experience, and perhaps a certain analytical coolness, to trust in exponential curves whose effects cannot be easily perceived. Moore’s law—the observation by Intel cofounder Gordon Moore that the number of transistors on an integrated circuit doubles roughly every 18 months—is another example of exponential change. For people like Kurzweil, it is the key example, because Moore’s law and its many derivatives suggest that just about any limit on computing power today will be overcome in short order. While Kurzweil was working on his reading machine, computers were improving, and they were indeed improving exponentially. The payoff came on January 13, 1976, when Walter Cronkite’s famous sign-off—“and that’s the way it is”—was read not by the anchorman but by the synthetic voice of a Kurzweil Reading Machine. Stevie Wonder was the first customer. The original reader was the size of a washing machine. It read slowly and cost $50,000. One day late last year, as a winter storm broke across New England, I stood in Kurzweil’s small office suite in suburban Boston, playing with the latest version. I hefted it in my hand, stuck it in my pocket, pulled it out again, then raised it above a book flopped open on the table. A bright light flashed, and a voice began reading aloud. The angle of the book, the curve of its pages, the uneven shadows—none of that was a problem. The mechanical voice picked up from the numerals on the upper left corner—... four hundred ten. The singularity is near. The continued opportunity to alleviate human distress is one key motivation for continuing technological advancement—and continued down the page in an artificial monotone. Even after three decades of improvement, Kurzweil’s reader is a dull companion. It expresses no emotion. However, it is functionally brilliant to the point of magic. It can handle hundreds of fonts and any size book. It doesn’t mind being held at an angle by an unsteady hand. Not only that, it also makes calls: Computers have become so fast and small they’ve nearly disappeared, and the Kurzweil reader is now just software running on a Nokia phone. In the late ‘70s, Kurzweil’s character-recognition algorithms were used to scan legal documents and articles from newspapers and magazines. The result was the Lexis and Nexis databases. And a few years later, Kurzweil released speech recognition software that is the direct ancestor of today’s robot customer service agents. Their irritating mistakes taking orders and answering questions would seem to offer convincing evidence that real AI is still many years away. But Kurzweil draws the opposite conclusion. He admits that not everything he has invented works exactly as we might wish. But if you will grant him exponential progress, the fact that we already have virtual robots standing in for retail clerks, and cell phones that read books out loud, is evidence that the world is about to change in even more fantastical ways. Look at it this way: If the series of numbers in the quiz mentioned earlier is linear and progresses for 100 steps, the final entry is 200. But if progress is exponential, then the final entry is 1,267,650,600,228,229,400,000,000,000,000. Computers will soon be smarter than humans. Nobody has to die. In a small medical office on the outskirts of Denver, with windows overlooking the dirty snow and the golden arches of a fast-food mini-mall, one of the world’s leading longevity physicians, Terry Grossman, works on keeping Ray Kurzweil alive. Kurzweil is not Grossman’s only client. The doctor charges $6,000 per appointment, and wealthy singularitarians from all over the world visit him to plan their leap into the future. Grossman’s patient today is Matt Philips, 32, who became independently wealthy when Yahoo bought the Internet advertising company where he worked for four years. A young medical technician is snipping locks of his hair, and another is extracting small vials of blood. Philips is in good shape at the moment, but he is aware that time marches on. “I’m dying slowly. I can’t feel it, but I know its happening, little by little, cell by cell,” he wrote on his intake questionnaire. Philips has read Kurzweil’s books. He is a smart, skeptical person and accepts that the future is not entirely predictable, but he also knows the meaning of upside. At worst, his money buys him new information about his health. At best, it makes him immortal. “The normal human lifespan is about 125 years,” Grossman tells him. But Philips wasn’t born until 1975, so he starts with an advantage. “I think somebody your age, and in your condition, has a reasonable chance of making it across the first bridge,” Grossman says. According to Grossman and other singularitarians, immortality will arrive in stages. First, lifestyle and aggressive antiaging therapies will allow more people to approach the 125-year limit of the natural human lifespan. This is bridge one. Meanwhile, advanced medical technology will begin to fix some of the underlying biological causes of aging, allowing this natural limit to be surpassed. This is bridge two. Finally, computers become so powerful that they can model human consciousness. This will permit us to download our personalities into nonbiological substrates. When we cross this third bridge, we become information. And then, as long as we maintain multiple copies of ourselves to protect against a system crash, we won’t die. Kurzweil himself started across the first bridge in 1988. That year, he confronted the risk that had been haunting him and began to treat his body as a machine. He read up on the latest nutritional research, adopted the Pritikin diet, cut his fat intake to 10 percent of his calories, lost 40 pounds, and cured both his high cholesterol and his incipient diabetes. Kurzweil wrote a book about his experience, The 10% Solution for a Healthy Life. But this was only the beginning. Kurzweil met Grossman at a Foresight Nanotech Institute meeting in 1999, and they became research partners. Their object of investigation was Kurzweil’s body. Having cured himself of his most pressing health problems, Kurzweil was interested in adopting the most advanced medical and nutritional technologies, but it wasn’t easy to find a doctor willing to tolerate his persistent questions. Grossman was building a new type of practice, focused not on illness but on the pursuit of optimal health and extreme longevity. The two men exchanged thousands of emails, sharing speculations about which cutting-edge discoveries could be safely tried. Though both Grossman and Kurzweil respect science, their approach is necessarily improvisational. If a therapy has some scientific promise and little risk, they’ll try it. Kurzweil gets phosphatidylcholine intravenously, on the theory that this will rejuvenate all his body’s tissues. He takes DHEA and testosterone. Both men use special filters to produce alkaline water, which they drink between meals in the hope that negatively charged ions in the water will scavenge free radicals and produce a variety of health benefits. This kind of thing may seem like quackery, especially when promoted by various New Age outfits touting the “pH miracle of living.” Kurzweil and Grossman justify it not so much with scientific citations—though they have a few—but with a tinkerer’s shrug. “Life is not a randomized, double-blind, placebo-controlled study,” Grossman explains. “We don’t have that luxury. We are operating with incomplete information. The best we can do is experiment with ourselves.” Obviously, Kurzweil has no plan for retirement. He intends to sustain himself indefinitely through his intelligence, which he hopes will only grow. A few years ago he deployed an automated system for making money on the stock market, called FatKat, which he uses to direct his own hedge fund. He also earns about $1 million a year in speaking fees. Meanwhile, he tries to safeguard his well-being. As a driver he is cautious. He frequently bicycles through the Boston suburbs, which is good for physical conditioning but also puts his immortality on the line. For most people, such risks blend into the background of life, concealed by a cheerful fatalism that under ordinary conditions we take as a sign of mental health. But of course Kurzweil objects to this fatalism. He wants us to try harder to survive. His plea is often ignored. Kurzweil has written about the loneliness of being a singularitarian. This may seem an odd complaint, given his large following, but there is something to it. A dozen of his fans may show up in Denver every month to initiate longevity treatments, but many of them, like Matt Philips, are simply hedging their bets. Most health fanatics remain agnostic, at best, on the question of immortality. Kurzweil predicts that by the early 2030s, most of our fallible internal organs will have been replaced by tiny robots. We’ll have “eliminated the heart, lungs, red and white blood cells, platelets, pancreas, thyroid and all the hormone-producing organs, kidneys, bladder, liver, lower esophagus, stomach, small intestines, large intestines, and bowel. What we have left at this point is the skeleton, skin, sex organs, sensory organs, mouth and upper esophagus, and brain.” In outlining these developments, Kurzweil’s tone is so calm and confident that he seems to be describing the world as it is today, rather than some distant, barely imaginable future. This is because his prediction falls out cleanly from the equations he’s proposed. Knowledge doubles every year, Kurzweil says. He has estimated the number of computations necessary to simulate a human brain. The rest is simple math. But wait. There may be something wrong. Kurzweil’s theory of accelerating change is meant to be a universal law, applicable wherever intelligence is found. It’s fine to say that knowledge doubles every year. But then again, what is a year? A year is an astronomical artifact. It is the length of time required by Earth to make one orbit around our unexceptional star. A year is important to our nature, to our biology, to our fantasies and dreams. But it is a strange unit to discover in a general law. “Doubling every year,” I say to Kurzweil, “makes your theory sound like a wish.” He’s not thrown off. A year, he replies, is just shorthand. The real equation for accelerating world knowledge is much more complicated than that. He has examined the evidence, and welcomes debate on the minor details. If you accept his basic premise of accelerating growth, he’ll yield a little on the date he predicts the singularity will occur. After all, concede accelerating growth and the exponential fuse is lit. At the end you get that big bang: an explosion in intelligence that yields immortal life. Despite all this, people continue to disbelieve. There is a lively discussion among experts about the validity of Moore’s law. Kurzweil pushes Moore’s law back to the dawn of time, and forward to the end of the universe. But many computer scientists and historians of technology wonder if it will last another decade. Some suspect that the acceleration of computing power has already slowed. There are also philosophical objections. Kurzweil’s theory is that super-intelligent computers will necessarily be human, because they will be modeled on the human brain. But there are other types of intelligence in the world—for instance, the intelligence of ant colonies—that are alien to humanity. Grant that a computer, or a network of computers, might awaken. The consciousness of the this fabulous AI might remain as incomprehensible to us as we are to the protozoa. Other pessimists point out that the brain is more than raw processing power. It also has a certain architecture, a certain design. It is attached to specific type of nervous system, it accepts only particular kinds of inputs. Even with better computational speed driving our thoughts, we might still be stuck in a kind of evolutionary dead end, incapable of radical self-improvement. And these are the merely intellectual protests Kurzweil receives. The fundamental cause for loneliness, if you are a prophet of the singularity, is probably more profound. It stems from the simple fact that the idea is so strange. “Death has been a ubiquitous, ever-present facet of human society,” says Kurzweil’s friend Martine Rothblatt, founder of Sirius radio and chair of United Therapeutics, a biotech firm on whose board Kurzweil sits. “To tell people you are going to defeat death is like telling people you are going to travel back in time. It has never been done. I would be surprised if people had a positive reaction.” “To tell people you are going to defeat death is like telling people you are going to travel back in time,” a Kurzweil friend says. “It’s never been done. I’d be surprised if people had a positive reaction.” To press his case, Kurzweil is writing and producing an autobiographical movie, with walk-ons by Alan Dershowitz and Tony Robbins. Kurzweil appears in two guises, as himself and as an intelligent computer named Ramona, played by an actress. Ramona has long been the inventor’s virtual alter ego and the expression of his most personal goals. “Women are more interesting than men,” he says, “and if it’s more interesting to be with a woman, it is probably more interesting to be a woman.” He hopes one day to bring Ramona to life, and to have genuine human experiences, both with her and as her. Kurzweil has been married for 32 years to his wife, Sonya Kurzweil. They have two children—one at Stanford University, one at Harvard Business School. “I don’t necessarily only want to be Ramona,” he says. “It’s not necessarily about gender confusion, it’s just about freedom to express yourself.” Kurzweil’s movie offers a taste of the drama such a future will bring. Ramona is on a quest to attain full legal rights as a person. She agrees to take a Turing test, the classic proof of artificial intelligence, but although Ramona does her best to masquerade as human, she falls victim to one of the test’s subtle flaws: Humans have limited intelligence. A computer that appears too smart will fail just as definitively as one that seems too dumb. “She loses because she is too clever!” Kurzweil says. The inventor’s sympathy with his robot heroine is heartfelt. “If you’re just very good at doing mathematical theorems and making stock market investments, you’re not going to pass the Turing test,” Kurzweil acknowledged in 2006 during a public debate with noted computer scientist David Gelernter. Kurzweil himself is brilliant at math, and pretty good at stock market investments. The great benefits of the singularity, for him, do not lie here. “Human emotion is really the cutting edge of human intelligence,” he says. “Being funny, expressing a loving sentiment—these are very complex behaviors.” One day, sitting in his office overlooking the suburban parking lot, I ask Kurzweil if being a singularitarian makes him happy. “If you took a poll of primitive man, happiness would be getting a fire to light more easily,” he says. “But we’ve expanded our horizon, and that kind of happiness is now the wrong thing to focus on. Extending our knowledge and casting a wider net of consciousness is the purpose of life.” Kurzweil expects that the world will soon be entirely saturated by thought. Even the stones may compute, he says, within 200 years. Every day he stays alive brings him closer to this climax in intelligence, and to the time when Ramona will be real. Kurzweil is a technical person, but his goal is not technical in this respect. Yes, he wants to become a robot. But the robots of his dreams are complex, funny, loving machines. They are as human as he hopes to be."
821,https://www.wired.com/2007/08/fine-were-simul/,Wired,2007,8,15,145.0," So are we all just a bunch of consciousness programs running on some post-singularity universe simulator? The New York Times' John Tierney wrote about this yesterday, with transhumanist bigwig and Oxford University philosopher Nick Bostrom providing the ideas: A more practical question is how to behave in a computer simulation. Your first impulse might be to say nothing matters anymore because nothing’s real. But just because your neural circuits are made of silicon (or whatever posthumans would use in their computers) instead of carbon doesn’t mean your feelings are any less real. Fine, we could be simulations. Now what? Tierney doesn't have time to do much more than scratch the surface, but that's okay. When it comes to cutting-edge prognostication, it's not the New York Times we turn to. It's science fiction, which has been kicking around digitally replicated consciousness for a few decades now."
822,https://www.wired.com/2007/07/the-game-of-che/,Wired,2007,7,20,219.0," When you play a game of checkers, you and your opponent can make an unlimited combination of moves as you play the game, eventually leading to one winner. Actually, there aren't an unlimited number of combinations. It turns out, there are a mere 500,000,000,000,000,000,000 combinations (500 quintillion) that can be made over the course of a game of checkers. Researchers from the University of Alberta's Computer Science department should know, they tried them all out. Jonathan Schaeffer and his colleagues started their work 18 years ago with the development of a checkers simulation called Chinook. The program used an average of 50 computers to grind through the moves in the game. Initially it used techniques developed by master checkers players to learn the best moves, but now the system knows the perfect series of plays to win the game at any point. A perfect opponent matched against Chinook can never hope to beat it; even if they play a perfect game, their best result is a draw. Chinook has been competing for checkers championships for years. It first battled humans in 1990, and finally won in 1994, earning a place in the Guinness Book of World Records for the first computer program to win a human championship. Think you've got the right moves? Try your luck against Chinook online."
823,https://www.wired.com/2007/05/poker-bots-lear/,Wired,2007,5,30,259.0," Poker-playing software programs are great at calculating odds and keeping, so to speak, a straight face. But the bluff -- that highest art of the game, the ability to intuit when and how to successfully play a low pair like a full house -- has always been beyond the grasp of their code. ""Computers are programmed to perform the best strategy, but bluffing is based on unexpected, illogical actions,"" says Evan Hurwitz, a computer scientist at the University of the Witwatersrand in South Africa. All that may have changed: Hurwitz and his colleagues developed an artificial intelligence that learned to bluff. Based on a neural network algorithm typically used to predict the stock market -- talk about unexpected and illogical actions! -- Hurwitz's bots weren't pre-programmed with the rules of a card game called lerpa.(Yes, the bot doesn't quite play *poker*, but it's the principles that matter.) Instead, they were pitted against each other and learned to play by inferring the game's rules from their own hands, those of their opponents and the outcome of the games. Eventually, one of the bots -- dubbed Randy -- suddenly started to bluff, having calculated that it increased his chances of winning against his still-cautious computer opponents. ""This demonstrates that computers can learn this peculiarly human behaviour,"" says Philippe de Wilde, a computer scientist at Heriot-WattUniversity in Edinburgh, UK. ""They generate the strategy from play, which is a very human way of learning."" So what else can a poker-playing, stock market-reading artificial intelligence learn to do? This definitely calls for Bruce Sterling."
