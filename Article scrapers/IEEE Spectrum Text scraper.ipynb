{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os.path\n",
    "from htmldate import find_date\n",
    "import pandas as pd\n",
    "def scrape(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except Exception:\n",
    "        return \"DELETE\"\n",
    "    html_content = response.text\n",
    "    soup = BeautifulSoup(html_content, 'lxml')  # You can also use 'html.parser'\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    textOut = \"\"\n",
    "    timeStamp = \"\"\n",
    "    try:\n",
    "        timeStamp = find_date(url)\n",
    "        year = timeStamp.split(\"-\")[0]\n",
    "        month = timeStamp.split(\"-\")[1]\n",
    "        day = timeStamp.split(\"-\")[2]\n",
    "    except Exception:\n",
    "        year = \"check\"\n",
    "        month = \"check\"\n",
    "        day = \"check\"\n",
    "    \n",
    "    removeList = []\n",
    "    authorBio = soup.find_all(\"div\",{\"class\":\"social-author__bio\"})\n",
    "    additionalBio = soup.find(\"div\",{\"class\":\"minibios-group\"})\n",
    "    comments = soup.find_all(\"div\",{\"class\":\"comment--item-text\"})\n",
    "    captions = soup.find_all(\"div\",{\"class\":\"photo-caption\"})\n",
    "    moreCaptions = soup.find_all(\"small\",{\"class\":\"image-media media-caption\"})\n",
    "    for m in moreCaptions:\n",
    "        removeList.append(m.get_text().replace(\"\\n\",\"\").strip())\n",
    "    miniBios = \"\"\n",
    "    for cap in captions:\n",
    "        removeList.append(cap.get_text().replace(\"\\n\",\"\").strip())\n",
    "    commentsList = []\n",
    "    for c in comments:\n",
    "        try:\n",
    "            commentsList.append(c.get_text().replace(\"\\n\",\"\").strip())\n",
    "        except Exception:\n",
    "            continue\n",
    "    if additionalBio != None:\n",
    "       miniBios = additionalBio.get_text().replace(\"\\n\",\"\").strip()\n",
    "    for a in authorBio:\n",
    "        removeList.append(a.get_text().replace(\"\\n\",\"\").strip())\n",
    "    isComment = False\n",
    "    removeList.extend([\"This sponsored article is brought to you by BESydney.\",\"Click to read more thoughts from:\"])\n",
    "    for p in paragraphs:\n",
    "        isComment = False\n",
    "        text = p.get_text().replace(\"\\n\",\"\").strip()\n",
    "        for comment in commentsList:\n",
    "            if text in comment:\n",
    "                isComment = True\n",
    "        if text != \"\" and text[-1] in '!.?:‚Äù\"' and text not in removeList and not isComment and text not in miniBios: #this varies between websites since some write in a more academic manner\n",
    "            textOut+= \" \" + text\n",
    "    length = 0\n",
    "    if textOut == \"\":\n",
    "        return \"DELETE\"\n",
    "    else:\n",
    "        length = len(textOut.split())\n",
    "        return [url,\"Wired\",year,month,day,length,textOut]\n",
    "df = []\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
