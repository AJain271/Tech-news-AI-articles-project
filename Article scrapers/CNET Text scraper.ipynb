{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os.path\n",
    "from htmldate import find_date\n",
    "import pandas as pd\n",
    "def scrape(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except Exception:\n",
    "        return \"DELETE\"\n",
    "    html_content = response.text\n",
    "    soup = BeautifulSoup(html_content, 'lxml')  # You can also use 'html.parser'\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    textOut = \"\"\n",
    "    timeStamp = \"\"\n",
    "    try:\n",
    "        timeStamp = find_date(url)\n",
    "        year = timeStamp.split(\"-\")[0]\n",
    "        month = timeStamp.split(\"-\")[1]\n",
    "        day = timeStamp.split(\"-\")[2]\n",
    "    except Exception:\n",
    "        year = \"check\"\n",
    "        month = \"check\"\n",
    "        day = \"check\"\n",
    "    \n",
    "    removeList = []\n",
    "    remove = soup.find_all(\"p\",{\"class\":\"u-speakableText-dek c-contentHeader_description g-outer-spacing-top-small\"})\n",
    "    captions = soup.find_all(\"span\",{\"class\":\"c-shortcodeImage_caption g-inner-spacing-right-small g-text-xxsmall\"})\n",
    "\n",
    "    capList = []\n",
    "    for c in captions:\n",
    "        capList.append(c.get_text().strip().replace(\"\\n\",\"\"))\n",
    "    for r in remove:\n",
    "        removeList.append(r.get_text().strip().replace(\"\\n\",\"\")) \n",
    "   \n",
    "    for block,p in enumerate(paragraphs):\n",
    "        text = p.get_text().replace(\"\\n\",\"\").strip()\n",
    "        if text != \"\" and text[-1] in '!.?:‚Äù)\"' and text not in removeList and text not in capList: #this varies between websites since some write in a more academic manner\n",
    "            textOut+= \" \" + text\n",
    "    length = 0\n",
    "    if textOut == \"\":\n",
    "        return \"DELETE\"\n",
    "    else:\n",
    "        length = len(textOut.split())\n",
    "        return [url,\"Wired\",year,month,day,length,textOut]\n",
    "df = []\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
