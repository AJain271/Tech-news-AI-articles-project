{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os.path\n",
    "from htmldate import find_date\n",
    "import pandas as pd\n",
    "def remove_nonAscii(text): \n",
    "    return ''.join(i for i in text if ord(i)<128)\n",
    "def scrape(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except Exception:\n",
    "        return \"DELETE\"\n",
    "    html_content = response.text\n",
    "    soup = BeautifulSoup(html_content, 'lxml')  # You can also use 'html.parser'\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    textOut = \"\"\n",
    "    timeStamp = \"\"\n",
    "    try:\n",
    "        timeStamp = find_date(url)\n",
    "        year = timeStamp.split(\"-\")[0]\n",
    "        month = timeStamp.split(\"-\")[1]\n",
    "        day = timeStamp.split(\"-\")[2]\n",
    "    except Exception:\n",
    "        year = \"check\"\n",
    "        month = \"check\"\n",
    "        day = \"check\"\n",
    "  \n",
    "  \n",
    "    removeList = []  \n",
    "\n",
    "    relatedPosts = soup.find_all(\"p\",{\"class\":\"mt-2 line-clamp-3 sm:line-clamp-2 font-serif xs:text-lg text-ellipsis break-words\"})\n",
    "    for r in relatedPosts:\n",
    "        removeList.append(remove_nonAscii(r.get_text().replace(\"\\n\",\"\").strip().replace(\"â\",\"\").replace(\"\",\"\")))\n",
    "    \n",
    "\n",
    "    removeList.extend([\"We may earn a commission when you buy through links on our sites.2024 GIZMODO USA LLC. All rights reserved.\",\" may earn a commission when you buy through links on our sites.2024 GIZMODO USA LLC. All rights reserved.\",\"Want to know more about AI, chatbots, and the future of machine learning? Check out our full coverage of artificial intelligence, or browse our guides to The Best Free AI Art Generators, The Best ChatGPT Alternatives, and Everything We Know About OpenAIs ChatGPT.\"])\n",
    "    \n",
    "    for block,p in enumerate(paragraphs):\n",
    "        \n",
    "        text = remove_nonAscii(p.get_text().replace(\"\\n\",\"\").strip()) \n",
    "      \n",
    "   \n",
    "\n",
    "    \n",
    "        if text != \"\" and text[-1] in '!.?:”)]\"' and text not in removeList and not (text[0:6] ==\"Update\" and block > len(paragraphs)-14): #this varies between websites a bit\n",
    "            textOut+= \" \" + text\n",
    "    length = 0\n",
    "    print(textOut.replace(\"â\",\"\"))\n",
    "    if textOut == \"\":\n",
    "        return \"DELETE\"\n",
    "    else:\n",
    "        length = len(textOut.split())\n",
    "        return [url,\"Wired\",year,month,day,length,textOut]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
