{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os.path\n",
    "from htmldate import find_date\n",
    "import pandas as pd\n",
    "def scrape(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except Exception:\n",
    "        return \"DELETE\"\n",
    "    html_content = response.text\n",
    "    soup = BeautifulSoup(html_content, 'lxml')  # You can also use 'html.parser'\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    textOut = \"\"\n",
    "    timeStamp = \"\"\n",
    "    try:\n",
    "        timeStamp = find_date(url)\n",
    "        year = timeStamp.split(\"-\")[0]\n",
    "        month = timeStamp.split(\"-\")[1]\n",
    "        day = timeStamp.split(\"-\")[2]\n",
    "    except Exception:\n",
    "        year = \"check\"\n",
    "        month = \"check\"\n",
    "        day = \"check\"\n",
    "    removeList = []\n",
    "    captions = soup.find_all(\"figcaption\")\n",
    "    for c in captions:\n",
    "        removeList.append(c.get_text().strip().replace(\"\\n\",\"\"))\n",
    "    authorBio = soup.find(\"div\",{\"class\":\"author-bio__bio\"})\n",
    "    if authorBio!=None:\n",
    "        removeList.append(authorBio.get_text().strip().replace(\"\\n\",\"\"))\n",
    "    #remove = soup.find_all(\"p\",{\"class\":\"u-speakableText-dek c-contentHeader_description g-outer-spacing-top-small\"})\n",
    "    #captions = soup.find_all(\"span\",{\"class\":\"c-shortcodeImage_caption g-inner-spacing-right-small g-text-xxsmall\"})\n",
    "    removeList.append(\"More OpenAI news:\")\n",
    "    for block,p in enumerate(paragraphs):\n",
    "        text = p.get_text().replace(\"\\n\",\"\").strip()\n",
    "        #print(text)\n",
    "        if text !=\"\" and ((text[0:4]==\"More\" and text[-1]==\":\" and block> len(paragraphs)-8) or (text[0:16]==\"Please follow me\" and block> len(paragraphs)-5)):\n",
    "            break\n",
    "        if text != \"\" and text[-1] in '!.?:”)“\"' and text not in removeList: #this varies between websites since some write in a more academic manner\n",
    "            textOut+= \" \" + text\n",
    "    length = 0\n",
    "    if textOut == \"\":\n",
    "        return \"DELETE\"\n",
    "    else:\n",
    "        length = len(textOut.split())\n",
    "        return [url,\"Wired\",year,month,day,length,textOut]\n",
    "df = []\n",
    "print(scrape(\"https://www.computerworld.com/article/1638293/microsoft-invested-13b-in-generative-ai-does-copilot-for-windows-show-it-s-a-bad-bet.html\")[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
